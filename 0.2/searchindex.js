Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.modules.tokenizers.Tokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.modules.tokenizers.Tokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_classifier_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceTokenizer", "TikTokenTokenizer", "Tokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 20, 21, 22, 30, 31, 32, 35, 39, 91, 97, 98, 99, 156, 160, 161, 164, 166, 167], "prompt": [1, 14, 15, 17, 18, 19, 21, 22, 23, 24, 25, 28, 30, 32, 35, 36, 38, 39, 40, 41, 106, 118, 129, 162, 163, 165], "chat": [1, 2, 15, 16, 19, 20, 24, 25, 28, 36, 41, 99], "includ": [1, 6, 7, 8, 15, 18, 54, 63, 75, 88, 99, 112, 122, 123, 127, 158, 160, 161, 162, 163, 164, 165, 166, 167], "some": [1, 6, 7, 16, 86, 114, 115, 156, 158, 160, 161, 162, 163, 164, 166, 167], "specif": [1, 7, 8, 10, 132, 161, 162, 163, 167], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 28, 30, 32, 35, 36, 39, 41, 120, 122, 123, 124, 125, 160, 161, 163, 164, 165, 166], "differ": [1, 7, 9, 28, 29, 30, 32, 118, 125, 146, 153, 158, 160, 161, 163, 165, 166, 167], "dataset": [1, 5, 7, 14, 17, 18, 20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 158, 164, 165], "model": [1, 2, 6, 7, 8, 10, 16, 21, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 122, 123, 124, 125, 128, 129, 132, 134, 140, 147, 148, 156, 158, 161, 162, 167], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 64, 65, 66, 89, 91, 102, 106, 107, 109, 111, 114, 117, 118, 122, 123, 124, 126, 127, 128, 129, 143, 144, 147, 155, 157, 159, 160, 162, 163, 164, 165, 166], "common": [1, 2, 4, 7, 160, 161, 162, 165, 166], "json": [1, 6, 24, 25, 99, 122, 160, 162, 163], "messag": [1, 15, 16, 19, 21, 24, 25, 27, 28, 36, 99, 118, 119, 120, 157, 160, 161, 162], "miscellan": 1, "function": [1, 7, 8, 10, 12, 28, 101, 102, 108, 110, 113, 116, 117, 121, 122, 129, 130, 136, 140, 146, 150, 158, 161, 162, 167], "us": [1, 2, 4, 6, 9, 10, 12, 16, 19, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 63, 68, 75, 79, 95, 101, 102, 104, 105, 106, 107, 108, 110, 113, 116, 118, 119, 121, 122, 123, 125, 126, 127, 129, 130, 131, 132, 134, 140, 141, 142, 143, 144, 150, 156, 157, 158, 160, 162, 164, 165, 166], "modifi": [1, 7, 8, 9, 108, 158, 163, 165, 166, 167], "For": [2, 5, 6, 7, 8, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 106, 122, 127, 128, 135, 144, 148, 150, 157, 160, 161, 162, 163, 164, 165, 166, 167], "detail": [2, 6, 34, 36, 41, 90, 99, 103, 121, 132, 140, 150, 160, 163, 164, 165, 166, 167], "usag": [2, 108, 125, 126, 151, 157, 160, 162, 163, 164, 165, 167], "guid": [2, 7, 9, 158, 161, 162, 164, 166], "pleas": [2, 5, 51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 121, 132, 140, 148, 157, 167], "see": [2, 5, 6, 9, 19, 21, 34, 36, 41, 44, 51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 90, 93, 94, 99, 100, 103, 111, 121, 125, 127, 132, 133, 140, 144, 148, 150, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "our": [2, 6, 8, 158, 161, 162, 163, 164, 166, 167], "tutori": [2, 6, 148, 158, 161, 162, 163, 164, 165, 166, 167], "support": [2, 6, 8, 9, 10, 21, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 58, 68, 79, 84, 86, 95, 98, 99, 101, 112, 123, 124, 126, 131, 134, 135, 158, 160, 161, 162, 163, 164, 165, 166, 167], "sever": 2, "wide": 2, "help": [2, 6, 19, 106, 122, 127, 156, 157, 158, 160, 161, 162, 163, 164, 165, 167], "quickli": [2, 7, 33, 161, 162], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 22, 23, 28, 33, 143, 144, 156, 157, 158, 160, 161, 162, 165, 166, 167], "fine": [2, 6, 8, 9, 31, 156, 158, 163, 166], "tune": [2, 3, 6, 7, 8, 9, 12, 31, 156, 157, 158, 160, 163, 166, 167], "also": [2, 6, 7, 8, 9, 10, 29, 36, 39, 43, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 99, 101, 106, 112, 130, 132, 134, 140, 144, 157, 160, 161, 162, 163, 164, 165, 166, 167], "like": [2, 6, 7, 8, 9, 28, 99, 124, 157, 160, 161, 162, 163, 164, 166], "These": [2, 4, 6, 7, 8, 10, 31, 127, 161, 162, 163, 164, 165, 166, 167], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 23, 27, 30, 31, 32, 35, 36, 38, 39, 40, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 106, 112, 113, 116, 117, 121, 122, 123, 125, 126, 128, 129, 131, 134, 138, 140, 146, 151, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "especi": [2, 158, 160, 163], "specifi": [2, 6, 7, 8, 10, 36, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 106, 107, 110, 121, 129, 132, 135, 140, 144, 148, 151, 160, 161, 162, 163, 164, 165, 167], "yaml": [2, 7, 8, 10, 11, 12, 29, 36, 39, 43, 127, 144, 158, 160, 161, 162, 163, 164, 165, 166, 167], "config": [2, 6, 9, 10, 11, 12, 13, 29, 36, 39, 43, 101, 116, 122, 126, 127, 144, 151, 158, 161, 162, 163, 165, 166, 167], "represent": [2, 166, 167], "abov": [2, 6, 108, 138, 157, 163, 165, 166, 167], "all": [3, 4, 8, 13, 28, 29, 31, 36, 101, 102, 106, 108, 113, 119, 122, 126, 127, 128, 138, 147, 153, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166], "famili": [3, 8, 35, 37, 41, 42, 44, 125, 158, 160, 165], "To": [3, 6, 7, 8, 9, 31, 122, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "download": [3, 6, 154, 157, 161, 162, 165, 166, 167], "8b": [3, 77, 81, 83, 96, 160, 161], "meta": [3, 6, 19, 105, 122, 123, 160, 161, 163, 164], "hf": [3, 6, 110, 122, 160, 161, 163, 164, 165], "token": [3, 6, 7, 8, 20, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 57, 58, 63, 67, 68, 75, 78, 79, 84, 86, 88, 90, 92, 95, 97, 99, 101, 105, 106, 107, 118, 119, 129, 132, 145, 160, 162, 163, 164, 165, 166, 167], "hf_token": 3, "70b": [3, 46, 49, 52, 65, 70, 73, 76, 80, 82, 165], "ignor": [3, 6, 99, 101, 102, 160], "pattern": [3, 119, 160], "origin": [3, 6, 34, 35, 108, 112, 161, 163, 165, 166, 167], "consolid": [3, 6, 165], "7b": [3, 6, 30, 32, 33, 35, 37, 39, 43, 44, 47, 50, 53, 56, 60, 66, 71, 74, 85, 87, 89, 91, 122, 123, 161, 164, 165, 166, 167], "2": [3, 6, 9, 27, 31, 41, 101, 118, 122, 123, 135, 145, 146, 149, 150, 151, 152, 161, 163, 164, 165, 166], "13b": [3, 6, 45, 48, 51, 64, 69, 72], "codellama": 3, "mini": [3, 96, 97, 98, 99, 100], "4k": [3, 97, 98, 99], "microsoft": [3, 98, 99], "ai": [3, 89, 101, 144, 161, 165], "v0": 3, "1": [3, 6, 8, 31, 41, 101, 106, 109, 110, 118, 119, 123, 125, 129, 138, 143, 144, 145, 146, 149, 150, 160, 161, 163, 164, 165, 166, 167], "mistralai": [3, 160], "size": [3, 6, 8, 10, 35, 38, 40, 101, 103, 104, 105, 106, 136, 138, 158, 160, 162, 163, 164, 165, 166], "2b": [3, 55, 59], "googl": [3, 55, 56], "gguf": 3, "can": [4, 6, 7, 8, 9, 10, 13, 20, 28, 29, 30, 32, 33, 35, 36, 37, 39, 43, 44, 104, 105, 113, 118, 121, 122, 125, 127, 132, 140, 143, 144, 148, 151, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "offer": 5, "allow": [5, 29, 116, 143, 160, 167], "seamless": 5, "transit": 5, "between": [5, 6, 122, 125, 162, 163, 165, 166, 167], "train": [5, 6, 8, 9, 19, 28, 29, 30, 31, 35, 36, 38, 39, 40, 41, 43, 101, 105, 106, 107, 108, 109, 122, 123, 124, 131, 134, 140, 151, 156, 158, 160, 161, 162, 163, 165, 166, 167], "interoper": [5, 6, 8, 158, 163, 167], "rest": [5, 161, 167], "ecosystem": [5, 6, 8, 158, 163, 165, 167], "comprehens": 5, "overview": [5, 7, 9, 156, 164, 166, 167], "deep": [5, 6, 7, 8, 9, 158, 164, 165], "dive": [5, 6, 7, 8, 9, 158, 164, 165], "enabl": [5, 7, 8, 9, 29, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 69, 70, 71, 72, 73, 74, 80, 81, 82, 83, 85, 87, 93, 94, 96, 100, 112, 150, 151, 165, 166, 167], "work": [5, 6, 8, 127, 158, 160, 163, 165, 167], "set": [5, 6, 7, 8, 9, 30, 31, 32, 33, 35, 37, 38, 39, 40, 41, 43, 44, 105, 106, 113, 115, 121, 130, 132, 138, 140, 148, 149, 150, 151, 158, 160, 161, 163, 164, 165, 166], "consumpt": [5, 29], "dure": [5, 6, 29, 30, 31, 35, 38, 40, 101, 103, 105, 106, 107, 108, 134, 161, 163, 165, 166, 167], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 26, 28, 29, 30, 31, 32, 41, 106, 113, 124, 127, 130, 132, 144, 151, 158, 160, 161, 162, 163, 164, 165], "debug": [5, 6, 7, 8, 160], "finetun": [5, 6, 7, 8, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 156, 158, 164, 165], "job": [5, 9, 150, 164], "variou": [5, 18], "walk": [6, 8, 143, 158, 161, 162, 163, 164, 167], "you": [6, 7, 8, 9, 10, 18, 19, 23, 28, 30, 32, 33, 35, 37, 39, 43, 44, 125, 127, 129, 143, 144, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "through": [6, 7, 8, 9, 102, 113, 158, 160, 161, 162, 163, 164, 167], "design": [6, 8], "behavior": [6, 140, 161, 162], "associ": [6, 7, 8, 54, 63, 75, 88, 129, 163, 166], "util": [6, 7, 8, 9, 10, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 158, 163, 164, 165, 167], "what": [6, 7, 9, 19, 21, 22, 33, 38, 40, 156, 161, 162, 163, 164, 165], "cover": [6, 7, 8, 9, 161, 163, 167], "how": [6, 7, 8, 9, 23, 121, 148, 156, 160, 161, 162, 163, 164, 165, 167], "we": [6, 7, 8, 9, 30, 31, 32, 33, 35, 37, 39, 43, 44, 101, 103, 105, 106, 112, 118, 122, 123, 124, 129, 131, 135, 140, 147, 158, 160, 161, 162, 163, 164, 165, 166, 167], "them": [6, 7, 28, 29, 30, 32, 39, 102, 108, 118, 160, 161, 162, 163, 166, 167], "scenario": [6, 29], "full": [6, 7, 8, 36, 39, 51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 116, 117, 118, 158, 160, 162, 165, 166], "compos": 6, "compon": [6, 8, 13, 146, 158, 162, 164, 166, 167], "which": [6, 7, 8, 29, 30, 31, 33, 35, 38, 40, 48, 49, 50, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 101, 105, 106, 107, 109, 116, 117, 118, 122, 123, 124, 126, 131, 141, 144, 148, 158, 160, 161, 162, 163, 164, 165, 166, 167], "plug": 6, "ani": [6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 25, 26, 28, 30, 32, 33, 36, 37, 39, 43, 44, 108, 114, 115, 116, 117, 118, 122, 123, 124, 126, 129, 137, 140, 150, 153, 160, 161, 162, 163, 164, 165, 166], "recip": [6, 7, 9, 10, 11, 12, 102, 116, 122, 123, 124, 158, 161, 162, 163, 165, 167], "evalu": [6, 8, 156, 158, 164, 166, 167], "gener": [6, 8, 14, 17, 22, 23, 28, 30, 31, 32, 37, 41, 113, 118, 149, 150, 151, 154, 156, 161, 162, 166, 167], "each": [6, 8, 15, 18, 29, 31, 48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 101, 105, 106, 107, 110, 116, 117, 118, 119, 146, 150, 151, 158, 160, 162, 163, 164, 165, 166], "make": [6, 7, 8, 9, 101, 107, 158, 160, 161, 163, 164, 165, 166, 167], "easi": [6, 8, 158, 162, 166], "understand": [6, 7, 8, 156, 158, 161, 162, 166, 167], "extend": [6, 8, 158], "befor": [6, 27, 30, 31, 32, 54, 58, 101, 106, 107, 112, 122, 160, 163], "let": [6, 7, 9, 160, 161, 162, 163, 164, 165, 166, 167], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 24, 25, 27, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 49, 50, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 99, 101, 103, 105, 106, 107, 108, 110, 111, 114, 116, 117, 119, 121, 122, 123, 126, 130, 132, 134, 140, 143, 148, 149, 158, 160, 161, 162, 164, 166, 167], "defin": [6, 7, 8, 102, 111, 112, 114, 162, 164, 166], "concept": [6, 163, 164], "In": [6, 7, 8, 28, 105, 112, 121, 140, 143, 144, 161, 163, 165, 166, 167], "ll": [6, 7, 8, 119, 129, 135, 158, 161, 162, 163, 164, 165, 167], "talk": 6, "about": [6, 8, 110, 144, 158, 160, 161, 163, 164, 165, 166, 167], "take": [6, 7, 8, 10, 102, 103, 108, 122, 124, 127, 130, 146, 161, 162, 163, 164, 165, 166, 167], "close": [6, 8, 141, 142, 143, 144, 166], "look": [6, 7, 8, 128, 143, 157, 161, 162, 163, 164, 165, 166], "veri": [6, 29, 106, 160, 163], "simpli": [6, 7, 31, 160, 161, 162, 163, 165, 167], "dictat": 6, "state_dict": [6, 108, 116, 122, 123, 124, 125, 126, 166, 167], "store": [6, 29, 141, 144, 166, 167], "file": [6, 7, 8, 9, 10, 11, 12, 118, 119, 122, 123, 124, 127, 141, 144, 151, 155, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167], "disk": [6, 33, 141], "weight": [6, 8, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 108, 111, 112, 116, 122, 123, 124, 125, 135, 140, 144, 156, 160, 161, 163, 164, 165, 166, 167], "string": [6, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 111, 118, 119, 120, 130, 131, 135, 160, 162], "kei": [6, 7, 9, 14, 17, 22, 23, 24, 28, 30, 32, 39, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 106, 107, 115, 116, 117, 122, 124, 126, 146, 151, 160, 163, 164, 166, 167], "identifi": 6, "state": [6, 8, 108, 114, 115, 116, 117, 122, 123, 124, 126, 128, 163, 165, 166, 167], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 36, 37, 39, 43, 44, 108, 114, 115, 116, 117, 122, 123, 124, 126, 128, 134, 137, 139, 145, 146, 147, 162], "If": [6, 7, 13, 14, 17, 18, 21, 22, 23, 24, 26, 27, 28, 30, 32, 35, 38, 39, 40, 41, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 105, 106, 107, 108, 110, 112, 117, 122, 123, 124, 125, 126, 129, 130, 131, 132, 134, 135, 137, 143, 144, 150, 151, 153, 157, 160, 161, 162, 163, 164, 165, 166], "don": [6, 7, 8, 144, 150, 160, 161, 162, 163, 164, 165, 167], "t": [6, 7, 8, 119, 131, 144, 150, 160, 161, 162, 163, 164, 165, 167], "match": [6, 28, 30, 32, 39, 117, 157, 160, 162, 163, 165, 166], "up": [6, 8, 9, 30, 31, 32, 33, 35, 37, 39, 43, 44, 128, 151, 160, 161, 162, 164, 165, 166, 167], "exactli": [6, 117], "those": [6, 125, 166], "definit": [6, 166], "either": [6, 117, 122, 129, 148, 160, 166, 167], "run": [6, 7, 9, 12, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 102, 103, 106, 108, 122, 123, 124, 126, 128, 138, 143, 144, 147, 157, 158, 161, 162, 164, 165, 166, 167], "explicit": 6, "error": [6, 7, 27, 122, 150, 160], "load": [6, 8, 28, 29, 30, 31, 32, 33, 116, 122, 123, 124, 126, 127, 143, 161, 162, 163, 165, 166], "rais": [6, 10, 13, 21, 24, 27, 36, 41, 101, 103, 106, 110, 116, 117, 122, 123, 124, 126, 131, 134, 137, 144, 146, 150, 153], "an": [6, 7, 8, 9, 10, 14, 20, 27, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 68, 79, 84, 86, 90, 95, 101, 106, 110, 111, 113, 114, 115, 121, 122, 123, 124, 126, 130, 132, 144, 151, 158, 160, 161, 162, 163, 164, 165, 166, 167], "except": [6, 20, 21, 162], "wors": 6, "silent": [6, 102], "succe": 6, "infer": [6, 19, 28, 54, 88, 101, 103, 105, 106, 107, 130, 156, 161, 163, 164, 165, 167], "expect": [6, 7, 10, 14, 17, 18, 22, 23, 28, 30, 32, 36, 39, 105, 117, 126, 144, 153, 161, 162, 166], "addit": [6, 7, 8, 10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 116, 121, 122, 123, 124, 131, 132, 137, 140, 141, 143, 144, 148, 158, 161, 164, 166], "line": [6, 8, 14, 127, 160, 162, 164, 165], "need": [6, 7, 8, 9, 18, 28, 31, 41, 101, 102, 106, 140, 143, 144, 147, 157, 160, 161, 162, 163, 164, 165, 166, 167], "shape": [6, 101, 103, 105, 106, 107, 110, 112, 129, 151], "valu": [6, 7, 25, 41, 45, 46, 47, 54, 55, 56, 58, 63, 64, 65, 66, 68, 75, 76, 77, 79, 84, 86, 88, 89, 90, 91, 95, 97, 101, 103, 104, 106, 107, 109, 116, 122, 125, 126, 127, 129, 141, 142, 143, 144, 146, 150, 160, 162, 164, 165, 166], "two": [6, 7, 27, 158, 163, 164, 165, 166, 167], "popular": [6, 158, 162, 163], "llama2": [6, 7, 8, 10, 19, 28, 30, 32, 33, 35, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 102, 106, 107, 118, 125, 156, 158, 160, 164, 165], "offici": [6, 19, 161, 164, 165], "implement": [6, 8, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 102, 104, 105, 109, 110, 111, 112, 122, 135, 143, 158, 162, 166, 167], "when": [6, 7, 8, 12, 20, 29, 31, 33, 101, 105, 106, 107, 108, 109, 116, 129, 132, 143, 147, 160, 163, 165, 166, 167], "llama": [6, 19, 28, 104, 105, 122, 123, 160, 161, 163, 164, 165, 166], "websit": 6, "get": [6, 7, 8, 9, 28, 118, 131, 133, 134, 136, 157, 158, 161, 162, 163, 164, 166], "access": [6, 7, 8, 29, 122, 128, 160, 163, 164], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 33, 101, 116, 122, 123, 124, 126, 128, 160, 161, 162, 163, 164, 165, 166, 167], "pth": [6, 163], "inspect": [6, 163, 166, 167], "content": [6, 20, 24, 25, 28, 118, 161, 162], "easili": [6, 7, 158, 162, 166, 167], "torch": [6, 7, 103, 106, 108, 109, 110, 124, 126, 128, 129, 130, 131, 134, 135, 137, 138, 145, 146, 147, 148, 149, 150, 151, 152, 153, 163, 164, 165, 166, 167], "import": [6, 7, 10, 36, 39, 43, 143, 144, 161, 162, 163, 164, 166, 167], "00": [6, 155, 159, 164], "mmap": [6, 163], "true": [6, 7, 20, 29, 30, 31, 34, 35, 36, 38, 39, 40, 43, 51, 52, 53, 54, 58, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 101, 106, 107, 108, 113, 118, 119, 121, 122, 123, 124, 132, 134, 137, 138, 140, 143, 151, 152, 160, 161, 162, 163, 165, 166, 167], "weights_onli": [6, 124], "map_loc": [6, 163], "cpu": [6, 8, 108, 131, 151, 157, 160, 163, 167], "tensor": [6, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 122, 129, 141, 142, 143, 144, 145, 146, 149, 166, 167], "item": 6, "print": [6, 9, 29, 35, 38, 40, 41, 118, 129, 152, 161, 162, 164, 166, 167], "f": [6, 9, 35, 38, 40, 161, 163, 166, 167], "tok_embed": [6, 106], "32000": [6, 10, 166], "4096": [6, 10, 30, 32, 33, 35, 37, 39, 43, 44, 101, 105, 162, 166], "len": [6, 29, 35, 38, 40, 106], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 28, 29, 30, 31, 32, 38, 40, 41, 42, 48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 86, 95, 96, 104, 105, 108, 109, 110, 113, 118, 119, 121, 122, 124, 127, 130, 131, 133, 135, 144, 149, 151, 152, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "contain": [6, 20, 24, 29, 31, 33, 43, 101, 103, 105, 106, 107, 111, 114, 115, 116, 118, 119, 122, 123, 124, 126, 127, 128, 134, 139, 143, 145, 146, 151, 161, 163, 165, 166], "input": [6, 14, 15, 17, 18, 22, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 41, 42, 43, 44, 101, 102, 104, 105, 106, 107, 112, 118, 122, 124, 145, 146, 150, 153, 161, 162, 166, 167], "embed": [6, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 104, 105, 106, 132, 161, 165], "tabl": [6, 161, 167], "call": [6, 10, 20, 102, 108, 116, 127, 141, 142, 143, 144, 147, 151, 161, 162, 166, 167], "layer": [6, 8, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 93, 94, 95, 96, 97, 100, 101, 106, 107, 112, 116, 117, 121, 132, 158, 165, 166, 167], "have": [6, 7, 10, 101, 103, 111, 117, 124, 126, 127, 132, 140, 143, 153, 157, 161, 162, 163, 164, 165, 166, 167], "dim": [6, 101, 102, 104, 105, 106], "most": [6, 7, 119, 161, 164, 166, 167], "within": [6, 7, 10, 28, 31, 41, 58, 68, 79, 84, 86, 95, 102, 129, 143, 150, 151, 160, 162, 163, 165, 166, 167], "hug": [6, 16, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 109, 158, 160, 164, 165], "face": [6, 16, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 109, 158, 160, 164, 165], "hub": [6, 160, 162, 164], "default": [6, 7, 16, 20, 24, 25, 26, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 58, 59, 60, 63, 64, 65, 66, 68, 69, 70, 71, 75, 76, 77, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 95, 96, 97, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112, 116, 118, 119, 122, 123, 124, 127, 129, 131, 136, 140, 141, 144, 145, 146, 149, 150, 151, 157, 160, 161, 162, 163, 165, 166, 167], "everi": [6, 8, 102, 143, 151, 157, 160, 167], "repo": [6, 122, 123, 125, 160, 163], "first": [6, 7, 10, 27, 31, 103, 106, 119, 122, 127, 156, 158, 161, 162, 163, 165, 166, 167], "big": [6, 163], "split": [6, 29, 31, 161, 162, 163], "across": [6, 8, 29, 122, 143, 150, 163, 165], "bin": [6, 160, 163], "correctli": [6, 8, 13, 116, 122, 157, 161, 164, 167], "piec": 6, "one": [6, 8, 27, 102, 110, 118, 124, 161, 162, 163, 164, 165, 167], "pytorch_model": [6, 163], "00001": [6, 160], "00002": [6, 160], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 31, 37, 58, 68, 79, 84, 86, 95, 112, 114, 116, 118, 123, 124, 126, 127, 129, 131, 132, 134, 135, 140, 160, 162, 163, 164, 165, 166, 167], "doe": [6, 21, 24, 28, 31, 54, 88, 98, 101, 106, 107, 111, 122, 124, 126, 127, 160, 161, 163], "fewer": [6, 101], "sinc": [6, 7, 10, 102, 122, 124, 161, 163, 165], "instead": [6, 8, 31, 36, 39, 43, 102, 103, 112, 160, 163, 165, 166], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 41, 43, 44, 111, 115, 117, 119, 122, 123, 124, 125, 126, 127, 128, 129, 130, 141, 142, 143, 144, 153, 160, 161, 163, 165], "caus": [6, 118], "try": [6, 7, 161, 163, 164, 165, 167], "same": [6, 7, 48, 49, 50, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 86, 88, 90, 95, 96, 97, 101, 103, 107, 118, 126, 127, 132, 144, 160, 161, 163, 165, 166, 167], "As": [6, 7, 8, 9, 112, 158, 163, 165, 167], "re": [6, 7, 119, 124, 158, 161, 163, 164, 165, 166], "care": [6, 102, 122, 124, 163, 165, 166], "end": [6, 8, 20, 29, 118, 119, 156, 158, 161, 165, 166], "number": [6, 8, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 106, 109, 122, 123, 124, 129, 136, 150, 151, 160, 164, 166], "just": [6, 14, 158, 160, 161, 162, 164, 165, 166], "save": [6, 8, 9, 108, 122, 123, 124, 126, 132, 140, 144, 156, 160, 161, 162, 163, 165, 166], "less": [6, 41, 163, 164, 165, 167], "prone": 6, "manag": [6, 29, 113, 149, 161], "invari": 6, "accept": [6, 7, 41, 118, 121, 162, 164, 167], "multipl": [6, 7, 8, 20, 28, 29, 101, 106, 107, 112, 141, 142, 143, 144, 146, 151, 164, 165], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 161, 162, 163], "worri": [6, 161, 164], "explicitli": [6, 111, 158, 166], "convert": [6, 24, 25, 28, 122, 145, 161, 163, 167], "time": [6, 54, 88, 118, 141, 143, 151, 160, 161, 162, 163, 165, 167], "produc": [6, 126, 167], "back": [6, 27, 113, 122, 162, 166, 167], "form": [6, 7, 8, 27, 160], "One": [6, 163], "advantag": [6, 166], "being": [6, 122, 123, 124, 128, 130, 167], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 24, 25, 31, 36, 39, 43, 48, 49, 50, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 101, 102, 110, 111, 116, 117, 121, 127, 139, 141, 142, 143, 144, 157, 158, 162, 163, 164, 165, 166, 167], "abl": [6, 8, 163, 164, 165], "post": [6, 147, 151, 167], "tool": [6, 162, 163, 164], "quantiz": [6, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 112, 124, 135, 156, 164, 167], "eval": [6, 156, 158], "without": [6, 7, 9, 14, 116, 157, 158, 161, 163, 166], "code": [6, 8, 45, 46, 47, 48, 49, 50, 51, 52, 53, 106, 154, 158, 162, 164], "chang": [6, 7, 9, 14, 124, 157, 160, 163, 164, 165, 166, 167], "OR": [6, 24], "convers": [6, 15, 16, 19, 21, 24, 25, 27, 28, 36, 41, 122, 124, 125, 158, 161, 162, 163, 165, 166, 167], "script": [6, 9, 160, 163, 164, 165], "wai": [6, 7, 28, 116, 160, 161, 162, 163, 164, 165], "surround": [6, 8, 158], "load_checkpoint": [6, 8, 122, 123, 124, 125], "save_checkpoint": [6, 8, 9, 122, 123, 124], "method": [6, 7, 8, 9, 12, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 108, 111, 114, 116, 126, 127, 135, 157, 158, 162, 163, 165, 166, 167], "convertor": 6, "avail": [6, 8, 44, 127, 130, 131, 138, 158, 160, 163, 165, 166], "here": [6, 7, 9, 14, 16, 17, 22, 23, 38, 104, 105, 160, 161, 162, 163, 164, 165, 166, 167], "three": [6, 8, 110, 164], "hfcheckpoint": 6, "read": [6, 122, 123, 124, 158], "write": [6, 8, 14, 122, 123, 124, 141, 161, 162, 164], "compat": [6, 122, 124], "transform": [6, 8, 28, 30, 32, 48, 49, 50, 54, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 106, 107, 109, 148, 166], "framework": [6, 8, 158], "mention": [6, 163, 167], "assum": [6, 14, 17, 18, 22, 23, 30, 32, 39, 101, 105, 106, 107, 109, 114, 119, 126, 128, 131, 140, 161, 163, 166], "checkpoint_dir": [6, 7, 122, 123, 124, 163, 165], "necessari": [6, 41, 141, 142, 143, 144, 161, 166], "easiest": [6, 163, 164], "sure": [6, 7, 161, 163, 164, 165, 166, 167], "everyth": [6, 8, 127, 158, 164], "follow": [6, 8, 24, 25, 28, 31, 101, 109, 124, 125, 126, 138, 144, 151, 156, 157, 160, 162, 163, 164, 165, 166, 167], "flow": [6, 28, 30, 31, 32, 167], "By": [6, 160, 165, 166, 167], "safetensor": [6, 122, 160], "output": [6, 18, 29, 35, 38, 41, 48, 49, 50, 54, 58, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 95, 96, 101, 102, 104, 105, 106, 107, 112, 115, 116, 117, 124, 129, 132, 142, 151, 157, 160, 161, 162, 163, 164, 165, 166, 167], "dir": [6, 144, 157, 160, 163, 164, 165], "output_dir": [6, 7, 122, 123, 124, 151, 163, 165, 166, 167], "argument": [6, 7, 10, 18, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 101, 121, 127, 132, 137, 141, 143, 144, 148, 160, 161, 162, 165, 166], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 106, 151, 160, 162, 163, 166, 167], "_component_": [6, 7, 9, 10, 29, 36, 39, 43, 151, 161, 162, 163, 165, 166], "fullmodelhfcheckpoint": [6, 163], "directori": [6, 7, 122, 123, 124, 141, 143, 144, 151, 160, 161, 163, 164, 165], "sort": [6, 122, 124], "id": [6, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 101, 105, 106, 107, 118, 119, 120, 122, 124, 129, 145, 146, 161, 162, 163], "so": [6, 7, 31, 122, 127, 157, 158, 161, 163, 164, 165, 166, 167], "order": [6, 8, 122, 124, 143, 144, 164], "matter": [6, 122, 124, 160, 166], "checkpoint_fil": [6, 7, 9, 122, 123, 124, 163, 165, 166, 167], "restart": [6, 160], "previou": [6, 31, 122, 123, 124], "more": [6, 7, 8, 34, 36, 41, 99, 103, 105, 116, 121, 124, 127, 144, 148, 150, 158, 160, 162, 163, 164, 165, 166, 167], "next": [6, 31, 129, 165, 167], "section": [6, 8, 134, 156, 163, 165, 167], "recipe_checkpoint": [6, 122, 123, 124], "null": [6, 7], "usual": [6, 105, 122, 144, 160, 163, 166], "model_typ": [6, 122, 123, 124, 163, 165], "resume_from_checkpoint": [6, 122, 123, 124], "fals": [6, 7, 20, 24, 25, 28, 29, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 106, 107, 112, 113, 116, 118, 119, 122, 123, 124, 138, 151, 160, 161, 162, 163, 165, 166, 167], "requir": [6, 7, 29, 33, 41, 43, 122, 124, 126, 135, 137, 138, 140, 143, 144, 146, 150, 151, 157, 160, 161, 162, 164, 167], "param": [6, 8, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 112, 114, 115, 117, 122, 140, 166, 167], "directli": [6, 7, 8, 10, 36, 39, 43, 121, 122, 160, 163, 164, 165, 166, 167], "ensur": [6, 7, 13, 27, 41, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 122, 124, 131, 158, 162, 164], "out": [6, 7, 8, 28, 30, 35, 36, 38, 40, 122, 123, 156, 158, 160, 161, 163, 164, 165, 166, 167], "case": [6, 8, 9, 20, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 122, 126, 131, 135, 140, 141, 148, 158, 160, 161, 162, 163, 165, 166, 167], "discrep": [6, 122], "along": [6, 165, 166], "found": [6, 7, 9, 104, 105, 160, 166, 167], "metacheckpoint": 6, "github": [6, 10, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 101, 104, 105, 109, 110, 116, 157, 162, 164], "repositori": [6, 19, 163, 164], "fullmodelmetacheckpoint": [6, 165], "torchtunecheckpoint": 6, "perform": [6, 31, 99, 102, 113, 129, 158, 161, 163, 165, 167], "current": [6, 31, 54, 58, 68, 79, 84, 86, 88, 95, 98, 99, 101, 103, 105, 106, 107, 123, 124, 132, 135, 136, 141, 143, 147, 150, 163, 164, 165], "test": [6, 7, 8, 158, 161], "complet": [6, 8, 14, 31, 37, 99, 161, 162, 163, 164, 165], "written": [6, 7, 8, 122, 123, 141, 142, 143, 144, 158], "begin": [6, 31, 118, 119, 161, 165, 167], "partit": [6, 122, 167], "ha": [6, 111, 113, 114, 117, 118, 124, 126, 153, 161, 162, 163, 164, 165, 166, 167], "standard": [6, 17, 24, 142, 158, 161, 163, 165], "key_1": [6, 124], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 163], "inform": [6, 20, 144, 148, 158, 160, 163, 164, 165], "subsequ": [6, 8], "recipe_st": [6, 122, 123, 124], "pt": [6, 9, 122, 123, 124, 163, 165], "epoch": [6, 8, 9, 109, 122, 123, 124, 160, 161, 163, 164, 165], "optim": [6, 7, 8, 29, 54, 88, 98, 109, 110, 124, 126, 128, 134, 146, 147, 151, 161, 163, 164, 165, 166, 167], "etc": [6, 8, 122, 134, 164], "prevent": [6, 31, 160], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 58, 106, 111, 118, 126, 147, 150, 161, 162, 163, 166, 167], "updat": [6, 7, 8, 103, 126, 151, 157, 161, 163, 164, 165, 166, 167], "hf_model_0001_0": [6, 163], "hf_model_0002_0": [6, 163], "both": [6, 29, 117, 160, 163, 166, 167], "adapt": [6, 111, 112, 113, 114, 115, 122, 123, 124, 161, 163, 166, 167], "merg": [6, 10, 11, 122, 163, 165, 167], "would": [6, 7, 9, 31, 106, 157, 161, 162, 163, 166, 167], "primari": [6, 7, 8, 164], "want": [6, 7, 8, 9, 10, 28, 129, 157, 160, 161, 162, 163, 164, 165, 166], "resum": [6, 8, 109, 122, 123, 124, 167], "initi": [6, 8, 12, 29, 31, 45, 46, 47, 55, 56, 64, 65, 66, 76, 77, 89, 91, 126, 137, 138, 164, 166, 167], "frozen": [6, 166, 167], "base": [6, 10, 30, 32, 41, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 93, 94, 95, 96, 97, 100, 105, 109, 110, 112, 113, 115, 116, 117, 122, 127, 130, 132, 140, 141, 156, 161, 163, 164, 165, 166, 167], "well": [6, 7, 8, 158, 160, 162, 163, 165, 167], "learnt": [6, 161, 163], "someth": [6, 8, 9, 161, 163], "NOT": [6, 54, 88], "refer": [6, 7, 8, 104, 105, 110, 113, 158, 166], "adapter_checkpoint": [6, 122, 123, 124], "adapter_0": [6, 163], "now": [6, 118, 126, 128, 161, 162, 163, 164, 165, 166, 167], "knowledg": 6, "creat": [6, 7, 10, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 76, 77, 80, 81, 82, 83, 85, 87, 89, 91, 93, 94, 96, 98, 100, 103, 109, 121, 122, 123, 124, 128, 141, 143, 160, 161, 162, 163, 165, 167], "simpl": [6, 8, 14, 17, 22, 23, 156, 162, 164, 166, 167], "forward": [6, 8, 101, 102, 104, 105, 106, 107, 110, 112, 134, 151, 165, 166, 167], "modeltyp": [6, 122, 123, 124], "llama2_13b": [6, 69], "right": [6, 122, 163, 165, 166], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 116, 126, 166], "successfulli": [6, 160, 164], "vocab": [6, 10, 106, 165], "70": [6, 76], "x": [6, 101, 102, 104, 105, 106, 107, 112, 129, 149, 166, 167], "randint": 6, "0": [6, 8, 31, 48, 49, 50, 51, 52, 53, 54, 58, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 84, 86, 88, 90, 95, 97, 101, 106, 109, 110, 112, 118, 129, 135, 143, 144, 145, 146, 150, 152, 155, 159, 161, 162, 163, 164, 165, 166, 167], "no_grad": 6, "6": [6, 31, 54, 58, 104, 145, 146, 163, 167], "3989": 6, "9": [6, 146, 163, 167], "0531": 6, "3": [6, 31, 96, 98, 99, 119, 125, 127, 133, 135, 145, 146, 149, 160, 161, 163, 164, 165, 167], "2375": 6, "5": [6, 7, 14, 109, 110, 145, 146, 163, 164, 165], "2822": 6, "4": [6, 7, 41, 101, 135, 145, 146, 152, 158, 160, 162, 163, 165, 166, 167], "4872": 6, "7469": 6, "8": [6, 35, 38, 40, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 69, 70, 71, 72, 73, 74, 80, 81, 82, 83, 85, 87, 93, 94, 96, 100, 146, 163, 166, 167], "6737": 6, "11": [6, 146, 163, 165, 167], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 145, 146], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 28, 30, 32, 39, 41, 116, 144, 160, 161, 162, 163, 164, 165, 166], "find": [6, 8, 9, 160, 163, 164, 166], "list": [6, 7, 15, 16, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 111, 112, 116, 117, 118, 119, 120, 122, 123, 124, 127, 129, 133, 145, 146, 161, 162, 164, 165], "builder": [6, 34, 37, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 76, 77, 80, 81, 82, 83, 85, 87, 89, 91, 93, 94, 96, 98, 100, 161, 162, 167], "hope": 6, "deeper": [6, 164], "insight": [6, 163], "happi": [6, 163], "thi": [7, 8, 9, 10, 17, 20, 23, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 41, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 98, 99, 101, 102, 105, 106, 107, 108, 109, 111, 113, 116, 117, 118, 121, 122, 123, 124, 126, 127, 129, 130, 131, 134, 138, 140, 141, 143, 144, 146, 147, 148, 150, 156, 157, 158, 160, 161, 162, 163, 164, 165, 166, 167], "pars": [7, 10, 11, 119, 127, 161, 164], "effect": 7, "cli": [7, 9, 11, 12, 157, 163, 164], "prerequisit": [7, 161, 162, 163, 164, 165, 166, 167], "Be": [7, 161, 163, 164, 165, 166, 167], "familiar": [7, 161, 163, 164, 165, 166, 167], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 157, 161, 162, 164], "instal": [7, 9, 138, 143, 144, 156, 160, 163, 164, 165, 166, 167], "fundament": 7, "There": [7, 15, 27, 161, 163, 164, 165, 166], "entri": [7, 8, 164], "point": [7, 8, 24, 25, 162, 163, 164, 165, 166, 167], "locat": [7, 160, 165, 166, 167], "thei": [7, 8, 20, 29, 106, 117, 127, 132, 160, 161, 162, 166], "truth": [7, 163, 165], "reproduc": 7, "overridden": [7, 102, 127, 151], "quick": [7, 29], "experiment": 7, "serv": [7, 121, 162, 166], "particular": [7, 28, 29, 41, 121, 162, 166, 167], "seed": [7, 8, 9, 150, 164], "shuffl": [7, 31], "devic": [7, 8, 116, 126, 130, 131, 134, 160, 161, 163, 164, 165, 166], "cuda": [7, 130, 131, 134, 151, 157, 163, 167], "dtype": [7, 8, 103, 106, 108, 131, 149, 153, 163, 167], "fp32": [7, 167], "enable_fsdp": 7, "mani": [7, 31, 162, 163], "object": [7, 10, 11, 15, 16, 19, 21, 101, 121, 135, 161], "keyword": [7, 10, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 108, 161, 162], "loss": [7, 8, 30, 35, 38, 40, 110, 164, 166, 167], "exampl": [7, 8, 9, 10, 12, 14, 17, 22, 23, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 101, 110, 111, 113, 118, 121, 122, 123, 125, 126, 129, 135, 143, 144, 145, 146, 149, 152, 154, 155, 157, 159, 160, 161, 162, 163, 165, 166, 167], "subfield": 7, "dotpath": 7, "wish": [7, 162], "exact": [7, 10, 163], "path": [7, 8, 9, 10, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 57, 67, 78, 92, 99, 118, 119, 122, 123, 124, 127, 151, 160, 161, 162, 163, 165, 166], "normal": [7, 28, 31, 104, 106, 107, 118, 161, 162, 166, 167], "python": [7, 119, 127, 133, 144, 150, 154, 160, 163], "alpaca_dataset": [7, 34, 162], "custom": [7, 8, 28, 30, 32, 36, 39, 43, 148, 158, 160, 163, 164, 165, 166], "train_on_input": [7, 24, 25, 28, 29, 30, 34, 35, 36, 38, 39, 40, 41, 161, 162], "onc": [7, 113, 163, 164, 165, 166, 167], "ve": [7, 103, 119, 160, 161, 162, 163, 165, 166], "instanc": [7, 10, 29, 68, 79, 84, 86, 95, 102, 108, 114, 115, 166], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 36, 160, 163, 167], "under": [7, 151, 162, 163, 165, 167], "preced": [7, 10, 160, 165, 166], "actual": [7, 9, 14, 17, 22, 23, 28, 161], "throw": 7, "notic": [7, 161, 162, 166], "miss": [7, 116, 117, 151, 166], "posit": [7, 10, 31, 54, 58, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 107, 165], "anoth": [7, 163], "handl": [7, 12, 20, 29, 118, 161, 163, 166, 167], "def": [7, 8, 9, 12, 121, 125, 161, 162, 166, 167], "dictconfig": [7, 8, 10, 11, 12, 13, 144, 151], "arg": [7, 10, 106, 108, 111, 120, 127, 142, 151], "tupl": [7, 10, 29, 41, 103, 108, 110, 118, 119, 121, 127, 136, 145, 146, 151, 153], "kwarg": [7, 10, 108, 111, 120, 127, 137, 141, 142, 143, 144, 148, 151, 162], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 57, 67, 78, 92, 99, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 126, 127, 130, 131, 133, 134, 135, 137, 139, 141, 142, 143, 144, 145, 146, 150, 151, 152, 153, 161, 162], "mean": [7, 101, 104, 106, 107, 140, 160, 161, 162, 164, 166], "pass": [7, 10, 28, 29, 30, 32, 33, 36, 37, 39, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 102, 108, 113, 117, 121, 124, 131, 132, 134, 137, 140, 143, 144, 148, 151, 160, 161, 162, 166, 167], "add": [7, 9, 28, 31, 119, 124, 125, 127, 162, 163, 165, 166, 167], "d": [7, 20, 101, 103, 106, 119, 160, 161, 166], "llama2_token": [7, 161, 163], "tmp": [7, 126, 161, 164, 165], "option": [7, 8, 14, 17, 18, 22, 23, 26, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 48, 49, 50, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 95, 96, 101, 105, 106, 107, 108, 116, 117, 118, 119, 122, 123, 124, 129, 130, 131, 133, 135, 141, 144, 150, 151, 157, 158, 160, 162, 163], "bool": [7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 108, 112, 116, 117, 118, 119, 120, 121, 122, 123, 124, 132, 134, 137, 138, 140, 143, 148, 151, 152, 161, 167], "max_seq_len": [7, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 118, 119, 161, 162], "int": [7, 9, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 93, 94, 95, 96, 97, 100, 101, 103, 104, 105, 106, 109, 112, 118, 119, 120, 121, 122, 123, 124, 129, 132, 136, 140, 141, 142, 143, 144, 145, 146, 148, 150, 151, 160, 161, 162, 166, 167], "512": [7, 34, 35, 162, 167], "instructdataset": [7, 34, 35, 38, 39, 40, 162], "alreadi": [7, 125, 137, 140, 157, 160, 162, 163, 166], "overwrit": [7, 124, 157, 160], "duplic": [7, 8, 158, 160], "sometim": 7, "than": [7, 27, 41, 101, 103, 121, 124, 125, 152, 153, 161, 162, 163, 164, 165, 166, 167], "resolv": [7, 11, 164], "alpaca": [7, 14, 29, 34, 35, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 162], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 141, 142, 143, 144], "disklogg": 7, "log_dir": [7, 141, 143, 144], "conveni": [7, 8, 160], "verifi": [7, 130, 131, 132, 161, 164, 166], "properli": [7, 116, 138, 160], "experi": [7, 144, 156, 158, 161, 165, 166], "wa": [7, 116, 161, 163, 165, 166, 167], "cp": [7, 157, 160, 161, 163, 164, 165], "7b_lora_single_devic": [7, 163, 164, 166, 167], "my_config": [7, 160], "discuss": [7, 99, 164, 166], "guidelin": 7, "while": [7, 8, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 102, 158, 163, 167], "mai": [7, 9, 132, 161, 162, 164, 166], "tempt": 7, "put": [7, 8, 164, 166], "much": [7, 163, 165, 166, 167], "give": [7, 162, 166], "maximum": [7, 26, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 119, 160], "flexibl": [7, 29, 162], "switch": 7, "encourag": [7, 166], "clariti": 7, "significantli": 7, "easier": [7, 163, 164], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 31, 33, 43, 99, 110, 162, 167], "expos": [7, 8, 124, 161, 164], "parent": [7, 160], "modul": [7, 10, 86, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 125, 128, 132, 140, 147, 148, 150, 161, 164, 166, 167], "__init__": [7, 8, 166, 167], "py": [7, 10, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 101, 103, 104, 105, 109, 110, 160, 163, 165], "guarante": 7, "stabil": [7, 158, 167], "underscor": 7, "_alpaca": 7, "collect": [7, 129, 164], "itself": 7, "via": [7, 9, 36, 39, 43, 112, 122, 166, 167], "pair": [7, 14, 42, 145, 146, 162], "k1": [7, 8], "v1": [7, 8, 44], "k2": [7, 8], "v2": [7, 8, 162], "lora_finetune_single_devic": [7, 160, 161, 163, 164, 165, 166, 167], "checkpoint": [7, 8, 108, 119, 122, 123, 124, 125, 126, 144, 148, 158, 160, 165, 166, 167], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 36, 39, 90, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 118, 119, 120, 122, 123, 124, 125, 126, 127, 141, 142, 143, 144, 161, 162, 164, 166, 167], "assign": [7, 33], "nest": 7, "dot": 7, "notat": [7, 101, 105, 106], "certain": [7, 151, 161], "flag": [7, 8, 30, 35, 38, 40, 121, 124, 132, 160, 167], "built": [7, 9, 42, 157, 161, 164, 167], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 28], "pytorch": [7, 8, 106, 108, 116, 121, 138, 143, 148, 150, 151, 156, 157, 158, 165, 166, 167], "llama3": [7, 28, 41, 76, 77, 78, 79, 80, 81, 82, 83, 95, 125, 129, 132, 156, 160, 162], "8b_full": [7, 160, 162], "adamw": [7, 166], "lr": [7, 109], "2e": 7, "fuse": [7, 147], "nproc_per_nod": [7, 162, 165, 166], "full_finetune_distribut": [7, 160, 162, 163, 164], "core": [8, 158, 162, 164, 167], "i": [8, 19, 21, 101, 106, 107, 108, 115, 119, 126, 129, 162, 163, 165, 167], "structur": [8, 15, 16, 19, 21, 24, 25, 28, 36, 161, 162, 163], "new": [8, 37, 89, 103, 125, 141, 143, 161, 163, 164, 165, 166, 167], "user": [8, 15, 16, 19, 20, 21, 24, 25, 27, 28, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 118, 161, 162, 164], "thought": [8, 158, 164, 167], "target": [8, 158], "pipelin": [8, 158], "llm": [8, 156, 158, 162, 163, 166], "eg": [8, 106, 122, 158], "meaning": [8, 158, 163], "featur": [8, 9, 157, 158, 163, 164], "fsdp": [8, 121, 126, 132, 140, 158, 164, 165], "activ": [8, 102, 134, 139, 148, 151, 158, 167], "gradient": [8, 140, 147, 151, 158, 163, 165, 166, 167], "accumul": [8, 147, 151, 158], "mix": [8, 160, 162, 163], "precis": [8, 108, 131, 158, 164, 167], "appli": [8, 28, 30, 32, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 100, 101, 104, 105, 106, 107, 116, 117, 148, 158, 167], "given": [8, 10, 14, 17, 18, 22, 23, 27, 112, 113, 120, 129, 130, 131, 135, 140, 147, 152, 158, 166], "complex": 8, "becom": [8, 157, 162], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 106, 125, 160, 162], "methodolog": 8, "reason": [8, 129, 163], "possibl": [8, 28, 31, 36, 160, 162], "trade": 8, "off": [8, 118, 163], "memori": [8, 29, 30, 31, 32, 33, 35, 37, 39, 43, 44, 108, 116, 132, 134, 139, 140, 151, 156, 158, 163, 164, 165], "vs": [8, 164], "qualiti": [8, 163, 166], "believ": 8, "best": [8, 161], "suit": [8, 164], "b": [8, 101, 103, 105, 106, 107, 112, 140, 144, 166, 167], "fit": [8, 28, 30, 31, 32, 33, 35, 37, 39, 43, 44, 162], "solut": 8, "result": [8, 118, 151, 163, 165, 166, 167], "meant": [8, 108, 126], "depend": [8, 9, 14, 122, 151, 160, 162, 163, 166, 167], "level": [8, 128, 133, 140, 158, 167], "expertis": 8, "routin": 8, "yourself": [8, 160, 165, 166], "exist": [8, 157, 160, 163, 164, 165, 167], "ad": [8, 90, 118, 124, 125, 161, 166, 167], "ones": 8, "modular": [8, 158], "build": [8, 36, 39, 43, 54, 63, 75, 88, 90, 158, 165, 166], "block": [8, 31, 48, 49, 50, 54, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 95, 96, 116, 117, 158], "wandb": [8, 9, 144, 164], "log": [8, 11, 110, 133, 134, 139, 141, 142, 143, 144, 163, 164, 165, 167], "fulli": [8, 29], "nativ": [8, 156, 158, 166, 167], "correct": [8, 17, 38, 104, 105, 106, 130, 158, 161, 162], "numer": [8, 158], "pariti": [8, 158], "verif": 8, "extens": [8, 124, 158], "comparison": [8, 166, 167], "benchmark": [8, 150, 158, 163, 165, 166], "limit": [8, 126, 162], "hidden": [8, 102], "behind": 8, "100": [8, 30, 35, 38, 40, 41, 129, 145, 146, 166, 167], "prefer": [8, 22, 42, 110, 146, 158, 160, 162], "over": [8, 109, 127, 158, 160, 163, 165, 166, 167], "unnecessari": 8, "abstract": [8, 15, 18, 120, 158, 164, 167], "No": [8, 124, 158], "inherit": [8, 127, 158, 162], "go": [8, 19, 21, 118, 158, 162, 163, 164, 167], "upon": [8, 29, 165], "figur": [8, 166, 167], "spectrum": 8, "decid": 8, "interact": [8, 156, 164], "start": [8, 9, 29, 119, 125, 157, 158, 161, 162, 163, 164], "paradigm": 8, "consist": [8, 44, 164], "configur": [8, 30, 32, 35, 36, 37, 38, 39, 40, 41, 43, 44, 58, 68, 79, 84, 95, 107, 158, 161, 164, 165, 166, 167], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 126, 128, 129, 130, 131, 132, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 156, 158, 160, 161, 162, 163, 164, 165, 166, 167], "command": [8, 9, 127, 157, 160, 161, 162, 163, 164, 165, 166, 167], "overrid": [8, 11, 12, 160, 163, 164, 165, 167], "togeth": [8, 31, 144, 164, 166], "valid": [8, 27, 116, 117, 153, 157, 163, 164], "environ": [8, 130, 138, 157, 160, 163, 164], "logic": [8, 125, 158, 164, 166], "api": [8, 9, 24, 51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 116, 160, 161, 163, 164, 165, 167], "closer": [8, 166], "monolith": [8, 158], "trainer": [8, 110], "A": [8, 9, 24, 25, 29, 31, 101, 106, 107, 108, 110, 112, 116, 118, 119, 121, 126, 127, 134, 135, 139, 140, 145, 146, 155, 156, 159, 160, 161, 163, 166, 167], "wrapper": [8, 118, 119, 126, 128, 160, 166], "around": [8, 28, 118, 119, 134, 160, 161, 163, 166, 167], "extern": [8, 162], "primarili": [8, 29, 166], "eleutherai": [8, 158, 166], "har": [8, 158, 166], "control": [8, 30, 35, 38, 40, 113, 150, 163], "multi": [8, 28, 101, 116, 165], "stage": 8, "distil": 8, "oper": [8, 29, 113, 150], "turn": [8, 20, 27, 28, 119, 161], "dataload": [8, 31, 35, 38, 40], "applic": [8, 101, 122, 123, 144], "clean": [8, 9, 34], "after": [8, 101, 103, 104, 106, 107, 116, 140, 141, 142, 143, 144, 161, 167], "process": [8, 9, 108, 136, 137, 150, 162, 164, 167], "group": [8, 101, 136, 137, 141, 142, 143, 144, 160, 165], "init_process_group": [8, 137], "backend": [8, 160], "gloo": 8, "els": [8, 127, 144, 158, 167], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 29, 124, 127, 132, 151, 162, 164, 165, 166], "stuff": 8, "carri": 8, "relev": [8, 20, 160, 163, 166], "interfac": [8, 15, 18, 29, 111, 162], "metric": [8, 164], "logger": [8, 133, 139, 141, 142, 143, 144, 164], "self": [8, 9, 31, 48, 49, 50, 54, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 101, 106, 107, 111, 116, 117, 122, 125, 126, 162, 166, 167], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 121, 132, 140, 148, 161], "_model": [8, 126], "_setup_model": 8, "_token": [8, 162], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 126, 128, 147, 151, 167], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 110, 150, 160, 165], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 31], "batch": [8, 31, 35, 38, 40, 101, 103, 105, 106, 110, 118, 145, 146, 151, 158, 162, 164, 165, 166], "enumer": 8, "_autocast": 8, "logit": [8, 129], "label": [8, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 110, 145, 146], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 141, 142, 143, 144], "step": [8, 31, 106, 109, 119, 128, 141, 142, 143, 144, 147, 151, 156, 163, 166, 167], "learn": [8, 29, 109, 158, 161, 162, 164, 165, 166, 167], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 23, 26, 27, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 107, 113, 115, 116, 117, 118, 119, 122, 123, 124, 125, 129, 130, 131, 133, 135, 139, 141, 142, 143, 144, 147, 148, 149, 150, 151, 153, 161, 162, 163], "fullfinetunerecip": 8, "direct": [8, 110, 146, 157], "wandblogg": [9, 166, 167], "workspac": 9, "seen": [9, 166, 167], "screenshot": 9, "below": [9, 14, 105, 121, 162, 165, 166, 167], "packag": [9, 143, 144, 157], "pip": [9, 143, 144, 157, 163, 165], "Then": [9, 113, 164], "login": [9, 144, 160, 163], "project": [9, 48, 49, 50, 54, 58, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 95, 96, 101, 102, 116, 117, 132, 144, 156, 161, 166, 167], "grab": [9, 165], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 138, 161], "exit": [9, 157, 160], "resourc": [9, 141, 142, 143, 144], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 39, 41, 43, 101, 105, 106, 107, 129, 161, 163], "desir": [9, 28, 149, 161], "suggest": 9, "approach": [9, 29, 162], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 163], "_output_dir": [9, 122, 123, 124], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 151], "type": [9, 10, 12, 20, 24, 25, 26, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 101, 103, 104, 105, 106, 107, 108, 110, 112, 114, 118, 119, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 145, 146, 148, 149, 150, 151, 152, 162, 163, 166, 167], "descript": [9, 36, 41, 160], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 24, 25, 28, 31, 35, 38, 40, 139, 162], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 80, 81, 82, 83, 89, 91, 93, 94, 96, 98, 99, 100, 101, 104, 105, 109, 110, 116, 121, 122, 123, 127, 133, 138, 143, 144, 148, 150, 157, 162, 163], "com": [10, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 101, 104, 105, 109, 110, 116, 157], "facebookresearch": [10, 104], "blob": [10, 48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96, 99, 101, 104, 105, 109, 110], "main": [10, 12, 99, 101, 104, 105, 157, 163, 165], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 106], "32": [10, 165, 166, 167], "num_head": [10, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106], "num_kv_head": [10, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103], "vocab_s": [10, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "must": [10, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 111, 119, 127, 167], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 101, 103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 124, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 146, 149, 150, 151, 152, 161, 162, 166, 167], "nn": [10, 101, 102, 103, 106, 107, 108, 111, 113, 114, 115, 121, 128, 140, 147, 148, 153, 166, 167], "parsed_yaml": 10, "embed_dim": [10, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 105, 107, 166], "valueerror": [10, 21, 24, 27, 36, 41, 101, 103, 106, 110, 122, 123, 124, 131, 134, 150, 153], "recipe_nam": 11, "rank": [11, 48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 112, 136, 138, 150, 164, 166, 167], "zero": [11, 103, 104, 163, 165], "displai": 11, "callabl": [12, 28, 30, 32, 106, 113, 121, 129, 132, 135, 140, 148], "With": [12, 163, 166, 167], "my_recip": 12, "foo": 12, "bar": [12, 158, 164], "instanti": [13, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 81, 84, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 126], "configerror": 13, "cannot": [13, 124, 165], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 134, 141, 142, 143, 144, 162, 163, 167], "templat": [14, 15, 17, 18, 22, 23, 28, 29, 30, 32, 35, 36, 38, 39, 40, 41], "style": [14, 31, 34, 35, 36, 41, 167], "slightli": 14, "describ": [14, 148, 162], "task": [14, 23, 29, 37, 161, 162, 163, 165, 166, 167], "further": [14, 160, 162, 166, 167], "context": [14, 16, 98, 113, 149, 151, 162], "respons": [14, 16, 110, 118, 162, 163, 164, 165], "appropri": [14, 16, 19, 21, 29, 109, 122, 162, 167], "request": [14, 131, 162, 163], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 162], "map": [14, 17, 18, 22, 23, 24, 25, 28, 29, 30, 31, 32, 39, 115, 122, 126, 128, 141, 142, 143, 144, 147, 151, 161, 162, 163, 166], "column_map": [14, 17, 18, 22, 23, 28, 29, 30, 32, 39, 162], "placehold": [14, 15, 17, 18, 22, 23, 28, 30, 32, 39, 162], "column": [14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 43, 101, 106, 107, 161, 162], "ident": [14, 17, 18, 21, 22, 23, 30, 31, 32, 39, 163], "poem": 14, "n": [14, 22, 101, 118, 119, 155, 159, 160, 161, 162], "nwrite": 14, "long": [14, 31, 161, 166], "where": [14, 17, 22, 23, 28, 29, 35, 38, 40, 101, 106, 112, 118, 132, 140, 146, 162], "me": 14, "role": [15, 20, 24, 25, 28, 118, 161, 162], "system": [15, 16, 19, 20, 21, 24, 25, 27, 28, 99, 118, 161, 162], "assist": [15, 16, 19, 20, 24, 25, 27, 28, 99, 118, 129, 161, 162], "accord": [15, 21, 161], "openai": [16, 24, 36, 162], "markup": 16, "languag": [16, 112, 129, 166], "It": [16, 21, 160, 161, 162, 167], "im_start": 16, "im_end": 16, "goe": [16, 113], "tag": [16, 19, 21, 28, 119, 141, 142, 143, 144, 161], "grammar": [17, 38, 162], "english": 17, "sentenc": [17, 31, 118], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 166], "lazi": 17, "dog": 17, "alwai": [18, 127], "human": [19, 25, 161], "pre": [19, 31, 43, 157, 161, 162], "taken": [19, 166, 167], "inst": [19, 21, 28, 161, 162], "sy": [19, 161, 162], "respect": [19, 29, 115, 151, 161, 162], "honest": [19, 161, 162], "am": [19, 21, 161, 162, 163, 165], "pari": [19, 21, 23, 162], "capit": [19, 21, 22, 23, 162], "franc": [19, 21, 22, 23, 162], "known": [19, 21, 118, 135, 162], "its": [19, 21, 31, 86, 101, 105, 106, 107, 147, 150, 160, 161, 162, 163, 165, 166], "stun": [19, 21, 162], "liter": [20, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 116, 117], "mask": [20, 30, 31, 35, 38, 40, 101, 106, 107, 118, 119, 161, 162], "ipython": 20, "eot": 20, "dataclass": [20, 161], "repres": [20, 146, 161], "individu": [20, 31, 134, 144, 148, 161, 162], "tiktoken": [20, 119, 165], "special": [20, 28, 99, 118, 119, 126, 162], "variabl": [20, 28, 29, 30, 32, 39, 138, 167], "writer": 20, "whether": [20, 24, 25, 28, 30, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 54, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 108, 112, 116, 117, 118, 119, 121, 131, 134, 161, 162], "correspond": [20, 111, 114, 131, 146, 164, 165], "consecut": [20, 27], "from_dict": [20, 161], "construct": [20, 166], "dictionari": [20, 31, 134, 139, 141, 142, 143, 144, 146, 163], "mistral": [21, 28, 41, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 125, 160, 161, 163, 164], "llama2chatformat": [21, 161, 162], "similar": [22, 37, 42, 43, 44, 116, 162, 163, 165, 166, 167], "stackexchangedpair": 22, "question": [22, 161, 162, 163, 165], "answer": [22, 161, 163, 165], "nanswer": 22, "summar": [23, 40, 161, 162], "dialogu": [23, 40, 161], "summari": [23, 29, 40, 134, 162], "dialog": 23, "hello": [23, 118, 161, 163, 165], "did": [23, 163, 165, 167], "know": [23, 161, 162, 163, 165, 166], "adher": [24, 25], "could": [24, 166], "remain": [24, 25, 109, 166], "unmask": [24, 25], "sharegpt": [25, 36], "gpt": [25, 101, 163], "eos_id": 26, "length": [26, 27, 29, 30, 31, 32, 33, 35, 37, 39, 41, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 98, 101, 103, 105, 106, 118, 119, 123, 145, 146], "last": [26, 31, 109, 162], "replac": [26, 30, 35, 38, 40, 108, 166], "forth": [27, 162], "come": [27, 111, 166], "empti": [27, 160], "shorter": 27, "min": [27, 166], "invalid": 27, "convert_to_messag": [28, 161], "chat_format": [28, 36, 41, 161, 162], "chatformat": [28, 36, 162], "load_dataset_kwarg": [28, 30, 32, 33, 36, 37, 39, 43, 44], "multiturn": [28, 161], "prepar": [28, 161], "truncat": [28, 30, 31, 32, 33, 37, 39, 41, 43, 44, 118, 119, 162], "encod": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 110, 118, 119, 120, 161], "decod": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 106, 118, 119, 120, 129, 161], "anyth": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "load_dataset": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 161, 162], "huggingfac": [28, 30, 32, 33, 36, 37, 39, 43, 44, 91, 98, 99, 109, 110, 122, 123, 160, 163], "co": [28, 30, 32, 33, 36, 37, 39, 43, 44, 91, 98, 99, 122, 123, 163], "doc": [28, 30, 32, 33, 36, 37, 39, 43, 44, 121, 127, 133, 138, 143, 144, 150, 160, 163], "en": [28, 30, 32, 33, 36, 37, 39, 43, 44], "package_refer": [28, 30, 32, 33, 36, 37, 39, 43, 44], "loading_method": [28, 30, 32, 33, 36, 37, 39, 43, 44], "text": [28, 31, 33, 36, 37, 43, 44, 118, 119, 120, 161, 163], "extra": [28, 157, 161, 166, 167], "still": [28, 127, 166, 167], "unless": 28, "check": [28, 36, 106, 116, 131, 138, 152, 156, 161, 163, 164, 166], "concaten": [29, 118, 120, 146], "sub": [29, 143], "unifi": [29, 91], "were": [29, 113, 161, 164], "simplifi": [29, 160, 166], "simultan": 29, "intern": [29, 127], "aggreg": 29, "transpar": 29, "index": [29, 31, 101, 105, 106, 107, 109, 145, 146, 157, 161, 163], "howev": [29, 99, 157], "constitu": 29, "might": [29, 160, 163], "larg": [29, 112, 151, 160, 167], "comput": [29, 63, 68, 75, 79, 101, 102, 105, 106, 110, 134, 150, 163, 167], "cumul": 29, "maintain": [29, 167], "indic": [29, 31, 101, 105, 106, 107, 121, 138, 161], "deleg": 29, "retriev": [29, 132], "lead": [29, 118], "high": [29, 158, 166], "scale": [29, 48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 112, 129, 166, 167], "consid": 29, "strategi": 29, "stream": [29, 133], "demand": 29, "deriv": [29, 102, 106, 107], "_dataset": 29, "_len": 29, "total": [29, 109, 136, 155, 159, 163, 165, 166], "combin": 29, "_index": 29, "lookup": 29, "dataset1": 29, "mycustomdataset": 29, "params1": 29, "dataset2": 29, "params2": 29, "concat_dataset": 29, "data_point": 29, "1500": 29, "element": [29, 119, 163], "accomplish": [29, 36, 39, 43], "instruct_dataset": [29, 162], "vicgal": [29, 162], "gpt4": [29, 162], "alpacainstructtempl": [29, 39, 162], "samsum": [29, 40, 162], "summarizetempl": [29, 161, 162], "focus": [29, 164], "enhanc": [29, 167], "divers": 29, "machin": [29, 130, 160, 163], "instructtempl": [30, 32, 162], "contribut": [30, 35, 38, 40], "disabl": [30, 32, 33, 37, 39, 43, 44, 113, 150], "recommend": [30, 32, 33, 35, 37, 39, 43, 44, 143, 161, 163, 167], "highest": [30, 32, 33, 35, 37, 39, 43, 44], "sequenc": [30, 31, 32, 33, 35, 37, 39, 41, 43, 44, 54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 118, 119, 145, 146, 161], "ds": [31, 41], "padding_idx": [31, 145, 146], "max_pack": 31, "split_across_pack": 31, "greedi": 31, "pack": [31, 34, 35, 36, 38, 39, 40, 41, 43, 101, 105, 106, 107], "done": [31, 116, 131, 140, 166, 167], "preprocess": 31, "outsid": [31, 150, 151, 163, 165, 166], "sampler": [31, 164], "part": [31, 161, 167], "buffer": 31, "enough": [31, 161], "attent": [31, 48, 49, 50, 54, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 98, 101, 103, 105, 106, 107, 116, 117, 165, 166, 167], "lower": [31, 166], "triangular": 31, "cross": 31, "attend": [31, 101, 106, 107], "rel": [31, 101, 105, 106, 107, 134, 166], "pad": [31, 129, 145, 146, 162], "max": [31, 41, 106, 109, 118, 160, 166], "wise": 31, "collat": [31, 145, 162], "made": [31, 36, 39, 43, 105, 163], "smaller": [31, 163, 165, 166, 167], "jam": 31, "vari": 31, "s1": [31, 118], "s2": [31, 118], "s3": 31, "s4": 31, "contamin": 31, "input_po": [31, 101, 103, 105, 106, 107], "matrix": 31, "causal": [31, 101, 106, 107], "continu": [31, 162], "increment": 31, "move": [31, 106], "entir": [31, 140, 161, 167], "avoid": [31, 104, 108, 150, 160, 167], "freeform": [33, 43], "unstructur": [33, 43, 44], "corpu": [33, 37, 43, 44], "local": [33, 43, 144, 150, 157, 160, 161, 163, 164], "tabular": [33, 43], "yahma": [34, 39], "variant": [34, 38, 40], "version": [34, 58, 68, 79, 84, 86, 95, 101, 129, 152, 157, 161, 165, 167], "page": [34, 44, 157, 158, 160, 164, 165], "tatsu": 35, "lab": 35, "codebas": [35, 38, 40, 163], "prior": [35, 36, 38, 39, 40, 41, 43], "alpaca_d": 35, "batch_siz": [35, 38, 40, 101, 103, 106, 107, 110, 163], "conversation_styl": [36, 162], "chatdataset": [36, 41, 161, 162], "friendli": [36, 39, 43, 129, 161], "huggingfaceh4": 36, "no_robot": 36, "chatmlformat": 36, "2096": [36, 39, 43], "packeddataset": [36, 39, 43, 162], "ccdv": 37, "cnn_dailymail": 37, "textcompletiondataset": [37, 43, 44, 162], "cnn": 37, "dailymail": 37, "articl": [37, 44], "extract": 37, "highlight": [37, 167], "liweili": 38, "c4_200m": 38, "mirror": [38, 40], "llama_recip": [38, 40], "grammar_d": 38, "alpaca_clean": 39, "samsum_d": 40, "open": [41, 55, 56, 162, 163], "orca": 41, "slimorca": 41, "dedup": 41, "1024": [41, 42, 162], "prescrib": 41, "least": [41, 165, 166], "though": [41, 161], "10": [41, 145, 146, 163, 165, 167], "351": 41, "82": [41, 163], "391": 41, "221": 41, "220": 41, "193": 41, "12": [41, 146, 157], "471": 41, "lvwerra": [42, 162], "stack": [42, 151, 162], "exchang": [42, 162], "preferencedataset": [42, 162], "stackexchangepair": 42, "omit": [43, 166], "allenai": [43, 162], "c4": [43, 162], "data_dir": [43, 162], "realnewslik": [43, 162], "wikitext": 44, "subset": [44, 58, 68, 79, 84, 86, 95, 114], "103": [44, 163], "raw": 44, "wikipedia": 44, "code_llama2": [45, 46, 47, 48, 49, 50, 51, 52, 53, 160], "transformerdecod": [45, 46, 47, 48, 49, 50, 51, 52, 53, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 100, 129, 166], "w": [45, 46, 47, 55, 56, 64, 65, 66, 76, 77, 89, 91, 143, 144, 161, 163, 166, 167], "arxiv": [45, 46, 47, 51, 52, 53, 61, 62, 64, 65, 66, 72, 73, 74, 82, 83, 93, 94, 100, 101, 104, 105, 110], "org": [45, 46, 47, 51, 52, 53, 61, 62, 64, 65, 66, 72, 73, 74, 82, 83, 93, 94, 100, 101, 104, 105, 110, 121, 127, 133, 138, 143, 148, 150, 157], "pdf": [45, 46, 47], "2308": [45, 46, 47], "12950": [45, 46, 47], "lora_attn_modul": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 116, 117, 166, 167], "q_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 116, 117, 166, 167], "k_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 116, 117, 166, 167], "v_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 116, 117, 166, 167], "output_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 101, 116, 117, 166, 167], "apply_lora_to_mlp": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 116, 117, 166], "apply_lora_to_output": [48, 49, 50, 51, 52, 53, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 116, 117, 166], "lora_rank": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 166], "lora_alpha": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 166], "float": [48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 93, 94, 95, 96, 97, 100, 101, 104, 109, 110, 112, 129, 134, 139, 141, 142, 143, 144, 166, 167], "16": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 69, 70, 71, 72, 73, 74, 80, 81, 82, 83, 85, 87, 93, 94, 96, 100, 146, 166, 167], "lora_dropout": [48, 49, 50, 51, 52, 53, 58, 68, 69, 70, 71, 72, 73, 74, 79, 84, 86, 95], "05": [48, 49, 50, 51, 52, 53, 63, 68, 69, 70, 71, 72, 73, 74, 75, 79, 84, 86, 88, 90, 95, 97], "quantize_bas": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 112, 167], "lora": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 112, 113, 116, 117, 122, 140, 156, 158, 161, 164, 165], "code_llama2_13b": 48, "tloen": [48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96], "8bb8579e403dc78e37fe81ffbb253c413007323f": [48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96], "l41": [48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96], "l43": [48, 49, 50, 59, 60, 69, 70, 71, 80, 81, 96], "linear": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 100, 106, 111, 112, 116, 117, 166, 167], "mlp": [48, 49, 50, 54, 58, 59, 60, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 90, 95, 96, 97, 106, 107, 116, 117, 165, 166], "final": [48, 49, 50, 54, 58, 63, 68, 69, 70, 71, 75, 79, 80, 81, 84, 85, 86, 87, 88, 95, 96, 102, 106, 113, 116, 117, 119, 163, 165, 166, 167], "low": [48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 112, 163, 166, 167], "approxim": [48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 112, 166], "factor": [48, 49, 50, 58, 59, 60, 68, 69, 70, 71, 79, 80, 81, 84, 85, 86, 87, 95, 96, 112, 163], "dropout": [48, 49, 50, 54, 58, 63, 68, 69, 70, 71, 75, 79, 84, 86, 88, 90, 95, 97, 101, 112, 166, 167], "probabl": [48, 49, 50, 58, 68, 69, 70, 71, 79, 84, 86, 95, 110, 112, 129, 163], "code_llama2_70b": 49, "code_llama2_7b": 50, "qlora": [51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 108, 156, 158, 165, 166], "per": [51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 103, 108, 160, 165, 167], "paper": [51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 166, 167], "ab": [51, 52, 53, 61, 62, 64, 65, 66, 72, 73, 74, 82, 83, 93, 94, 100, 101, 104, 105, 110], "2305": [51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100, 101, 110], "14314": [51, 52, 53, 61, 62, 72, 73, 74, 82, 83, 93, 94, 100], "lora_code_llama2_13b": 51, "lora_code_llama2_70b": 52, "lora_code_llama2_7b": 53, "head_dim": [54, 58, 101, 103, 106], "intermediate_dim": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "attn_dropout": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 106], "norm_ep": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "1e": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 104], "06": [54, 58, 104, 166], "rope_bas": [54, 58, 75, 79, 84, 86, 88, 90, 95, 97], "10000": [54, 58, 84, 86, 88, 90, 95, 97, 105], "norm_embed": [54, 58], "gemmatransformerdecod": [54, 55, 56, 58, 59, 60, 61, 62], "transformerdecoderlay": [54, 63, 75, 88, 106], "rm": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "norm": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 106, 107], "space": [54, 63, 75, 88, 106], "slide": [54, 88, 98], "window": [54, 88, 98, 162], "vocabulari": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "queri": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 106, 107, 165], "head": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 125, 165], "mha": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 106], "dimens": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101, 103, 105, 106, 112, 165, 166, 167], "intermedi": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 124, 148, 165, 167], "onto": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101], "scaled_dot_product_attent": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97, 101], "epsilon": [54, 58, 63, 68, 75, 79, 84, 86, 88, 90, 95, 97], "rotari": [54, 58, 84, 86, 88, 90, 95, 97, 105, 165], "10_000": [54, 58, 84, 86, 88, 90, 97], "gemma": [55, 56, 57, 58, 59, 60, 61, 62, 125], "blog": [55, 56], "technolog": [55, 56], "develop": [55, 56, 167], "sentencepiecetoken": [57, 67, 92, 161], "becaus": [58, 103, 106, 124, 160, 161, 163, 165], "ti": 58, "gemma_2b": 59, "gemma_7b": 60, "lora_gemma_2b": 61, "lora_gemma_7b": 62, "kvcach": [63, 68, 75, 79, 95, 101, 106], "scale_hidden_dim_for_mlp": [63, 68, 75, 79], "2307": [64, 65, 66], "09288": [64, 65, 66], "llama2_70b": 70, "llama2_7b": [71, 166], "lora_llama2_13b": 72, "lora_llama2_70b": 73, "lora_llama2_7b": [74, 166], "500000": [75, 79], "tiktokentoken": [78, 161], "llama3_70b": 80, "llama3_8b": [81, 129, 165], "lora_llama3_70b": 82, "lora_llama3_8b": 83, "num_class": [86, 90], "classifi": [86, 87, 90, 91, 94, 162], "announc": 89, "classif": [90, 125], "ray2333": 91, "reward": [91, 110], "feedback": 91, "lora_mistral_7b": 93, "lora_mistral_classifier_7b": 94, "phi3": [95, 96, 98, 99, 100, 125, 160], "phi3_mini": [96, 125], "ref": [98, 99, 144], "phi": [98, 99, 125], "128k": 98, "nor": 98, "phi3minisentencepiecetoken": 99, "tokenizer_config": 99, "spm": 99, "lm": 99, "eo": [99, 118, 119, 161, 162], "bo": [99, 118, 119, 161, 162], "unk": 99, "augment": [99, 167], "endoftext": 99, "opt": [99, 157, 164], "cite": 99, "better": [99, 158, 161, 162, 163], "51": 99, "lora_phi3_mini": 100, "pos_embed": [101, 166], "kv_cach": 101, "gqa": 101, "introduc": [101, 104, 112, 161, 162, 166, 167], "13245v1": 101, "multihead": 101, "extrem": 101, "share": [101, 162, 163], "mqa": 101, "credit": 101, "document": [101, 121, 132, 140, 160, 162], "lightn": 101, "lit": 101, "lit_gpt": 101, "v": [101, 106, 166], "k": [101, 166], "q": [101, 166], "n_kv_head": 101, "calcul": [101, 106, 165], "e": [101, 108, 111, 115, 122, 126, 134, 151, 157, 163, 165, 166, 167], "g": [101, 111, 122, 134, 151, 165, 166, 167], "rotarypositionalembed": [101, 166], "cach": [101, 103, 105, 106, 157, 160], "rope": [101, 105], "seq_length": [101, 107, 129], "boolean": [101, 106, 107, 121], "softmax": [101, 106, 107], "row": [101, 106, 107, 161], "j": [101, 106, 107], "seq_len": 101, "bigger": 101, "n_h": [101, 105], "num": [101, 105], "n_kv": 101, "kv": [101, 103, 106], "emb": [101, 106], "h_d": [101, 105], "gate_proj": 102, "down_proj": 102, "up_proj": 102, "silu": 102, "feed": [102, 107], "network": [102, 113, 166, 167], "fed": [102, 161], "multipli": 102, "subclass": [102, 127], "although": [102, 166], "afterward": 102, "former": 102, "regist": [102, 108, 147, 167], "hook": [102, 108, 147, 167], "latter": 102, "standalon": 103, "past": 103, "expand": 103, "dpython": [103, 106, 108], "reset": [103, 106, 134], "k_val": 103, "v_val": 103, "h": [103, 157, 160], "longer": [103, 162], "ep": 104, "root": [104, 143, 144], "squar": 104, "1910": 104, "07467": 104, "verfic": [104, 105], "small": [104, 163], "divis": 104, "propos": 105, "2104": 105, "09864": 105, "l80": 105, "upto": 105, "init": [105, 134, 144, 167], "exceed": 105, "freq": 105, "recomput": 105, "geometr": 105, "progress": [105, 164], "rotat": 105, "angl": 105, "todo": 105, "effici": [105, 116, 132, 156, 158, 163, 164, 166], "belong": [106, 128], "reduc": [106, 158, 162, 166, 167], "statement": 106, "improv": [106, 132, 163, 165, 166], "readabl": [106, 163], "At": 106, "arang": 106, "prompt_length": 106, "causal_mask": 106, "m_": 106, "seq": 106, "reset_cach": 106, "setup_cach": 106, "attn": [107, 166, 167], "causalselfattent": [107, 166], "sa_norm": 107, "mlp_norm": 107, "ff": 107, "common_util": 108, "bfloat16": [108, 149, 163, 164, 165, 166], "offload_to_cpu": 108, "nf4": [108, 167], "restor": 108, "higher": [108, 165, 167], "offload": [108, 167], "increas": [108, 109, 165, 166], "peak": [108, 134, 139, 163, 165, 166, 167], "gpu": [108, 160, 163, 164, 165, 166, 167], "_register_state_dict_hook": 108, "m": [108, 119, 129, 161], "mymodul": 108, "_after_": 108, "nf4tensor": [108, 167], "unquant": [108, 163, 167], "unus": 108, "num_warmup_step": 109, "num_training_step": 109, "num_cycl": [109, 151], "last_epoch": 109, "lambdalr": 109, "rate": [109, 158, 164], "schedul": [109, 151, 164], "linearli": 109, "decreas": [109, 162, 166, 167], "cosin": 109, "v4": 109, "23": [109, 165], "src": 109, "l104": 109, "warmup": [109, 151], "phase": 109, "wave": 109, "half": 109, "lr_schedul": 109, "beta": 110, "label_smooth": 110, "loss_typ": 110, "sigmoid": 110, "dpo": [110, 113, 146], "18290": 110, "trl": 110, "librari": [110, 127, 131, 133, 150, 156, 158, 160, 162, 167], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 110, "dpo_train": 110, "l844": 110, "temperatur": [110, 129, 163], "uncertainti": 110, "hing": 110, "ipo": 110, "kto_pair": 110, "policy_chosen_logp": 110, "policy_rejected_logp": 110, "reference_chosen_logp": 110, "reference_rejected_logp": 110, "polici": [110, 113, 121, 132, 140, 148], "chosen": [110, 151, 162], "reject": [110, 162], "chosen_reward": 110, "rejected_reward": 110, "unknown": 110, "peft": [111, 112, 113, 114, 115, 116, 117, 122, 166, 167], "protocol": 111, "adapter_param": [111, 112, 113, 114, 115], "proj": 111, "in_dim": [111, 112, 166, 167], "out_dim": [111, 112, 166, 167], "bia": [111, 112, 166, 167], "loralinear": [111, 166, 167], "alpha": [112, 166, 167], "use_bia": 112, "perturb": 112, "decomposit": [112, 166], "matric": [112, 140, 166, 167], "trainabl": [112, 115, 140, 166, 167], "mapsto": 112, "w_0x": 112, "r": [112, 119, 166], "bax": 112, "lora_a": [112, 166, 167], "lora_b": [112, 166, 167], "temporarili": 113, "neural": [113, 166, 167], "treat": [113, 127, 161], "attribut": [113, 118, 128], "caller": 113, "whose": [113, 147], "yield": 113, "get_adapter_param": [115, 166], "base_miss": 116, "base_unexpect": 116, "lora_miss": 116, "lora_unexpect": 116, "validate_state_dict_for_lora": [116, 166], "unlik": [116, 163, 165], "reli": 116, "unexpect": 116, "strict": [116, 166], "pull": [116, 160], "120600": 116, "assertionerror": [116, 117, 146], "nonempti": 116, "full_model_state_dict_kei": 117, "lora_state_dict_kei": 117, "base_model_state_dict_kei": 117, "confirm": [117, 157], "determin": 117, "lora_modul": 117, "complement": 117, "disjoint": 117, "union": [117, 141, 142, 143, 144, 148, 150], "non": [117, 118], "overlap": 117, "sentencepieceprocessor": 118, "pretrain": [118, 119, 160, 161, 164, 166, 167], "spm_model": [118, 161], "tokenized_text": 118, "world": [118, 136, 138, 163], "add_bo": [118, 119, 120, 161], "add_eo": [118, 119, 120, 161], "31587": 118, "29644": 118, "102": 118, "trim_leading_whitespac": 118, "prefix": 118, "unbatch": 118, "prepend": [118, 119], "append": [118, 157], "trim": 118, "whitespac": 118, "underli": [118, 167], "sentencepiec": [118, 165], "due": [118, 166, 167], "tokenize_messag": [118, 119, 120, 161, 162], "problem": 118, "slice": 118, "tokenizer_path": 118, "separ": [118, 122, 161, 164, 165, 166, 167], "concat": 118, "1788": 118, "2643": 118, "13": [118, 146, 163, 165, 167], "1792": 118, "9508": 118, "465": 118, "22137": 118, "2933": 118, "join": 118, "llama3_tiktoken": 119, "p": [119, 121, 126, 166, 167], "l": 119, "all_special_token": 119, "bos_token": 119, "begin_of_text": [119, 161], "eos_token": 119, "end_of_text": 119, "start_header_id": [119, 161], "end_header_id": [119, 161], "step_id": 119, "eom_id": 119, "eot_id": [119, 161], "python_tag": 119, "identif": 119, "regex": 119, "second": [119, 163, 165, 166, 167], "uniqu": [119, 125], "256": [119, 163, 165], "header": [119, 161], "token_id": [119, 120], "truncate_at_eo": 119, "tokenize_head": 119, "datatyp": [121, 167], "denot": 121, "integ": [121, 145, 150], "auto_wrap_polici": [121, 132, 148], "submodul": [121, 140], "obei": 121, "contract": 121, "get_fsdp_polici": 121, "modules_to_wrap": [121, 132, 140], "min_num_param": 121, "my_fsdp_polici": 121, "recurs": [121, 140, 143], "isinst": [121, 162], "sum": [121, 166], "numel": [121, 166], "1000": 121, "functool": 121, "partial": 121, "stabl": [121, 138, 143, 150, 157], "html": [121, 127, 133, 138, 143, 148, 150, 156], "alia": 121, "safe_seri": 122, "from_pretrain": 122, "0001_of_0003": 122, "0002_of_0003": 122, "preserv": [122, 167], "weight_map": [122, 163], "convert_weight": 122, "_model_typ": [122, 125], "intermediate_checkpoint": [122, 123, 124], "_weight_map": 122, "shard": [123, 165], "wip": 123, "larger": [124, 163, 165], "present": 124, "down": [124, 162, 166, 167], "qualnam": 125, "boundari": 125, "distinguish": 125, "gate": [125, 160, 164], "my_new_model": 125, "my_custom_state_dict_map": 125, "mistral_reward": 125, "mistral_classifi": 125, "optim_map": 126, "bare": 126, "bone": 126, "distribut": [126, 130, 137, 138, 148, 150, 158, 160, 164, 165], "optim_dict": [126, 128, 147], "cfg_optim": 126, "ckpt": 126, "optim_ckpt": 126, "placeholder_optim_dict": 126, "optiminbackwardwrapp": 126, "get_optim_kei": 126, "arbitrari": [126, 166], "hyperparamet": [126, 158, 164, 166, 167], "optim_ckpt_map": 126, "runtimeerror": [126, 131, 137], "loadabl": 126, "argpars": 127, "argumentpars": 127, "builtin": 127, "said": 127, "noth": 127, "consult": 127, "info": [127, 164], "parse_known_arg": 127, "namespac": 127, "act": 127, "precid": 127, "parse_arg": 127, "properti": [127, 166], "too": [127, 165], "optimizerinbackwardwrapp": 128, "top": [128, 163, 167], "named_paramet": 128, "max_generated_token": 129, "pad_id": 129, "top_k": [129, 163], "stop_token": 129, "custom_generate_next_token": 129, "condit": [129, 138, 160, 162], "bsz": 129, "predict": 129, "prune": [129, 167], "stop": 129, "compil": [129, 163, 165, 167], "generate_next_token": 129, "llama3_token": [129, 161, 165], "hi": [129, 161], "my": [129, 160, 161, 162, 163, 165], "jeremi": 129, "float32": 131, "bf16": [131, 167], "inde": [131, 163], "kernel": 131, "isn": [131, 160], "hardwar": [131, 158, 162, 163, 166], "memory_efficient_fsdp_wrap": 132, "maxim": [132, 140, 156, 158], "been": [132, 161, 165], "workload": 132, "15": [132, 146, 161, 163, 166, 167], "alongsid": 132, "ac": 132, "fullyshardeddataparallel": [132, 140], "fsdppolicytyp": [132, 140], "handler": 133, "reset_stat": 134, "track": 134, "alloc": [134, 139, 140, 165, 167], "reserv": [134, 139, 161, 167], "stat": [134, 139, 167], "int4": 135, "4w": [135, 163, 165], "recogn": 135, "int4weightonlyquant": [135, 163, 165], "int8weightonlyquant": 135, "8w": 135, "int4weightonlygptqquant": 135, "gptq": 135, "int8dynactint4weightquant": 135, "8da4w": 135, "int8dynactint4weightqatquant": 135, "qat": 135, "mode": [135, 163], "aka": 136, "master": 138, "port": [138, 160], "address": 138, "hold": [138, 164], "peak_memory_act": 139, "peak_memory_alloc": 139, "peak_memory_reserv": 139, "get_memory_stat": 139, "own": [140, 150, 160, 161, 162, 163, 166], "unit": [140, 158], "hierarch": 140, "requires_grad": [140, 166, 167], "filenam": 141, "log_": 141, "unixtimestamp": 141, "txt": [141, 162, 164], "thread": 141, "safe": 141, "flush": [141, 142, 143, 144], "ndarrai": [141, 142, 143, 144], "scalar": [141, 142, 143, 144], "record": [141, 142, 143, 144, 151], "payload": [141, 142, 143, 144], "organize_log": 143, "tensorboard": 143, "subdirectori": 143, "compar": [143, 152, 163, 166, 167], "logdir": 143, "startup": 143, "tree": [143, 162, 163], "tfevent": 143, "encount": 143, "frontend": 143, "organ": [143, 160], "accordingli": 143, "my_log_dir": 143, "view": [143, 163, 164], "my_metr": [143, 144], "termin": [143, 144], "entiti": 144, "bias": 144, "sent": 144, "usernam": 144, "my_project": 144, "my_ent": 144, "my_group": 144, "importerror": 144, "account": [144, 166, 167], "log_config": 144, "link": [144, 163], "capecap": 144, "6053ofw0": 144, "torchtune_config_j67sb73v": 144, "ignore_idx": [145, 146], "longest": 145, "token_pair": 145, "input_id": 146, "chosen_input_id": [146, 162], "chosen_label": [146, 162], "rejected_input_id": [146, 162], "rejected_label": [146, 162], "14": [146, 167], "17": [146, 163, 166], "18": [146, 165], "19": [146, 163, 165, 167], "20": 146, "soon": 147, "readi": [147, 156, 161], "grad": 147, "achiev": [147, 163, 165, 166, 167], "acwrappolicytyp": 148, "author": [148, 158, 164, 167], "fsdp_adavnced_tutori": 148, "insid": 149, "contextmanag": 149, "debug_mod": 150, "pseudo": 150, "random": [150, 164], "commonli": [150, 163, 166, 167], "numpi": 150, "determinist": 150, "global": [150, 162], "warn": 150, "nondeterminist": 150, "addition": [150, 162, 166], "cudnn": 150, "set_deterministic_debug_mod": 150, "algorithm": 150, "profile_memori": 151, "with_stack": 151, "record_shap": 151, "with_flop": 151, "wait_step": 151, "warmup_step": 151, "active_step": 151, "profil": 151, "layout": 151, "trace": 151, "profileract": 151, "gradient_accumul": 151, "sensibl": 151, "default_schedul": 151, "speed": [151, 165, 167], "reduct": [151, 166], "iter": [151, 153, 167], "scope": 151, "flop": 151, "wait": 151, "cycl": 151, "repeat": 151, "greater": 152, "equal": 152, "against": [152, 167], "__version__": 152, "named_param": 153, "generated_examples_python": 154, "zip": 154, "galleri": [154, 159], "sphinx": 154, "000": [155, 159, 165], "execut": [155, 159], "generated_exampl": 155, "mem": [155, 159], "mb": [155, 159], "topic": 156, "gentl": 156, "introduct": 156, "first_finetune_tutori": 156, "workflow": [156, 162, 164, 166], "requisit": 157, "proper": [157, 164], "host": [157, 160, 164], "latest": [157, 164, 167], "And": [157, 163, 165], "ls": [157, 160, 163, 164, 165], "welcom": [157, 160], "show": [157, 160, 161, 166], "greatest": [157, 164], "contributor": 157, "cd": [157, 163], "even": [157, 160, 161, 162, 165, 166, 167], "commit": 157, "branch": 157, "url": 157, "whl": 157, "therebi": [157, 167], "forc": 157, "reinstal": 157, "suffix": 157, "cu121": 157, "On": [158, 166], "pointer": 158, "emphas": 158, "aspect": 158, "simplic": 158, "component": 158, "reus": 158, "prove": 158, "democrat": 158, "box": [158, 167], "zoo": 158, "varieti": [158, 166], "techniqu": [158, 163, 164, 166], "integr": [158, 163, 164, 165, 166, 167], "excit": 158, "checkout": 158, "quickstart": 158, "attain": 158, "chekckpoint": 158, "embodi": 158, "philosophi": 158, "usabl": 158, "composit": 158, "hard": [158, 162], "outlin": 158, "unecessari": 158, "never": 158, "thoroughli": 158, "short": 160, "subcommand": 160, "anytim": 160, "symlink": 160, "auto": 160, "wrote": 160, "readm": 160, "md": 160, "lot": [160, 163], "recent": 160, "releas": [160, 165], "agre": 160, "term": 160, "perman": 160, "eat": 160, "bandwith": 160, "storag": [160, 167], "00030": 160, "ootb": 160, "full_finetune_single_devic": [160, 162, 163, 164], "7b_full_low_memori": [160, 163, 164], "8b_full_single_devic": [160, 162], "mini_full_low_memori": 160, "7b_full": [160, 163, 164], "13b_full": [160, 163, 164], "70b_full": 160, "edit": 160, "clobber": 160, "destin": 160, "lora_finetune_distribut": [160, 165, 166], "torchrun": 160, "8b_lora_single_devic": [160, 161, 165], "launch": [160, 161, 164], "nproc": 160, "node": 160, "worker": 160, "nnode": [160, 166], "minimum_nod": 160, "maximum_nod": 160, "fail": 160, "rdzv": 160, "rendezv": 160, "endpoint": 160, "8b_lora": [160, 165], "bypass": 160, "vice": 160, "versa": 160, "fancy_lora": 160, "8b_fancy_lora": 160, "sai": [160, 161, 164], "align": 161, "intend": 161, "nice": 161, "meet": 161, "overhaul": 161, "yet": [161, 163], "untrain": 161, "accompani": 161, "who": 161, "influenti": 161, "hip": 161, "hop": 161, "artist": [161, 165], "2pac": 161, "rakim": 161, "c": 161, "na": 161, "flavor": [161, 162], "msg": 161, "formatted_messag": [161, 162], "nyou": [161, 162], "nwho": 161, "why": [161, 164, 166], "user_messag": 161, "text_cont": 161, "518": 161, "25580": 161, "29962": 161, "3532": 161, "14816": 161, "29903": 161, "6778": 161, "_spm_model": 161, "piece_to_id": 161, "vector": 161, "place": 161, "manual": [161, 167], "529": 161, "29879": 161, "29958": 161, "nhere": 161, "special_token": 161, "128000": 161, "128009": 161, "pure": 161, "That": 161, "won": [161, 163, 165], "mess": 161, "govern": 161, "prime": 161, "strictli": 161, "lightweight": 161, "ask": 161, "untouch": 161, "nsummari": 161, "robust": 161, "csv": [161, 162], "onlin": 161, "forum": 161, "panda": 161, "pd": 161, "df": 161, "read_csv": 161, "your_fil": 161, "nrow": 161, "tolist": 161, "iloc": 161, "gp": 161, "receiv": 161, "commun": [161, 162, 163], "satellit": 161, "thing": [161, 167], "message_convert": 161, "input_msg": 161, "output_msg": 161, "assistant_messag": 161, "But": [161, 163, 165, 166], "mistralchatformat": 161, "custom_dataset": 161, "2048": 161, "data_fil": [161, 162], "honor": 161, "copi": [161, 163, 164, 165, 167], "folder": 161, "custom_8b_lora_single_devic": 161, "steer": 162, "wheel": 162, "publicli": 162, "great": [162, 163], "hood": [162, 163, 167], "text_completion_dataset": 162, "padded_col": 162, "upper": 162, "constraint": [162, 166], "slow": [162, 167], "signific": 162, "speedup": [162, 165], "minim": [162, 164, 166, 167], "my_data": 162, "fix": 162, "goal": 162, "agnost": 162, "respond": 162, "anim": 162, "plant": 162, "miner": 162, "oak": 162, "copper": 162, "ore": 162, "eleph": 162, "customtempl": 162, "cl": 162, "chat_dataset": 162, "quit": [162, 167], "similarli": 162, "incorpor": 162, "advanc": 162, "customchatformat": 162, "concatdataset": 162, "drive": 162, "rajpurkar": 162, "io": 162, "squad": 162, "explor": 162, "rlhf": 162, "few": [162, 165, 166, 167], "adjust": 162, "chosen_messag": 162, "transformed_sampl": 162, "key_chosen": 162, "rejected_messag": 162, "key_reject": 162, "c_mask": 162, "np": 162, "cross_entropy_ignore_idx": 162, "r_mask": 162, "stack_exchanged_paired_dataset": 162, "had": 162, "stackexchangedpairedtempl": 162, "response_j": 162, "response_k": 162, "rl": 162, "favorit": [163, 165, 166], "seemlessli": 163, "beyond": [163, 167], "connect": 163, "amount": 163, "natur": 163, "export": 163, "mobil": 163, "phone": 163, "leverag": [163, 165, 167], "plai": 163, "freez": [163, 166], "percentag": 163, "learnabl": 163, "keep": [163, 166], "16gb": [163, 166], "rtx": 163, "3090": 163, "4090": 163, "hour": 163, "7b_qlora_single_devic": [163, 164, 167], "473": 163, "98": [163, 167], "gb": [163, 165, 166, 167], "50": 163, "484": 163, "01": [163, 164], "fact": [163, 165, 166], "third": 163, "realli": 163, "eleuther_ev": [163, 165], "eleuther_evalu": [163, 165], "lm_eval": [163, 165], "plan": 163, "custom_eval_config": [163, 165], "truthfulqa_mc2": [163, 165, 166], "measur": [163, 165], "propens": [163, 165], "shot": [163, 165], "accuraci": [163, 165, 166, 167], "baselin": [163, 166], "324": 163, "loglikelihood": 163, "195": 163, "121": 163, "27": 163, "197": 163, "acc": 163, "388": 163, "38": 163, "shown": 163, "489": 163, "48": [163, 167], "seem": 163, "custom_generation_config": [163, 165], "kick": 163, "300": 163, "interest": 163, "site": 163, "visit": 163, "bai": 163, "area": 163, "92": [163, 165], "exploratorium": 163, "san": 163, "francisco": 163, "magazin": 163, "awesom": 163, "bridg": 163, "pretti": 163, "cool": 163, "96": [163, 167], "61": 163, "sec": [163, 165], "25": 163, "83": 163, "99": [163, 166], "72": 163, "littl": 163, "saw": 163, "took": [163, 165], "torchao": [163, 165, 167], "bit": [163, 165, 166, 167], "custom_quantization_config": [163, 165], "68": 163, "76": 163, "69": 163, "95": [163, 165], "67": 163, "engin": [163, 165], "fullmodeltorchtunecheckpoint": [163, 165], "groupsiz": [163, 165], "park": 163, "sit": 163, "hill": 163, "beauti": 163, "62": [163, 165], "85": 163, "sped": 163, "almost": [163, 165, 166], "3x": [163, 165], "benefit": 163, "doesn": 163, "fast": 163, "clone": [163, 166, 167], "assumpt": 163, "satisfi": 163, "new_dir": 163, "output_dict": 163, "sd_1": 163, "sd_2": 163, "dump": 163, "convert_hf_checkpoint": 163, "checkpoint_path": 163, "justin": 163, "school": 163, "math": 163, "teacher": 163, "ws": 163, "94": [163, 165], "28": 163, "bandwidth": [163, 165], "1391": 163, "84": 163, "thats": 163, "seamlessli": 163, "authent": [163, 164], "hopefulli": 163, "gave": 163, "grant": 164, "minut": 164, "agreement": 164, "altern": 164, "hackabl": 164, "singularli": 164, "technic": 164, "purpos": [164, 165], "depth": 164, "principl": 164, "boilerpl": 164, "substanti": [164, 166], "custom_config": 164, "replic": 164, "lorafinetunerecipesingledevic": 164, "lora_finetune_output": 164, "log_1713194212": 164, "52": 164, "3697006702423096": 164, "25880": [164, 167], "24": [164, 165], "55": 164, "83it": 164, "monitor": 164, "tqdm": 164, "interv": 164, "e2": 164, "focu": 165, "128": [165, 166], "theta": 165, "gain": 165, "illustr": 165, "basic": 165, "observ": 165, "consum": [165, 167], "vram": [165, 166], "overal": 165, "8b_qlora_single_devic": 165, "coupl": [165, 166, 167], "meta_model_0": 165, "122": 165, "sarah": 165, "busi": 165, "mum": 165, "young": 165, "children": 165, "live": 165, "north": 165, "east": 165, "england": 165, "135": 165, "88": 165, "138": 165, "346": 165, "09": 165, "139": 165, "31": 165, "far": 165, "drill": 165, "90": 165, "93": 165, "91": 165, "104": 165, "four": [165, 166], "again": 165, "jake": 165, "disciplin": 165, "passion": 165, "draw": 165, "paint": 165, "57": [165, 166, 167], "broader": 165, "teach": 166, "straight": 166, "unfamiliar": 166, "oppos": [166, 167], "momentum": 166, "relat": 166, "aghajanyan": 166, "et": 166, "al": 166, "hypothes": 166, "intrins": 166, "often": 166, "eight": 166, "practic": 166, "imag": 166, "left": 166, "blue": 166, "rememb": 166, "approx": 166, "15m": 166, "8192": 166, "65k": 166, "frozen_out": [166, 167], "lora_out": [166, 167], "base_model": 166, "choos": 166, "lora_model": 166, "lora_llama_2_7b": [166, 167], "alon": 166, "in_featur": 166, "out_featur": 166, "inplac": 166, "feel": 166, "free": 166, "whenev": 166, "peft_util": 166, "set_trainable_param": 166, "fetch": 166, "lora_param": 166, "total_param": 166, "trainable_param": 166, "2f": 166, "6742609920": 166, "4194304": 166, "7b_lora": 166, "my_model_checkpoint_path": [166, 167], "tokenizer_checkpoint": [166, 167], "my_tokenizer_checkpoint_path": [166, 167], "factori": 166, "benefici": 166, "impact": 166, "minor": 166, "good": 166, "64": 166, "lora_experiment_1": 166, "smooth": [166, 167], "curv": [166, 167], "500": 166, "ran": 166, "footprint": 166, "commod": 166, "cogniz": 166, "ax": 166, "parallel": 166, "truthfulqa": 166, "previous": 166, "475": 166, "87": 166, "508": 166, "86": 166, "504": 166, "04": 166, "514": 166, "lowest": 166, "absolut": 166, "4gb": 166, "tradeoff": 166, "potenti": 166, "highli": 167, "vanilla": 167, "held": 167, "therefor": 167, "bespok": 167, "normalfloat": 167, "8x": 167, "retain": 167, "vast": 167, "major": 167, "degrad": 167, "normatfloat": 167, "doubl": 167, "themselv": 167, "deepdiv": 167, "idea": 167, "distinct": 167, "de": 167, "incur": 167, "counterpart": 167, "set_default_devic": 167, "qlora_linear": 167, "memory_alloc": 167, "177": 167, "152": 167, "byte": 167, "del": 167, "empty_cach": 167, "lora_linear": 167, "081": 167, "344": 167, "qlora_llama2_7b": 167, "qlora_model": 167, "essenti": 167, "reparametrize_as_dtype_state_dict_post_hook": 167, "35": 167, "40": 167, "29": 167, "slower": 167, "149": 167, "9157477021217346": 167, "02": 167, "08": 167, "15it": 167, "nightli": 167, "200": 167, "hundr": 167, "228": 167, "8158286809921265": 167, "59": 167, "95it": 167, "exercis": 167, "portion": 167, "linear_nf4": 167, "to_nf4": 167, "linear_weight": 167, "autograd": 167, "regular": 167, "incom": 167}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "StackExchangedPairedTemplate"], [23, 1, 1, "", "SummarizeTemplate"], [24, 0, 1, "", "get_openai_messages"], [25, 0, 1, "", "get_sharegpt_messages"], [26, 0, 1, "", "truncate"], [27, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.StackExchangedPairedTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[23, 2, 1, "", "format"]], "torchtune.datasets": [[28, 1, 1, "", "ChatDataset"], [29, 1, 1, "", "ConcatDataset"], [30, 1, 1, "", "InstructDataset"], [31, 1, 1, "", "PackedDataset"], [32, 1, 1, "", "PreferenceDataset"], [33, 1, 1, "", "TextCompletionDataset"], [34, 0, 1, "", "alpaca_cleaned_dataset"], [35, 0, 1, "", "alpaca_dataset"], [36, 0, 1, "", "chat_dataset"], [37, 0, 1, "", "cnn_dailymail_articles_dataset"], [38, 0, 1, "", "grammar_dataset"], [39, 0, 1, "", "instruct_dataset"], [40, 0, 1, "", "samsum_dataset"], [41, 0, 1, "", "slimorca_dataset"], [42, 0, 1, "", "stack_exchanged_paired_dataset"], [43, 0, 1, "", "text_completion_dataset"], [44, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[45, 0, 1, "", "code_llama2_13b"], [46, 0, 1, "", "code_llama2_70b"], [47, 0, 1, "", "code_llama2_7b"], [48, 0, 1, "", "lora_code_llama2_13b"], [49, 0, 1, "", "lora_code_llama2_70b"], [50, 0, 1, "", "lora_code_llama2_7b"], [51, 0, 1, "", "qlora_code_llama2_13b"], [52, 0, 1, "", "qlora_code_llama2_70b"], [53, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[54, 0, 1, "", "gemma"], [55, 0, 1, "", "gemma_2b"], [56, 0, 1, "", "gemma_7b"], [57, 0, 1, "", "gemma_tokenizer"], [58, 0, 1, "", "lora_gemma"], [59, 0, 1, "", "lora_gemma_2b"], [60, 0, 1, "", "lora_gemma_7b"], [61, 0, 1, "", "qlora_gemma_2b"], [62, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[63, 0, 1, "", "llama2"], [64, 0, 1, "", "llama2_13b"], [65, 0, 1, "", "llama2_70b"], [66, 0, 1, "", "llama2_7b"], [67, 0, 1, "", "llama2_tokenizer"], [68, 0, 1, "", "lora_llama2"], [69, 0, 1, "", "lora_llama2_13b"], [70, 0, 1, "", "lora_llama2_70b"], [71, 0, 1, "", "lora_llama2_7b"], [72, 0, 1, "", "qlora_llama2_13b"], [73, 0, 1, "", "qlora_llama2_70b"], [74, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[75, 0, 1, "", "llama3"], [76, 0, 1, "", "llama3_70b"], [77, 0, 1, "", "llama3_8b"], [78, 0, 1, "", "llama3_tokenizer"], [79, 0, 1, "", "lora_llama3"], [80, 0, 1, "", "lora_llama3_70b"], [81, 0, 1, "", "lora_llama3_8b"], [82, 0, 1, "", "qlora_llama3_70b"], [83, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[84, 0, 1, "", "lora_mistral"], [85, 0, 1, "", "lora_mistral_7b"], [86, 0, 1, "", "lora_mistral_classifier"], [87, 0, 1, "", "lora_mistral_classifier_7b"], [88, 0, 1, "", "mistral"], [89, 0, 1, "", "mistral_7b"], [90, 0, 1, "", "mistral_classifier"], [91, 0, 1, "", "mistral_classifier_7b"], [92, 0, 1, "", "mistral_tokenizer"], [93, 0, 1, "", "qlora_mistral_7b"], [94, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.phi3": [[95, 0, 1, "", "lora_phi3"], [96, 0, 1, "", "lora_phi3_mini"], [97, 0, 1, "", "phi3"], [98, 0, 1, "", "phi3_mini"], [99, 0, 1, "", "phi3_mini_tokenizer"], [100, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[101, 1, 1, "", "CausalSelfAttention"], [102, 1, 1, "", "FeedForward"], [103, 1, 1, "", "KVCache"], [104, 1, 1, "", "RMSNorm"], [105, 1, 1, "", "RotaryPositionalEmbeddings"], [106, 1, 1, "", "TransformerDecoder"], [107, 1, 1, "", "TransformerDecoderLayer"], [109, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[101, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[102, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[103, 2, 1, "", "reset"], [103, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[104, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[105, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[106, 2, 1, "", "forward"], [106, 2, 1, "", "reset_caches"], [106, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[107, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[108, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[110, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[110, 2, 1, "", "forward"]], "torchtune.modules.peft": [[111, 1, 1, "", "AdapterModule"], [112, 1, 1, "", "LoRALinear"], [113, 0, 1, "", "disable_adapter"], [114, 0, 1, "", "get_adapter_params"], [115, 0, 1, "", "set_trainable_params"], [116, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [117, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[111, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[112, 2, 1, "", "adapter_params"], [112, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[118, 1, 1, "", "SentencePieceTokenizer"], [119, 1, 1, "", "TikTokenTokenizer"], [120, 1, 1, "", "Tokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[118, 2, 1, "", "decode"], [118, 2, 1, "", "encode"], [118, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[119, 2, 1, "", "decode"], [119, 2, 1, "", "encode"], [119, 2, 1, "", "tokenize_message"], [119, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.Tokenizer": [[120, 2, 1, "", "decode"], [120, 2, 1, "", "encode"], [120, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[121, 4, 1, "", "FSDPPolicyType"], [122, 1, 1, "", "FullModelHFCheckpointer"], [123, 1, 1, "", "FullModelMetaCheckpointer"], [124, 1, 1, "", "FullModelTorchTuneCheckpointer"], [125, 1, 1, "", "ModelType"], [126, 1, 1, "", "OptimizerInBackwardWrapper"], [127, 1, 1, "", "TuneRecipeArgumentParser"], [128, 0, 1, "", "create_optim_in_bwd_wrapper"], [129, 0, 1, "", "generate"], [130, 0, 1, "", "get_device"], [131, 0, 1, "", "get_dtype"], [132, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [133, 0, 1, "", "get_logger"], [134, 0, 1, "", "get_memory_stats"], [135, 0, 1, "", "get_quantizer_mode"], [136, 0, 1, "", "get_world_size_and_rank"], [137, 0, 1, "", "init_distributed"], [138, 0, 1, "", "is_distributed"], [139, 0, 1, "", "log_memory_stats"], [140, 0, 1, "", "lora_fsdp_wrap_policy"], [145, 0, 1, "", "padded_collate"], [146, 0, 1, "", "padded_collate_dpo"], [147, 0, 1, "", "register_optim_in_bwd_hooks"], [148, 0, 1, "", "set_activation_checkpointing"], [149, 0, 1, "", "set_default_dtype"], [150, 0, 1, "", "set_seed"], [151, 0, 1, "", "setup_torch_profiler"], [152, 0, 1, "", "torch_version_ge"], [153, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[122, 2, 1, "", "load_checkpoint"], [122, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[123, 2, 1, "", "load_checkpoint"], [123, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[124, 2, 1, "", "load_checkpoint"], [124, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[125, 3, 1, "", "GEMMA"], [125, 3, 1, "", "LLAMA2"], [125, 3, 1, "", "LLAMA3"], [125, 3, 1, "", "MISTRAL"], [125, 3, 1, "", "MISTRAL_REWARD"], [125, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[126, 2, 1, "", "get_optim_key"], [126, 2, 1, "", "load_state_dict"], [126, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[127, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[141, 1, 1, "", "DiskLogger"], [142, 1, 1, "", "StdoutLogger"], [143, 1, 1, "", "TensorBoardLogger"], [144, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[141, 2, 1, "", "close"], [141, 2, 1, "", "log"], [141, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[142, 2, 1, "", "close"], [142, 2, 1, "", "log"], [142, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[143, 2, 1, "", "close"], [143, 2, 1, "", "log"], [143, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[144, 2, 1, "", "close"], [144, 2, 1, "", "log"], [144, 2, 1, "", "log_config"], [144, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 121, 156, 158, 160, 163, 165, 166, 167], "config": [0, 7, 8, 160, 164], "data": [1, 5, 161], "text": [1, 162, 165], "templat": [1, 161, 162], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 161, 162], "exampl": 2, "gener": [2, 129, 163, 165], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 160, 163, 164, 165, 166], "llama3": [3, 75, 161, 165], "llama2": [3, 63, 161, 163, 166, 167], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 88], "gemma": [3, 54], "modul": 4, "compon": [4, 7], "build": [4, 157, 167], "block": 4, "token": [4, 120, 161], "peft": 4, "util": [4, 5, 121], "loss": 4, "checkpoint": [5, 6, 9, 163], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 162, 166, 167], "manag": 5, "perform": [5, 166], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 158, 163], "format": [6, 162], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 163, 166, 167], "put": [6, 167], "thi": 6, "all": [6, 7, 167], "togeth": [6, 167], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 162], "us": [7, 8, 161, 163, 167], "instanti": [7, 10], "referenc": 7, "other": [7, 163], "field": 7, "interpol": 7, "valid": [7, 13, 160], "your": [7, 8, 163, 164], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 158, 166, 167], "ar": 8, "recip": [8, 160, 164, 166], "script": 8, "run": [8, 160, 163], "cli": [8, 160], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "stackexchangedpairedtempl": 22, "summarizetempl": 23, "get_openai_messag": 24, "get_sharegpt_messag": 25, "truncat": 26, "validate_messag": 27, "chatdataset": 28, "concatdataset": 29, "instructdataset": 30, "packeddataset": 31, "preferencedataset": 32, "textcompletiondataset": 33, "alpaca_cleaned_dataset": 34, "alpaca_dataset": 35, "chat_dataset": 36, "cnn_dailymail_articles_dataset": 37, "grammar_dataset": 38, "instruct_dataset": 39, "samsum_dataset": 40, "slimorca_dataset": 41, "stack_exchanged_paired_dataset": 42, "text_completion_dataset": 43, "wikitext_dataset": 44, "code_llama2_13b": 45, "code_llama2_70b": 46, "code_llama2_7b": 47, "lora_code_llama2_13b": 48, "lora_code_llama2_70b": 49, "lora_code_llama2_7b": 50, "qlora_code_llama2_13b": 51, "qlora_code_llama2_70b": 52, "qlora_code_llama2_7b": 53, "gemma_2b": 55, "gemma_7b": 56, "gemma_token": 57, "lora_gemma": 58, "lora_gemma_2b": 59, "lora_gemma_7b": 60, "qlora_gemma_2b": 61, "qlora_gemma_7b": 62, "llama2_13b": 64, "llama2_70b": 65, "llama2_7b": 66, "llama2_token": 67, "lora_llama2": 68, "lora_llama2_13b": 69, "lora_llama2_70b": 70, "lora_llama2_7b": 71, "qlora_llama2_13b": 72, "qlora_llama2_70b": 73, "qlora_llama2_7b": 74, "llama3_70b": 76, "llama3_8b": 77, "llama3_token": 78, "lora_llama3": 79, "lora_llama3_70b": 80, "lora_llama3_8b": 81, "qlora_llama3_70b": 82, "qlora_llama3_8b": 83, "lora_mistr": 84, "lora_mistral_7b": 85, "lora_mistral_classifi": 86, "lora_mistral_classifier_7b": 87, "mistral_7b": 89, "mistral_classifi": 90, "mistral_classifier_7b": 91, "mistral_token": 92, "qlora_mistral_7b": 93, "qlora_mistral_classifier_7b": 94, "lora_phi3": 95, "lora_phi3_mini": 96, "phi3": 97, "phi3_mini": 98, "phi3_mini_token": 99, "qlora_phi3_mini": 100, "causalselfattent": 101, "todo": [101, 107], "feedforward": 102, "kvcach": 103, "rmsnorm": 104, "rotarypositionalembed": 105, "transformerdecod": 106, "transformerdecoderlay": 107, "reparametrize_as_dtype_state_dict_post_hook": 108, "get_cosine_schedule_with_warmup": 109, "dpoloss": 110, "adaptermodul": 111, "loralinear": 112, "disable_adapt": 113, "get_adapter_param": 114, "set_trainable_param": 115, "validate_missing_and_unexpected_for_lora": 116, "validate_state_dict_for_lora": 117, "sentencepiecetoken": 118, "tiktokentoken": 119, "fsdppolicytyp": 121, "fullmodelhfcheckpoint": 122, "fullmodelmetacheckpoint": 123, "fullmodeltorchtunecheckpoint": 124, "modeltyp": 125, "optimizerinbackwardwrapp": 126, "tunerecipeargumentpars": 127, "create_optim_in_bwd_wrapp": 128, "get_devic": 130, "get_dtyp": 131, "get_full_finetune_fsdp_wrap_polici": 132, "get_logg": 133, "get_memory_stat": 134, "get_quantizer_mod": 135, "get_world_size_and_rank": 136, "init_distribut": 137, "is_distribut": 138, "log_memory_stat": 139, "lora_fsdp_wrap_polici": 140, "disklogg": 141, "stdoutlogg": 142, "tensorboardlogg": 143, "wandblogg": 144, "padded_col": 145, "padded_collate_dpo": 146, "register_optim_in_bwd_hook": 147, "set_activation_checkpoint": 148, "set_default_dtyp": 149, "set_se": 150, "setup_torch_profil": 151, "torch_version_g": 152, "validate_expected_param_dtyp": 153, "comput": [155, 159], "time": [155, 159], "welcom": 156, "document": 156, "get": [156, 160, 165], "start": [156, 160], "tutori": 156, "instal": 157, "instruct": [157, 162, 165], "via": [157, 165], "pypi": 157, "git": 157, "clone": 157, "nightli": 157, "kei": 158, "concept": 158, "design": 158, "principl": 158, "download": [160, 163, 164], "list": 160, "built": [160, 162], "copi": 160, "fine": [161, 162, 164, 165], "tune": [161, 162, 164, 165], "chat": [161, 162], "chang": 161, "from": [161, 167], "prompt": 161, "special": 161, "when": 161, "should": 161, "i": 161, "custom": [161, 162], "hug": [162, 163], "face": [162, 163], "set": 162, "max": 162, "sequenc": 162, "length": 162, "sampl": 162, "pack": 162, "unstructur": 162, "corpu": 162, "multipl": 162, "local": 162, "remot": 162, "fulli": 162, "end": 163, "workflow": 163, "7b": 163, "finetun": [163, 166, 167], "evalu": [163, 165], "eleutherai": [163, 165], "s": [163, 165], "eval": [163, 165], "har": [163, 165], "speed": 163, "up": 163, "quantiz": [163, 165], "librari": 163, "upload": 163, "hub": 163, "first": 164, "llm": 164, "select": 164, "modifi": 164, "train": 164, "next": 164, "step": 164, "meta": 165, "8b": 165, "access": 165, "our": 165, "faster": 165, "how": 166, "doe": 166, "work": 166, "appli": 166, "trade": 166, "off": 166, "qlora": 167, "save": 167, "deep": 167, "dive": 167}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})