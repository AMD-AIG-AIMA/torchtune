Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_classifier_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 20, 21, 22, 30, 31, 32, 35, 39, 77, 103, 110, 111, 112, 170, 174, 175, 178, 180, 181], "prompt": [1, 14, 15, 17, 18, 19, 21, 22, 23, 24, 25, 28, 30, 32, 35, 36, 38, 39, 40, 41, 54, 64, 77, 95, 107, 119, 134, 143, 176, 177, 179], "chat": [1, 2, 15, 16, 19, 20, 24, 25, 28, 36, 41, 64, 112], "includ": [1, 6, 7, 8, 15, 18, 55, 64, 65, 78, 87, 100, 112, 125, 136, 137, 141, 172, 174, 175, 176, 177, 178, 179, 180, 181], "some": [1, 6, 7, 16, 98, 127, 128, 170, 172, 174, 175, 176, 177, 178, 180, 181], "specif": [1, 4, 7, 8, 10, 146, 175, 176, 177, 181], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 28, 30, 32, 35, 36, 39, 41, 64, 77, 136, 137, 138, 139, 174, 175, 177, 178, 179, 180], "differ": [1, 7, 9, 28, 29, 30, 32, 95, 131, 139, 160, 167, 172, 174, 175, 177, 179, 180, 181], "dataset": [1, 5, 7, 14, 17, 18, 20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 172, 178, 179], "model": [1, 2, 6, 7, 8, 10, 16, 21, 28, 29, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 133, 134, 136, 137, 138, 139, 142, 143, 146, 148, 154, 161, 162, 170, 172, 175, 176, 181], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 56, 57, 66, 67, 68, 81, 95, 101, 103, 112, 115, 119, 120, 122, 124, 127, 130, 131, 133, 136, 137, 138, 140, 141, 142, 143, 157, 158, 161, 169, 171, 173, 174, 176, 177, 178, 179, 180], "common": [1, 2, 4, 7, 134, 174, 175, 176, 179, 180], "json": [1, 6, 24, 25, 81, 112, 133, 136, 174, 176, 177], "messag": [1, 15, 16, 19, 21, 24, 25, 27, 28, 36, 54, 64, 77, 95, 107, 112, 134, 171, 174, 175, 176], "miscellan": 1, "function": [1, 7, 8, 10, 12, 28, 114, 115, 121, 123, 126, 129, 130, 135, 136, 143, 144, 150, 154, 160, 164, 172, 175, 176, 181], "us": [1, 2, 4, 6, 9, 10, 12, 16, 19, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 64, 65, 70, 78, 81, 82, 87, 90, 108, 112, 114, 115, 117, 118, 119, 120, 121, 123, 126, 129, 131, 132, 135, 136, 137, 139, 140, 141, 143, 144, 145, 146, 148, 154, 155, 156, 157, 158, 164, 170, 171, 172, 174, 176, 178, 179, 180], "modifi": [1, 7, 8, 9, 121, 172, 177, 179, 180, 181], "For": [2, 5, 6, 7, 8, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 43, 44, 55, 59, 64, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 119, 136, 141, 142, 149, 158, 162, 164, 171, 174, 175, 176, 177, 178, 179, 180, 181], "detail": [2, 6, 34, 36, 41, 64, 102, 112, 116, 135, 146, 154, 164, 174, 177, 178, 179, 180, 181], "usag": [2, 121, 139, 140, 165, 171, 174, 176, 177, 178, 179, 181], "guid": [2, 7, 9, 172, 175, 176, 178, 180], "pleas": [2, 5, 51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 135, 146, 154, 162, 171, 181], "see": [2, 5, 6, 9, 19, 21, 34, 36, 41, 44, 51, 52, 53, 62, 63, 64, 74, 75, 76, 85, 86, 93, 94, 102, 105, 106, 112, 113, 116, 124, 135, 139, 141, 146, 147, 154, 158, 162, 164, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "our": [2, 6, 8, 172, 175, 176, 177, 178, 180, 181], "tutori": [2, 6, 64, 162, 172, 175, 176, 177, 178, 179, 180, 181], "support": [2, 6, 8, 9, 10, 21, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 59, 70, 82, 90, 96, 98, 108, 111, 112, 114, 125, 137, 138, 140, 145, 148, 149, 172, 174, 175, 176, 177, 178, 179, 180, 181], "sever": 2, "wide": 2, "help": [2, 6, 19, 119, 136, 141, 170, 171, 172, 174, 175, 176, 177, 178, 179, 181], "quickli": [2, 7, 33, 175, 176], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 22, 23, 28, 33, 64, 157, 158, 170, 171, 172, 174, 175, 176, 179, 180, 181], "fine": [2, 6, 8, 9, 31, 64, 170, 172, 177, 180], "tune": [2, 3, 6, 7, 8, 9, 12, 31, 64, 170, 171, 172, 174, 177, 180, 181], "also": [2, 6, 7, 8, 9, 10, 29, 36, 39, 43, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 112, 114, 119, 125, 144, 146, 148, 154, 158, 171, 174, 175, 176, 177, 178, 179, 180, 181], "like": [2, 6, 7, 8, 9, 28, 112, 138, 171, 174, 175, 176, 177, 178, 180], "These": [2, 4, 6, 7, 8, 10, 31, 141, 175, 176, 177, 178, 179, 180, 181], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 23, 27, 30, 31, 32, 35, 36, 38, 39, 40, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 119, 125, 126, 129, 130, 135, 136, 137, 139, 140, 142, 143, 145, 148, 152, 154, 160, 165, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "especi": [2, 172, 174, 177], "specifi": [2, 6, 7, 8, 10, 36, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 119, 120, 123, 135, 143, 146, 149, 154, 158, 162, 165, 174, 175, 176, 177, 178, 179, 181], "yaml": [2, 7, 8, 10, 11, 12, 29, 36, 39, 43, 141, 158, 172, 174, 175, 176, 177, 178, 179, 180, 181], "config": [2, 6, 9, 10, 11, 12, 13, 29, 36, 39, 43, 114, 129, 136, 140, 141, 158, 165, 172, 175, 176, 177, 179, 180, 181], "represent": [2, 180, 181], "abov": [2, 3, 6, 121, 152, 171, 177, 179, 180, 181], "all": [3, 4, 8, 13, 28, 29, 31, 36, 81, 112, 114, 115, 119, 121, 126, 136, 140, 141, 142, 152, 161, 167, 168, 170, 172, 173, 174, 175, 176, 177, 178, 179, 180], "famili": [3, 8, 35, 37, 41, 42, 44, 139, 172, 174, 179], "To": [3, 6, 7, 8, 9, 31, 136, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "download": [3, 6, 168, 171, 175, 176, 179, 180, 181], "8b": [3, 80, 84, 86, 89, 92, 94, 109, 174, 175], "meta": [3, 6, 19, 64, 77, 118, 136, 137, 174, 175, 177, 178], "hf": [3, 6, 107, 123, 136, 174, 175, 177, 178, 179], "token": [3, 6, 7, 8, 20, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 55, 58, 59, 64, 65, 69, 70, 77, 78, 81, 82, 87, 90, 95, 96, 98, 100, 102, 104, 107, 108, 110, 112, 114, 118, 119, 120, 131, 132, 133, 134, 143, 146, 159, 174, 176, 177, 178, 179, 180, 181], "hf_token": 3, "70b": [3, 46, 49, 52, 67, 72, 75, 79, 83, 85, 88, 91, 93, 179], "ignor": [3, 6, 107, 112, 114, 115, 174], "pattern": [3, 132, 174], "origin": [3, 6, 34, 35, 121, 125, 175, 177, 179, 180, 181], "consolid": [3, 6, 179], "weight": [3, 6, 8, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 114, 121, 124, 125, 129, 131, 136, 137, 138, 139, 149, 154, 158, 170, 174, 175, 177, 178, 179, 180, 181], "you": [3, 6, 7, 8, 9, 10, 18, 19, 23, 28, 30, 32, 33, 35, 37, 39, 43, 44, 139, 141, 143, 157, 158, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 28, 29, 30, 32, 33, 35, 36, 37, 39, 43, 44, 64, 95, 117, 118, 126, 131, 132, 135, 136, 139, 141, 146, 154, 157, 158, 162, 165, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "instead": [3, 6, 8, 31, 36, 39, 43, 115, 116, 125, 174, 177, 179, 180], "The": [3, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 28, 29, 30, 31, 32, 38, 40, 41, 42, 48, 49, 50, 54, 59, 60, 61, 64, 70, 71, 72, 73, 77, 82, 83, 84, 90, 91, 92, 95, 96, 98, 107, 108, 109, 117, 118, 121, 122, 123, 126, 131, 132, 133, 134, 135, 136, 138, 141, 144, 145, 147, 149, 158, 163, 165, 166, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "reus": [3, 172], "llama3_token": [3, 143, 175, 179], "builder": [3, 6, 34, 37, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 57, 60, 61, 62, 63, 66, 67, 68, 71, 72, 73, 74, 75, 76, 79, 80, 83, 84, 85, 86, 88, 89, 91, 92, 93, 94, 97, 99, 101, 103, 105, 106, 109, 111, 113, 175, 176, 181], "class": [3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 36, 39, 54, 64, 77, 95, 102, 107, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 127, 128, 131, 132, 136, 137, 138, 139, 140, 141, 155, 156, 157, 158, 175, 176, 178, 180, 181], "7b": [3, 6, 30, 32, 33, 35, 37, 39, 43, 44, 47, 50, 53, 57, 61, 68, 73, 76, 97, 99, 101, 103, 136, 137, 175, 178, 179, 180, 181], "2": [3, 6, 9, 27, 31, 41, 54, 64, 77, 95, 107, 114, 131, 132, 134, 136, 137, 149, 159, 160, 163, 164, 165, 166, 175, 177, 178, 179, 180], "13b": [3, 6, 45, 48, 51, 66, 71, 74], "codellama": 3, "mini": [3, 107, 109, 110, 111, 112, 113], "4k": [3, 110, 111, 112], "microsoft": [3, 111, 112], "ai": [3, 101, 114, 158, 175, 179], "v0": 3, "mistralai": [3, 174], "size": [3, 6, 8, 10, 35, 38, 40, 114, 116, 117, 118, 119, 150, 152, 172, 174, 176, 177, 178, 179, 180], "2b": [3, 56, 60], "googl": [3, 56, 57], "gguf": 3, "perform": [4, 6, 31, 64, 112, 115, 126, 143, 172, 175, 177, 179, 181], "direct": [4, 8, 123, 160, 171], "encod": [4, 54, 64, 77, 95, 107, 123, 131, 132, 134, 175], "text": [4, 28, 31, 33, 36, 37, 43, 44, 64, 77, 95, 107, 131, 132, 175, 177], "id": [4, 6, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 64, 77, 95, 107, 114, 118, 119, 120, 131, 132, 133, 136, 138, 143, 159, 160, 175, 176, 177], "decod": [4, 55, 59, 65, 70, 77, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 119, 131, 132, 143, 175], "typic": [4, 7, 31, 33, 43, 112, 123, 176, 181], "byte": [4, 132, 181], "pair": [4, 7, 14, 42, 132, 159, 160, 176], "underli": [4, 95, 131, 181], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 121, 124, 127, 129, 140, 141, 149, 171, 172, 176, 177, 179, 180, 181], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 25, 26, 28, 30, 32, 33, 36, 37, 39, 43, 44, 64, 95, 121, 127, 128, 129, 130, 131, 134, 136, 137, 138, 140, 143, 151, 154, 164, 167, 174, 175, 176, 177, 178, 179, 180], "offer": 5, "allow": [5, 29, 129, 157, 174, 181], "seamless": 5, "transit": 5, "between": [5, 6, 136, 139, 176, 177, 179, 180, 181], "train": [5, 6, 8, 9, 19, 28, 29, 30, 31, 35, 36, 38, 39, 40, 41, 43, 64, 114, 118, 119, 120, 121, 122, 136, 137, 138, 145, 148, 154, 165, 170, 172, 174, 175, 176, 177, 179, 180, 181], "interoper": [5, 6, 8, 172, 177, 181], "rest": [5, 175, 181], "ecosystem": [5, 6, 8, 172, 177, 179, 181], "comprehens": 5, "overview": [5, 7, 9, 170, 178, 180, 181], "deep": [5, 6, 7, 8, 9, 172, 178, 179], "dive": [5, 6, 7, 8, 9, 172, 178, 179], "enabl": [5, 7, 8, 9, 29, 48, 49, 50, 51, 52, 53, 60, 61, 62, 63, 71, 72, 73, 74, 75, 76, 83, 84, 85, 86, 91, 92, 93, 94, 97, 99, 105, 106, 109, 113, 125, 164, 165, 179, 180, 181], "work": [5, 6, 8, 141, 172, 174, 177, 179, 181], "set": [5, 6, 7, 8, 9, 30, 31, 32, 33, 35, 37, 38, 39, 40, 41, 43, 44, 77, 87, 90, 107, 118, 119, 126, 128, 135, 144, 146, 152, 154, 162, 163, 164, 165, 172, 174, 175, 177, 178, 179, 180], "consumpt": [5, 29], "dure": [5, 6, 29, 30, 31, 35, 38, 40, 114, 116, 118, 119, 120, 121, 148, 175, 177, 179, 180, 181], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 26, 28, 29, 30, 31, 32, 41, 119, 126, 138, 141, 144, 146, 158, 165, 172, 174, 175, 176, 177, 178, 179], "debug": [5, 6, 7, 8, 174], "finetun": [5, 6, 7, 8, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 170, 172, 178, 179], "job": [5, 9, 164, 178], "variou": [5, 18], "walk": [6, 8, 157, 172, 175, 176, 177, 178, 181], "through": [6, 7, 8, 9, 115, 126, 172, 174, 175, 176, 177, 178, 181], "design": [6, 8], "behavior": [6, 154, 175, 176], "associ": [6, 7, 8, 55, 65, 78, 87, 100, 143, 177, 180], "util": [6, 7, 8, 9, 10, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 172, 177, 178, 179, 181], "what": [6, 7, 9, 19, 21, 22, 33, 38, 40, 170, 175, 176, 177, 178, 179], "cover": [6, 7, 8, 9, 175, 177, 181], "how": [6, 7, 8, 9, 23, 135, 162, 170, 174, 175, 176, 177, 178, 179, 181], "we": [6, 7, 8, 9, 30, 31, 32, 33, 35, 37, 39, 43, 44, 64, 95, 114, 116, 118, 119, 125, 136, 137, 138, 143, 145, 149, 154, 161, 172, 174, 175, 176, 177, 178, 179, 180, 181], "them": [6, 7, 28, 29, 30, 32, 39, 54, 64, 95, 107, 115, 121, 134, 174, 175, 176, 177, 180, 181], "scenario": [6, 29], "full": [6, 7, 8, 36, 39, 51, 52, 53, 54, 62, 63, 64, 74, 75, 76, 85, 86, 93, 94, 95, 105, 106, 107, 113, 129, 130, 134, 172, 174, 176, 179, 180], "compos": 6, "compon": [6, 8, 13, 160, 172, 176, 178, 180, 181], "which": [6, 7, 8, 29, 30, 31, 33, 35, 38, 40, 48, 49, 50, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 90, 91, 92, 95, 96, 97, 98, 99, 100, 102, 108, 109, 110, 114, 118, 119, 120, 122, 129, 130, 131, 136, 137, 138, 140, 145, 155, 158, 162, 172, 174, 175, 176, 177, 178, 179, 180, 181], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 115, 129, 136, 137, 138, 172, 175, 176, 177, 179, 181], "evalu": [6, 8, 170, 172, 178, 180, 181], "gener": [6, 8, 14, 17, 22, 23, 28, 30, 31, 32, 37, 41, 64, 95, 126, 163, 164, 165, 168, 170, 175, 176, 180, 181], "each": [6, 8, 15, 18, 29, 31, 48, 49, 50, 54, 59, 60, 61, 64, 70, 71, 72, 73, 77, 82, 83, 84, 90, 91, 92, 95, 96, 97, 98, 99, 107, 108, 109, 114, 118, 119, 120, 123, 129, 130, 134, 160, 164, 165, 172, 174, 176, 177, 178, 179, 180], "make": [6, 7, 8, 9, 114, 120, 172, 174, 175, 177, 178, 179, 180, 181], "easi": [6, 8, 172, 176, 180], "understand": [6, 7, 8, 170, 172, 175, 176, 180, 181], "extend": [6, 8, 172], "befor": [6, 27, 30, 31, 32, 55, 59, 114, 119, 120, 125, 132, 136, 174, 177], "let": [6, 7, 9, 174, 175, 176, 177, 178, 179, 180, 181], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 24, 25, 27, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 49, 50, 54, 64, 70, 71, 72, 73, 77, 82, 83, 84, 90, 91, 92, 95, 96, 97, 98, 99, 107, 108, 109, 112, 114, 116, 118, 119, 120, 121, 123, 124, 127, 129, 130, 135, 136, 137, 140, 144, 146, 148, 154, 157, 162, 163, 172, 174, 175, 176, 178, 180, 181], "defin": [6, 7, 8, 115, 124, 125, 127, 176, 178, 180], "concept": [6, 177, 178], "In": [6, 7, 8, 28, 118, 125, 135, 154, 157, 158, 175, 177, 179, 180, 181], "ll": [6, 7, 8, 143, 149, 172, 175, 176, 177, 178, 179, 181], "talk": 6, "about": [6, 8, 123, 158, 172, 174, 175, 177, 178, 179, 180, 181], "take": [6, 7, 8, 10, 115, 116, 121, 136, 138, 141, 144, 160, 175, 176, 177, 178, 179, 180, 181], "close": [6, 8, 155, 156, 157, 158, 180], "look": [6, 7, 8, 142, 157, 171, 175, 176, 177, 178, 179, 180], "veri": [6, 29, 119, 174, 177], "simpli": [6, 7, 31, 174, 175, 176, 177, 179, 181], "dictat": 6, "state_dict": [6, 121, 129, 136, 137, 138, 139, 140, 180, 181], "store": [6, 29, 155, 158, 180, 181], "file": [6, 7, 8, 9, 10, 11, 12, 54, 64, 77, 81, 95, 107, 112, 131, 132, 133, 136, 137, 138, 141, 155, 158, 165, 169, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181], "disk": [6, 33, 155], "string": [6, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 64, 77, 95, 107, 124, 131, 132, 134, 144, 145, 149, 174, 176], "kei": [6, 7, 9, 14, 17, 22, 23, 24, 28, 30, 32, 39, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 119, 120, 128, 129, 130, 136, 138, 140, 160, 165, 174, 177, 178, 180, 181], "identifi": 6, "state": [6, 8, 121, 127, 128, 129, 130, 136, 137, 138, 140, 142, 177, 179, 180, 181], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 36, 37, 39, 43, 44, 77, 107, 121, 127, 128, 129, 130, 132, 133, 136, 137, 138, 140, 142, 148, 151, 153, 159, 160, 161, 176], "If": [6, 7, 13, 14, 17, 18, 21, 22, 23, 24, 26, 27, 28, 30, 32, 35, 38, 39, 40, 41, 65, 70, 77, 78, 82, 87, 90, 96, 98, 100, 102, 107, 108, 110, 114, 118, 119, 120, 121, 123, 125, 130, 136, 137, 138, 139, 140, 143, 144, 145, 146, 148, 149, 151, 157, 158, 164, 165, 167, 171, 174, 175, 176, 177, 178, 179, 180], "don": [6, 7, 8, 158, 164, 174, 175, 176, 177, 178, 179, 181], "t": [6, 7, 8, 145, 158, 164, 174, 175, 176, 177, 178, 179, 181], "match": [6, 28, 30, 32, 39, 107, 130, 171, 174, 176, 177, 179, 180], "up": [6, 8, 9, 30, 31, 32, 33, 35, 37, 39, 43, 44, 132, 142, 165, 174, 175, 176, 178, 179, 180, 181], "exactli": [6, 130], "those": [6, 139, 180], "definit": [6, 180], "either": [6, 130, 136, 143, 162, 174, 180, 181], "run": [6, 7, 9, 12, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 115, 116, 119, 121, 136, 137, 138, 140, 142, 152, 157, 158, 161, 171, 172, 175, 176, 178, 179, 180, 181], "explicit": 6, "error": [6, 7, 27, 136, 164, 174], "load": [6, 8, 28, 29, 30, 31, 32, 33, 129, 136, 137, 138, 140, 141, 157, 175, 176, 177, 179, 180], "rais": [6, 10, 13, 21, 24, 27, 36, 41, 107, 114, 116, 119, 123, 129, 130, 136, 137, 138, 140, 145, 148, 151, 158, 160, 164, 167], "an": [6, 7, 8, 9, 10, 14, 20, 27, 29, 38, 40, 44, 70, 82, 90, 96, 98, 102, 108, 114, 119, 123, 124, 126, 127, 128, 135, 136, 137, 138, 140, 144, 146, 158, 165, 172, 174, 175, 176, 177, 178, 179, 180, 181], "except": [6, 20, 21, 134, 176], "wors": 6, "silent": [6, 115], "succe": 6, "infer": [6, 19, 28, 55, 64, 100, 114, 116, 118, 119, 120, 144, 170, 175, 177, 178, 179, 181], "expect": [6, 7, 10, 14, 17, 18, 22, 23, 28, 30, 32, 36, 39, 118, 130, 140, 158, 167, 175, 176, 180], "addit": [6, 7, 8, 10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 64, 129, 135, 136, 137, 138, 145, 146, 151, 154, 155, 157, 158, 162, 172, 175, 178, 180], "line": [6, 8, 14, 141, 174, 176, 178, 179], "need": [6, 7, 8, 9, 18, 28, 31, 41, 114, 115, 119, 154, 157, 158, 161, 171, 174, 175, 176, 177, 178, 179, 180, 181], "shape": [6, 114, 116, 118, 119, 120, 123, 125, 143, 165], "valu": [6, 7, 25, 41, 45, 46, 47, 55, 56, 57, 59, 65, 66, 67, 68, 70, 78, 79, 80, 82, 87, 88, 89, 90, 96, 98, 100, 101, 102, 103, 108, 110, 114, 116, 117, 119, 120, 122, 129, 136, 139, 140, 141, 143, 155, 156, 157, 158, 160, 164, 174, 176, 178, 179, 180], "two": [6, 7, 27, 172, 177, 178, 179, 180, 181], "popular": [6, 172, 176, 177], "llama2": [6, 7, 8, 10, 19, 28, 30, 32, 33, 35, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 115, 119, 120, 139, 170, 172, 174, 178, 179], "offici": [6, 19, 175, 178, 179], "implement": [6, 8, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 64, 95, 107, 115, 117, 118, 122, 123, 124, 125, 136, 149, 157, 172, 176, 180, 181], "when": [6, 7, 8, 12, 20, 29, 31, 33, 64, 114, 118, 119, 120, 121, 122, 129, 143, 146, 157, 161, 174, 177, 179, 180, 181], "llama": [6, 19, 28, 64, 77, 117, 118, 136, 137, 174, 175, 177, 178, 179, 180], "websit": 6, "get": [6, 7, 8, 9, 28, 64, 95, 145, 147, 148, 150, 171, 172, 175, 176, 177, 178, 180], "access": [6, 7, 8, 29, 136, 142, 174, 177, 178], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 33, 114, 129, 136, 137, 138, 140, 142, 174, 175, 176, 177, 178, 179, 180, 181], "pth": [6, 177], "inspect": [6, 177, 180, 181], "content": [6, 20, 24, 25, 28, 54, 64, 95, 107, 134, 175, 176], "easili": [6, 7, 172, 176, 180, 181], "torch": [6, 7, 116, 119, 121, 122, 123, 138, 140, 142, 143, 144, 145, 148, 149, 151, 152, 159, 160, 161, 162, 163, 164, 165, 166, 167, 177, 178, 179, 180, 181], "import": [6, 7, 10, 36, 39, 43, 157, 158, 175, 176, 177, 178, 180, 181], "00": [6, 169, 173, 178], "mmap": [6, 177], "true": [6, 7, 20, 29, 30, 31, 34, 35, 36, 38, 39, 40, 43, 51, 52, 53, 54, 55, 59, 62, 63, 64, 74, 75, 76, 77, 85, 86, 93, 94, 95, 105, 106, 107, 113, 114, 119, 120, 121, 126, 131, 132, 134, 135, 136, 137, 138, 146, 148, 151, 152, 154, 157, 165, 166, 174, 175, 176, 177, 179, 180, 181], "weights_onli": [6, 138], "map_loc": [6, 177], "cpu": [6, 8, 121, 145, 165, 171, 174, 177, 181], "tensor": [6, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 136, 143, 155, 156, 157, 158, 159, 160, 163, 180, 181], "item": 6, "print": [6, 9, 29, 35, 38, 40, 41, 54, 64, 77, 95, 107, 131, 132, 134, 143, 166, 175, 176, 178, 180, 181], "f": [6, 9, 35, 38, 40, 175, 177, 180, 181], "tok_embed": [6, 119], "32000": [6, 10, 180], "4096": [6, 10, 30, 32, 33, 35, 37, 39, 43, 44, 114, 118, 176, 180], "len": [6, 29, 35, 38, 40, 119], "292": 6, "contain": [6, 20, 24, 29, 31, 33, 43, 54, 64, 77, 81, 95, 107, 112, 114, 116, 118, 119, 120, 124, 127, 128, 129, 132, 134, 136, 137, 138, 140, 141, 142, 148, 153, 157, 159, 160, 165, 175, 177, 179, 180], "input": [6, 14, 15, 17, 18, 22, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 41, 42, 43, 44, 95, 107, 114, 115, 117, 118, 119, 120, 125, 131, 132, 136, 138, 159, 160, 164, 167, 175, 176, 180, 181], "embed": [6, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 117, 118, 119, 146, 175, 179], "tabl": [6, 175, 181], "call": [6, 10, 20, 115, 121, 129, 141, 155, 156, 157, 158, 161, 165, 175, 176, 180, 181], "layer": [6, 8, 48, 49, 50, 51, 52, 53, 55, 59, 60, 61, 62, 63, 65, 70, 71, 72, 73, 74, 75, 76, 78, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 105, 106, 108, 109, 110, 113, 114, 119, 120, 125, 129, 130, 135, 146, 172, 179, 180, 181], "have": [6, 7, 10, 114, 116, 124, 130, 138, 140, 141, 146, 154, 157, 167, 171, 175, 176, 177, 178, 179, 180, 181], "dim": [6, 114, 115, 117, 118, 119], "most": [6, 7, 175, 178, 180, 181], "within": [6, 7, 10, 28, 31, 41, 59, 70, 82, 90, 96, 98, 108, 115, 143, 157, 164, 165, 174, 176, 177, 179, 180, 181], "hug": [6, 16, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 81, 112, 122, 133, 172, 174, 178, 179], "face": [6, 16, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 81, 112, 122, 133, 172, 174, 178, 179], "hub": [6, 174, 176, 178], "default": [6, 7, 16, 20, 24, 25, 26, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 59, 60, 61, 64, 65, 66, 67, 68, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 107, 108, 109, 110, 112, 114, 115, 117, 118, 119, 120, 121, 122, 123, 125, 129, 131, 132, 134, 136, 137, 138, 141, 143, 145, 150, 154, 155, 158, 159, 160, 163, 164, 165, 171, 174, 175, 176, 177, 179, 180, 181], "everi": [6, 8, 115, 157, 165, 171, 174, 181], "repo": [6, 136, 137, 139, 174, 177], "first": [6, 7, 10, 27, 31, 116, 119, 136, 141, 170, 172, 175, 176, 177, 179, 180, 181], "big": [6, 177], "split": [6, 29, 31, 132, 175, 176, 177], "across": [6, 8, 29, 136, 157, 164, 177, 179], "bin": [6, 174, 177], "correctli": [6, 8, 13, 129, 136, 171, 175, 178, 181], "piec": 6, "one": [6, 8, 27, 54, 64, 95, 107, 115, 123, 134, 138, 175, 176, 177, 178, 179, 181], "pytorch_model": [6, 177], "00001": [6, 174], "00002": [6, 174], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 31, 37, 59, 70, 82, 90, 96, 98, 108, 125, 127, 129, 131, 137, 138, 140, 141, 143, 145, 146, 148, 149, 154, 174, 176, 177, 178, 179, 180, 181], "doe": [6, 21, 24, 28, 31, 55, 64, 100, 111, 114, 119, 120, 124, 134, 136, 138, 140, 141, 174, 175, 177], "fewer": [6, 114], "sinc": [6, 7, 10, 115, 136, 138, 175, 177, 179], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 41, 43, 44, 124, 128, 130, 132, 136, 137, 138, 139, 140, 141, 142, 143, 144, 155, 156, 157, 158, 167, 174, 175, 177, 179], "caus": [6, 95, 131], "try": [6, 7, 175, 177, 178, 179, 181], "same": [6, 7, 48, 49, 50, 54, 60, 61, 64, 65, 70, 71, 72, 73, 78, 82, 83, 84, 91, 92, 95, 96, 98, 100, 102, 107, 108, 109, 110, 114, 116, 120, 134, 140, 141, 146, 158, 174, 175, 177, 179, 180, 181], "As": [6, 7, 8, 9, 125, 172, 177, 179, 181], "re": [6, 7, 138, 172, 175, 177, 178, 179, 180], "care": [6, 115, 136, 138, 177, 179, 180], "end": [6, 8, 20, 29, 77, 95, 132, 170, 172, 175, 179, 180], "number": [6, 8, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 119, 122, 136, 137, 138, 143, 150, 164, 165, 174, 178, 180], "just": [6, 14, 172, 174, 175, 176, 178, 179, 180], "save": [6, 8, 9, 121, 136, 137, 138, 140, 146, 154, 158, 170, 174, 175, 176, 177, 179, 180], "less": [6, 41, 177, 178, 179, 181], "prone": 6, "manag": [6, 29, 126, 163, 175], "invari": 6, "accept": [6, 7, 41, 135, 176, 178, 181], "multipl": [6, 7, 8, 20, 28, 29, 114, 119, 120, 125, 155, 156, 157, 158, 160, 165, 178, 179], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 175, 176, 177], "worri": [6, 175, 178], "explicitli": [6, 124, 172, 180], "convert": [6, 24, 25, 28, 136, 159, 175, 177, 181], "time": [6, 54, 55, 64, 95, 100, 107, 134, 155, 157, 165, 174, 175, 176, 177, 179, 181], "produc": [6, 140, 181], "back": [6, 27, 126, 136, 176, 180, 181], "form": [6, 7, 8, 27, 174], "One": [6, 177], "advantag": [6, 180], "being": [6, 136, 137, 138, 142, 144, 181], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 24, 25, 31, 36, 39, 43, 48, 49, 50, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 102, 108, 109, 110, 114, 115, 123, 124, 129, 130, 135, 141, 153, 155, 156, 157, 158, 171, 172, 176, 177, 178, 179, 180, 181], "abl": [6, 8, 177, 178, 179], "post": [6, 161, 165, 181], "tool": [6, 176, 177, 178], "quantiz": [6, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 125, 138, 149, 170, 178, 181], "eval": [6, 170, 172], "without": [6, 7, 9, 14, 129, 171, 172, 175, 177, 180], "code": [6, 8, 45, 46, 47, 48, 49, 50, 51, 52, 53, 119, 168, 172, 176, 178], "chang": [6, 7, 9, 14, 138, 171, 174, 177, 178, 179, 180, 181], "OR": [6, 24], "convers": [6, 15, 16, 19, 21, 24, 25, 27, 28, 36, 41, 136, 138, 139, 172, 175, 176, 177, 179, 180, 181], "script": [6, 9, 174, 177, 178, 179], "wai": [6, 7, 28, 129, 174, 175, 176, 177, 178, 179], "surround": [6, 8, 172], "load_checkpoint": [6, 8, 136, 137, 138, 139], "save_checkpoint": [6, 8, 9, 136, 137, 138], "convertor": 6, "avail": [6, 8, 44, 141, 144, 145, 152, 172, 174, 177, 179, 180], "here": [6, 7, 9, 14, 16, 17, 22, 23, 38, 117, 118, 174, 175, 176, 177, 178, 179, 180, 181], "three": [6, 8, 123, 178], "hfcheckpoint": 6, "read": [6, 136, 137, 138, 172], "write": [6, 8, 14, 136, 137, 138, 155, 175, 176, 178], "compat": [6, 136, 138], "transform": [6, 8, 28, 30, 32, 48, 49, 50, 55, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 102, 108, 109, 110, 119, 120, 122, 162, 180], "framework": [6, 8, 172], "mention": [6, 177, 181], "assum": [6, 14, 17, 18, 22, 23, 30, 32, 39, 114, 118, 119, 120, 122, 127, 132, 140, 142, 145, 154, 175, 177, 180], "checkpoint_dir": [6, 7, 136, 137, 138, 177, 179], "necessari": [6, 41, 155, 156, 157, 158, 175, 180], "easiest": [6, 177, 178], "sure": [6, 7, 175, 177, 178, 179, 180, 181], "everyth": [6, 8, 141, 172, 178], "follow": [6, 8, 24, 25, 28, 31, 114, 122, 138, 139, 140, 152, 158, 165, 170, 171, 174, 176, 177, 178, 179, 180, 181], "flow": [6, 28, 30, 31, 32, 181], "By": [6, 174, 179, 180, 181], "safetensor": [6, 136, 174], "output": [6, 18, 29, 35, 38, 41, 48, 49, 50, 55, 59, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 108, 109, 114, 115, 117, 118, 119, 120, 125, 128, 129, 130, 138, 143, 146, 156, 165, 171, 174, 175, 176, 177, 178, 179, 180, 181], "dir": [6, 158, 171, 174, 177, 178, 179], "output_dir": [6, 7, 136, 137, 138, 165, 177, 179, 180, 181], "argument": [6, 7, 10, 18, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 114, 135, 141, 146, 151, 155, 157, 158, 162, 174, 175, 176, 179, 180], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 119, 165, 174, 176, 177, 180, 181], "_component_": [6, 7, 9, 10, 29, 36, 39, 43, 165, 175, 176, 177, 179, 180], "fullmodelhfcheckpoint": [6, 177], "directori": [6, 7, 136, 137, 138, 155, 157, 158, 165, 174, 175, 177, 178, 179], "sort": [6, 136, 138], "so": [6, 7, 31, 136, 141, 171, 172, 175, 177, 178, 179, 180, 181], "order": [6, 8, 136, 138, 157, 158, 178], "matter": [6, 136, 138, 174, 180], "checkpoint_fil": [6, 7, 9, 136, 137, 138, 177, 179, 180, 181], "restart": [6, 174], "previou": [6, 31, 136, 137, 138], "more": [6, 7, 8, 34, 36, 41, 64, 112, 116, 118, 129, 135, 138, 141, 158, 162, 164, 172, 174, 176, 177, 178, 179, 180, 181], "next": [6, 31, 143, 179, 181], "section": [6, 8, 148, 170, 177, 179, 181], "recipe_checkpoint": [6, 136, 137, 138], "null": [6, 7], "usual": [6, 118, 136, 158, 174, 177, 180], "model_typ": [6, 136, 137, 138, 177, 179], "resume_from_checkpoint": [6, 136, 137, 138], "fals": [6, 7, 20, 24, 25, 28, 29, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 113, 114, 119, 120, 125, 126, 129, 131, 136, 137, 138, 152, 165, 174, 175, 176, 177, 179, 180, 181], "requir": [6, 7, 29, 33, 41, 43, 64, 136, 138, 140, 149, 151, 152, 154, 157, 158, 160, 164, 165, 171, 174, 175, 176, 178, 181], "param": [6, 8, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 125, 127, 128, 130, 136, 154, 180, 181], "directli": [6, 7, 8, 10, 36, 39, 43, 135, 136, 174, 177, 178, 179, 180, 181], "ensur": [6, 7, 13, 27, 41, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 136, 138, 145, 172, 176, 178], "out": [6, 7, 8, 28, 30, 35, 36, 38, 40, 136, 137, 170, 172, 174, 175, 177, 178, 179, 180, 181], "case": [6, 8, 9, 20, 65, 70, 78, 82, 96, 98, 100, 102, 108, 110, 114, 136, 140, 145, 149, 154, 155, 162, 172, 174, 175, 176, 177, 179, 180, 181], "discrep": [6, 136], "along": [6, 179, 180], "found": [6, 7, 9, 117, 118, 174, 180, 181], "metacheckpoint": 6, "github": [6, 10, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 114, 117, 118, 122, 123, 129, 171, 176, 178], "repositori": [6, 19, 177, 178], "fullmodelmetacheckpoint": [6, 179], "torchtunecheckpoint": 6, "current": [6, 31, 55, 59, 70, 82, 90, 96, 98, 100, 108, 111, 112, 114, 116, 118, 119, 120, 137, 138, 146, 149, 150, 155, 157, 161, 164, 177, 178, 179], "test": [6, 7, 8, 172, 175], "complet": [6, 8, 14, 31, 37, 112, 175, 176, 177, 178, 179], "written": [6, 7, 8, 136, 137, 155, 156, 157, 158, 172], "begin": [6, 31, 64, 95, 132, 175, 179, 181], "partit": [6, 136, 181], "ha": [6, 64, 95, 124, 126, 127, 130, 138, 140, 167, 175, 176, 177, 178, 179, 180, 181], "standard": [6, 17, 24, 87, 90, 156, 172, 175, 177, 179], "key_1": [6, 138], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 177], "inform": [6, 20, 158, 162, 172, 174, 177, 178, 179], "subsequ": [6, 8], "recipe_st": [6, 136, 137, 138], "pt": [6, 9, 136, 137, 138, 177, 179], "epoch": [6, 8, 9, 122, 136, 137, 138, 174, 175, 177, 178, 179], "optim": [6, 7, 8, 29, 55, 64, 100, 111, 122, 123, 138, 140, 142, 148, 160, 161, 165, 175, 177, 178, 179, 180, 181], "etc": [6, 8, 136, 148, 178], "prevent": [6, 31, 174], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 59, 64, 95, 119, 124, 140, 161, 164, 175, 176, 177, 180, 181], "updat": [6, 7, 8, 116, 140, 165, 171, 175, 177, 178, 179, 180, 181], "hf_model_0001_0": [6, 177], "hf_model_0002_0": [6, 177], "both": [6, 29, 130, 174, 177, 180, 181], "adapt": [6, 124, 125, 126, 127, 128, 136, 137, 138, 175, 177, 180, 181], "merg": [6, 10, 11, 136, 177, 179, 181], "would": [6, 7, 9, 31, 119, 171, 175, 176, 177, 180, 181], "primari": [6, 7, 8, 178], "want": [6, 7, 8, 9, 10, 28, 143, 171, 174, 175, 176, 177, 178, 179, 180], "resum": [6, 8, 122, 136, 137, 138, 181], "initi": [6, 8, 12, 29, 31, 45, 46, 47, 56, 57, 66, 67, 68, 79, 80, 88, 89, 101, 103, 140, 151, 152, 178, 180, 181], "frozen": [6, 180, 181], "base": [6, 10, 30, 32, 41, 48, 49, 50, 51, 52, 53, 55, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 105, 106, 108, 109, 110, 113, 118, 122, 123, 125, 126, 128, 129, 130, 136, 141, 144, 146, 154, 155, 170, 175, 177, 178, 179, 180, 181], "well": [6, 7, 8, 172, 174, 176, 177, 179, 181], "learnt": [6, 175, 177], "someth": [6, 8, 9, 175, 177], "NOT": [6, 55, 100], "refer": [6, 7, 8, 117, 118, 123, 126, 172, 180], "adapter_checkpoint": [6, 136, 137, 138], "adapter_0": [6, 177], "now": [6, 140, 142, 175, 176, 177, 178, 179, 180, 181], "knowledg": 6, "creat": [6, 7, 10, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 57, 60, 61, 62, 63, 66, 67, 68, 71, 72, 73, 74, 75, 76, 79, 80, 83, 84, 85, 86, 88, 89, 91, 92, 93, 94, 97, 99, 101, 103, 105, 106, 109, 111, 113, 116, 122, 135, 136, 137, 138, 142, 155, 157, 174, 175, 176, 177, 179, 181], "simpl": [6, 8, 14, 17, 22, 23, 170, 176, 178, 180, 181], "forward": [6, 8, 114, 115, 117, 118, 119, 120, 123, 125, 148, 165, 179, 180, 181], "modeltyp": [6, 136, 137, 138], "llama2_13b": [6, 71], "right": [6, 136, 177, 179, 180], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 129, 140, 180], "successfulli": [6, 174, 178], "vocab": [6, 10, 119, 179], "70": [6, 79], "x": [6, 114, 115, 117, 118, 119, 120, 125, 143, 163, 180, 181], "randint": 6, "0": [6, 8, 31, 48, 49, 50, 51, 52, 53, 54, 55, 59, 64, 65, 70, 71, 72, 73, 74, 75, 76, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 114, 119, 122, 123, 125, 134, 143, 149, 157, 158, 159, 160, 164, 166, 169, 173, 175, 176, 177, 178, 179, 180, 181], "1": [6, 8, 31, 41, 54, 64, 77, 87, 88, 89, 90, 91, 92, 93, 94, 95, 107, 114, 119, 122, 123, 131, 132, 134, 137, 139, 143, 152, 157, 158, 159, 160, 163, 164, 174, 175, 177, 178, 179, 180, 181], "no_grad": 6, "6": [6, 31, 55, 59, 117, 159, 160, 177, 181], "3989": 6, "9": [6, 160, 177, 181], "0531": 6, "3": [6, 31, 77, 109, 111, 112, 139, 141, 147, 149, 159, 160, 163, 174, 175, 177, 178, 179, 181], "2375": 6, "5": [6, 7, 14, 122, 123, 159, 160, 177, 178, 179], "2822": 6, "4": [6, 7, 41, 114, 149, 159, 160, 166, 172, 174, 176, 177, 179, 180, 181], "4872": 6, "7469": 6, "8": [6, 35, 38, 40, 48, 49, 50, 51, 52, 53, 60, 61, 62, 63, 71, 72, 73, 74, 75, 76, 83, 84, 85, 86, 91, 92, 93, 94, 97, 99, 105, 106, 109, 113, 160, 177, 180, 181], "6737": 6, "11": [6, 160, 177, 179, 181], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 159, 160], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 28, 30, 32, 39, 41, 129, 134, 158, 174, 175, 176, 177, 178, 179, 180], "find": [6, 8, 9, 174, 177, 178, 180], "list": [6, 7, 15, 16, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 113, 124, 125, 129, 130, 131, 132, 134, 136, 137, 138, 141, 143, 147, 159, 160, 175, 176, 178, 179], "hope": 6, "deeper": [6, 178], "insight": [6, 177], "happi": [6, 177], "thi": [7, 8, 9, 10, 17, 20, 23, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 41, 43, 44, 55, 59, 64, 65, 70, 77, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 111, 112, 114, 115, 118, 119, 120, 121, 122, 124, 126, 129, 130, 131, 132, 134, 135, 136, 137, 138, 140, 141, 143, 144, 145, 148, 152, 154, 155, 157, 158, 160, 161, 162, 164, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181], "pars": [7, 10, 11, 133, 141, 175, 178], "effect": 7, "cli": [7, 9, 11, 12, 171, 177, 178], "prerequisit": [7, 175, 176, 177, 178, 179, 180, 181], "Be": [7, 175, 177, 178, 179, 180, 181], "familiar": [7, 175, 177, 178, 179, 180, 181], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 171, 175, 176, 178], "instal": [7, 9, 152, 157, 158, 170, 174, 177, 178, 179, 180, 181], "fundament": 7, "There": [7, 15, 27, 175, 177, 178, 179, 180], "entri": [7, 8, 178], "point": [7, 8, 24, 25, 134, 176, 177, 178, 179, 180, 181], "locat": [7, 174, 179, 180, 181], "thei": [7, 8, 20, 29, 119, 130, 141, 146, 174, 175, 176, 180], "truth": [7, 177, 179], "reproduc": 7, "overridden": [7, 115, 141, 165], "quick": [7, 29], "experiment": 7, "serv": [7, 134, 135, 176, 180], "particular": [7, 28, 29, 41, 135, 176, 180, 181], "seed": [7, 8, 9, 164, 178], "shuffl": [7, 31], "devic": [7, 8, 129, 140, 144, 145, 148, 174, 175, 177, 178, 179, 180], "cuda": [7, 144, 145, 148, 165, 171, 177, 181], "dtype": [7, 8, 116, 119, 121, 145, 163, 167, 177, 181], "fp32": [7, 181], "enable_fsdp": 7, "mani": [7, 31, 176, 177], "object": [7, 10, 11, 15, 16, 19, 21, 114, 135, 149, 175], "keyword": [7, 10, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 121, 175, 176], "loss": [7, 8, 30, 35, 38, 40, 123, 178, 180, 181], "exampl": [7, 8, 9, 10, 12, 14, 17, 22, 23, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 43, 44, 54, 64, 77, 95, 107, 114, 123, 124, 126, 131, 132, 134, 135, 136, 137, 139, 140, 143, 149, 157, 158, 159, 160, 163, 166, 168, 169, 171, 173, 174, 175, 176, 177, 179, 180, 181], "subfield": 7, "dotpath": 7, "wish": [7, 176], "exact": [7, 10, 177], "path": [7, 8, 9, 10, 28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 58, 64, 69, 77, 81, 95, 104, 107, 112, 131, 132, 133, 136, 137, 138, 141, 165, 174, 175, 176, 177, 179, 180], "normal": [7, 28, 31, 64, 95, 117, 119, 120, 131, 175, 176, 180, 181], "python": [7, 141, 147, 158, 164, 168, 174, 177], "alpaca_dataset": [7, 34, 176], "custom": [7, 8, 28, 30, 32, 36, 39, 43, 162, 172, 174, 177, 178, 179, 180], "train_on_input": [7, 24, 25, 28, 29, 30, 34, 35, 36, 38, 39, 40, 41, 175, 176], "onc": [7, 126, 177, 178, 179, 180, 181], "ve": [7, 116, 174, 175, 176, 177, 179, 180], "instanc": [7, 10, 29, 70, 82, 90, 96, 98, 108, 115, 121, 127, 128, 180], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 36, 174, 177, 181], "under": [7, 165, 176, 177, 179, 181], "preced": [7, 10, 174, 179, 180], "actual": [7, 9, 14, 17, 22, 23, 28, 175], "throw": 7, "notic": [7, 175, 176, 180], "miss": [7, 129, 130, 165, 180], "posit": [7, 10, 31, 55, 59, 96, 98, 100, 102, 108, 110, 114, 116, 118, 119, 120, 179], "anoth": [7, 177], "handl": [7, 12, 20, 29, 64, 95, 131, 132, 175, 177, 180, 181], "def": [7, 8, 9, 12, 135, 139, 175, 176, 180, 181], "dictconfig": [7, 8, 10, 11, 12, 13, 158, 165], "arg": [7, 10, 119, 121, 124, 141, 156, 165], "tupl": [7, 10, 29, 41, 54, 64, 77, 95, 107, 116, 121, 123, 134, 135, 141, 150, 159, 160, 165, 167], "kwarg": [7, 10, 121, 124, 141, 151, 155, 156, 157, 158, 162, 165, 176], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 58, 64, 69, 77, 81, 95, 104, 107, 112, 121, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 140, 141, 144, 145, 147, 148, 149, 151, 153, 155, 156, 157, 158, 159, 160, 164, 165, 166, 167, 175, 176], "mean": [7, 114, 117, 119, 120, 154, 174, 175, 176, 178, 180], "pass": [7, 10, 28, 29, 30, 32, 33, 36, 37, 39, 43, 44, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 115, 121, 126, 130, 132, 135, 138, 145, 146, 148, 151, 154, 157, 158, 162, 165, 174, 175, 176, 180, 181], "add": [7, 9, 28, 31, 64, 134, 138, 139, 141, 176, 177, 179, 180, 181], "d": [7, 20, 114, 116, 119, 174, 175, 180], "llama2_token": [7, 175, 177], "tmp": [7, 140, 175, 178, 179], "llama2token": [7, 69], "option": [7, 8, 14, 17, 18, 22, 23, 26, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 48, 49, 50, 54, 59, 60, 61, 64, 65, 70, 71, 72, 73, 77, 78, 81, 82, 83, 84, 87, 90, 91, 92, 95, 96, 97, 98, 99, 107, 108, 109, 112, 114, 118, 119, 120, 121, 129, 130, 131, 134, 136, 137, 138, 143, 144, 145, 147, 149, 155, 158, 164, 165, 171, 172, 174, 176, 177], "modeltoken": [7, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 134, 175, 176], "bool": [7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 113, 119, 121, 125, 129, 130, 131, 132, 134, 135, 136, 137, 138, 146, 148, 151, 152, 154, 157, 162, 165, 166, 175, 181], "max_seq_len": [7, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 55, 59, 64, 65, 70, 77, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 114, 116, 118, 119, 134, 175, 176], "int": [7, 9, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 105, 106, 107, 108, 109, 110, 113, 114, 116, 117, 118, 119, 122, 125, 131, 132, 133, 134, 135, 136, 137, 138, 143, 146, 150, 154, 155, 156, 157, 158, 159, 160, 162, 164, 165, 174, 175, 176, 180, 181], "512": [7, 34, 35, 176, 181], "instructdataset": [7, 34, 35, 38, 39, 40, 176], "alreadi": [7, 139, 151, 154, 171, 174, 176, 177, 180], "overwrit": [7, 138, 171, 174], "duplic": [7, 8, 172, 174], "sometim": 7, "than": [7, 27, 41, 114, 116, 135, 138, 139, 166, 167, 175, 176, 177, 178, 179, 180, 181], "resolv": [7, 11, 178], "alpaca": [7, 14, 29, 34, 35, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 176], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 155, 156, 157, 158], "disklogg": 7, "log_dir": [7, 155, 157, 158], "conveni": [7, 8, 174], "verifi": [7, 144, 145, 146, 175, 178, 180], "properli": [7, 129, 152, 174], "experi": [7, 158, 170, 172, 175, 179, 180], "wa": [7, 129, 175, 177, 179, 180, 181], "cp": [7, 171, 174, 175, 177, 178, 179], "7b_lora_single_devic": [7, 177, 178, 180, 181], "my_config": [7, 174], "discuss": [7, 112, 178, 180], "guidelin": 7, "while": [7, 8, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 115, 172, 177, 181], "mai": [7, 9, 146, 175, 176, 178, 180], "tempt": 7, "put": [7, 8, 178, 180], "much": [7, 177, 179, 180, 181], "give": [7, 176, 180], "maximum": [7, 26, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 55, 59, 65, 70, 77, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 118, 119, 174], "flexibl": [7, 29, 176], "switch": 7, "encourag": [7, 64, 180], "clariti": 7, "significantli": 7, "easier": [7, 177, 178], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 138, 175, 178], "parent": [7, 174], "modul": [7, 10, 98, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 139, 142, 146, 154, 161, 162, 164, 178, 180, 181], "__init__": [7, 8, 180, 181], "py": [7, 10, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 114, 116, 117, 118, 122, 123, 174, 177, 179], "guarante": 7, "stabil": [7, 172, 181], "underscor": 7, "_alpaca": 7, "collect": [7, 143, 178], "itself": 7, "via": [7, 9, 36, 39, 43, 125, 136, 180, 181], "k1": [7, 8], "v1": [7, 8, 44], "k2": [7, 8], "v2": [7, 8, 176], "lora_finetune_single_devic": [7, 174, 175, 177, 178, 179, 180, 181], "checkpoint": [7, 8, 121, 132, 136, 137, 138, 139, 140, 158, 162, 172, 174, 179, 180, 181], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "assign": [7, 33], "nest": 7, "dot": 7, "notat": [7, 114, 118, 119], "certain": [7, 165, 175], "flag": [7, 8, 30, 35, 38, 40, 135, 138, 146, 174, 181], "built": [7, 9, 42, 171, 175, 178, 181], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 28], "pytorch": [7, 8, 64, 119, 121, 129, 135, 152, 157, 162, 164, 165, 170, 171, 172, 179, 180, 181], "llama3": [7, 28, 41, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 108, 139, 143, 146, 170, 174, 176], "8b_full": [7, 174, 176], "adamw": [7, 180], "lr": [7, 122], "2e": 7, "fuse": [7, 161], "nproc_per_nod": [7, 176, 179, 180], "full_finetune_distribut": [7, 174, 176, 177, 178], "core": [8, 172, 176, 178, 181], "i": [8, 19, 21, 114, 119, 120, 121, 128, 140, 143, 176, 177, 179, 181], "structur": [8, 15, 16, 19, 21, 24, 25, 28, 36, 81, 112, 175, 176, 177], "new": [8, 37, 101, 116, 139, 155, 157, 175, 177, 178, 179, 180, 181], "user": [8, 15, 16, 19, 20, 21, 24, 25, 27, 28, 54, 64, 65, 70, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 114, 134, 175, 176, 178], "thought": [8, 172, 178, 181], "target": [8, 172], "pipelin": [8, 172], "llm": [8, 170, 172, 176, 177, 180], "eg": [8, 119, 136, 172], "meaning": [8, 172, 177], "featur": [8, 9, 171, 172, 177, 178], "fsdp": [8, 135, 140, 146, 154, 172, 178, 179], "activ": [8, 115, 148, 153, 162, 165, 172, 181], "gradient": [8, 154, 161, 165, 172, 177, 179, 180, 181], "accumul": [8, 161, 165, 172], "mix": [8, 174, 176, 177], "precis": [8, 121, 145, 172, 178, 181], "appli": [8, 28, 30, 32, 48, 49, 50, 51, 52, 53, 55, 59, 60, 61, 62, 63, 64, 65, 70, 71, 72, 73, 74, 75, 76, 78, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 105, 106, 108, 109, 113, 114, 117, 118, 119, 120, 129, 130, 162, 172, 181], "given": [8, 10, 14, 17, 18, 22, 23, 27, 125, 126, 143, 144, 145, 149, 154, 161, 166, 172, 180], "complex": 8, "becom": [8, 171, 176], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 119, 139, 174, 176], "methodolog": 8, "reason": [8, 143, 177], "possibl": [8, 28, 31, 36, 174, 176], "trade": 8, "off": [8, 64, 95, 177], "memori": [8, 29, 30, 31, 32, 33, 35, 37, 39, 43, 44, 121, 129, 146, 148, 153, 154, 165, 170, 172, 177, 178, 179], "vs": [8, 178], "qualiti": [8, 177, 180], "believ": 8, "best": [8, 175], "suit": [8, 178], "b": [8, 114, 116, 118, 119, 120, 125, 154, 158, 180, 181], "fit": [8, 28, 30, 31, 32, 33, 35, 37, 39, 43, 44, 176], "solut": 8, "result": [8, 54, 64, 95, 107, 134, 165, 177, 179, 180, 181], "meant": [8, 121, 140], "depend": [8, 9, 14, 136, 165, 174, 176, 177, 180, 181], "level": [8, 142, 147, 154, 172, 181], "expertis": 8, "routin": 8, "yourself": [8, 174, 179, 180], "exist": [8, 171, 174, 177, 178, 179, 181], "ad": [8, 95, 102, 131, 138, 139, 175, 180, 181], "ones": 8, "modular": [8, 172], "build": [8, 36, 39, 43, 55, 65, 78, 87, 100, 102, 172, 179, 180], "block": [8, 31, 48, 49, 50, 55, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 108, 109, 129, 130, 172], "wandb": [8, 9, 158, 178], "log": [8, 11, 123, 147, 148, 153, 155, 156, 157, 158, 177, 178, 179, 181], "fulli": [8, 29], "nativ": [8, 170, 172, 180, 181], "correct": [8, 17, 38, 117, 118, 119, 144, 172, 175, 176], "numer": [8, 172], "pariti": [8, 172], "verif": 8, "extens": [8, 138, 172], "comparison": [8, 180, 181], "benchmark": [8, 164, 172, 177, 179, 180], "limit": [8, 140, 176], "hidden": [8, 115], "behind": 8, "100": [8, 30, 35, 38, 40, 41, 143, 159, 160, 180, 181], "prefer": [8, 22, 42, 123, 160, 172, 174, 176], "over": [8, 122, 141, 172, 174, 177, 179, 180, 181], "unnecessari": 8, "abstract": [8, 15, 18, 172, 178, 181], "No": [8, 138, 172], "inherit": [8, 141, 172, 176], "go": [8, 19, 21, 54, 64, 95, 107, 134, 172, 176, 177, 178, 181], "upon": [8, 29, 179], "figur": [8, 180, 181], "spectrum": 8, "decid": 8, "interact": [8, 170, 178], "start": [8, 9, 29, 134, 139, 171, 172, 175, 176, 177, 178], "paradigm": 8, "consist": [8, 44, 178], "configur": [8, 30, 32, 35, 36, 37, 38, 39, 40, 41, 43, 44, 59, 70, 77, 82, 90, 96, 107, 108, 120, 172, 175, 178, 179, 180, 181], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 142, 143, 144, 145, 146, 147, 148, 149, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 170, 172, 174, 175, 176, 177, 178, 179, 180, 181], "command": [8, 9, 141, 171, 174, 175, 176, 177, 178, 179, 180, 181], "overrid": [8, 11, 12, 174, 177, 178, 179, 181], "togeth": [8, 31, 158, 178, 180], "valid": [8, 27, 129, 130, 167, 171, 177, 178], "environ": [8, 144, 152, 171, 174, 177, 178], "logic": [8, 139, 172, 178, 180], "api": [8, 9, 24, 51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 129, 174, 175, 177, 178, 179, 181], "closer": [8, 180], "monolith": [8, 172], "trainer": [8, 123], "A": [8, 9, 24, 25, 29, 31, 54, 64, 95, 107, 114, 119, 120, 121, 123, 125, 129, 131, 132, 134, 135, 140, 141, 148, 149, 153, 154, 159, 160, 169, 170, 173, 174, 175, 177, 180, 181], "wrapper": [8, 131, 132, 140, 142, 174, 180], "around": [8, 28, 64, 95, 131, 132, 148, 174, 175, 177, 180, 181], "extern": [8, 176], "primarili": [8, 29, 180], "eleutherai": [8, 172, 180], "har": [8, 172, 180], "control": [8, 30, 35, 38, 40, 126, 164, 177], "multi": [8, 28, 114, 129, 179], "stage": 8, "distil": 8, "oper": [8, 29, 126, 164], "turn": [8, 20, 27, 28, 175], "dataload": [8, 31, 35, 38, 40], "applic": [8, 114, 136, 137, 158], "clean": [8, 9, 34], "after": [8, 107, 114, 116, 117, 119, 120, 129, 154, 155, 156, 157, 158, 175, 181], "process": [8, 9, 121, 150, 151, 164, 176, 178, 181], "group": [8, 114, 150, 151, 155, 156, 157, 158, 174, 179], "init_process_group": [8, 151], "backend": [8, 174], "gloo": 8, "els": [8, 141, 158, 172, 181], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 29, 138, 141, 146, 165, 176, 178, 179, 180], "stuff": 8, "carri": 8, "relev": [8, 20, 174, 177, 180], "interfac": [8, 15, 18, 29, 124, 176], "metric": [8, 178], "logger": [8, 147, 153, 155, 156, 157, 158, 178], "self": [8, 9, 31, 48, 49, 50, 55, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 102, 108, 109, 110, 114, 119, 120, 124, 129, 130, 136, 139, 140, 176, 180, 181], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 135, 146, 154, 162, 175], "_model": [8, 140], "_setup_model": 8, "_token": [8, 176], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 140, 142, 161, 165, 181], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 123, 164, 174, 179], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 31], "batch": [8, 31, 35, 38, 40, 114, 116, 118, 119, 123, 159, 160, 165, 172, 176, 178, 179, 180], "enumer": 8, "_autocast": 8, "logit": [8, 143], "label": [8, 28, 30, 31, 32, 33, 35, 36, 37, 39, 41, 42, 43, 44, 123, 159, 160], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 155, 156, 157, 158], "step": [8, 31, 119, 122, 142, 155, 156, 157, 158, 161, 165, 170, 177, 180, 181], "learn": [8, 29, 122, 172, 175, 176, 178, 179, 180, 181], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 23, 26, 27, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 54, 64, 65, 70, 77, 78, 81, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 112, 114, 116, 118, 119, 120, 126, 128, 129, 130, 131, 134, 136, 137, 138, 139, 143, 144, 145, 147, 149, 153, 155, 156, 157, 158, 161, 162, 163, 164, 165, 167, 175, 176, 177], "fullfinetunerecip": 8, "wandblogg": [9, 180, 181], "workspac": 9, "seen": [9, 180, 181], "screenshot": 9, "below": [9, 14, 118, 135, 176, 179, 180, 181], "packag": [9, 157, 158, 171], "pip": [9, 157, 158, 171, 177, 179], "Then": [9, 126, 178], "login": [9, 158, 174, 177], "project": [9, 48, 49, 50, 55, 59, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 108, 109, 114, 115, 129, 130, 146, 158, 170, 175, 180, 181], "grab": [9, 179], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 152, 175], "exit": [9, 171, 174], "resourc": [9, 155, 156, 157, 158], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 39, 41, 43, 114, 118, 119, 120, 143, 175, 177], "desir": [9, 28, 163, 175], "suggest": 9, "approach": [9, 29, 176], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 177], "_output_dir": [9, 136, 137, 138], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 165], "type": [9, 10, 12, 20, 24, 25, 26, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 116, 117, 118, 119, 120, 121, 123, 125, 127, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 159, 160, 162, 163, 164, 165, 166, 176, 177, 180, 181], "descript": [9, 36, 41, 174], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 24, 25, 28, 31, 35, 38, 40, 153, 176], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 57, 60, 61, 62, 63, 64, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 83, 84, 85, 86, 91, 92, 93, 94, 101, 103, 105, 106, 109, 111, 112, 113, 114, 117, 118, 122, 123, 129, 135, 136, 137, 141, 147, 152, 157, 158, 162, 164, 171, 176, 177], "com": [10, 48, 49, 50, 60, 61, 64, 71, 72, 73, 77, 83, 84, 91, 92, 109, 114, 117, 118, 122, 123, 129, 171], "facebookresearch": [10, 117], "blob": [10, 48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109, 112, 114, 117, 118, 122, 123], "main": [10, 12, 64, 112, 114, 117, 118, 171, 177, 179], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 119], "32": [10, 179, 180, 181], "num_head": [10, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 118, 119], "num_kv_head": [10, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116], "vocab_s": [10, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "must": [10, 29, 124, 141, 181], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 114, 116, 117, 118, 119, 120, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 159, 160, 163, 164, 165, 166, 175, 176, 180, 181], "nn": [10, 114, 115, 116, 119, 120, 121, 124, 126, 127, 128, 135, 142, 154, 161, 162, 167, 180, 181], "parsed_yaml": 10, "embed_dim": [10, 55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 118, 120, 180], "valueerror": [10, 21, 24, 27, 36, 41, 107, 114, 116, 119, 123, 136, 137, 138, 145, 148, 164, 167], "recipe_nam": 11, "rank": [11, 48, 49, 50, 59, 60, 61, 70, 71, 72, 73, 82, 83, 84, 90, 91, 92, 96, 97, 98, 99, 108, 109, 125, 150, 152, 164, 178, 180, 181], "zero": [11, 116, 117, 177, 179], "displai": 11, "callabl": [12, 28, 30, 32, 119, 126, 135, 143, 146, 149, 154, 162], "With": [12, 177, 180, 181], "my_recip": 12, "foo": 12, "bar": [12, 172, 178], "instanti": [13, 45, 46, 47, 48, 49, 50, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 96, 97, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 140], "configerror": 13, "cannot": [13, 138, 179], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 33, 35, 36, 38, 39, 40, 41, 42, 43, 44, 64, 148, 155, 156, 157, 158, 176, 177, 181], "templat": [14, 15, 17, 18, 22, 23, 28, 29, 30, 32, 35, 36, 38, 39, 40, 41, 64], "style": [14, 31, 34, 35, 36, 41, 181], "slightli": 14, "describ": [14, 64, 77, 162, 176], "task": [14, 23, 29, 37, 175, 176, 177, 179, 180, 181], "further": [14, 174, 176, 180, 181], "context": [14, 16, 111, 126, 163, 165, 176], "respons": [14, 16, 54, 64, 95, 107, 123, 134, 176, 177, 178, 179], "appropri": [14, 16, 19, 21, 29, 122, 136, 176, 181], "request": [14, 145, 176, 177], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 176], "map": [14, 17, 18, 22, 23, 24, 25, 28, 29, 30, 31, 32, 39, 77, 107, 128, 132, 133, 136, 140, 142, 155, 156, 157, 158, 161, 165, 175, 176, 177, 180], "column_map": [14, 17, 18, 22, 23, 28, 29, 30, 32, 39, 176], "placehold": [14, 15, 17, 18, 22, 23, 28, 30, 32, 39, 176], "column": [14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 43, 114, 119, 120, 175, 176], "ident": [14, 17, 18, 21, 22, 23, 30, 31, 32, 39, 177], "poem": 14, "n": [14, 22, 54, 64, 95, 107, 114, 134, 169, 173, 174, 175, 176], "nwrite": 14, "long": [14, 31, 132, 175, 180], "where": [14, 17, 22, 23, 28, 29, 35, 38, 40, 64, 95, 114, 119, 125, 131, 146, 154, 160, 176], "me": 14, "role": [15, 20, 24, 25, 28, 54, 64, 95, 107, 134, 175, 176], "system": [15, 16, 19, 20, 21, 24, 25, 27, 28, 54, 64, 95, 107, 112, 134, 175, 176], "assist": [15, 16, 19, 20, 24, 25, 27, 28, 54, 64, 95, 107, 112, 134, 143, 175, 176], "accord": [15, 21, 175], "openai": [16, 24, 36, 176], "markup": 16, "languag": [16, 125, 143, 180], "It": [16, 21, 174, 175, 176, 181], "im_start": 16, "im_end": 16, "goe": [16, 126], "tag": [16, 19, 21, 28, 155, 156, 157, 158, 175], "grammar": [17, 38, 176], "english": 17, "sentenc": [17, 31, 95], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 180], "lazi": 17, "dog": 17, "alwai": [18, 141], "human": [19, 25, 175], "pre": [19, 31, 43, 64, 171, 175, 176], "taken": [19, 180, 181], "inst": [19, 21, 28, 64, 175, 176], "sy": [19, 64, 175, 176], "respect": [19, 29, 128, 165, 175, 176], "honest": [19, 175, 176], "am": [19, 21, 175, 176, 177, 179], "pari": [19, 21, 23, 176], "capit": [19, 21, 22, 23, 176], "franc": [19, 21, 22, 23, 176], "known": [19, 21, 64, 95, 149, 176], "its": [19, 21, 31, 98, 114, 118, 119, 120, 161, 164, 174, 175, 176, 177, 179, 180], "stun": [19, 21, 176], "liter": [20, 48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 129, 130], "mask": [20, 30, 31, 35, 38, 40, 54, 64, 77, 95, 107, 114, 119, 120, 134, 175, 176], "ipython": 20, "eot": 20, "dataclass": [20, 175], "repres": [20, 160, 175], "individu": [20, 31, 148, 158, 162, 175, 176], "tiktoken": [20, 77, 132, 179], "special": [20, 28, 64, 77, 81, 95, 107, 112, 132, 133, 134, 140, 176], "variabl": [20, 28, 29, 30, 32, 39, 152, 181], "writer": 20, "whether": [20, 24, 25, 28, 30, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 55, 59, 60, 61, 70, 71, 72, 73, 77, 82, 83, 84, 90, 91, 92, 95, 96, 97, 98, 99, 107, 108, 109, 121, 125, 129, 130, 131, 132, 135, 145, 148, 175, 176], "correspond": [20, 124, 127, 145, 160, 178, 179], "consecut": [20, 27], "from_dict": [20, 175], "construct": [20, 180], "dictionari": [20, 31, 148, 153, 155, 156, 157, 158, 160, 177], "mistral": [21, 28, 41, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 139, 174, 175, 177, 178], "llama2chatformat": [21, 64, 175, 176], "similar": [22, 37, 42, 43, 44, 129, 176, 177, 179, 180, 181], "stackexchangedpair": 22, "question": [22, 175, 176, 177, 179], "answer": [22, 175, 177, 179], "nanswer": 22, "summar": [23, 40, 175, 176], "dialogu": [23, 40, 175], "summari": [23, 29, 40, 148, 176], "dialog": 23, "hello": [23, 54, 64, 77, 95, 107, 131, 132, 175, 177, 179], "did": [23, 177, 179, 181], "know": [23, 175, 176, 177, 179, 180], "adher": [24, 25], "could": [24, 180], "remain": [24, 25, 122, 180], "unmask": [24, 25], "sharegpt": [25, 36], "gpt": [25, 114, 177], "eos_id": [26, 132, 134], "length": [26, 27, 29, 30, 31, 32, 33, 35, 37, 39, 41, 43, 44, 54, 55, 59, 64, 65, 70, 77, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 111, 114, 116, 118, 119, 132, 134, 137, 159, 160], "last": [26, 31, 122, 176], "replac": [26, 30, 35, 38, 40, 121, 180], "forth": [27, 176], "come": [27, 124, 180], "empti": [27, 174], "shorter": 27, "min": [27, 180], "invalid": 27, "convert_to_messag": [28, 175], "chat_format": [28, 36, 41, 175, 176], "chatformat": [28, 36, 176], "load_dataset_kwarg": [28, 30, 32, 33, 36, 37, 39, 43, 44], "multiturn": [28, 175], "prepar": [28, 175], "truncat": [28, 30, 31, 32, 33, 37, 39, 41, 43, 44, 54, 64, 77, 95, 107, 132, 134, 176], "tokenize_messag": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 64, 77, 95, 107, 134, 175, 176], "anyth": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "load_dataset": [28, 30, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 175, 176], "huggingfac": [28, 30, 32, 33, 36, 37, 39, 43, 44, 103, 111, 112, 122, 123, 136, 137, 174, 177], "co": [28, 30, 32, 33, 36, 37, 39, 43, 44, 103, 111, 112, 136, 137, 177], "doc": [28, 30, 32, 33, 36, 37, 39, 43, 44, 64, 77, 135, 141, 147, 152, 157, 158, 164, 174, 177], "en": [28, 30, 32, 33, 36, 37, 39, 43, 44], "package_refer": [28, 30, 32, 33, 36, 37, 39, 43, 44], "loading_method": [28, 30, 32, 33, 36, 37, 39, 43, 44], "extra": [28, 171, 175, 180, 181], "still": [28, 141, 180, 181], "unless": 28, "check": [28, 36, 119, 129, 145, 152, 166, 170, 175, 177, 178, 180], "concaten": [29, 54, 64, 95, 107, 134, 160], "sub": [29, 157], "unifi": [29, 103], "were": [29, 126, 175, 178], "simplifi": [29, 174, 180], "simultan": 29, "intern": [29, 141], "aggreg": 29, "transpar": 29, "index": [29, 31, 114, 118, 119, 120, 122, 159, 160, 171, 175, 177], "howev": [29, 112, 171], "constitu": 29, "might": [29, 174, 177], "larg": [29, 125, 165, 174, 181], "comput": [29, 65, 70, 78, 82, 87, 90, 114, 115, 118, 119, 123, 148, 164, 177, 181], "cumul": 29, "maintain": [29, 181], "indic": [29, 31, 114, 118, 119, 120, 135, 152, 175], "deleg": 29, "retriev": [29, 146], "lead": [29, 95, 131], "high": [29, 172, 180], "scale": [29, 48, 49, 50, 59, 60, 61, 70, 71, 72, 73, 82, 83, 84, 90, 91, 92, 96, 97, 98, 99, 108, 109, 125, 143, 180, 181], "consid": 29, "strategi": 29, "stream": [29, 147], "demand": 29, "deriv": [29, 115, 119, 120], "_dataset": 29, "_len": 29, "total": [29, 122, 150, 169, 173, 177, 179, 180], "combin": 29, "_index": 29, "lookup": 29, "dataset1": 29, "mycustomdataset": 29, "params1": 29, "dataset2": 29, "params2": 29, "concat_dataset": 29, "data_point": 29, "1500": 29, "element": [29, 177], "accomplish": [29, 36, 39, 43], "instruct_dataset": [29, 176], "vicgal": [29, 176], "gpt4": [29, 176], "alpacainstructtempl": [29, 39, 176], "samsum": [29, 40, 176], "summarizetempl": [29, 175, 176], "focus": [29, 178], "enhanc": [29, 181], "divers": 29, "machin": [29, 144, 174, 177], "instructtempl": [30, 32, 176], "contribut": [30, 35, 38, 40], "disabl": [30, 32, 33, 37, 39, 43, 44, 126, 164], "recommend": [30, 32, 33, 35, 37, 39, 43, 44, 157, 175, 177, 181], "highest": [30, 32, 33, 35, 37, 39, 43, 44], "sequenc": [30, 31, 32, 33, 35, 37, 39, 41, 43, 44, 54, 55, 59, 64, 65, 70, 77, 78, 82, 87, 90, 95, 96, 98, 100, 102, 107, 108, 110, 114, 116, 118, 119, 132, 134, 159, 160, 175], "ds": [31, 41], "padding_idx": [31, 159, 160], "max_pack": 31, "split_across_pack": 31, "greedi": 31, "pack": [31, 34, 35, 36, 38, 39, 40, 41, 43, 114, 118, 119, 120], "done": [31, 129, 145, 154, 180, 181], "preprocess": 31, "outsid": [31, 164, 165, 177, 179, 180], "sampler": [31, 178], "part": [31, 175, 181], "buffer": 31, "enough": [31, 175], "attent": [31, 48, 49, 50, 55, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 102, 108, 109, 110, 111, 114, 116, 118, 119, 120, 129, 130, 179, 180, 181], "lower": [31, 180], "triangular": 31, "cross": 31, "attend": [31, 114, 119, 120], "rel": [31, 114, 118, 119, 120, 148, 180], "pad": [31, 143, 159, 160, 176], "max": [31, 41, 54, 64, 95, 107, 119, 122, 132, 134, 174, 180], "wise": 31, "collat": [31, 159, 176], "made": [31, 36, 39, 43, 118, 177], "smaller": [31, 177, 179, 180, 181], "jam": 31, "vari": 31, "s1": [31, 64, 95, 131], "s2": [31, 64, 95, 131], "s3": 31, "s4": 31, "contamin": 31, "input_po": [31, 114, 116, 118, 119, 120], "matrix": 31, "causal": [31, 114, 119, 120], "continu": [31, 176], "increment": 31, "move": [31, 119], "entir": [31, 154, 175, 181], "avoid": [31, 117, 121, 164, 174, 181], "freeform": [33, 43], "unstructur": [33, 43, 44], "corpu": [33, 37, 43, 44], "local": [33, 43, 81, 112, 158, 164, 171, 174, 175, 177, 178], "tabular": [33, 43], "yahma": [34, 39], "variant": [34, 38, 40], "version": [34, 59, 70, 82, 90, 96, 98, 108, 114, 143, 166, 171, 175, 179, 181], "page": [34, 44, 171, 172, 174, 178, 179], "tatsu": 35, "lab": 35, "codebas": [35, 38, 40, 177], "prior": [35, 36, 38, 39, 40, 41, 43], "alpaca_d": 35, "batch_siz": [35, 38, 40, 114, 116, 119, 120, 123, 177], "conversation_styl": [36, 176], "chatdataset": [36, 41, 175, 176], "friendli": [36, 39, 43, 143, 175], "huggingfaceh4": 36, "no_robot": 36, "chatmlformat": 36, "2096": [36, 39, 43], "packeddataset": [36, 39, 43, 176], "ccdv": 37, "cnn_dailymail": 37, "textcompletiondataset": [37, 43, 44, 176], "cnn": 37, "dailymail": 37, "articl": [37, 44], "extract": [37, 133], "highlight": [37, 181], "liweili": 38, "c4_200m": 38, "mirror": [38, 40], "llama_recip": [38, 40], "grammar_d": 38, "alpaca_clean": 39, "samsum_d": 40, "open": [41, 56, 57, 176, 177], "orca": 41, "slimorca": 41, "dedup": 41, "1024": [41, 42, 176], "prescrib": 41, "least": [41, 179, 180], "though": [41, 175], "10": [41, 159, 160, 177, 179, 181], "351": 41, "82": [41, 177], "391": 41, "221": 41, "220": 41, "193": 41, "12": [41, 160, 171], "471": 41, "lvwerra": [42, 176], "stack": [42, 165, 176], "exchang": [42, 176], "preferencedataset": [42, 176], "stackexchangepair": 42, "omit": [43, 180], "allenai": [43, 176], "c4": [43, 176], "data_dir": [43, 176], "realnewslik": [43, 176], "wikitext": 44, "subset": [44, 59, 70, 82, 90, 96, 98, 108, 127], "103": [44, 177], "raw": 44, "wikipedia": 44, "code_llama2": [45, 46, 47, 48, 49, 50, 51, 52, 53, 174], "transformerdecod": [45, 46, 47, 48, 49, 50, 51, 52, 53, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 108, 109, 110, 111, 113, 143, 180], "w": [45, 46, 47, 56, 57, 66, 67, 68, 79, 80, 88, 89, 101, 103, 157, 158, 175, 177, 180, 181], "arxiv": [45, 46, 47, 51, 52, 53, 62, 63, 66, 67, 68, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 114, 117, 118, 123], "org": [45, 46, 47, 51, 52, 53, 62, 63, 64, 66, 67, 68, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 114, 117, 118, 123, 135, 141, 147, 152, 157, 162, 164, 171], "pdf": [45, 46, 47], "2308": [45, 46, 47], "12950": [45, 46, 47], "lora_attn_modul": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 129, 130, 180, 181], "q_proj": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 114, 129, 130, 180, 181], "k_proj": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 114, 129, 130, 180, 181], "v_proj": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 114, 129, 130, 180, 181], "output_proj": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 114, 129, 130, 180, 181], "apply_lora_to_mlp": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 129, 130, 180], "apply_lora_to_output": [48, 49, 50, 51, 52, 53, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 129, 130, 180], "lora_rank": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 180], "lora_alpha": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 180], "float": [48, 49, 50, 51, 52, 53, 55, 59, 60, 61, 62, 63, 65, 70, 71, 72, 73, 74, 75, 76, 78, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 102, 105, 106, 108, 109, 110, 113, 114, 117, 122, 123, 125, 143, 148, 153, 155, 156, 157, 158, 180, 181], "16": [48, 49, 50, 51, 52, 53, 60, 61, 62, 63, 71, 72, 73, 74, 75, 76, 83, 84, 85, 86, 91, 92, 93, 94, 97, 99, 105, 106, 109, 113, 160, 180, 181], "lora_dropout": [48, 49, 50, 51, 52, 53, 59, 70, 71, 72, 73, 74, 75, 76, 82, 90, 96, 98, 108], "05": [48, 49, 50, 51, 52, 53, 65, 70, 71, 72, 73, 74, 75, 76, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "quantize_bas": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 125, 181], "lora": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 125, 126, 129, 130, 136, 154, 170, 172, 175, 178, 179], "code_llama2_13b": 48, "tloen": [48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109], "8bb8579e403dc78e37fe81ffbb253c413007323f": [48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109], "l41": [48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109], "l43": [48, 49, 50, 60, 61, 71, 72, 73, 83, 84, 91, 92, 109], "linear": [48, 49, 50, 51, 52, 53, 59, 60, 61, 62, 63, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 108, 109, 113, 119, 124, 125, 129, 130, 180, 181], "mlp": [48, 49, 50, 55, 59, 60, 61, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 102, 108, 109, 110, 119, 120, 129, 130, 179, 180], "final": [48, 49, 50, 55, 59, 65, 70, 71, 72, 73, 78, 82, 83, 84, 87, 90, 91, 92, 96, 97, 98, 99, 100, 108, 109, 115, 119, 126, 129, 130, 177, 179, 180, 181], "low": [48, 49, 50, 59, 60, 61, 70, 71, 72, 73, 82, 83, 84, 90, 91, 92, 96, 97, 98, 99, 108, 109, 125, 177, 180, 181], "approxim": [48, 49, 50, 59, 60, 61, 70, 71, 72, 73, 82, 83, 84, 90, 91, 92, 96, 97, 98, 99, 108, 109, 125, 180], "factor": [48, 49, 50, 59, 60, 61, 70, 71, 72, 73, 82, 83, 84, 90, 91, 92, 96, 97, 98, 99, 108, 109, 125, 177], "dropout": [48, 49, 50, 55, 59, 65, 70, 71, 72, 73, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 125, 180, 181], "probabl": [48, 49, 50, 59, 70, 71, 72, 73, 82, 90, 96, 98, 108, 123, 125, 143, 177], "code_llama2_70b": 49, "code_llama2_7b": 50, "qlora": [51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 121, 170, 172, 179, 180], "per": [51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 116, 121, 174, 179, 181], "paper": [51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 180, 181], "ab": [51, 52, 53, 62, 63, 66, 67, 68, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 114, 117, 118, 123], "2305": [51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113, 114, 123], "14314": [51, 52, 53, 62, 63, 74, 75, 76, 85, 86, 93, 94, 105, 106, 113], "lora_code_llama2_13b": 51, "lora_code_llama2_70b": 52, "lora_code_llama2_7b": 53, "gemma": [54, 56, 57, 58, 59, 60, 61, 62, 63, 139], "sentencepiec": [54, 64, 95, 107, 131, 179], "pretrain": [54, 64, 77, 95, 107, 131, 132, 174, 175, 178, 180, 181], "spm_model": [54, 64, 95, 107, 131, 175], "tokenized_text": [54, 64, 77, 95, 107, 131, 132], "world": [54, 64, 77, 95, 107, 131, 132, 150, 152, 177], "add_bo": [54, 64, 77, 95, 107, 131, 132, 175], "add_eo": [54, 64, 77, 95, 107, 131, 132, 175], "31587": [54, 64, 77, 95, 107, 131, 132], "29644": [54, 64, 77, 95, 107, 131, 132], "102": [54, 64, 77, 95, 107, 131, 132], "tokenizer_path": [54, 64, 95, 107], "separ": [54, 64, 95, 107, 134, 136, 175, 178, 179, 180, 181], "concat": [54, 64, 95, 107, 134], "1788": [54, 64, 95, 107, 134], "2643": [54, 64, 95, 107, 134], "13": [54, 64, 95, 107, 134, 160, 177, 179, 181], "1792": [54, 64, 95, 107, 134], "9508": [54, 64, 95, 107, 134], "465": [54, 64, 95, 107, 134], "22137": [54, 64, 95, 107, 134], "2933": [54, 64, 95, 107, 134], "join": [54, 64, 95, 107, 134], "attribut": [54, 64, 95, 107, 126, 134, 142], "head_dim": [55, 59, 114, 116, 119], "intermediate_dim": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "attn_dropout": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 119], "norm_ep": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "1e": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 117], "06": [55, 59, 117, 180], "rope_bas": [55, 59, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "10000": [55, 59, 96, 98, 100, 102, 108, 110, 118], "norm_embed": [55, 59], "gemmatransformerdecod": [55, 56, 57, 59, 60, 61, 62, 63], "transformerdecoderlay": [55, 65, 78, 87, 100, 119], "rm": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "norm": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 119, 120], "space": [55, 65, 78, 87, 100, 119], "slide": [55, 100, 111], "window": [55, 100, 111, 176], "vocabulari": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "queri": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 119, 120, 179], "head": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 118, 119, 139, 179], "mha": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 119], "dimens": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114, 116, 118, 119, 125, 179, 180, 181], "intermedi": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 138, 162, 179, 181], "onto": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114], "scaled_dot_product_attent": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110, 114], "epsilon": [55, 59, 65, 70, 78, 82, 87, 90, 96, 98, 100, 102, 108, 110], "rotari": [55, 59, 96, 98, 100, 102, 108, 110, 118, 179], "10_000": [55, 59, 96, 98, 100, 102, 110], "blog": [56, 57], "technolog": [56, 57], "develop": [56, 57, 181], "gemmatoken": 58, "becaus": [59, 116, 119, 138, 174, 175, 177, 179], "ti": 59, "gemma_2b": 60, "gemma_7b": 61, "lora_gemma_2b": 62, "lora_gemma_7b": 63, "card": [64, 77], "regist": [64, 77, 81, 107, 112, 115, 121, 161, 181], "uniqu": [64, 139], "strongli": 64, "beforehand": 64, "html": [64, 135, 141, 147, 152, 157, 162, 164, 170], "problem": [64, 95], "due": [64, 95, 131, 180, 181], "whitespac": [64, 95, 131], "prepend": [64, 77, 95, 131], "slice": [64, 95], "kvcach": [65, 70, 78, 82, 87, 90, 108, 114, 119], "scale_hidden_dim_for_mlp": [65, 70, 78, 82, 87, 90], "2307": [66, 67, 68], "09288": [66, 67, 68], "llama2_70b": 72, "llama2_7b": [73, 180], "lora_llama2_13b": 74, "lora_llama2_70b": 75, "lora_llama2_7b": [76, 180], "special_token": [77, 107, 132, 175], "left": [77, 107, 180], "canon": [77, 81, 107, 112], "tt_model": [77, 132], "token_id": [77, 95, 132], "truncate_at_eo": [77, 132], "tokenize_head": 77, "header": [77, 175], "500000": [78, 82, 87, 90], "special_tokens_path": [81, 112], "llama3token": [81, 175], "similarli": [81, 112, 176], "llama3_70b": 83, "llama3_8b": [84, 143, 179], "lora_llama3_70b": 85, "lora_llama3_8b": 86, "gqa": [87, 90, 114], "mqa": [87, 90, 114], "llama3_1": [88, 89, 90, 91, 92, 93, 94], "llama3_1_70b": 91, "llama3_1_8b": 92, "lora_llama3_1_70b": 93, "lora_llama3_1_8b": 94, "trim_leading_whitespac": [95, 131], "unbatch": [95, 131], "bo": [95, 112, 131, 134, 175, 176], "append": [95, 107, 131, 171], "eo": [95, 107, 112, 131, 134, 175, 176], "trim": [95, 131], "num_class": [98, 102], "classifi": [98, 99, 102, 103, 106, 176], "announc": 101, "classif": [102, 139], "ray2333": 103, "reward": [103, 123], "feedback": 103, "mistraltoken": [104, 175], "lora_mistral_7b": 105, "lora_mistral_classifier_7b": 106, "phi3": [107, 108, 109, 111, 112, 113, 139, 174], "ignore_system_prompt": 107, "phi3_mini": [109, 139], "ref": [111, 112, 158], "phi": [111, 112, 139], "128k": 111, "nor": 111, "phi3minitoken": 112, "tokenizer_config": 112, "spm": 112, "lm": 112, "unk": 112, "augment": [112, 181], "endoftext": 112, "opt": [112, 171, 178], "cite": 112, "better": [112, 172, 175, 176, 177], "51": 112, "phi3minisentencepiecebasetoken": 112, "lora_phi3_mini": 113, "pos_embed": [114, 180], "kv_cach": 114, "introduc": [114, 117, 125, 175, 176, 180, 181], "13245v1": 114, "multihead": 114, "extrem": 114, "share": [114, 176, 177], "credit": 114, "document": [114, 135, 146, 154, 174, 176], "lightn": 114, "lit": 114, "lit_gpt": 114, "v": [114, 119, 180], "k": [114, 180], "q": [114, 180], "n_kv_head": 114, "calcul": [114, 119, 179], "e": [114, 121, 124, 128, 136, 140, 148, 165, 171, 177, 179, 180, 181], "g": [114, 124, 136, 148, 165, 179, 180, 181], "rotarypositionalembed": [114, 180], "cach": [114, 116, 118, 119, 171, 174], "rope": [114, 118], "seq_length": [114, 120, 143], "boolean": [114, 119, 120, 135], "softmax": [114, 119, 120], "row": [114, 119, 120, 175], "j": [114, 119, 120], "seq_len": 114, "bigger": 114, "n_h": [114, 118], "num": [114, 118], "n_kv": 114, "kv": [114, 116, 119], "emb": [114, 119], "h_d": [114, 118], "gate_proj": 115, "down_proj": 115, "up_proj": 115, "silu": 115, "feed": [115, 120], "network": [115, 126, 180, 181], "fed": [115, 175], "multipli": 115, "subclass": [115, 141], "although": [115, 180], "afterward": 115, "former": 115, "hook": [115, 121, 161, 181], "latter": 115, "standalon": 116, "past": 116, "expand": 116, "dpython": [116, 119, 121], "reset": [116, 119, 148], "k_val": 116, "v_val": 116, "h": [116, 171, 174], "longer": [116, 176], "ep": 117, "root": [117, 157, 158], "squar": 117, "1910": 117, "07467": 117, "verfic": [117, 118], "small": [117, 177], "divis": 117, "propos": 118, "2104": 118, "09864": 118, "l80": 118, "upto": 118, "init": [118, 148, 158, 181], "exceed": 118, "freq": 118, "recomput": 118, "geometr": 118, "progress": [118, 178], "rotat": 118, "angl": 118, "todo": 118, "effici": [118, 129, 146, 170, 172, 177, 178, 180], "belong": [119, 142], "reduc": [119, 172, 176, 180, 181], "statement": 119, "improv": [119, 132, 146, 177, 179, 180], "readabl": [119, 177], "caches_are_en": 119, "At": 119, "arang": 119, "prompt_length": 119, "causal_mask": 119, "m_": 119, "seq": 119, "reset_cach": 119, "setup_cach": 119, "attn": [120, 180, 181], "causalselfattent": [120, 180], "sa_norm": 120, "mlp_norm": 120, "ff": 120, "common_util": 121, "bfloat16": [121, 163, 177, 178, 179, 180], "offload_to_cpu": 121, "nf4": [121, 181], "restor": 121, "higher": [121, 179, 181], "offload": [121, 181], "increas": [121, 122, 179, 180], "peak": [121, 148, 153, 177, 179, 180, 181], "gpu": [121, 174, 177, 178, 179, 180, 181], "_register_state_dict_hook": 121, "m": [121, 143, 175], "mymodul": 121, "_after_": 121, "nf4tensor": [121, 181], "unquant": [121, 177, 181], "unus": 121, "num_warmup_step": 122, "num_training_step": 122, "num_cycl": [122, 165], "last_epoch": 122, "lambdalr": 122, "rate": [122, 172, 178], "schedul": [122, 165, 178], "linearli": 122, "decreas": [122, 176, 180, 181], "cosin": 122, "v4": 122, "23": [122, 179], "src": 122, "l104": 122, "warmup": [122, 165], "phase": 122, "wave": 122, "half": 122, "lr_schedul": 122, "beta": 123, "label_smooth": 123, "loss_typ": 123, "sigmoid": 123, "dpo": [123, 126, 160], "18290": 123, "trl": 123, "librari": [123, 141, 145, 147, 164, 170, 172, 174, 176, 181], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 123, "dpo_train": 123, "l844": 123, "temperatur": [123, 143, 177], "uncertainti": 123, "hing": 123, "ipo": 123, "kto_pair": 123, "policy_chosen_logp": 123, "policy_rejected_logp": 123, "reference_chosen_logp": 123, "reference_rejected_logp": 123, "polici": [123, 126, 135, 146, 154, 162], "chosen": [123, 165, 176], "reject": [123, 176], "chosen_reward": 123, "rejected_reward": 123, "unknown": 123, "peft": [124, 125, 126, 127, 128, 129, 130, 136, 180, 181], "protocol": 124, "adapter_param": [124, 125, 126, 127, 128], "proj": 124, "in_dim": [124, 125, 180, 181], "out_dim": [124, 125, 180, 181], "bia": [124, 125, 180, 181], "loralinear": [124, 180, 181], "alpha": [125, 180, 181], "use_bia": 125, "perturb": 125, "decomposit": [125, 180], "matric": [125, 154, 180, 181], "trainabl": [125, 128, 154, 180, 181], "mapsto": 125, "w_0x": 125, "r": [125, 180], "bax": 125, "lora_a": [125, 180, 181], "lora_b": [125, 180, 181], "temporarili": 126, "neural": [126, 180, 181], "treat": [126, 141, 175], "caller": 126, "whose": [126, 161], "yield": 126, "get_adapter_param": [128, 180], "base_miss": 129, "base_unexpect": 129, "lora_miss": 129, "lora_unexpect": 129, "validate_state_dict_for_lora": [129, 180], "unlik": [129, 177, 179], "reli": [129, 134], "unexpect": 129, "strict": [129, 180], "pull": [129, 174], "120600": 129, "assertionerror": [129, 130, 160], "nonempti": 129, "full_model_state_dict_kei": 130, "lora_state_dict_kei": 130, "base_model_state_dict_kei": 130, "confirm": [130, 171], "determin": 130, "lora_modul": 130, "complement": 130, "disjoint": 130, "union": [130, 155, 156, 157, 158, 162, 164], "non": 130, "overlap": 130, "light": 131, "sentencepieceprocessor": 131, "addition": [131, 132, 164, 176, 180], "prefix": 131, "bos_id": [132, 134], "lightweight": [132, 175], "break": 132, "substr": 132, "repetit": 132, "speed": [132, 165, 179, 181], "identif": 132, "regex": 132, "chunk": 132, "present": [132, 138], "absent": 132, "tokenizer_json_path": 133, "heavili": 134, "datatyp": [135, 181], "denot": 135, "integ": [135, 159, 164], "auto_wrap_polici": [135, 146, 162], "submodul": [135, 154], "obei": 135, "contract": 135, "get_fsdp_polici": 135, "modules_to_wrap": [135, 146, 154], "min_num_param": 135, "my_fsdp_polici": 135, "recurs": [135, 154, 157], "isinst": [135, 176], "sum": [135, 180], "p": [135, 140, 180, 181], "numel": [135, 180], "1000": 135, "functool": 135, "partial": 135, "stabl": [135, 152, 157, 164, 171], "alia": 135, "safe_seri": 136, "from_pretrain": 136, "0001_of_0003": 136, "0002_of_0003": 136, "preserv": [136, 181], "weight_map": [136, 177], "convert_weight": 136, "_model_typ": [136, 139], "intermediate_checkpoint": [136, 137, 138], "_weight_map": 136, "shard": [137, 179], "wip": 137, "larger": [138, 177, 179], "down": [138, 176, 180, 181], "qualnam": 139, "boundari": 139, "distinguish": 139, "gate": [139, 174, 178], "my_new_model": 139, "my_custom_state_dict_map": 139, "mistral_reward": 139, "mistral_classifi": 139, "optim_map": 140, "bare": 140, "bone": 140, "distribut": [140, 144, 151, 152, 162, 164, 172, 174, 178, 179], "optim_dict": [140, 142, 161], "cfg_optim": 140, "ckpt": 140, "optim_ckpt": 140, "placeholder_optim_dict": 140, "optiminbackwardwrapp": 140, "get_optim_kei": 140, "arbitrari": [140, 180], "hyperparamet": [140, 172, 178, 180, 181], "optim_ckpt_map": 140, "runtimeerror": [140, 145, 151], "loadabl": 140, "argpars": 141, "argumentpars": 141, "builtin": 141, "said": 141, "noth": 141, "consult": 141, "info": [141, 178], "parse_known_arg": 141, "namespac": 141, "act": 141, "precid": 141, "parse_arg": 141, "properti": [141, 180], "too": [141, 179], "optimizerinbackwardwrapp": 142, "top": [142, 177, 181], "named_paramet": 142, "max_generated_token": 143, "pad_id": 143, "top_k": [143, 177], "stop_token": 143, "custom_generate_next_token": 143, "condit": [143, 152, 174, 176], "bsz": 143, "predict": 143, "prune": [143, 181], "stop": 143, "compil": [143, 177, 179, 181], "generate_next_token": 143, "hi": [143, 175], "my": [143, 174, 175, 176, 177, 179], "jeremi": 143, "float32": 145, "bf16": [145, 181], "inde": [145, 177], "kernel": 145, "isn": [145, 174], "hardwar": [145, 172, 176, 177, 180], "memory_efficient_fsdp_wrap": 146, "maxim": [146, 154, 170, 172], "been": [146, 175, 179], "workload": 146, "15": [146, 160, 175, 177, 180, 181], "alongsid": 146, "ac": 146, "fullyshardeddataparallel": [146, 154], "fsdppolicytyp": [146, 154], "handler": 147, "reset_stat": 148, "track": 148, "alloc": [148, 153, 154, 179, 181], "reserv": [148, 153, 175, 181], "stat": [148, 153, 181], "int4": 149, "4w": [149, 177, 179], "recogn": 149, "int4weightonlyquant": [149, 177, 179], "int8weightonlyquant": 149, "8w": 149, "int4weightonlygptqquant": 149, "gptq": 149, "int8dynactint4weightquant": 149, "8da4w": 149, "int8dynactint4weightqatquant": 149, "qat": 149, "mode": [149, 177], "aka": 150, "master": 152, "port": [152, 174], "address": 152, "hold": [152, 178], "peak_memory_act": 153, "peak_memory_alloc": 153, "peak_memory_reserv": 153, "get_memory_stat": 153, "own": [154, 164, 174, 175, 176, 177, 180], "unit": [154, 172], "hierarch": 154, "requires_grad": [154, 180, 181], "filenam": 155, "log_": 155, "unixtimestamp": 155, "txt": [155, 176, 178], "thread": 155, "safe": 155, "flush": [155, 156, 157, 158], "ndarrai": [155, 156, 157, 158], "scalar": [155, 156, 157, 158], "record": [155, 156, 157, 158, 165], "payload": [155, 156, 157, 158], "organize_log": 157, "tensorboard": 157, "subdirectori": 157, "compar": [157, 166, 177, 180, 181], "logdir": 157, "startup": 157, "tree": [157, 176, 177], "tfevent": 157, "encount": 157, "frontend": 157, "organ": [157, 174], "accordingli": 157, "my_log_dir": 157, "view": [157, 177, 178], "my_metr": [157, 158], "termin": [157, 158], "entiti": 158, "bias": 158, "sent": 158, "usernam": 158, "my_project": 158, "my_ent": 158, "my_group": 158, "importerror": 158, "account": [158, 180, 181], "log_config": 158, "link": [158, 177], "capecap": 158, "6053ofw0": 158, "torchtune_config_j67sb73v": 158, "ignore_idx": [159, 160], "longest": 159, "token_pair": 159, "input_id": 160, "chosen_input_id": [160, 176], "chosen_label": [160, 176], "rejected_input_id": [160, 176], "rejected_label": [160, 176], "14": [160, 181], "17": [160, 177, 180], "18": [160, 179], "19": [160, 177, 179, 181], "20": 160, "soon": 161, "readi": [161, 170, 175], "grad": 161, "achiev": [161, 177, 179, 180, 181], "acwrappolicytyp": 162, "author": [162, 172, 178, 181], "fsdp_adavnced_tutori": 162, "insid": 163, "contextmanag": 163, "debug_mod": 164, "pseudo": 164, "random": [164, 178], "commonli": [164, 177, 180, 181], "numpi": 164, "determinist": 164, "global": [164, 176], "warn": 164, "nondeterminist": 164, "cudnn": 164, "set_deterministic_debug_mod": 164, "algorithm": 164, "profile_memori": 165, "with_stack": 165, "record_shap": 165, "with_flop": 165, "wait_step": 165, "warmup_step": 165, "active_step": 165, "profil": 165, "layout": 165, "trace": 165, "profileract": 165, "gradient_accumul": 165, "sensibl": 165, "default_schedul": 165, "reduct": [165, 180], "iter": [165, 167, 181], "scope": 165, "flop": 165, "wait": 165, "cycl": 165, "repeat": 165, "greater": 166, "equal": 166, "against": [166, 181], "__version__": 166, "named_param": 167, "generated_examples_python": 168, "zip": 168, "galleri": [168, 173], "sphinx": 168, "000": [169, 173, 179], "execut": [169, 173], "generated_exampl": 169, "mem": [169, 173], "mb": [169, 173], "topic": 170, "gentl": 170, "introduct": 170, "first_finetune_tutori": 170, "workflow": [170, 176, 178, 180], "requisit": 171, "proper": [171, 178], "host": [171, 174, 178], "latest": [171, 178, 181], "And": [171, 177, 179], "ls": [171, 174, 177, 178, 179], "welcom": [171, 174], "show": [171, 174, 175, 180], "greatest": [171, 178], "contributor": 171, "cd": [171, 177], "even": [171, 174, 175, 176, 179, 180, 181], "commit": 171, "branch": 171, "url": 171, "whl": 171, "therebi": [171, 181], "forc": 171, "reinstal": 171, "suffix": 171, "cu121": 171, "On": [172, 180], "pointer": 172, "emphas": 172, "aspect": 172, "simplic": 172, "component": 172, "prove": 172, "democrat": 172, "box": [172, 181], "zoo": 172, "varieti": [172, 180], "techniqu": [172, 177, 178, 180], "integr": [172, 177, 178, 179, 180, 181], "excit": 172, "checkout": 172, "quickstart": 172, "attain": 172, "chekckpoint": 172, "embodi": 172, "philosophi": 172, "usabl": 172, "composit": 172, "hard": [172, 176], "outlin": 172, "unecessari": 172, "never": 172, "thoroughli": 172, "short": 174, "subcommand": 174, "anytim": 174, "symlink": 174, "auto": 174, "wrote": 174, "readm": 174, "md": 174, "lot": [174, 177], "recent": 174, "releas": [174, 179], "agre": 174, "term": 174, "perman": 174, "eat": 174, "bandwith": 174, "storag": [174, 181], "00030": 174, "ootb": 174, "full_finetune_single_devic": [174, 176, 177, 178], "7b_full_low_memori": [174, 177, 178], "8b_full_single_devic": [174, 176], "mini_full_low_memori": 174, "7b_full": [174, 177, 178], "13b_full": [174, 177, 178], "70b_full": 174, "edit": 174, "clobber": 174, "destin": 174, "lora_finetune_distribut": [174, 179, 180], "torchrun": 174, "8b_lora_single_devic": [174, 175, 179], "launch": [174, 175, 178], "nproc": 174, "node": 174, "worker": 174, "nnode": [174, 180], "minimum_nod": 174, "maximum_nod": 174, "fail": 174, "rdzv": 174, "rendezv": 174, "endpoint": 174, "8b_lora": [174, 179], "bypass": 174, "vice": 174, "versa": 174, "fancy_lora": 174, "8b_fancy_lora": 174, "sai": [174, 175, 178], "align": 175, "intend": 175, "nice": 175, "meet": 175, "overhaul": 175, "begin_of_text": 175, "start_header_id": 175, "end_header_id": 175, "eot_id": 175, "yet": [175, 177], "untrain": 175, "accompani": 175, "who": 175, "influenti": 175, "hip": 175, "hop": 175, "artist": [175, 179], "2pac": 175, "rakim": 175, "c": 175, "na": 175, "flavor": [175, 176], "msg": 175, "formatted_messag": [175, 176], "nyou": [175, 176], "nwho": 175, "why": [175, 178, 180], "user_messag": 175, "518": 175, "25580": 175, "29962": 175, "3532": 175, "14816": 175, "29903": 175, "6778": 175, "_spm_model": 175, "piece_to_id": 175, "vector": 175, "place": 175, "manual": [175, 181], "529": 175, "29879": 175, "29958": 175, "nhere": 175, "128000": 175, "128009": 175, "pure": 175, "That": 175, "won": [175, 177, 179], "mess": 175, "govern": 175, "prime": 175, "strictli": 175, "ask": 175, "untouch": 175, "nsummari": 175, "robust": 175, "csv": [175, 176], "onlin": 175, "forum": 175, "panda": 175, "pd": 175, "df": 175, "read_csv": 175, "your_fil": 175, "nrow": 175, "tolist": 175, "iloc": 175, "gp": 175, "receiv": 175, "commun": [175, 176, 177], "satellit": 175, "thing": [175, 181], "message_convert": 175, "input_msg": 175, "output_msg": 175, "assistant_messag": 175, "But": [175, 177, 179, 180], "mistralchatformat": 175, "custom_dataset": 175, "2048": 175, "data_fil": [175, 176], "honor": 175, "copi": [175, 177, 178, 179, 181], "folder": 175, "custom_8b_lora_single_devic": 175, "steer": 176, "wheel": 176, "publicli": 176, "great": [176, 177], "hood": [176, 177, 181], "text_completion_dataset": 176, "padded_col": 176, "upper": 176, "constraint": [176, 180], "slow": [176, 181], "signific": 176, "speedup": [176, 179], "minim": [176, 178, 180, 181], "my_data": 176, "fix": 176, "goal": 176, "agnost": 176, "respond": 176, "anim": 176, "plant": 176, "miner": 176, "oak": 176, "copper": 176, "ore": 176, "eleph": 176, "customtempl": 176, "cl": 176, "chat_dataset": 176, "quit": [176, 181], "incorpor": 176, "advanc": 176, "customchatformat": 176, "concatdataset": 176, "drive": 176, "rajpurkar": 176, "io": 176, "squad": 176, "explor": 176, "rlhf": 176, "few": [176, 179, 180, 181], "adjust": 176, "chosen_messag": 176, "transformed_sampl": 176, "key_chosen": 176, "rejected_messag": 176, "key_reject": 176, "c_mask": 176, "np": 176, "cross_entropy_ignore_idx": 176, "r_mask": 176, "stack_exchanged_paired_dataset": 176, "had": 176, "stackexchangedpairedtempl": 176, "response_j": 176, "response_k": 176, "rl": 176, "favorit": [177, 179, 180], "seemlessli": 177, "beyond": [177, 181], "connect": 177, "amount": 177, "natur": 177, "export": 177, "mobil": 177, "phone": 177, "leverag": [177, 179, 181], "plai": 177, "freez": [177, 180], "percentag": 177, "learnabl": 177, "keep": [177, 180], "16gb": [177, 180], "rtx": 177, "3090": 177, "4090": 177, "hour": 177, "7b_qlora_single_devic": [177, 178, 181], "473": 177, "98": [177, 181], "gb": [177, 179, 180, 181], "50": 177, "484": 177, "01": [177, 178], "fact": [177, 179, 180], "third": 177, "realli": 177, "eleuther_ev": [177, 179], "eleuther_evalu": [177, 179], "lm_eval": [177, 179], "plan": 177, "custom_eval_config": [177, 179], "truthfulqa_mc2": [177, 179, 180], "measur": [177, 179], "propens": [177, 179], "shot": [177, 179], "accuraci": [177, 179, 180, 181], "baselin": [177, 180], "324": 177, "loglikelihood": 177, "195": 177, "121": 177, "27": 177, "second": [177, 179, 180, 181], "197": 177, "acc": 177, "388": 177, "38": 177, "shown": 177, "489": 177, "48": [177, 181], "seem": 177, "custom_generation_config": [177, 179], "kick": 177, "300": 177, "interest": 177, "site": 177, "visit": 177, "bai": 177, "area": 177, "92": [177, 179], "exploratorium": 177, "san": 177, "francisco": 177, "magazin": 177, "awesom": 177, "bridg": 177, "pretti": 177, "cool": 177, "96": [177, 181], "61": 177, "sec": [177, 179], "25": 177, "83": 177, "99": [177, 180], "72": 177, "littl": 177, "saw": 177, "took": [177, 179], "torchao": [177, 179, 181], "bit": [177, 179, 180, 181], "custom_quantization_config": [177, 179], "68": 177, "76": 177, "69": 177, "95": [177, 179], "67": 177, "engin": [177, 179], "fullmodeltorchtunecheckpoint": [177, 179], "groupsiz": [177, 179], "256": [177, 179], "park": 177, "sit": 177, "hill": 177, "beauti": 177, "62": [177, 179], "85": 177, "sped": 177, "almost": [177, 179, 180], "3x": [177, 179], "benefit": 177, "doesn": 177, "fast": 177, "clone": [177, 180, 181], "assumpt": 177, "satisfi": 177, "new_dir": 177, "output_dict": 177, "sd_1": 177, "sd_2": 177, "dump": 177, "convert_hf_checkpoint": 177, "checkpoint_path": 177, "justin": 177, "school": 177, "math": 177, "teacher": 177, "ws": 177, "94": [177, 179], "28": 177, "bandwidth": [177, 179], "1391": 177, "84": 177, "thats": 177, "seamlessli": 177, "authent": [177, 178], "hopefulli": 177, "gave": 177, "grant": 178, "minut": 178, "agreement": 178, "altern": 178, "hackabl": 178, "singularli": 178, "technic": 178, "purpos": [178, 179], "depth": 178, "principl": 178, "boilerpl": 178, "substanti": [178, 180], "custom_config": 178, "replic": 178, "lorafinetunerecipesingledevic": 178, "lora_finetune_output": 178, "log_1713194212": 178, "52": 178, "3697006702423096": 178, "25880": [178, 181], "24": [178, 179], "55": 178, "83it": 178, "monitor": 178, "tqdm": 178, "interv": 178, "e2": 178, "focu": 179, "128": [179, 180], "theta": 179, "gain": 179, "illustr": 179, "basic": 179, "observ": 179, "consum": [179, 181], "vram": [179, 180], "overal": 179, "8b_qlora_single_devic": 179, "coupl": [179, 180, 181], "meta_model_0": 179, "122": 179, "sarah": 179, "busi": 179, "mum": 179, "young": 179, "children": 179, "live": 179, "north": 179, "east": 179, "england": 179, "135": 179, "88": 179, "138": 179, "346": 179, "09": 179, "139": 179, "31": 179, "far": 179, "drill": 179, "90": 179, "93": 179, "91": 179, "104": 179, "four": [179, 180], "again": 179, "jake": 179, "disciplin": 179, "passion": 179, "draw": 179, "paint": 179, "57": [179, 180, 181], "broader": 179, "teach": 180, "straight": 180, "unfamiliar": 180, "oppos": [180, 181], "momentum": 180, "relat": 180, "aghajanyan": 180, "et": 180, "al": 180, "hypothes": 180, "intrins": 180, "often": 180, "eight": 180, "practic": 180, "imag": 180, "blue": 180, "rememb": 180, "approx": 180, "15m": 180, "8192": 180, "65k": 180, "frozen_out": [180, 181], "lora_out": [180, 181], "base_model": 180, "choos": 180, "lora_model": 180, "lora_llama_2_7b": [180, 181], "alon": 180, "in_featur": 180, "out_featur": 180, "inplac": 180, "feel": 180, "free": 180, "whenev": 180, "peft_util": 180, "set_trainable_param": 180, "fetch": 180, "lora_param": 180, "total_param": 180, "trainable_param": 180, "2f": 180, "6742609920": 180, "4194304": 180, "7b_lora": 180, "my_model_checkpoint_path": [180, 181], "tokenizer_checkpoint": [180, 181], "my_tokenizer_checkpoint_path": [180, 181], "factori": 180, "benefici": 180, "impact": 180, "minor": 180, "good": 180, "64": 180, "lora_experiment_1": 180, "smooth": [180, 181], "curv": [180, 181], "500": 180, "ran": 180, "footprint": 180, "commod": 180, "cogniz": 180, "ax": 180, "parallel": 180, "truthfulqa": 180, "previous": 180, "475": 180, "87": 180, "508": 180, "86": 180, "504": 180, "04": 180, "514": 180, "lowest": 180, "absolut": 180, "4gb": 180, "tradeoff": 180, "potenti": 180, "highli": 181, "vanilla": 181, "held": 181, "therefor": 181, "bespok": 181, "normalfloat": 181, "8x": 181, "retain": 181, "vast": 181, "major": 181, "degrad": 181, "normatfloat": 181, "doubl": 181, "themselv": 181, "deepdiv": 181, "idea": 181, "distinct": 181, "de": 181, "incur": 181, "counterpart": 181, "set_default_devic": 181, "qlora_linear": 181, "memory_alloc": 181, "177": 181, "152": 181, "del": 181, "empty_cach": 181, "lora_linear": 181, "081": 181, "344": 181, "qlora_llama2_7b": 181, "qlora_model": 181, "essenti": 181, "reparametrize_as_dtype_state_dict_post_hook": 181, "35": 181, "40": 181, "29": 181, "slower": 181, "149": 181, "9157477021217346": 181, "02": 181, "08": 181, "15it": 181, "nightli": 181, "200": 181, "hundr": 181, "228": 181, "8158286809921265": 181, "59": 181, "95it": 181, "exercis": 181, "portion": 181, "linear_nf4": 181, "to_nf4": 181, "linear_weight": 181, "autograd": 181, "regular": 181, "incom": 181}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "StackExchangedPairedTemplate"], [23, 1, 1, "", "SummarizeTemplate"], [24, 0, 1, "", "get_openai_messages"], [25, 0, 1, "", "get_sharegpt_messages"], [26, 0, 1, "", "truncate"], [27, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.StackExchangedPairedTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[23, 2, 1, "", "format"]], "torchtune.datasets": [[28, 1, 1, "", "ChatDataset"], [29, 1, 1, "", "ConcatDataset"], [30, 1, 1, "", "InstructDataset"], [31, 1, 1, "", "PackedDataset"], [32, 1, 1, "", "PreferenceDataset"], [33, 1, 1, "", "TextCompletionDataset"], [34, 0, 1, "", "alpaca_cleaned_dataset"], [35, 0, 1, "", "alpaca_dataset"], [36, 0, 1, "", "chat_dataset"], [37, 0, 1, "", "cnn_dailymail_articles_dataset"], [38, 0, 1, "", "grammar_dataset"], [39, 0, 1, "", "instruct_dataset"], [40, 0, 1, "", "samsum_dataset"], [41, 0, 1, "", "slimorca_dataset"], [42, 0, 1, "", "stack_exchanged_paired_dataset"], [43, 0, 1, "", "text_completion_dataset"], [44, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[45, 0, 1, "", "code_llama2_13b"], [46, 0, 1, "", "code_llama2_70b"], [47, 0, 1, "", "code_llama2_7b"], [48, 0, 1, "", "lora_code_llama2_13b"], [49, 0, 1, "", "lora_code_llama2_70b"], [50, 0, 1, "", "lora_code_llama2_7b"], [51, 0, 1, "", "qlora_code_llama2_13b"], [52, 0, 1, "", "qlora_code_llama2_70b"], [53, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[54, 1, 1, "", "GemmaTokenizer"], [55, 0, 1, "", "gemma"], [56, 0, 1, "", "gemma_2b"], [57, 0, 1, "", "gemma_7b"], [58, 0, 1, "", "gemma_tokenizer"], [59, 0, 1, "", "lora_gemma"], [60, 0, 1, "", "lora_gemma_2b"], [61, 0, 1, "", "lora_gemma_7b"], [62, 0, 1, "", "qlora_gemma_2b"], [63, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[54, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[64, 1, 1, "", "Llama2Tokenizer"], [65, 0, 1, "", "llama2"], [66, 0, 1, "", "llama2_13b"], [67, 0, 1, "", "llama2_70b"], [68, 0, 1, "", "llama2_7b"], [69, 0, 1, "", "llama2_tokenizer"], [70, 0, 1, "", "lora_llama2"], [71, 0, 1, "", "lora_llama2_13b"], [72, 0, 1, "", "lora_llama2_70b"], [73, 0, 1, "", "lora_llama2_7b"], [74, 0, 1, "", "qlora_llama2_13b"], [75, 0, 1, "", "qlora_llama2_70b"], [76, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[64, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[77, 1, 1, "", "Llama3Tokenizer"], [78, 0, 1, "", "llama3"], [79, 0, 1, "", "llama3_70b"], [80, 0, 1, "", "llama3_8b"], [81, 0, 1, "", "llama3_tokenizer"], [82, 0, 1, "", "lora_llama3"], [83, 0, 1, "", "lora_llama3_70b"], [84, 0, 1, "", "lora_llama3_8b"], [85, 0, 1, "", "qlora_llama3_70b"], [86, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[77, 2, 1, "", "decode"], [77, 2, 1, "", "tokenize_message"], [77, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[87, 0, 1, "", "llama3_1"], [88, 0, 1, "", "llama3_1_70b"], [89, 0, 1, "", "llama3_1_8b"], [90, 0, 1, "", "lora_llama3_1"], [91, 0, 1, "", "lora_llama3_1_70b"], [92, 0, 1, "", "lora_llama3_1_8b"], [93, 0, 1, "", "qlora_llama3_1_70b"], [94, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[95, 1, 1, "", "MistralTokenizer"], [96, 0, 1, "", "lora_mistral"], [97, 0, 1, "", "lora_mistral_7b"], [98, 0, 1, "", "lora_mistral_classifier"], [99, 0, 1, "", "lora_mistral_classifier_7b"], [100, 0, 1, "", "mistral"], [101, 0, 1, "", "mistral_7b"], [102, 0, 1, "", "mistral_classifier"], [103, 0, 1, "", "mistral_classifier_7b"], [104, 0, 1, "", "mistral_tokenizer"], [105, 0, 1, "", "qlora_mistral_7b"], [106, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.mistral.MistralTokenizer": [[95, 2, 1, "", "decode"], [95, 2, 1, "", "encode"], [95, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[107, 1, 1, "", "Phi3MiniTokenizer"], [108, 0, 1, "", "lora_phi3"], [109, 0, 1, "", "lora_phi3_mini"], [110, 0, 1, "", "phi3"], [111, 0, 1, "", "phi3_mini"], [112, 0, 1, "", "phi3_mini_tokenizer"], [113, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[107, 2, 1, "", "decode"], [107, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[114, 1, 1, "", "CausalSelfAttention"], [115, 1, 1, "", "FeedForward"], [116, 1, 1, "", "KVCache"], [117, 1, 1, "", "RMSNorm"], [118, 1, 1, "", "RotaryPositionalEmbeddings"], [119, 1, 1, "", "TransformerDecoder"], [120, 1, 1, "", "TransformerDecoderLayer"], [122, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[114, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[115, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[116, 2, 1, "", "reset"], [116, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[117, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[118, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[119, 2, 1, "", "caches_are_enabled"], [119, 2, 1, "", "forward"], [119, 2, 1, "", "reset_caches"], [119, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[120, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[121, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[123, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[123, 2, 1, "", "forward"]], "torchtune.modules.peft": [[124, 1, 1, "", "AdapterModule"], [125, 1, 1, "", "LoRALinear"], [126, 0, 1, "", "disable_adapter"], [127, 0, 1, "", "get_adapter_params"], [128, 0, 1, "", "set_trainable_params"], [129, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [130, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[124, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[125, 2, 1, "", "adapter_params"], [125, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[131, 1, 1, "", "SentencePieceBaseTokenizer"], [132, 1, 1, "", "TikTokenBaseTokenizer"], [133, 0, 1, "", "parse_hf_tokenizer_json"], [134, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[131, 2, 1, "", "decode"], [131, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[132, 2, 1, "", "decode"], [132, 2, 1, "", "encode"]], "torchtune.utils": [[135, 4, 1, "", "FSDPPolicyType"], [136, 1, 1, "", "FullModelHFCheckpointer"], [137, 1, 1, "", "FullModelMetaCheckpointer"], [138, 1, 1, "", "FullModelTorchTuneCheckpointer"], [139, 1, 1, "", "ModelType"], [140, 1, 1, "", "OptimizerInBackwardWrapper"], [141, 1, 1, "", "TuneRecipeArgumentParser"], [142, 0, 1, "", "create_optim_in_bwd_wrapper"], [143, 0, 1, "", "generate"], [144, 0, 1, "", "get_device"], [145, 0, 1, "", "get_dtype"], [146, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [147, 0, 1, "", "get_logger"], [148, 0, 1, "", "get_memory_stats"], [149, 0, 1, "", "get_quantizer_mode"], [150, 0, 1, "", "get_world_size_and_rank"], [151, 0, 1, "", "init_distributed"], [152, 0, 1, "", "is_distributed"], [153, 0, 1, "", "log_memory_stats"], [154, 0, 1, "", "lora_fsdp_wrap_policy"], [159, 0, 1, "", "padded_collate"], [160, 0, 1, "", "padded_collate_dpo"], [161, 0, 1, "", "register_optim_in_bwd_hooks"], [162, 0, 1, "", "set_activation_checkpointing"], [163, 0, 1, "", "set_default_dtype"], [164, 0, 1, "", "set_seed"], [165, 0, 1, "", "setup_torch_profiler"], [166, 0, 1, "", "torch_version_ge"], [167, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[136, 2, 1, "", "load_checkpoint"], [136, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[137, 2, 1, "", "load_checkpoint"], [137, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[138, 2, 1, "", "load_checkpoint"], [138, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[139, 3, 1, "", "GEMMA"], [139, 3, 1, "", "LLAMA2"], [139, 3, 1, "", "LLAMA3"], [139, 3, 1, "", "MISTRAL"], [139, 3, 1, "", "MISTRAL_REWARD"], [139, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[140, 2, 1, "", "get_optim_key"], [140, 2, 1, "", "load_state_dict"], [140, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[141, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[155, 1, 1, "", "DiskLogger"], [156, 1, 1, "", "StdoutLogger"], [157, 1, 1, "", "TensorBoardLogger"], [158, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[155, 2, 1, "", "close"], [155, 2, 1, "", "log"], [155, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[156, 2, 1, "", "close"], [156, 2, 1, "", "log"], [156, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[157, 2, 1, "", "close"], [157, 2, 1, "", "log"], [157, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[158, 2, 1, "", "close"], [158, 2, 1, "", "log"], [158, 2, 1, "", "log_config"], [158, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 135, 170, 172, 174, 177, 179, 180, 181], "config": [0, 7, 8, 174, 178], "data": [1, 5, 175], "text": [1, 176, 179], "templat": [1, 175, 176], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 175, 176], "exampl": 2, "gener": [2, 143, 177, 179], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 174, 177, 178, 179, 180], "llama3": [3, 78, 175, 179], "1": 3, "llama2": [3, 65, 175, 177, 180, 181], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 100], "gemma": [3, 55], "modul": 4, "compon": [4, 7], "build": [4, 171, 181], "block": 4, "base": 4, "token": [4, 175], "util": [4, 5, 135], "peft": 4, "loss": 4, "checkpoint": [5, 6, 9, 177], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 176, 180, 181], "manag": 5, "perform": [5, 180], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 172, 177], "format": [6, 176], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 177, 180, 181], "put": [6, 181], "thi": 6, "all": [6, 7, 181], "togeth": [6, 181], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 176], "us": [7, 8, 175, 177, 181], "instanti": [7, 10], "referenc": 7, "other": [7, 177], "field": 7, "interpol": 7, "valid": [7, 13, 174], "your": [7, 8, 177, 178], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 172, 180, 181], "ar": 8, "recip": [8, 174, 178, 180], "script": 8, "run": [8, 174, 177], "cli": [8, 174], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "stackexchangedpairedtempl": 22, "summarizetempl": 23, "get_openai_messag": 24, "get_sharegpt_messag": 25, "truncat": 26, "validate_messag": 27, "chatdataset": 28, "concatdataset": 29, "instructdataset": 30, "packeddataset": 31, "preferencedataset": 32, "textcompletiondataset": 33, "alpaca_cleaned_dataset": 34, "alpaca_dataset": 35, "chat_dataset": 36, "cnn_dailymail_articles_dataset": 37, "grammar_dataset": 38, "instruct_dataset": 39, "samsum_dataset": 40, "slimorca_dataset": 41, "stack_exchanged_paired_dataset": 42, "text_completion_dataset": 43, "wikitext_dataset": 44, "code_llama2_13b": 45, "code_llama2_70b": 46, "code_llama2_7b": 47, "lora_code_llama2_13b": 48, "lora_code_llama2_70b": 49, "lora_code_llama2_7b": 50, "qlora_code_llama2_13b": 51, "qlora_code_llama2_70b": 52, "qlora_code_llama2_7b": 53, "gemmatoken": 54, "gemma_2b": 56, "gemma_7b": 57, "gemma_token": 58, "lora_gemma": 59, "lora_gemma_2b": 60, "lora_gemma_7b": 61, "qlora_gemma_2b": 62, "qlora_gemma_7b": 63, "llama2token": 64, "llama2_13b": 66, "llama2_70b": 67, "llama2_7b": 68, "llama2_token": 69, "lora_llama2": 70, "lora_llama2_13b": 71, "lora_llama2_70b": 72, "lora_llama2_7b": 73, "qlora_llama2_13b": 74, "qlora_llama2_70b": 75, "qlora_llama2_7b": 76, "llama3token": 77, "llama3_70b": 79, "llama3_8b": 80, "llama3_token": 81, "lora_llama3": 82, "lora_llama3_70b": 83, "lora_llama3_8b": 84, "qlora_llama3_70b": 85, "qlora_llama3_8b": 86, "llama3_1": 87, "llama3_1_70b": 88, "llama3_1_8b": 89, "lora_llama3_1": 90, "lora_llama3_1_70b": 91, "lora_llama3_1_8b": 92, "qlora_llama3_1_70b": 93, "qlora_llama3_1_8b": 94, "mistraltoken": 95, "lora_mistr": 96, "lora_mistral_7b": 97, "lora_mistral_classifi": 98, "lora_mistral_classifier_7b": 99, "mistral_7b": 101, "mistral_classifi": 102, "mistral_classifier_7b": 103, "mistral_token": 104, "qlora_mistral_7b": 105, "qlora_mistral_classifier_7b": 106, "phi3minitoken": 107, "lora_phi3": 108, "lora_phi3_mini": 109, "phi3": 110, "phi3_mini": 111, "phi3_mini_token": 112, "qlora_phi3_mini": 113, "causalselfattent": 114, "todo": [114, 120], "feedforward": 115, "kvcach": 116, "rmsnorm": 117, "rotarypositionalembed": 118, "transformerdecod": 119, "transformerdecoderlay": 120, "reparametrize_as_dtype_state_dict_post_hook": 121, "get_cosine_schedule_with_warmup": 122, "dpoloss": 123, "adaptermodul": 124, "loralinear": 125, "disable_adapt": 126, "get_adapter_param": 127, "set_trainable_param": 128, "validate_missing_and_unexpected_for_lora": 129, "validate_state_dict_for_lora": 130, "sentencepiecebasetoken": 131, "tiktokenbasetoken": 132, "parse_hf_tokenizer_json": 133, "tokenize_messages_no_special_token": 134, "fsdppolicytyp": 135, "fullmodelhfcheckpoint": 136, "fullmodelmetacheckpoint": 137, "fullmodeltorchtunecheckpoint": 138, "modeltyp": 139, "optimizerinbackwardwrapp": 140, "tunerecipeargumentpars": 141, "create_optim_in_bwd_wrapp": 142, "get_devic": 144, "get_dtyp": 145, "get_full_finetune_fsdp_wrap_polici": 146, "get_logg": 147, "get_memory_stat": 148, "get_quantizer_mod": 149, "get_world_size_and_rank": 150, "init_distribut": 151, "is_distribut": 152, "log_memory_stat": 153, "lora_fsdp_wrap_polici": 154, "disklogg": 155, "stdoutlogg": 156, "tensorboardlogg": 157, "wandblogg": 158, "padded_col": 159, "padded_collate_dpo": 160, "register_optim_in_bwd_hook": 161, "set_activation_checkpoint": 162, "set_default_dtyp": 163, "set_se": 164, "setup_torch_profil": 165, "torch_version_g": 166, "validate_expected_param_dtyp": 167, "comput": [169, 173], "time": [169, 173], "welcom": 170, "document": 170, "get": [170, 174, 179], "start": [170, 174], "tutori": 170, "instal": 171, "instruct": [171, 176, 179], "via": [171, 179], "pypi": 171, "git": 171, "clone": 171, "nightli": 171, "kei": 172, "concept": 172, "design": 172, "principl": 172, "download": [174, 177, 178], "list": 174, "built": [174, 176], "copi": 174, "fine": [175, 176, 178, 179], "tune": [175, 176, 178, 179], "chat": [175, 176], "chang": 175, "from": [175, 181], "prompt": 175, "special": 175, "when": 175, "should": 175, "i": 175, "custom": [175, 176], "hug": [176, 177], "face": [176, 177], "set": 176, "max": 176, "sequenc": 176, "length": 176, "sampl": 176, "pack": 176, "unstructur": 176, "corpu": 176, "multipl": 176, "local": 176, "remot": 176, "fulli": 176, "end": 177, "workflow": 177, "7b": 177, "finetun": [177, 180, 181], "evalu": [177, 179], "eleutherai": [177, 179], "s": [177, 179], "eval": [177, 179], "har": [177, 179], "speed": 177, "up": 177, "quantiz": [177, 179], "librari": 177, "upload": 177, "hub": 177, "first": 178, "llm": 178, "select": 178, "modifi": 178, "train": 178, "next": 178, "step": 178, "meta": 179, "8b": 179, "access": 179, "our": 179, "faster": 179, "how": 180, "doe": 180, "work": 180, "appli": 180, "trade": 180, "off": 180, "qlora": 181, "save": 181, "deep": 181, "dive": 181}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})