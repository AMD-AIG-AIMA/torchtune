Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.loss.IPOLoss", "generated/torchtune.modules.loss.PPOLoss", "generated/torchtune.modules.loss.RSOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.left_padded_collate", "generated/torchtune.modules.rlhf.padded_collate_dpo", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.loss.IPOLoss.rst", "generated/torchtune.modules.loss.PPOLoss.rst", "generated/torchtune.modules.loss.RSOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.left_padded_collate.rst", "generated/torchtune.modules.rlhf.padded_collate_dpo.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Llama2ChatFormat", "Message", "MistralChatFormat", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.Role", "ShareGPTToMessages", "StackExchangedPairedTemplate", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "left_padded_collate", "padded_collate_dpo", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 19, 23, 28, 36, 37, 38, 39, 42, 46, 50, 91, 117, 124, 125, 126, 201, 205, 206, 209, 211, 212, 213], "prompt": [1, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 34, 36, 38, 39, 42, 43, 45, 46, 47, 48, 65, 75, 91, 109, 121, 134, 160, 175, 207, 208, 210], "chat": [1, 2, 15, 16, 20, 21, 27, 30, 31, 34, 39, 43, 75, 126], "includ": [1, 6, 7, 8, 15, 19, 24, 25, 39, 55, 66, 75, 76, 92, 101, 114, 126, 144, 155, 164, 168, 169, 173, 203, 205, 206, 207, 208, 209, 210, 211, 213], "some": [1, 6, 7, 16, 112, 146, 147, 201, 203, 205, 206, 207, 208, 209, 211, 212, 213], "specif": [1, 4, 7, 8, 10, 39, 45, 47, 48, 156, 178, 206, 207, 208, 212, 213], "format": [1, 2, 5, 14, 15, 16, 19, 21, 22, 23, 28, 30, 34, 36, 38, 39, 42, 43, 45, 46, 47, 48, 75, 91, 156, 165, 168, 169, 170, 171, 205, 206, 208, 209, 210, 211], "differ": [1, 7, 9, 35, 36, 38, 52, 53, 54, 109, 136, 139, 153, 157, 171, 198, 203, 205, 206, 208, 210, 211, 212, 213], "dataset": [1, 5, 7, 14, 18, 19, 20, 22, 27, 28, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 139, 140, 203, 209, 210, 212], "model": [1, 2, 6, 7, 8, 10, 16, 18, 22, 23, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 155, 156, 159, 160, 161, 168, 169, 170, 171, 174, 175, 178, 180, 186, 192, 193, 201, 203, 206, 207, 213], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 19, 20, 21, 22, 27, 28, 31, 34, 35, 36, 37, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 67, 68, 77, 78, 79, 80, 95, 109, 115, 117, 126, 129, 134, 135, 136, 138, 139, 140, 142, 143, 146, 149, 157, 159, 162, 164, 165, 168, 169, 170, 172, 173, 174, 175, 189, 190, 192, 200, 202, 204, 205, 207, 208, 209, 210, 211, 212], "common": [1, 2, 4, 7, 160, 205, 206, 207, 210, 211, 212], "json": [1, 6, 20, 27, 30, 31, 34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 95, 126, 159, 168, 205, 207, 208, 212], "schema": 1, "convers": [1, 6, 15, 16, 21, 23, 27, 30, 31, 33, 34, 39, 43, 48, 168, 170, 171, 203, 206, 207, 208, 211, 213], "list": [1, 6, 7, 15, 16, 21, 22, 23, 24, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 59, 60, 61, 62, 63, 64, 65, 70, 71, 72, 73, 74, 75, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 123, 127, 136, 143, 144, 148, 149, 152, 153, 155, 156, 157, 158, 160, 162, 163, 164, 168, 169, 170, 173, 175, 179, 191, 206, 207, 208, 209, 210, 212], "miscellan": 1, "us": [1, 2, 3, 4, 6, 9, 10, 12, 16, 18, 21, 22, 24, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 75, 76, 82, 92, 95, 96, 101, 104, 122, 126, 128, 129, 132, 133, 134, 135, 136, 137, 139, 141, 145, 148, 150, 151, 157, 158, 162, 163, 165, 167, 168, 169, 171, 172, 173, 175, 176, 177, 178, 180, 186, 187, 188, 189, 190, 195, 201, 202, 203, 205, 207, 209, 210, 211, 212], "modifi": [1, 7, 8, 9, 137, 203, 208, 210, 211, 212, 213], "For": [2, 5, 6, 7, 8, 20, 24, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 66, 70, 75, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 134, 136, 163, 164, 168, 173, 174, 181, 190, 193, 195, 202, 205, 206, 207, 208, 209, 210, 211, 212, 213], "detail": [2, 6, 34, 36, 38, 39, 40, 41, 43, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 75, 116, 131, 136, 141, 167, 178, 186, 195, 205, 207, 208, 209, 210, 211, 212, 213], "usag": [2, 137, 171, 172, 196, 202, 205, 207, 208, 209, 210, 212, 213], "guid": [2, 7, 9, 203, 206, 207, 209, 211], "pleas": [2, 5, 17, 29, 52, 53, 54, 55, 62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 136, 167, 178, 186, 193, 202, 208, 210, 213], "see": [2, 5, 6, 9, 17, 21, 23, 29, 34, 36, 38, 39, 40, 41, 43, 45, 46, 47, 48, 50, 51, 62, 63, 64, 73, 74, 75, 87, 88, 89, 90, 99, 100, 107, 108, 116, 119, 120, 127, 131, 136, 143, 167, 171, 173, 178, 179, 186, 190, 193, 195, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "our": [2, 6, 8, 203, 206, 207, 208, 209, 211, 212, 213], "tutori": [2, 6, 75, 193, 203, 206, 207, 208, 209, 210, 211, 212, 213], "support": [2, 3, 6, 8, 9, 10, 22, 23, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 70, 82, 96, 104, 110, 112, 122, 125, 126, 128, 130, 136, 142, 144, 163, 165, 169, 170, 172, 177, 180, 181, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "sever": 2, "wide": 2, "help": [2, 6, 21, 134, 136, 168, 173, 201, 202, 203, 205, 206, 207, 208, 209, 212, 213], "quickli": [2, 7, 24, 40, 206, 207], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 24, 28, 34, 40, 53, 54, 55, 75, 136, 189, 190, 201, 202, 203, 205, 206, 207, 210, 211, 212, 213], "fine": [2, 6, 8, 9, 22, 37, 39, 50, 75, 201, 203, 208, 211, 212], "tune": [2, 3, 6, 7, 8, 9, 12, 22, 37, 39, 50, 75, 201, 202, 203, 205, 208, 211, 212, 213], "also": [2, 6, 7, 8, 9, 10, 35, 43, 46, 50, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 126, 128, 134, 144, 176, 178, 180, 186, 190, 202, 205, 206, 207, 208, 209, 210, 211, 212, 213], "like": [2, 4, 6, 7, 8, 9, 34, 126, 136, 170, 202, 205, 206, 207, 208, 209, 211, 212], "These": [2, 4, 6, 7, 8, 10, 37, 136, 162, 173, 206, 207, 208, 209, 210, 211, 212, 213], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 19, 20, 21, 23, 24, 25, 27, 28, 33, 36, 37, 38, 39, 42, 43, 45, 46, 47, 48, 53, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 75, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 134, 136, 144, 145, 148, 149, 151, 153, 162, 164, 167, 168, 169, 171, 172, 174, 175, 177, 180, 184, 186, 196, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "especi": [2, 203, 205, 208], "specifi": [2, 6, 7, 8, 10, 18, 39, 43, 76, 82, 92, 96, 101, 104, 128, 134, 135, 167, 175, 178, 181, 186, 190, 193, 196, 205, 206, 207, 208, 209, 210, 212, 213], "yaml": [2, 7, 8, 10, 11, 12, 35, 43, 46, 50, 173, 190, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "config": [2, 6, 9, 10, 11, 12, 13, 35, 43, 46, 50, 128, 148, 168, 172, 173, 190, 196, 203, 206, 207, 208, 210, 211, 212, 213], "represent": [2, 211, 212, 213], "abov": [2, 3, 6, 137, 164, 184, 202, 208, 210, 211, 212, 213], "all": [3, 4, 8, 13, 24, 35, 37, 39, 43, 55, 95, 126, 128, 129, 134, 136, 137, 145, 161, 163, 164, 168, 172, 173, 174, 184, 192, 198, 199, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212], "famili": [3, 6, 8, 42, 44, 48, 49, 51, 171, 203, 205, 210], "request": [3, 14, 177, 207, 208], "access": [3, 6, 7, 8, 35, 168, 174, 205, 207, 208, 209], "hug": [3, 6, 16, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 95, 126, 138, 159, 203, 205, 209, 210], "face": [3, 6, 16, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 95, 126, 138, 159, 203, 205, 209, 210], "To": [3, 6, 7, 8, 9, 37, 136, 168, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "download": [3, 6, 199, 202, 206, 207, 210, 211, 212, 213], "8b": [3, 94, 98, 100, 103, 106, 108, 123, 205, 206, 212], "meta": [3, 6, 21, 75, 91, 133, 168, 169, 205, 206, 208, 209], "hf": [3, 6, 121, 139, 140, 142, 168, 205, 206, 208, 209, 210], "token": [3, 6, 7, 8, 22, 32, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 65, 66, 69, 70, 75, 76, 81, 82, 91, 92, 95, 96, 101, 104, 109, 110, 112, 114, 116, 118, 121, 122, 124, 126, 128, 133, 134, 135, 136, 141, 151, 152, 154, 155, 156, 157, 158, 159, 160, 162, 175, 178, 191, 205, 207, 208, 209, 210, 211, 212, 213], "hf_token": 3, "70b": [3, 57, 60, 63, 78, 84, 88, 93, 97, 99, 102, 105, 107, 210], "ignor": [3, 6, 50, 121, 128, 129, 205], "pattern": [3, 158, 205], "origin": [3, 6, 41, 42, 137, 144, 206, 208, 210, 211, 212, 213], "consolid": [3, 6], "weight": [3, 6, 8, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 128, 137, 139, 143, 144, 148, 157, 168, 169, 170, 171, 181, 186, 190, 201, 205, 206, 208, 209, 210, 211, 212, 213], "you": [3, 6, 7, 8, 9, 10, 19, 21, 22, 24, 34, 36, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 136, 164, 171, 173, 175, 189, 190, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "can": [3, 4, 6, 7, 8, 9, 10, 13, 22, 24, 25, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 75, 109, 132, 133, 136, 145, 157, 158, 163, 165, 167, 168, 171, 173, 178, 186, 189, 190, 193, 196, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "instead": [3, 6, 8, 37, 43, 46, 50, 55, 129, 131, 136, 144, 205, 210, 211, 212], "The": [3, 6, 7, 8, 9, 12, 13, 14, 15, 16, 19, 21, 22, 23, 28, 33, 34, 35, 36, 37, 38, 39, 45, 47, 49, 52, 53, 54, 55, 59, 60, 61, 65, 70, 71, 72, 75, 82, 83, 84, 85, 86, 91, 96, 97, 98, 104, 105, 106, 109, 110, 112, 121, 122, 123, 130, 132, 133, 136, 137, 138, 139, 140, 141, 142, 145, 150, 152, 155, 156, 157, 158, 159, 160, 162, 164, 165, 166, 167, 168, 170, 173, 176, 177, 179, 181, 190, 194, 196, 197, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "reus": [3, 203], "llama3_token": [3, 175, 206, 210], "builder": [3, 6, 41, 44, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 71, 72, 73, 74, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 97, 98, 99, 100, 102, 103, 105, 106, 107, 108, 111, 113, 115, 117, 119, 120, 123, 125, 127, 206, 207, 213], "class": [3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 34, 35, 36, 37, 38, 39, 40, 43, 46, 52, 53, 54, 65, 75, 80, 91, 109, 112, 116, 117, 121, 128, 129, 130, 131, 132, 133, 134, 135, 136, 139, 140, 141, 142, 143, 144, 146, 147, 155, 156, 157, 158, 161, 162, 168, 169, 170, 171, 172, 173, 187, 188, 189, 190, 206, 207, 209, 211, 213], "7b": [3, 6, 36, 38, 40, 42, 44, 46, 50, 51, 58, 61, 64, 68, 72, 79, 80, 85, 86, 89, 90, 111, 113, 115, 117, 120, 168, 169, 206, 209, 210, 211, 213], "2": [3, 6, 9, 33, 37, 48, 52, 53, 65, 75, 91, 109, 121, 128, 136, 140, 141, 152, 153, 154, 157, 158, 160, 163, 164, 165, 168, 169, 181, 191, 194, 195, 196, 197, 206, 208, 209, 210, 211, 212], "13b": [3, 6, 56, 59, 62, 77, 83, 87], "codellama": 3, "mini": [3, 121, 123, 124, 125, 126, 127], "4k": [3, 124, 125, 126], "microsoft": [3, 125, 126], "none": [3, 8, 9, 11, 13, 14, 18, 19, 20, 27, 28, 32, 33, 34, 36, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 50, 51, 55, 65, 75, 76, 82, 91, 92, 95, 96, 101, 104, 109, 121, 126, 128, 131, 133, 134, 135, 136, 141, 145, 147, 148, 149, 150, 151, 157, 160, 165, 168, 169, 170, 171, 175, 176, 177, 179, 181, 185, 187, 188, 189, 190, 192, 193, 194, 195, 196, 198, 206, 207, 208, 212], "ai": [3, 39, 115, 128, 190, 206, 210], "v0": 3, "mistralai": [3, 205], "size": [3, 6, 8, 10, 42, 45, 47, 53, 54, 55, 128, 131, 132, 133, 134, 136, 150, 151, 162, 163, 165, 166, 182, 184, 203, 205, 207, 208, 209, 210, 211, 212], "2b": [3, 67, 71], "googl": [3, 67, 68], "gguf": 3, "vision": [3, 39, 55], "compon": [3, 6, 8, 13, 39, 153, 203, 207, 209, 211, 213], "multimod": [3, 22, 39], "encod": [3, 4, 39, 55, 65, 75, 91, 109, 121, 139, 155, 157, 158, 160, 162, 206], "perform": [4, 6, 37, 75, 129, 136, 145, 161, 175, 203, 206, 208, 210, 212, 213], "direct": [4, 8, 139, 153, 202], "text": [4, 22, 24, 25, 34, 36, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 50, 51, 75, 91, 109, 121, 155, 157, 158, 160, 162, 206, 208, 212], "id": [4, 6, 34, 36, 37, 38, 40, 42, 43, 44, 46, 49, 50, 51, 75, 91, 109, 121, 128, 133, 134, 135, 152, 153, 155, 156, 157, 158, 159, 160, 162, 168, 170, 175, 191, 206, 207, 208], "decod": [4, 66, 70, 76, 82, 91, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 134, 155, 157, 158, 175, 206], "typic": [4, 7, 37, 39, 40, 50, 126, 139, 207, 212, 213], "byte": [4, 158, 213], "pair": [4, 7, 14, 49, 140, 153, 158, 191, 207], "underli": [4, 109, 157, 213], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 34, 36, 38, 40, 42, 43, 44, 46, 49, 50, 51, 137, 143, 146, 148, 155, 156, 165, 172, 173, 181, 202, 203, 207, 211, 213], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 19, 24, 28, 30, 31, 32, 34, 36, 38, 39, 40, 43, 44, 46, 50, 51, 54, 75, 109, 130, 137, 146, 147, 148, 149, 155, 156, 157, 160, 168, 169, 170, 172, 175, 183, 186, 195, 198, 205, 206, 207, 209, 211, 212], "function": [4, 6, 7, 8, 10, 12, 34, 53, 54, 55, 128, 129, 136, 137, 139, 141, 145, 148, 149, 153, 167, 168, 175, 176, 182, 186, 195, 203, 206, 207, 213], "preprocess": [4, 37, 136], "imag": [4, 22, 39, 52, 53, 54, 55, 136, 162, 163, 164, 165, 166, 211], "algorithm": [4, 150, 195], "ppo": [4, 139, 141, 150, 151], "offer": 5, "allow": [5, 35, 148, 164, 189, 205, 212, 213], "seamless": 5, "transit": 5, "between": [5, 6, 140, 141, 151, 168, 171, 207, 208, 210, 211, 212, 213], "train": [5, 6, 8, 9, 18, 21, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 75, 128, 130, 133, 134, 135, 137, 138, 139, 168, 169, 170, 177, 180, 186, 196, 201, 203, 205, 206, 207, 208, 210, 211, 212, 213], "interoper": [5, 6, 8, 203, 208, 213], "rest": [5, 206, 213], "ecosystem": [5, 6, 8, 203, 208, 210, 213], "comprehens": 5, "overview": [5, 7, 9, 201, 209, 211, 213], "deep": [5, 6, 7, 8, 9, 203, 209, 210], "dive": [5, 6, 7, 8, 9, 203, 209, 210], "enabl": [5, 7, 8, 9, 35, 59, 60, 61, 62, 63, 64, 71, 72, 73, 74, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 105, 106, 107, 108, 111, 113, 119, 120, 123, 127, 144, 195, 196, 210, 211, 213], "work": [5, 6, 8, 173, 203, 205, 208, 210, 213], "set": [5, 6, 7, 8, 9, 22, 36, 37, 38, 40, 42, 44, 45, 46, 47, 48, 50, 51, 76, 82, 91, 92, 96, 101, 104, 110, 112, 114, 116, 121, 122, 124, 128, 133, 134, 145, 147, 165, 167, 176, 178, 184, 186, 193, 194, 195, 196, 203, 205, 206, 208, 209, 210, 211, 212], "consumpt": [5, 35], "dure": [5, 6, 36, 37, 42, 45, 47, 48, 128, 131, 133, 134, 135, 136, 137, 180, 206, 208, 210, 211, 212, 213], "provid": [5, 6, 7, 8, 10, 14, 16, 20, 23, 32, 35, 36, 37, 38, 55, 134, 136, 139, 145, 170, 173, 176, 178, 190, 196, 203, 205, 206, 207, 208, 209, 210], "debug": [5, 6, 7, 8, 205], "finetun": [5, 6, 7, 8, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 201, 203, 209, 210], "job": [5, 9, 195, 209], "variou": [5, 19], "walk": [6, 8, 189, 203, 206, 207, 208, 209, 212, 213], "through": [6, 7, 8, 9, 55, 129, 136, 145, 203, 205, 206, 207, 208, 209, 212, 213], "design": [6, 8], "behavior": [6, 186, 206, 207], "associ": [6, 7, 8, 55, 66, 76, 92, 101, 114, 175, 208, 211], "util": [6, 7, 8, 9, 10, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 203, 208, 209, 210, 212, 213], "what": [6, 7, 9, 21, 23, 28, 39, 45, 47, 136, 201, 206, 207, 208, 209, 210], "cover": [6, 7, 8, 9, 206, 208, 213], "how": [6, 7, 8, 9, 136, 167, 193, 201, 205, 206, 207, 208, 209, 210, 212, 213], "we": [6, 7, 8, 9, 36, 37, 38, 39, 40, 42, 44, 46, 50, 51, 75, 109, 128, 131, 133, 134, 136, 139, 144, 163, 164, 168, 169, 170, 175, 177, 181, 186, 192, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "them": [6, 7, 35, 36, 38, 46, 65, 75, 109, 121, 129, 136, 137, 160, 205, 206, 207, 208, 211, 212, 213], "scenario": [6, 35, 39], "full": [6, 7, 8, 17, 29, 43, 46, 51, 62, 63, 64, 65, 73, 74, 75, 87, 88, 89, 90, 99, 100, 107, 108, 109, 119, 120, 121, 127, 148, 149, 160, 203, 205, 207, 208, 210, 211, 212], "compos": [6, 136], "which": [6, 7, 8, 35, 36, 37, 40, 42, 45, 47, 48, 50, 59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 91, 96, 97, 98, 104, 105, 106, 109, 110, 111, 112, 113, 122, 123, 128, 133, 134, 135, 136, 138, 148, 149, 157, 164, 168, 169, 170, 172, 177, 187, 190, 193, 203, 205, 206, 207, 208, 209, 211, 212, 213], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 129, 148, 168, 169, 170, 203, 206, 207, 208, 210, 213], "evalu": [6, 8, 201, 203, 209, 211, 213], "gener": [6, 8, 14, 28, 34, 36, 37, 38, 44, 50, 75, 109, 145, 150, 194, 195, 196, 199, 201, 206, 207, 211, 212, 213], "each": [6, 8, 15, 19, 24, 25, 35, 37, 39, 52, 53, 54, 55, 59, 60, 61, 65, 70, 71, 72, 75, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 109, 110, 111, 112, 113, 121, 122, 123, 128, 133, 134, 135, 136, 139, 140, 142, 148, 149, 150, 151, 153, 160, 162, 164, 166, 195, 196, 203, 205, 207, 208, 209, 211, 212], "make": [6, 7, 8, 9, 128, 135, 136, 203, 205, 206, 208, 209, 210, 211, 212, 213], "easi": [6, 8, 203, 207, 211], "understand": [6, 7, 8, 201, 203, 206, 207, 211, 213], "extend": [6, 8, 203], "befor": [6, 24, 33, 36, 37, 38, 52, 53, 55, 66, 70, 128, 134, 135, 136, 144, 158, 168, 205, 208, 212], "let": [6, 7, 9, 205, 206, 207, 208, 209, 210, 211, 213], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 20, 21, 23, 27, 30, 31, 33, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 59, 60, 61, 65, 75, 82, 83, 84, 85, 86, 91, 96, 97, 98, 104, 105, 106, 109, 110, 111, 112, 113, 121, 122, 123, 126, 128, 131, 133, 134, 135, 136, 137, 139, 140, 142, 143, 146, 148, 149, 154, 158, 164, 167, 168, 169, 172, 176, 178, 180, 186, 189, 193, 194, 203, 205, 206, 207, 209, 211, 212, 213], "defin": [6, 7, 8, 24, 34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 129, 143, 144, 146, 151, 207, 209, 211], "concept": [6, 208, 209], "In": [6, 7, 8, 34, 53, 54, 55, 133, 136, 140, 144, 164, 167, 186, 189, 190, 206, 208, 210, 211, 212, 213], "ll": [6, 7, 8, 175, 181, 203, 206, 207, 208, 209, 210, 212, 213], "talk": 6, "about": [6, 8, 136, 139, 190, 203, 205, 206, 208, 209, 210, 211, 212, 213], "take": [6, 7, 8, 10, 39, 129, 131, 136, 137, 153, 168, 170, 173, 176, 206, 207, 208, 209, 210, 211, 213], "close": [6, 8, 187, 188, 189, 190, 211], "look": [6, 7, 8, 174, 189, 202, 206, 207, 208, 209, 210, 211, 212], "veri": [6, 35, 134, 205, 208], "simpli": [6, 7, 20, 37, 39, 139, 140, 205, 206, 207, 208, 210, 213], "dictat": 6, "state_dict": [6, 137, 148, 168, 169, 170, 171, 172, 211, 213], "store": [6, 39, 187, 190, 211, 213], "file": [6, 7, 8, 9, 10, 11, 12, 34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 65, 75, 91, 95, 109, 121, 126, 157, 158, 159, 168, 169, 170, 173, 187, 190, 196, 200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213], "disk": [6, 40, 187], "string": [6, 22, 24, 42, 43, 44, 49, 65, 75, 91, 109, 121, 143, 155, 157, 158, 160, 176, 177, 181, 205, 207], "kei": [6, 7, 9, 14, 28, 30, 34, 36, 38, 39, 46, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 134, 135, 147, 148, 149, 153, 168, 170, 172, 196, 205, 208, 209, 211, 213], "identifi": 6, "state": [6, 8, 136, 137, 139, 146, 147, 148, 149, 150, 168, 169, 170, 172, 174, 208, 210, 211, 213], "dict": [6, 7, 8, 9, 10, 14, 18, 19, 20, 22, 24, 27, 28, 30, 31, 34, 36, 38, 39, 40, 43, 44, 45, 46, 47, 48, 50, 51, 91, 121, 137, 146, 147, 148, 149, 152, 153, 155, 156, 158, 159, 161, 168, 169, 170, 172, 174, 180, 183, 185, 191, 192, 207], "If": [6, 7, 13, 14, 19, 20, 22, 23, 27, 28, 30, 32, 33, 34, 36, 38, 39, 42, 45, 46, 47, 48, 55, 76, 82, 91, 92, 96, 101, 104, 121, 128, 133, 134, 135, 136, 137, 144, 149, 164, 165, 168, 169, 170, 171, 172, 175, 176, 177, 178, 180, 181, 183, 189, 190, 195, 196, 198, 202, 205, 206, 207, 208, 209, 210, 211, 212], "don": [6, 7, 8, 190, 195, 205, 206, 207, 208, 209, 213], "t": [6, 7, 8, 140, 177, 190, 195, 205, 206, 207, 208, 209, 213], "match": [6, 36, 38, 46, 121, 149, 164, 202, 205, 207, 208, 210, 211], "up": [6, 8, 9, 36, 37, 38, 40, 42, 44, 46, 50, 51, 158, 162, 163, 165, 174, 196, 205, 206, 207, 209, 210, 211, 213], "exactli": [6, 149, 212], "those": [6, 171, 208, 210, 211], "definit": [6, 211], "either": [6, 39, 149, 168, 175, 193, 205, 211, 212, 213], "run": [6, 7, 9, 12, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 129, 131, 134, 137, 168, 169, 170, 172, 174, 184, 189, 190, 192, 202, 203, 206, 207, 209, 210, 211, 212, 213], "explicit": 6, "error": [6, 7, 17, 33, 131, 168, 195, 205], "load": [6, 8, 34, 35, 36, 37, 38, 39, 40, 42, 44, 45, 47, 48, 49, 50, 51, 148, 168, 169, 170, 172, 173, 189, 206, 207, 208, 210, 211], "rais": [6, 10, 13, 23, 30, 33, 34, 36, 43, 121, 128, 131, 134, 136, 148, 149, 153, 160, 168, 169, 170, 172, 177, 180, 183, 190, 195, 198], "an": [6, 7, 8, 9, 10, 14, 33, 34, 35, 36, 40, 45, 47, 50, 51, 52, 53, 54, 82, 96, 104, 110, 112, 116, 122, 128, 131, 134, 136, 139, 143, 145, 146, 147, 162, 163, 164, 165, 167, 168, 169, 170, 172, 176, 178, 190, 196, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "except": [6, 22, 23, 160, 207], "wors": 6, "silent": [6, 129], "succe": 6, "infer": [6, 21, 34, 39, 66, 75, 114, 128, 131, 133, 134, 135, 176, 201, 206, 208, 209, 210, 212, 213], "expect": [6, 7, 10, 14, 18, 19, 20, 27, 28, 34, 36, 38, 39, 43, 45, 46, 47, 48, 133, 149, 172, 190, 198, 206, 207, 211, 212], "addit": [6, 7, 8, 10, 34, 36, 38, 39, 40, 43, 44, 46, 50, 51, 75, 139, 148, 167, 168, 169, 170, 177, 178, 183, 186, 187, 189, 190, 193, 203, 206, 209, 211], "line": [6, 8, 14, 173, 205, 207, 209, 210], "need": [6, 7, 8, 9, 19, 24, 34, 37, 39, 128, 129, 134, 136, 186, 189, 190, 192, 202, 205, 206, 207, 208, 209, 210, 211, 213], "shape": [6, 52, 53, 54, 55, 128, 131, 133, 134, 135, 136, 139, 140, 141, 142, 144, 150, 151, 152, 154, 162, 164, 166, 175, 196], "valu": [6, 7, 27, 31, 56, 57, 58, 66, 67, 68, 70, 76, 77, 78, 79, 80, 82, 92, 93, 94, 96, 101, 102, 103, 104, 110, 112, 114, 115, 116, 117, 122, 124, 128, 131, 132, 134, 135, 138, 141, 148, 150, 151, 153, 154, 168, 171, 172, 173, 175, 187, 188, 189, 190, 195, 205, 207, 209, 210, 211, 212], "two": [6, 7, 18, 33, 53, 136, 154, 162, 164, 203, 208, 209, 210, 211, 212, 213], "popular": [6, 203, 207, 208], "llama2": [6, 7, 8, 10, 21, 34, 36, 38, 39, 40, 42, 44, 46, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 129, 134, 135, 171, 201, 203, 205, 209, 210, 212], "offici": [6, 21, 206, 209, 210], "implement": [6, 8, 34, 36, 38, 40, 42, 43, 44, 46, 49, 50, 51, 65, 75, 109, 121, 129, 132, 133, 136, 138, 139, 140, 141, 142, 143, 144, 155, 156, 168, 181, 189, 203, 207, 211, 212, 213], "when": [6, 7, 8, 12, 35, 37, 39, 40, 50, 75, 128, 133, 134, 135, 136, 137, 138, 148, 151, 163, 165, 175, 178, 189, 192, 205, 208, 210, 211, 212, 213], "llama": [6, 21, 34, 75, 91, 132, 133, 168, 169, 205, 206, 208, 209, 210, 211], "websit": 6, "get": [6, 7, 8, 9, 34, 39, 75, 109, 177, 179, 180, 182, 202, 203, 206, 207, 208, 209, 211, 212], "singl": [6, 7, 10, 14, 15, 16, 19, 21, 23, 28, 30, 31, 35, 37, 39, 40, 50, 53, 54, 55, 80, 91, 117, 128, 136, 148, 168, 169, 170, 171, 172, 174, 205, 206, 207, 208, 209, 210, 211, 213], "pth": [6, 208], "inspect": [6, 208, 211, 213], "content": [6, 15, 20, 22, 24, 25, 27, 30, 31, 34, 39, 65, 75, 109, 121, 160, 206, 207], "easili": [6, 7, 203, 207, 211, 212, 213], "torch": [6, 7, 52, 53, 54, 130, 131, 134, 136, 137, 138, 139, 140, 141, 142, 150, 151, 152, 153, 154, 164, 165, 166, 170, 172, 174, 175, 176, 177, 180, 181, 183, 184, 191, 192, 193, 194, 195, 196, 197, 198, 208, 209, 210, 211, 213], "import": [6, 7, 10, 43, 46, 50, 136, 139, 189, 190, 206, 207, 208, 209, 210, 211, 212, 213], "00": [6, 200, 204, 209], "mmap": [6, 208], "true": [6, 7, 22, 35, 36, 37, 40, 41, 42, 43, 45, 46, 47, 48, 50, 51, 55, 62, 63, 64, 65, 66, 70, 73, 74, 75, 87, 88, 89, 90, 91, 99, 100, 107, 108, 109, 119, 120, 121, 127, 128, 134, 135, 137, 141, 145, 150, 154, 157, 158, 160, 162, 164, 167, 168, 169, 170, 178, 180, 183, 184, 186, 189, 196, 197, 205, 206, 207, 208, 210, 211, 212, 213], "weights_onli": [6, 170], "map_loc": [6, 208], "cpu": [6, 8, 137, 177, 196, 202, 205, 208, 213], "tensor": [6, 52, 53, 54, 55, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 144, 150, 151, 152, 153, 154, 164, 165, 166, 168, 175, 187, 188, 189, 190, 191, 194, 211, 213], "item": 6, "print": [6, 9, 35, 42, 45, 47, 48, 65, 75, 91, 109, 121, 136, 157, 158, 160, 175, 197, 206, 207, 209, 211, 212, 213], "f": [6, 9, 42, 45, 47, 206, 208, 211, 213], "tok_embed": [6, 134], "32000": [6, 10, 211], "4096": [6, 10, 36, 38, 40, 42, 44, 46, 50, 51, 128, 133, 207, 211, 212], "len": [6, 35, 42, 45, 47, 134, 136], "292": 6, "contain": [6, 18, 22, 30, 37, 39, 40, 50, 65, 75, 91, 95, 109, 121, 126, 128, 131, 133, 134, 135, 143, 146, 147, 148, 150, 152, 153, 154, 158, 160, 163, 168, 169, 170, 172, 173, 174, 180, 185, 189, 191, 196, 206, 208, 210, 211], "input": [6, 14, 18, 19, 28, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 91, 109, 121, 128, 129, 130, 132, 133, 134, 135, 136, 144, 152, 153, 157, 158, 162, 165, 166, 168, 170, 191, 195, 198, 206, 207, 211, 213], "embed": [6, 52, 53, 54, 55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 132, 133, 134, 136, 178, 206, 210, 212], "tabl": [6, 206, 208, 210, 213], "call": [6, 10, 22, 24, 39, 129, 136, 137, 148, 173, 187, 188, 189, 190, 192, 196, 206, 207, 211, 213], "layer": [6, 8, 55, 59, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 74, 76, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 116, 117, 119, 120, 122, 123, 124, 127, 128, 134, 135, 136, 144, 148, 149, 167, 178, 203, 210, 211, 212, 213], "have": [6, 7, 10, 18, 53, 54, 55, 128, 131, 136, 143, 149, 162, 164, 170, 172, 173, 178, 186, 189, 198, 202, 206, 207, 208, 209, 210, 211, 212, 213], "dim": [6, 128, 129, 132, 133, 134], "most": [6, 7, 24, 163, 206, 209, 211, 213], "within": [6, 7, 10, 34, 37, 52, 70, 82, 96, 104, 110, 112, 122, 129, 136, 175, 189, 195, 196, 205, 207, 211, 213], "hub": [6, 39, 205, 207, 209], "default": [6, 7, 16, 18, 20, 22, 27, 30, 31, 32, 34, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 70, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 126, 128, 129, 132, 133, 134, 135, 137, 138, 139, 144, 148, 150, 151, 152, 153, 157, 158, 160, 168, 169, 170, 173, 175, 177, 182, 186, 187, 190, 191, 194, 195, 196, 202, 205, 206, 207, 208, 210, 211, 212, 213], "everi": [6, 8, 52, 53, 54, 129, 136, 189, 196, 202, 205, 213], "repo": [6, 168, 169, 171, 205, 208], "first": [6, 7, 10, 33, 37, 55, 131, 134, 136, 154, 168, 173, 201, 203, 206, 207, 208, 210, 211, 212, 213], "big": 6, "split": [6, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 158, 206, 207, 208, 212], "across": [6, 8, 35, 168, 189, 195, 208, 210, 212], "bin": [6, 205, 208], "correctli": [6, 8, 13, 148, 168, 202, 206, 209, 213], "piec": 6, "one": [6, 8, 18, 33, 65, 75, 109, 121, 129, 136, 160, 164, 170, 206, 207, 208, 209, 210, 213], "pytorch_model": [6, 208], "00001": [6, 205], "00002": [6, 205], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 22, 37, 39, 44, 55, 70, 82, 96, 104, 110, 112, 122, 136, 144, 146, 148, 157, 168, 169, 170, 172, 173, 175, 177, 178, 180, 181, 186, 205, 207, 208, 209, 211, 212, 213], "doe": [6, 23, 30, 34, 37, 50, 66, 75, 114, 125, 128, 134, 135, 143, 160, 168, 170, 172, 173, 205, 206, 208, 212], "fewer": [6, 128], "sinc": [6, 7, 10, 39, 129, 164, 165, 168, 170, 206, 208, 210, 212], "mismatch": 6, "name": [6, 7, 9, 11, 14, 18, 19, 20, 27, 28, 36, 38, 40, 45, 46, 47, 48, 50, 51, 143, 147, 149, 158, 168, 169, 170, 171, 172, 173, 174, 175, 176, 187, 188, 189, 190, 198, 205, 206, 208, 210, 212], "caus": [6, 109, 157, 165], "try": [6, 7, 206, 208, 209, 210, 213], "same": [6, 7, 24, 52, 53, 59, 60, 61, 65, 71, 72, 75, 83, 84, 85, 86, 97, 98, 105, 106, 109, 121, 123, 131, 135, 136, 141, 154, 160, 172, 173, 178, 190, 205, 206, 208, 210, 211, 212, 213], "As": [6, 7, 8, 9, 144, 203, 208, 213], "re": [6, 7, 170, 203, 206, 208, 209, 211], "care": [6, 129, 168, 170, 208, 210, 211], "end": [6, 8, 22, 40, 50, 91, 109, 158, 160, 201, 203, 206, 210, 211, 212], "number": [6, 8, 34, 36, 37, 38, 40, 42, 43, 44, 46, 49, 50, 51, 52, 53, 55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 134, 136, 138, 162, 163, 168, 169, 170, 175, 182, 195, 196, 205, 209, 211], "just": [6, 14, 203, 205, 206, 207, 209, 210, 211, 212], "save": [6, 8, 9, 137, 168, 169, 170, 172, 178, 186, 190, 201, 205, 206, 207, 208, 210, 211, 212], "less": [6, 208, 209, 210, 213], "prone": 6, "manag": [6, 35, 145, 194, 206], "invari": 6, "accept": [6, 7, 167, 207, 209, 213], "multipl": [6, 7, 8, 22, 34, 35, 39, 128, 134, 135, 136, 144, 153, 163, 164, 187, 188, 189, 190, 196, 209, 210], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 206, 207, 208, 212], "worri": [6, 206, 209], "explicitli": [6, 143, 203, 211], "convert": [6, 18, 20, 27, 30, 31, 34, 39, 45, 47, 48, 168, 191, 206, 208, 212, 213], "time": [6, 65, 66, 75, 109, 114, 121, 150, 160, 187, 189, 196, 205, 206, 207, 208, 210, 213], "produc": [6, 172, 212, 213], "back": [6, 33, 145, 168, 207, 211, 213], "form": [6, 7, 8, 33, 39, 205], "One": [6, 212], "advantag": [6, 141, 150, 211], "being": [6, 39, 168, 169, 170, 174, 176, 212, 213], "should": [6, 7, 8, 14, 15, 19, 20, 21, 22, 23, 24, 27, 30, 31, 37, 43, 46, 50, 59, 60, 61, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 116, 122, 123, 124, 128, 129, 136, 141, 143, 148, 149, 150, 166, 167, 173, 185, 187, 188, 189, 190, 202, 203, 207, 208, 209, 210, 211, 212, 213], "abl": [6, 8, 208, 209, 212], "post": [6, 136, 192, 196, 208, 210, 212, 213], "tool": [6, 22, 24, 39, 207, 208, 209], "quantiz": [6, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 144, 170, 181, 201, 209, 213], "eval": [6, 201, 203, 212], "without": [6, 7, 9, 14, 148, 164, 165, 202, 203, 206, 208, 211, 212], "code": [6, 8, 56, 57, 58, 59, 60, 61, 62, 63, 64, 134, 199, 203, 207, 209], "chang": [6, 7, 9, 14, 18, 170, 202, 205, 208, 209, 210, 211, 212, 213], "OR": [6, 30], "script": [6, 9, 205, 207, 208, 209, 210], "wai": [6, 7, 34, 39, 148, 205, 206, 207, 208, 209, 210], "surround": [6, 8, 203], "load_checkpoint": [6, 8, 168, 169, 170, 171], "save_checkpoint": [6, 8, 9, 168, 169, 170], "map": [6, 14, 18, 19, 20, 24, 27, 28, 30, 31, 34, 35, 36, 37, 38, 45, 46, 47, 48, 91, 121, 147, 158, 159, 168, 172, 174, 187, 188, 189, 190, 192, 196, 206, 207, 208, 211], "exampl": [6, 7, 8, 9, 10, 12, 14, 20, 24, 28, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 55, 65, 75, 91, 109, 121, 128, 136, 139, 140, 142, 143, 145, 152, 153, 154, 157, 158, 160, 163, 164, 165, 166, 167, 168, 169, 171, 172, 175, 181, 189, 190, 191, 194, 197, 199, 200, 202, 204, 205, 206, 207, 208, 210, 211, 212, 213], "appli": [6, 8, 34, 36, 38, 39, 59, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 119, 120, 122, 123, 127, 128, 132, 133, 134, 135, 148, 149, 193, 203, 213], "permut": 6, "certain": [6, 7, 196, 206], "ensur": [6, 7, 13, 33, 39, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 168, 170, 177, 203, 207, 209], "behav": 6, "further": [6, 14, 136, 205, 207, 211, 212, 213], "illustr": [6, 210], "whilst": 6, "other": [6, 8, 10, 18, 24, 35, 140, 170, 173, 178, 196, 207, 209, 210, 211, 212], "phi3": [6, 121, 122, 123, 125, 126, 127, 171, 205], "own": [6, 24, 186, 195, 205, 206, 207, 208, 210, 211], "found": [6, 7, 9, 132, 133, 168, 169, 170, 205, 211, 213], "folder": [6, 206], "three": [6, 8, 39, 139, 140, 142, 209], "read": [6, 168, 169, 170, 203], "write": [6, 8, 14, 168, 169, 170, 187, 206, 207, 209], "compat": [6, 168, 170, 212], "transform": [6, 8, 18, 20, 34, 36, 38, 39, 45, 47, 48, 55, 59, 60, 61, 66, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 116, 122, 123, 124, 134, 135, 136, 138, 162, 163, 164, 165, 166, 193, 211, 212], "framework": [6, 8, 203], "mention": [6, 208, 213], "assum": [6, 14, 19, 20, 27, 28, 36, 38, 45, 46, 47, 48, 128, 133, 134, 135, 138, 146, 158, 172, 174, 177, 186, 206, 208, 211], "checkpoint_dir": [6, 7, 168, 169, 170, 208, 210, 212], "necessari": [6, 39, 187, 188, 189, 190, 206, 211], "easiest": [6, 208, 209], "sure": [6, 7, 206, 208, 209, 210, 211, 212, 213], "everyth": [6, 8, 173, 203, 209], "follow": [6, 8, 20, 22, 24, 27, 30, 31, 34, 37, 39, 128, 138, 141, 162, 163, 170, 171, 172, 184, 190, 196, 201, 202, 205, 207, 208, 209, 210, 211, 212, 213], "flow": [6, 34, 36, 37, 38, 212, 213], "By": [6, 205, 211, 212, 213], "safetensor": [6, 168, 205], "output": [6, 18, 19, 35, 36, 39, 42, 45, 47, 48, 55, 59, 60, 61, 66, 70, 76, 80, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 117, 122, 123, 128, 129, 130, 132, 133, 134, 135, 136, 144, 147, 148, 149, 162, 165, 170, 175, 178, 188, 196, 202, 205, 206, 207, 208, 209, 210, 211, 213], "dir": [6, 190, 202, 205, 208, 209, 210, 212], "output_dir": [6, 7, 168, 169, 170, 196, 208, 210, 211, 212, 213], "here": [6, 7, 9, 14, 16, 28, 45, 132, 133, 205, 206, 207, 208, 209, 210, 211, 212, 213], "argument": [6, 7, 10, 17, 19, 29, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 128, 167, 173, 178, 183, 187, 189, 190, 193, 205, 206, 207, 211, 212], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 134, 196, 205, 207, 208, 211, 213], "_component_": [6, 7, 9, 10, 35, 43, 46, 50, 196, 206, 207, 208, 210, 211, 212], "fullmodelhfcheckpoint": [6, 208], "directori": [6, 7, 168, 169, 170, 187, 189, 190, 196, 205, 206, 207, 208, 209, 210], "sort": [6, 168, 170], "so": [6, 7, 37, 136, 164, 168, 173, 202, 203, 206, 208, 209, 210, 211, 212, 213], "order": [6, 8, 168, 170, 189, 190, 209], "matter": [6, 168, 170, 205, 211], "checkpoint_fil": [6, 7, 9, 168, 169, 170, 208, 210, 211, 212, 213], "restart": [6, 205], "previou": [6, 37, 168, 169, 170], "more": [6, 7, 8, 24, 34, 36, 38, 39, 40, 41, 43, 45, 46, 47, 48, 50, 51, 75, 131, 133, 136, 148, 167, 170, 173, 190, 193, 195, 203, 205, 207, 208, 209, 210, 211, 212, 213], "next": [6, 37, 50, 55, 136, 162, 175, 210, 213], "section": [6, 8, 180, 201, 208, 210, 213], "recipe_checkpoint": [6, 168, 169, 170, 212], "null": [6, 7, 212], "usual": [6, 133, 154, 168, 190, 205, 208, 211], "model_typ": [6, 168, 169, 170, 208, 210, 212], "resume_from_checkpoint": [6, 168, 169, 170], "fals": [6, 7, 18, 20, 22, 27, 30, 31, 34, 35, 36, 37, 41, 42, 43, 45, 46, 47, 48, 50, 51, 55, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 123, 127, 128, 134, 135, 144, 145, 148, 154, 157, 164, 168, 169, 170, 184, 196, 205, 206, 207, 208, 210, 211, 212, 213], "requir": [6, 7, 35, 39, 40, 50, 75, 153, 168, 170, 172, 181, 183, 184, 186, 189, 190, 195, 196, 202, 205, 206, 207, 209, 212, 213], "param": [6, 8, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 144, 146, 147, 149, 168, 186, 211, 212, 213], "directli": [6, 7, 8, 10, 39, 43, 46, 50, 139, 167, 168, 205, 208, 209, 210, 211, 212, 213], "out": [6, 7, 8, 36, 42, 43, 45, 47, 48, 162, 168, 169, 201, 203, 205, 206, 208, 209, 210, 211, 213], "case": [6, 8, 9, 22, 24, 53, 54, 55, 136, 168, 172, 177, 181, 186, 187, 193, 203, 205, 206, 207, 208, 210, 211, 213], "discrep": [6, 168], "along": [6, 211], "github": [6, 10, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 128, 132, 133, 138, 139, 140, 141, 142, 148, 202, 207, 208, 209, 210], "repositori": [6, 21, 34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 208, 209], "fullmodelmetacheckpoint": [6, 210, 212], "current": [6, 37, 66, 70, 82, 96, 104, 110, 112, 114, 122, 125, 128, 131, 133, 134, 135, 141, 169, 170, 178, 181, 182, 187, 189, 192, 195, 207, 209, 210, 212], "test": [6, 7, 8, 203, 206], "complet": [6, 8, 14, 37, 44, 50, 126, 140, 206, 207, 208, 209, 210], "written": [6, 7, 8, 168, 169, 187, 188, 189, 190, 203], "begin": [6, 37, 50, 75, 109, 136, 158, 206, 210, 213], "partit": [6, 168, 213], "ha": [6, 75, 109, 136, 143, 145, 146, 149, 154, 170, 172, 198, 206, 207, 208, 209, 210, 211, 213], "standard": [6, 17, 30, 39, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 188, 203, 206, 208, 210], "key_1": [6, 170], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 208], "inform": [6, 190, 193, 203, 205, 208, 209], "subsequ": [6, 8, 136, 162], "recipe_st": [6, 168, 169, 170], "pt": [6, 9, 168, 169, 170, 208, 210, 212], "epoch": [6, 8, 9, 138, 168, 169, 170, 205, 206, 208, 209, 210, 212], "optim": [6, 7, 8, 35, 66, 75, 114, 125, 138, 139, 141, 142, 153, 170, 172, 174, 180, 192, 196, 206, 208, 209, 210, 211, 213], "etc": [6, 8, 168, 180, 209], "prevent": [6, 37, 139, 205], "flood": 6, "overwritten": 6, "note": [6, 7, 19, 70, 134, 143, 172, 192, 195, 206, 207, 208, 211, 212, 213], "updat": [6, 7, 8, 24, 131, 139, 141, 161, 172, 196, 202, 206, 208, 209, 210, 211, 212, 213], "hf_model_0001_0": [6, 208], "hf_model_0002_0": [6, 208], "both": [6, 35, 149, 205, 208, 211, 212, 213], "adapt": [6, 143, 144, 145, 146, 147, 168, 169, 170, 206, 208, 211, 213], "merg": [6, 10, 11, 168, 208, 210, 213], "would": [6, 7, 9, 24, 37, 134, 136, 140, 202, 206, 207, 208, 211, 213], "addition": [6, 157, 158, 195, 207, 211], "option": [6, 7, 8, 14, 18, 19, 20, 27, 28, 32, 34, 36, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 50, 51, 54, 55, 59, 60, 61, 65, 70, 71, 72, 75, 76, 82, 83, 84, 85, 86, 91, 92, 95, 96, 97, 98, 101, 104, 105, 106, 109, 110, 111, 112, 113, 121, 122, 123, 126, 128, 133, 134, 135, 136, 137, 141, 148, 149, 150, 151, 155, 157, 160, 164, 165, 168, 169, 170, 175, 176, 177, 179, 181, 187, 190, 195, 196, 202, 203, 205, 207, 208], "save_adapter_weights_onli": 6, "choos": [6, 211], "primari": [6, 7, 8, 39, 209], "want": [6, 7, 8, 9, 10, 34, 39, 163, 164, 175, 202, 205, 206, 207, 208, 209, 210, 211], "resum": [6, 8, 138, 168, 169, 170, 213], "initi": [6, 8, 12, 35, 37, 56, 57, 58, 67, 68, 77, 78, 79, 80, 93, 94, 102, 103, 115, 117, 139, 172, 183, 184, 209, 211, 213], "frozen": [6, 139, 211, 213], "base": [6, 10, 22, 24, 36, 38, 39, 59, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 116, 119, 120, 122, 123, 124, 127, 133, 138, 139, 140, 142, 144, 145, 147, 148, 149, 151, 168, 173, 176, 178, 186, 187, 201, 206, 208, 209, 210, 211, 213], "well": [6, 7, 8, 203, 205, 207, 208, 210, 213], "learnt": [6, 206, 208], "someth": [6, 8, 9, 206, 208, 212], "NOT": [6, 66, 114], "refer": [6, 7, 8, 132, 133, 136, 139, 140, 141, 142, 145, 151, 203, 211, 212], "adapter_checkpoint": [6, 168, 169, 170], "adapter_0": [6, 208], "now": [6, 172, 174, 206, 207, 208, 209, 210, 211, 212, 213], "knowledg": 6, "creat": [6, 7, 10, 20, 24, 37, 39, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 71, 72, 73, 74, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 97, 98, 99, 100, 102, 103, 105, 106, 107, 108, 111, 113, 115, 117, 119, 120, 123, 125, 127, 131, 136, 138, 167, 168, 169, 170, 174, 187, 189, 205, 206, 207, 208, 213], "simpl": [6, 8, 14, 28, 136, 201, 207, 209, 211, 212, 213], "forward": [6, 8, 52, 53, 54, 128, 129, 130, 132, 133, 134, 135, 136, 139, 140, 141, 142, 144, 180, 196, 210, 211, 213], "modeltyp": [6, 168, 169, 170], "llama2_13b": [6, 83], "right": [6, 168, 208, 210, 211], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 148, 172, 211], "successfulli": [6, 205, 209], "vocab": [6, 10, 134, 210], "70": [6, 93], "x": [6, 52, 53, 54, 128, 129, 130, 132, 133, 134, 135, 136, 144, 175, 194, 211, 212, 213], "randint": 6, "0": [6, 8, 37, 55, 59, 60, 61, 62, 63, 64, 65, 66, 70, 75, 76, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 128, 134, 136, 138, 139, 140, 141, 142, 144, 152, 153, 154, 160, 164, 175, 181, 189, 190, 191, 195, 197, 200, 204, 206, 207, 208, 209, 210, 211, 212, 213], "1": [6, 8, 37, 48, 52, 53, 65, 75, 76, 82, 91, 92, 96, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 116, 121, 122, 124, 128, 134, 136, 138, 139, 140, 141, 142, 152, 153, 157, 158, 160, 164, 165, 169, 171, 175, 184, 189, 190, 191, 194, 195, 205, 206, 208, 209, 211, 212, 213], "no_grad": 6, "6": [6, 37, 66, 70, 132, 136, 152, 153, 166, 191, 212, 213], "3989": 6, "9": [6, 136, 153, 208, 212, 213], "0531": 6, "3": [6, 37, 55, 91, 123, 125, 126, 136, 152, 153, 164, 165, 166, 171, 173, 179, 181, 191, 194, 205, 206, 208, 209, 210, 212, 213], "2375": 6, "5": [6, 7, 14, 136, 138, 139, 152, 153, 154, 164, 191, 208, 209, 210], "2822": 6, "4": [6, 7, 55, 128, 136, 152, 153, 163, 181, 191, 197, 203, 205, 207, 208, 210, 211, 212, 213], "4872": 6, "7469": 6, "8": [6, 42, 45, 47, 59, 60, 61, 62, 63, 64, 71, 72, 73, 74, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 105, 106, 107, 108, 111, 113, 119, 120, 123, 127, 136, 152, 153, 208, 211, 212, 213], "6737": 6, "11": [6, 136, 153, 208, 212, 213], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 136, 141, 152, 153, 162, 191], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 136], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 22, 36, 38, 46, 148, 160, 190, 205, 206, 207, 208, 209, 210, 211, 212], "find": [6, 8, 9, 139, 205, 208, 209, 211], "hope": 6, "deeper": [6, 209], "insight": [6, 208], "happi": [6, 208], "thi": [7, 8, 9, 10, 17, 18, 20, 22, 29, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 66, 70, 75, 76, 82, 91, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 125, 126, 128, 129, 133, 134, 135, 136, 137, 138, 139, 140, 141, 143, 145, 148, 149, 151, 153, 157, 158, 160, 162, 167, 168, 169, 170, 172, 173, 175, 176, 177, 180, 184, 186, 187, 189, 190, 192, 193, 195, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "pars": [7, 10, 11, 159, 173, 206, 209], "effect": [7, 212], "cli": [7, 9, 11, 12, 202, 208, 209], "prerequisit": [7, 206, 207, 208, 209, 210, 211, 212, 213], "Be": [7, 206, 208, 209, 210, 211, 212, 213], "familiar": [7, 206, 208, 209, 210, 211, 212, 213], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 202, 206, 207, 209], "instal": [7, 9, 184, 189, 190, 201, 205, 207, 208, 209, 210, 211, 212, 213], "fundament": [7, 212], "There": [7, 33, 53, 164, 206, 209, 210, 211], "entri": [7, 8, 209], "point": [7, 8, 30, 31, 160, 207, 208, 209, 210, 211, 212, 213], "locat": [7, 205, 207, 210, 211, 212, 213], "thei": [7, 8, 35, 55, 134, 136, 149, 173, 178, 205, 206, 207, 211, 212], "truth": [7, 208, 210], "reproduc": 7, "overridden": [7, 129, 173, 196], "quick": 7, "experiment": 7, "serv": [7, 160, 167, 207, 211], "particular": [7, 34, 35, 39, 167, 207, 211, 213], "seed": [7, 8, 9, 195, 209, 212], "shuffl": [7, 37, 212], "devic": [7, 8, 148, 172, 176, 177, 180, 205, 206, 208, 209, 210, 211], "cuda": [7, 176, 177, 180, 196, 202, 208, 213], "dtype": [7, 8, 131, 134, 137, 177, 194, 198, 208, 212, 213], "fp32": [7, 212, 213], "enable_fsdp": 7, "mani": [7, 37, 207, 208], "object": [7, 10, 11, 15, 16, 21, 23, 45, 47, 55, 128, 139, 167, 181, 206], "keyword": [7, 10, 34, 36, 38, 39, 40, 43, 44, 46, 50, 51, 137, 206, 207], "loss": [7, 8, 22, 24, 36, 39, 42, 45, 47, 48, 139, 140, 141, 142, 209, 211, 213], "subfield": 7, "dotpath": [7, 207], "wish": [7, 207], "exact": [7, 10, 208], "path": [7, 8, 9, 10, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 65, 69, 75, 81, 91, 95, 109, 118, 121, 126, 157, 158, 159, 168, 169, 170, 173, 196, 205, 206, 207, 208, 210, 211], "normal": [7, 34, 37, 39, 75, 109, 130, 132, 134, 135, 157, 206, 207, 211, 212, 213], "python": [7, 173, 179, 190, 195, 199, 205, 207, 208, 212], "alpaca_dataset": [7, 41, 207], "custom": [7, 8, 24, 34, 36, 38, 39, 43, 46, 50, 193, 203, 205, 209, 210, 211], "train_on_input": [7, 18, 20, 27, 30, 31, 34, 35, 36, 41, 42, 43, 45, 46, 47, 48, 206, 207], "onc": [7, 24, 145, 208, 209, 210, 211, 213], "ve": [7, 131, 205, 206, 207, 208, 210, 211], "instanc": [7, 10, 34, 35, 36, 82, 96, 104, 110, 112, 122, 129, 137, 146, 147, 211], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 43, 205, 208, 213], "under": [7, 196, 207, 213], "preced": [7, 10, 205, 210, 211], "actual": [7, 9, 14, 18, 28, 34, 39, 206, 212], "throw": 7, "notic": [7, 52, 53, 54, 136, 206, 207, 211], "miss": [7, 148, 149, 196, 211], "posit": [7, 10, 37, 52, 53, 54, 55, 66, 70, 110, 112, 114, 116, 122, 124, 128, 131, 133, 134, 135, 136, 210], "anoth": [7, 39, 208], "handl": [7, 12, 35, 39, 75, 109, 157, 158, 206, 208, 211, 213], "def": [7, 8, 9, 12, 167, 171, 206, 207, 211, 213], "dictconfig": [7, 8, 10, 11, 12, 13, 190, 196], "arg": [7, 10, 25, 54, 130, 134, 137, 143, 155, 156, 161, 173, 188, 196, 212], "tupl": [7, 10, 24, 54, 65, 75, 91, 109, 121, 131, 136, 137, 139, 140, 141, 142, 150, 151, 153, 154, 156, 160, 163, 164, 165, 167, 173, 182, 196, 198], "kwarg": [7, 10, 25, 130, 137, 143, 155, 156, 161, 173, 183, 187, 188, 189, 190, 193, 196, 207], "str": [7, 10, 11, 14, 18, 19, 20, 22, 24, 27, 28, 30, 31, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 65, 69, 75, 81, 91, 95, 109, 118, 121, 126, 137, 143, 144, 146, 147, 148, 149, 152, 153, 155, 156, 157, 158, 159, 168, 169, 170, 171, 172, 173, 176, 177, 179, 180, 181, 183, 185, 187, 188, 189, 190, 191, 195, 196, 197, 198, 206, 207], "mean": [7, 128, 132, 134, 135, 150, 186, 205, 206, 207, 209, 211, 212], "pass": [7, 10, 22, 24, 34, 35, 36, 38, 39, 40, 43, 44, 45, 46, 47, 48, 50, 51, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 129, 137, 141, 145, 149, 158, 167, 170, 177, 178, 180, 183, 186, 189, 190, 193, 196, 205, 206, 207, 211, 212, 213], "add": [7, 9, 34, 37, 39, 40, 50, 75, 91, 136, 158, 160, 170, 171, 173, 207, 208, 210, 211, 213], "d": [7, 22, 128, 131, 134, 205, 206, 211, 212], "llama2_token": [7, 206, 208], "tmp": [7, 172, 206, 209], "llama2token": [7, 81], "modeltoken": [7, 22, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 160, 206, 207], "bool": [7, 18, 20, 22, 27, 30, 31, 34, 36, 37, 40, 41, 42, 43, 45, 46, 47, 48, 50, 51, 55, 59, 60, 61, 62, 63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 123, 127, 134, 137, 144, 148, 149, 150, 154, 156, 157, 158, 160, 164, 167, 168, 169, 170, 178, 180, 183, 184, 186, 189, 193, 196, 197, 206, 213], "max_seq_len": [7, 10, 32, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 65, 66, 70, 75, 76, 82, 91, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 128, 131, 133, 134, 152, 160, 206, 207, 212], "int": [7, 9, 32, 34, 36, 37, 38, 40, 41, 42, 43, 44, 46, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 76, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 119, 120, 121, 122, 123, 124, 127, 128, 131, 132, 133, 134, 136, 138, 144, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 175, 178, 182, 186, 187, 188, 189, 190, 191, 193, 195, 196, 205, 206, 207, 211, 213], "512": [7, 41, 42, 55, 207, 213], "instructdataset": [7, 41, 42, 46, 207], "alreadi": [7, 171, 183, 186, 202, 205, 207, 208, 211], "overwrit": [7, 170, 202, 205], "duplic": [7, 8, 203, 205], "sometim": 7, "than": [7, 33, 128, 131, 136, 139, 167, 170, 171, 197, 198, 206, 207, 208, 209, 210, 211, 213], "resolv": [7, 11, 209], "alpaca": [7, 14, 35, 41, 42, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 207], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 187, 188, 189, 190], "disklogg": 7, "log_dir": [7, 187, 189, 190], "conveni": [7, 8, 205], "verifi": [7, 176, 177, 178, 206, 209, 211], "properli": [7, 148, 184, 205], "experi": [7, 190, 201, 203, 206, 210, 211], "wa": [7, 53, 54, 55, 136, 148, 206, 211, 212, 213], "cp": [7, 202, 205, 206, 208, 209, 210, 212], "7b_lora_single_devic": [7, 208, 209, 211, 213], "my_config": [7, 205], "discuss": [7, 208, 209, 210, 211], "guidelin": 7, "while": [7, 8, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 129, 203, 208, 212, 213], "mai": [7, 9, 136, 178, 206, 207, 209, 211], "tempt": 7, "put": [7, 8, 209, 211, 212], "much": [7, 208, 210, 211, 212, 213], "give": [7, 207, 211], "maximum": [7, 32, 34, 36, 37, 38, 40, 42, 43, 44, 46, 49, 50, 51, 52, 53, 55, 66, 70, 76, 82, 91, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 133, 134, 152, 163, 164, 165, 205], "flexibl": [7, 35, 207], "switch": 7, "encourag": [7, 75, 211], "clariti": 7, "significantli": [7, 139], "easier": [7, 208, 209], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 170, 206, 209], "parent": [7, 205], "modul": [7, 10, 45, 47, 52, 53, 54, 55, 112, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 171, 174, 178, 186, 192, 193, 195, 207, 209, 211, 213], "__init__": [7, 8, 211, 213], "py": [7, 10, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 128, 131, 132, 133, 138, 139, 140, 141, 142, 205, 208, 210], "guarante": 7, "stabil": [7, 203, 212, 213], "underscor": 7, "_alpaca": 7, "collect": [7, 175, 209], "itself": 7, "via": [7, 9, 43, 46, 50, 144, 168, 211, 213], "k1": [7, 8], "v1": [7, 8, 51], "k2": [7, 8], "v2": [7, 8, 207], "lora_finetune_single_devic": [7, 205, 206, 208, 209, 210, 211, 213], "checkpoint": [7, 8, 137, 158, 168, 169, 170, 171, 172, 190, 193, 203, 205, 210, 211, 212, 213], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "assign": [7, 39], "nest": 7, "dot": 7, "notat": [7, 128, 133, 134, 150, 151], "flag": [7, 8, 22, 36, 42, 45, 47, 48, 167, 170, 178, 205, 213], "built": [7, 9, 49, 202, 206, 209, 213], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 34], "pytorch": [7, 8, 75, 134, 137, 148, 167, 184, 189, 193, 195, 196, 201, 202, 203, 208, 210, 211, 212, 213], "llama3": [7, 34, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 122, 171, 175, 178, 201, 205, 207, 208], "8b_full": [7, 205, 207], "adamw": [7, 211], "lr": [7, 138], "2e": 7, "fuse": [7, 192, 212], "nproc_per_nod": [7, 207, 210, 211, 212], "full_finetune_distribut": [7, 205, 207, 208, 209], "core": [8, 39, 203, 207, 209, 213], "i": [8, 21, 22, 23, 128, 134, 135, 136, 137, 147, 172, 175, 207, 208, 210, 212, 213], "structur": [8, 15, 16, 20, 21, 23, 25, 27, 30, 31, 34, 39, 43, 95, 126, 162, 206, 207, 208, 212], "new": [8, 20, 27, 44, 45, 47, 48, 115, 131, 171, 187, 189, 206, 208, 209, 210, 211, 213], "user": [8, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 33, 34, 39, 65, 75, 76, 82, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 128, 160, 165, 206, 207, 209, 212], "thought": [8, 203, 209, 213], "target": [8, 203], "pipelin": [8, 203], "llm": [8, 201, 203, 207, 208, 210, 211], "eg": [8, 134, 168, 203], "meaning": [8, 203, 208], "featur": [8, 9, 202, 203, 208, 209], "fsdp": [8, 167, 172, 178, 186, 203, 209, 210], "activ": [8, 129, 180, 185, 193, 196, 203, 212, 213], "gradient": [8, 186, 192, 196, 203, 208, 210, 211, 213], "accumul": [8, 192, 196, 203], "mix": [8, 130, 205, 207, 208], "precis": [8, 130, 137, 177, 203, 209, 213], "given": [8, 10, 14, 19, 28, 33, 42, 44, 45, 47, 48, 49, 51, 140, 144, 145, 151, 155, 156, 175, 176, 177, 181, 186, 192, 197, 203, 211], "complex": 8, "becom": [8, 136, 140, 202, 207], "harder": 8, "anticip": 8, "architectur": [8, 21, 23, 134, 136, 171, 205, 207], "methodolog": 8, "reason": [8, 175, 208, 212], "possibl": [8, 37, 43, 163, 164, 205, 207], "trade": 8, "off": [8, 24, 75, 109, 208, 212], "memori": [8, 35, 36, 37, 38, 40, 42, 44, 46, 50, 51, 137, 148, 178, 180, 185, 186, 196, 201, 203, 208, 209, 210, 212], "vs": [8, 140, 209], "qualiti": [8, 208, 211, 212], "believ": 8, "best": [8, 164, 206], "suit": [8, 209], "b": [8, 128, 131, 133, 134, 135, 144, 150, 151, 186, 190, 211, 213], "fit": [8, 34, 36, 37, 38, 40, 42, 44, 46, 50, 51, 136, 139, 140, 164, 165, 207], "solut": [8, 140], "result": [8, 55, 65, 75, 109, 121, 136, 160, 162, 196, 208, 210, 211, 212, 213], "meant": [8, 137, 172], "depend": [8, 9, 14, 168, 196, 205, 207, 208, 211, 213], "level": [8, 39, 161, 174, 179, 186, 203, 213], "expertis": 8, "routin": 8, "yourself": [8, 205, 210, 211], "exist": [8, 202, 205, 208, 209, 210, 213], "ad": [8, 24, 52, 53, 54, 109, 116, 136, 157, 170, 171, 206, 207, 211, 212, 213], "ones": 8, "modular": [8, 203], "build": [8, 43, 46, 50, 55, 66, 76, 92, 101, 114, 116, 203, 210, 211], "block": [8, 37, 59, 60, 61, 66, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 122, 123, 148, 149, 203], "wandb": [8, 9, 190, 209], "log": [8, 11, 139, 140, 141, 142, 179, 180, 185, 187, 188, 189, 190, 208, 209, 210, 211, 213], "fulli": [8, 35], "nativ": [8, 201, 203, 211, 212, 213], "correct": [8, 17, 45, 132, 133, 134, 176, 203, 206, 207], "numer": [8, 203, 212], "pariti": [8, 203], "verif": 8, "extens": [8, 170, 203], "comparison": [8, 211, 213], "benchmark": [8, 195, 203, 208, 210, 211, 212], "limit": [8, 164, 165, 172, 207, 212], "hidden": [8, 55, 129, 136], "behind": 8, "100": [8, 36, 42, 45, 47, 48, 153, 175, 191, 211, 213], "prefer": [8, 28, 49, 139, 140, 141, 142, 153, 203, 205, 207], "over": [8, 39, 138, 139, 140, 173, 203, 205, 208, 211, 213], "unnecessari": 8, "abstract": [8, 15, 19, 155, 156, 203, 209, 213], "No": [8, 170, 203], "inherit": [8, 173, 203, 207], "go": [8, 21, 23, 55, 65, 75, 109, 121, 136, 160, 203, 207, 208, 209, 213], "upon": [8, 35, 210], "figur": [8, 211, 213], "spectrum": 8, "decid": 8, "interact": [8, 201, 209], "start": [8, 9, 160, 171, 202, 203, 206, 207, 208, 209, 212], "avail": [8, 51, 173, 176, 177, 184, 203, 205, 208, 210, 211], "paradigm": 8, "consist": [8, 51, 209], "configur": [8, 36, 38, 39, 42, 43, 44, 45, 46, 47, 48, 50, 51, 70, 82, 91, 96, 104, 110, 121, 122, 135, 203, 206, 209, 210, 211, 212, 213], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 124, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 174, 175, 176, 177, 178, 179, 180, 181, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213], "command": [8, 9, 173, 202, 205, 206, 207, 208, 209, 210, 211, 212, 213], "overrid": [8, 11, 12, 205, 208, 209, 210, 213], "togeth": [8, 37, 190, 209, 211, 212], "valid": [8, 33, 148, 149, 151, 198, 202, 208, 209], "environ": [8, 176, 184, 202, 205, 207, 208, 209, 212], "logic": [8, 39, 156, 171, 203, 209, 211], "api": [8, 9, 17, 29, 30, 39, 62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 148, 205, 206, 209, 210, 213], "closer": [8, 211], "monolith": [8, 203], "trainer": [8, 139, 140, 142], "A": [8, 9, 17, 29, 30, 31, 35, 37, 55, 65, 75, 109, 121, 128, 134, 135, 136, 137, 139, 140, 141, 142, 144, 148, 150, 151, 152, 153, 154, 157, 158, 160, 162, 164, 167, 171, 172, 173, 180, 181, 185, 186, 191, 200, 201, 204, 205, 206, 211, 212, 213], "wrapper": [8, 130, 157, 158, 172, 174, 205, 211], "around": [8, 34, 39, 75, 109, 130, 157, 158, 180, 205, 206, 208, 211, 212, 213], "extern": [8, 207], "primarili": [8, 35, 211], "eleutherai": [8, 51, 203, 211, 212], "har": [8, 203, 211, 212], "control": [8, 36, 42, 45, 47, 48, 140, 145, 195, 208], "multi": [8, 34, 128, 148, 210], "stage": [8, 136], "distil": 8, "oper": [8, 136, 145, 161, 195, 212], "turn": [8, 22, 33, 34, 206], "dataload": [8, 37, 42, 45, 47], "applic": [8, 128, 168, 169, 190], "clean": [8, 9, 41], "after": [8, 24, 39, 91, 121, 128, 131, 132, 134, 135, 148, 154, 186, 187, 188, 189, 190, 206, 208, 210, 212, 213], "process": [8, 9, 39, 55, 136, 137, 182, 183, 195, 207, 209, 212, 213], "group": [8, 128, 182, 183, 187, 188, 189, 190, 205, 210, 212], "init_process_group": [8, 183], "backend": [8, 205, 212], "gloo": 8, "els": [8, 173, 190, 203, 213], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "stuff": 8, "carri": [8, 39], "relev": [8, 205, 208, 211], "interfac": [8, 15, 19, 24, 25, 35, 143, 161, 207], "metric": [8, 209, 212], "logger": [8, 179, 185, 187, 188, 189, 190, 209], "self": [8, 9, 37, 59, 60, 61, 66, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 116, 122, 123, 124, 128, 134, 135, 143, 148, 149, 168, 171, 172, 207, 211, 213], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 167, 178, 186, 193, 206], "_model": [8, 172], "_setup_model": 8, "_token": [8, 207], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 172, 174, 192, 196, 213], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 139, 141, 195, 205, 210, 212], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 37], "batch": [8, 37, 42, 45, 47, 53, 128, 131, 133, 134, 136, 139, 140, 142, 150, 151, 152, 153, 191, 196, 203, 207, 209, 210, 211], "enumer": 8, "_autocast": 8, "logit": [8, 175], "label": [8, 34, 36, 37, 38, 40, 42, 43, 44, 46, 48, 49, 50, 51, 139, 153, 191], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 187, 188, 189, 190], "step": [8, 37, 39, 134, 138, 150, 174, 187, 188, 189, 190, 192, 196, 201, 208, 211, 212, 213], "learn": [8, 35, 138, 140, 203, 206, 207, 209, 210, 211, 212, 213], "decor": [8, 12], "recipe_main": [8, 12], "fullfinetunerecip": 8, "wandblogg": [9, 211, 213], "workspac": 9, "seen": [9, 211, 213], "screenshot": 9, "below": [9, 14, 133, 167, 207, 210, 211, 213], "packag": [9, 189, 190, 202, 207], "pip": [9, 189, 190, 202, 208, 210], "Then": [9, 145, 209], "login": [9, 190, 205, 208], "project": [9, 55, 59, 60, 61, 66, 70, 76, 80, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 117, 122, 123, 128, 129, 136, 148, 149, 171, 178, 190, 201, 206, 211, 213], "grab": [9, 210], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 53, 54, 55, 136, 184, 206, 212], "exit": [9, 202, 205], "resourc": [9, 187, 188, 189, 190, 212], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 18, 19, 20, 21, 22, 23, 27, 28, 30, 31, 34, 36, 37, 38, 39, 40, 46, 48, 50, 128, 133, 134, 135, 136, 142, 161, 162, 175, 206, 208], "desir": [9, 34, 39, 165, 194, 206], "suggest": 9, "approach": [9, 35, 207], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 208], "_output_dir": [9, 168, 169, 170], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 196], "type": [9, 10, 12, 22, 30, 31, 32, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 126, 128, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 144, 146, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 191, 193, 194, 195, 196, 197, 198, 207, 208, 211, 212, 213], "descript": [9, 43, 205], "whatev": 9, "metadata": [9, 212], "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": [9, 212], "add_fil": 9, "log_artifact": 9, "field": [10, 18, 19, 22, 30, 31, 34, 37, 39, 42, 45, 47, 185, 207], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 34, 36, 38, 40, 43, 44, 46, 50, 51, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 71, 72, 73, 74, 75, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 97, 98, 99, 100, 105, 106, 107, 108, 115, 117, 119, 120, 123, 125, 126, 127, 128, 132, 133, 136, 138, 139, 140, 141, 142, 148, 150, 162, 167, 168, 169, 173, 179, 184, 189, 190, 193, 195, 202, 207, 208, 210], "com": [10, 59, 60, 61, 71, 72, 75, 83, 84, 85, 86, 91, 97, 98, 105, 106, 123, 128, 132, 133, 138, 139, 140, 141, 142, 148, 202, 208, 210], "facebookresearch": [10, 132], "blob": [10, 59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123, 126, 128, 132, 133, 138, 139, 140, 141, 142], "main": [10, 12, 75, 126, 128, 132, 133, 202, 208, 210], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 134, 136], "32": [10, 136, 210, 211, 212, 213], "num_head": [10, 55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 133, 134], "num_kv_head": [10, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131], "vocab_s": [10, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "must": [10, 24, 35, 143, 173, 213], "return": [10, 12, 14, 15, 16, 19, 21, 22, 23, 24, 28, 30, 31, 32, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 126, 128, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 172, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 191, 194, 195, 196, 197, 206, 207, 211, 213], "nn": [10, 128, 129, 131, 134, 135, 136, 137, 143, 145, 146, 147, 167, 174, 186, 192, 193, 198, 211, 213], "parsed_yaml": 10, "embed_dim": [10, 52, 53, 54, 55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 133, 135, 136, 211], "valueerror": [10, 23, 30, 33, 34, 36, 43, 121, 128, 134, 136, 168, 169, 170, 177, 180, 195, 198], "recipe_nam": 11, "rank": [11, 59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 110, 111, 112, 113, 122, 123, 144, 182, 184, 195, 209, 211, 213], "zero": [11, 131, 132, 208, 210, 212], "displai": 11, "callabl": [12, 34, 36, 38, 39, 134, 145, 167, 175, 178, 181, 186, 193], "With": [12, 208, 211, 212, 213], "my_recip": 12, "foo": 12, "bar": [12, 203, 209], "instanti": [13, 24, 56, 57, 58, 59, 60, 61, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 92, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 110, 111, 112, 113, 114, 115, 116, 117, 118, 122, 123, 124, 125, 126, 172], "configerror": 13, "cannot": [13, 170, 210], "data": [14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 75, 136, 139, 142, 161, 180, 187, 188, 189, 190, 207, 208, 212, 213], "templat": [14, 17, 19, 24, 25, 28, 29, 34, 35, 36, 38, 39, 42, 43, 45, 46, 47, 48, 75], "style": [14, 37, 41, 42, 43, 48, 213], "slightli": 14, "describ": [14, 75, 91, 193, 207], "task": [14, 17, 29, 35, 39, 44, 206, 207, 208, 210, 211, 212, 213], "context": [14, 16, 125, 145, 194, 196, 207], "respons": [14, 16, 18, 22, 39, 65, 75, 109, 121, 139, 140, 142, 150, 151, 160, 207, 208, 209, 210], "appropri": [14, 16, 21, 22, 23, 35, 138, 168, 207, 213], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 19, 21, 22, 23, 28, 207], "column_map": [14, 18, 19, 20, 27, 28, 35, 36, 38, 45, 46, 47, 48, 207], "placehold": [14, 19, 28, 36, 38, 46, 207], "column": [14, 18, 19, 20, 27, 28, 36, 38, 39, 40, 45, 46, 47, 48, 50, 128, 134, 135, 206, 207, 212], "ident": [14, 19, 20, 23, 27, 28, 36, 37, 38, 45, 46, 47, 48, 140, 208, 212], "poem": 14, "n": [14, 17, 24, 28, 29, 65, 75, 109, 121, 128, 136, 160, 164, 200, 204, 205, 206, 207, 212], "nwrite": 14, "long": [14, 37, 158, 206, 207, 211], "where": [14, 24, 28, 34, 42, 45, 47, 53, 75, 80, 109, 117, 128, 134, 136, 139, 140, 141, 144, 150, 153, 154, 157, 162, 164, 178, 186, 207], "me": 14, "tag": [15, 16, 21, 23, 24, 34, 39, 187, 188, 189, 190, 206], "system": [15, 16, 20, 21, 22, 23, 24, 25, 26, 27, 30, 31, 33, 34, 39, 65, 75, 109, 121, 160, 206, 207], "assist": [15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 30, 31, 33, 34, 39, 65, 75, 109, 121, 126, 160, 175, 206, 207], "role": [15, 20, 22, 24, 25, 27, 30, 31, 34, 39, 65, 75, 109, 121, 160, 206, 207], "prepend": [15, 24, 25, 75, 91, 109, 157], "append": [15, 24, 25, 91, 109, 121, 157, 202, 207], "messag": [15, 16, 18, 20, 21, 23, 24, 25, 27, 30, 31, 33, 34, 39, 43, 45, 47, 48, 65, 75, 91, 109, 121, 156, 160, 202, 205, 206, 207], "accord": [15, 23, 206], "openai": [16, 30, 43, 141, 207], "markup": 16, "languag": [16, 139, 144, 175, 211], "It": [16, 22, 23, 24, 39, 136, 139, 205, 206, 207, 213], "im_start": 16, "im_end": 16, "goe": [16, 145], "functool": [17, 29, 167], "partial": [17, 29, 167], "_prompt_templ": [17, 29, 45, 47], "prompttempl": [17, 29, 39, 45, 47, 48], "english": 17, "ncorrect": 17, "grammar": [17, 45, 207], "user_messag": [17, 29, 206], "assistant_messag": [17, 29, 206], "equival": [18, 53, 140, 142], "respect": [18, 21, 35, 147, 164, 196, 206, 207], "whether": [18, 20, 22, 27, 30, 31, 34, 36, 40, 42, 43, 45, 46, 47, 48, 50, 51, 59, 60, 61, 66, 70, 71, 72, 82, 83, 84, 85, 86, 91, 96, 97, 98, 104, 105, 106, 109, 110, 111, 112, 113, 121, 122, 123, 137, 144, 148, 149, 157, 158, 167, 177, 180, 206, 207], "keep": [18, 208, 211], "alwai": [19, 45, 47, 48, 140, 173], "dataclass": [20, 206], "remain": [20, 27, 30, 31, 138, 211], "unmask": [20, 27, 30, 31], "human": [21, 22, 27, 31, 139, 141, 142, 206], "pre": [21, 37, 39, 50, 75, 136, 202, 206, 207], "taken": [21, 211, 213], "inst": [21, 23, 34, 39, 75, 206, 207], "sy": [21, 75, 206, 207], "honest": [21, 206, 207], "am": [21, 23, 206, 207, 208, 210], "pari": [21, 23, 207], "capit": [21, 23, 28, 207], "franc": [21, 23, 28, 207], "known": [21, 23, 75, 109, 181, 207, 212], "its": [21, 23, 37, 112, 128, 133, 134, 135, 140, 192, 195, 205, 206, 207, 208, 210, 211], "stun": [21, 23, 207], "liter": [22, 24, 26, 59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 148, 149], "ipython": [22, 24, 26, 39], "union": [22, 45, 47, 48, 50, 51, 149, 187, 188, 189, 190, 193, 195], "mask": [22, 24, 36, 37, 39, 42, 45, 47, 48, 65, 75, 91, 109, 121, 128, 134, 135, 141, 150, 156, 160, 162, 206, 207], "eot": [22, 91], "repres": [22, 52, 53, 136, 153, 164, 206, 212], "individu": [22, 37, 180, 190, 193, 206, 207], "interleav": [22, 162], "tokenize_messag": [22, 34, 36, 38, 40, 42, 43, 44, 46, 49, 50, 51, 65, 75, 91, 109, 121, 156, 160, 206, 207], "attach": 22, "special": [22, 34, 39, 75, 91, 95, 109, 121, 126, 136, 155, 156, 158, 159, 160, 162, 172, 207], "writer": 22, "dictionari": [22, 24, 37, 39, 152, 153, 180, 185, 187, 188, 189, 190, 191, 208], "hello": [22, 65, 75, 91, 109, 121, 157, 158, 206, 208, 210], "world": [22, 65, 75, 91, 109, 121, 157, 158, 182, 184, 208], "calcul": [22, 24, 128, 134, 136, 141, 150, 151, 164, 210], "correspond": [22, 141, 143, 146, 150, 153, 177, 209, 210, 212], "consecut": [22, 33, 162], "e": [22, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 128, 136, 137, 143, 147, 162, 164, 168, 172, 180, 196, 202, 208, 210, 211, 212, 213], "properti": [22, 173, 211], "contains_media": 22, "non": [22, 149, 151], "from_dict": [22, 206], "construct": [22, 162, 211], "text_cont": [22, 206], "mistral": [23, 34, 39, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 171, 205, 206, 208, 209], "llama2chatformat": [23, 75, 206, 207], "achiev": [24, 192, 208, 210, 211, 212, 213], "prepend_tag": 24, "append_tag": 24, "thu": [24, 39, 140, 212], "consid": [24, 35, 39, 53, 54, 55, 136], "come": [24, 33, 143, 211], "alia": [26, 167], "adher": [27, 30, 31], "sharegpt": [27, 31, 43], "gpt": [27, 31, 128, 208], "similar": [28, 44, 49, 50, 51, 139, 148, 207, 208, 210, 211, 213], "stackexchangedpair": 28, "question": [28, 206, 207, 208, 210], "answer": [28, 206, 208, 210], "nanswer": 28, "summar": [29, 47, 206, 207], "dialogu": [29, 47, 206], "nsummari": [29, 206], "summari": [29, 35, 47, 136, 180, 207], "could": [30, 211], "eos_id": [32, 91, 158, 160], "length": [32, 33, 35, 36, 37, 38, 40, 42, 44, 46, 50, 51, 65, 66, 70, 75, 76, 82, 91, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 125, 128, 131, 133, 134, 150, 151, 152, 153, 158, 160, 162, 169, 191], "last": [32, 37, 50, 138, 151, 207], "replac": [32, 36, 42, 45, 47, 48, 137, 211], "forth": [33, 207], "empti": [33, 205], "shorter": 33, "min": [33, 164, 211], "invalid": 33, "convert_to_messag": [34, 206], "chat_format": [34, 43, 206, 207], "chatformat": [34, 43, 207], "load_dataset_kwarg": [34, 36, 38, 39, 40, 43, 44, 46, 50, 51], "multiturn": [34, 206], "prepar": [34, 206, 212], "truncat": [34, 36, 37, 38, 40, 44, 46, 50, 51, 65, 75, 91, 109, 121, 154, 158, 160, 207], "local": [34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 95, 126, 190, 195, 202, 205, 206, 208, 209], "g": [34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 128, 136, 143, 162, 164, 168, 180, 196, 210, 211, 212, 213], "csv": [34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 206, 207], "filepath": [34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51], "data_fil": [34, 36, 38, 39, 40, 43, 45, 46, 47, 48, 50, 51, 206, 207], "load_dataset": [34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 206, 207], "huggingfac": [34, 36, 38, 40, 43, 44, 46, 50, 51, 117, 125, 126, 138, 139, 140, 142, 168, 169, 205, 208], "co": [34, 36, 38, 40, 43, 44, 46, 50, 51, 117, 125, 126, 168, 169, 208], "doc": [34, 36, 38, 39, 40, 43, 44, 46, 50, 51, 75, 91, 167, 173, 179, 184, 189, 190, 195, 205, 207, 208], "en": [34, 36, 38, 40, 43, 44, 46, 50, 51, 212], "package_refer": [34, 36, 38, 40, 43, 44, 46, 50, 51], "loading_method": [34, 36, 38, 40, 43, 44, 46, 50, 51], "extra": [34, 39, 202, 206, 211, 212, 213], "still": [34, 39, 173, 211, 212, 213], "unless": 34, "concaten": [35, 65, 75, 109, 121, 153, 156, 160], "sub": [35, 189], "unifi": [35, 117], "were": [35, 136, 141, 145, 206, 209, 212], "simplifi": [35, 139, 205, 211], "simultan": 35, "intern": [35, 173], "aggreg": 35, "transpar": 35, "index": [35, 37, 128, 133, 134, 135, 138, 151, 152, 153, 191, 202, 206, 208], "howev": [35, 126, 202], "constitu": 35, "might": [35, 205, 208], "larg": [35, 144, 196, 205, 213], "comput": [35, 39, 76, 82, 92, 96, 101, 104, 128, 129, 133, 134, 139, 140, 142, 162, 163, 180, 195, 208, 212, 213], "cumul": 35, "maintain": [35, 213], "indic": [35, 36, 37, 55, 128, 133, 134, 135, 136, 141, 150, 154, 162, 167, 184, 206], "deleg": 35, "retriev": [35, 39, 178], "lead": [35, 109, 157], "high": [35, 39, 203, 211], "scale": [35, 59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 110, 111, 112, 113, 122, 123, 140, 144, 151, 164, 175, 211, 212, 213], "strategi": 35, "stream": [35, 179], "demand": 35, "deriv": [35, 129, 134, 135], "dataset1": 35, "mycustomdataset": 35, "params1": 35, "dataset2": 35, "params2": 35, "concat_dataset": 35, "total": [35, 138, 141, 151, 182, 200, 204, 208, 210, 211], "data_point": 35, "1500": 35, "element": [35, 208], "accomplish": [35, 43, 46, 50], "instruct_dataset": [35, 207], "vicgal": [35, 207], "gpt4": [35, 207], "alpacainstructtempl": [35, 46, 207], "samsum": [35, 47, 207], "summarizetempl": [35, 206, 207], "focus": [35, 209], "enhanc": [35, 136, 213], "divers": 35, "machin": [35, 142, 176, 205, 208], "instructtempl": [36, 38, 207], "contribut": [36, 42, 45, 47, 48, 141, 151], "variabl": [36, 38, 46, 171, 184, 207, 213], "disabl": [36, 38, 40, 44, 46, 50, 51, 145, 195, 212], "recommend": [36, 38, 40, 42, 44, 46, 50, 51, 189, 206, 208, 213], "highest": [36, 38, 40, 42, 44, 46, 50, 51], "sequenc": [36, 37, 38, 40, 42, 44, 46, 50, 51, 65, 66, 70, 75, 76, 82, 91, 92, 96, 101, 104, 109, 110, 112, 114, 116, 121, 122, 124, 128, 131, 133, 134, 136, 151, 152, 153, 154, 158, 160, 162, 191, 206], "ds": [37, 48], "padding_idx": [37, 152, 153, 191], "max_pack": 37, "split_across_pack": [37, 50], "greedi": 37, "pack": [37, 41, 42, 43, 45, 46, 47, 48, 50, 51, 128, 133, 134, 135, 212], "done": [37, 148, 177, 186, 211, 212, 213], "outsid": [37, 195, 196, 211], "sampler": [37, 209], "part": [37, 142, 206, 213], "buffer": 37, "enough": [37, 206], "attent": [37, 55, 59, 60, 61, 66, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 116, 122, 123, 124, 125, 128, 131, 133, 134, 135, 148, 149, 162, 210, 211, 213], "lower": [37, 211], "triangular": 37, "cross": [37, 162], "attend": [37, 128, 134, 135, 162], "rel": [37, 128, 133, 134, 135, 139, 180, 211], "pad": [37, 136, 141, 151, 152, 153, 154, 164, 165, 191, 207], "max": [37, 65, 75, 109, 121, 134, 136, 138, 158, 160, 205, 211], "wise": 37, "collat": [37, 191, 207], "made": [37, 43, 46, 50, 133, 208], "smaller": [37, 140, 208, 210, 211, 212, 213], "jam": 37, "vari": 37, "s1": [37, 75, 109, 157], "s2": [37, 75, 109, 157], "s3": 37, "s4": 37, "contamin": 37, "input_po": [37, 128, 131, 133, 134, 135], "matrix": 37, "causal": [37, 128, 134, 135], "continu": [37, 136, 207], "increment": 37, "move": [37, 50, 134], "entir": [37, 50, 186, 206, 213], "avoid": [37, 50, 132, 136, 137, 140, 195, 205, 212, 213], "sentenc": [37, 50, 109], "message_transform": 39, "model_transform": [39, 45, 47, 48], "prompt_templ": [39, 45, 47, 48], "filter_fn": 39, "supervis": 39, "remot": 39, "At": [39, 134], "uniqu": [39, 75, 171], "extract": [39, 44, 159], "becaus": [39, 70, 131, 134, 136, 170, 205, 206, 212], "against": [39, 197, 212, 213], "round": [39, 212], "involv": [39, 212], "incorpor": [39, 139, 207], "media": 39, "unit": [39, 186, 203], "row": [39, 128, 134, 135, 164, 206], "happen": 39, "ti": [39, 70], "agnost": [39, 207], "treat": [39, 136, 145, 173, 206], "final": [39, 59, 60, 61, 66, 70, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 122, 123, 129, 134, 145, 148, 149, 208, 210, 211, 213], "modal": 39, "minimum": 39, "gear": 39, "whenev": [39, 211], "commun": [39, 206, 207, 208], "chatmlformat": [39, 43], "filter": [39, 212], "prior": [39, 42, 43, 45, 46, 47, 48, 50, 51], "ref": [39, 125, 126, 190], "add_eo": [40, 50, 65, 75, 91, 109, 121, 157, 158, 206], "freeform": [40, 50], "unstructur": [40, 50, 51], "corpu": [40, 44, 50, 51], "tabular": [40, 50], "txt": [40, 50, 187, 207, 209], "eo": [40, 50, 109, 121, 126, 157, 160, 206, 207], "yahma": [41, 46], "variant": [41, 45, 47], "version": [41, 70, 82, 96, 104, 110, 112, 122, 128, 175, 197, 202, 206, 210, 212, 213], "page": [41, 51, 202, 203, 205, 209, 210], "tatsu": 42, "lab": 42, "codebas": [42, 45, 47, 208], "anyth": [42, 44, 49], "subset": [42, 44, 45, 47, 48, 49, 51, 70, 82, 96, 104, 110, 112, 122, 146], "10": [42, 44, 45, 47, 48, 49, 51, 136, 153, 191, 208, 210, 212, 213], "alpaca_d": 42, "batch_siz": [42, 45, 47, 128, 131, 134, 135, 139, 140, 142, 152, 154, 208, 212], "conversation_styl": [43, 207], "chatdataset": [43, 206, 207], "friendli": [43, 46, 50, 175, 206], "check": [43, 52, 53, 54, 55, 134, 136, 148, 177, 184, 197, 201, 206, 208, 209, 211], "huggingfaceh4": 43, "no_robot": 43, "2096": [43, 46, 50], "packeddataset": [43, 45, 46, 47, 48, 50, 51, 207], "ccdv": 44, "cnn_dailymail": 44, "textcompletiondataset": [44, 50, 51, 207], "cnn": 44, "dailymail": 44, "articl": [44, 51], "highlight": [44, 213], "_transform": [45, 47, 136], "liweili": 45, "c4_200m": 45, "sftdataset": [45, 47, 48], "mirror": [45, 47], "llama_recip": [45, 47], "grammarerrorcorrectiontempl": [45, 47], "grammar_d": 45, "alpaca_clean": 46, "samsung": 47, "samsum_d": 47, "open": [48, 67, 68, 207, 208], "orca": 48, "slimorca": 48, "dedup": 48, "351": 48, "82": 48, "391": 48, "221": 48, "220": 48, "193": 48, "12": [48, 136, 153, 202, 212], "471": 48, "lvwerra": [49, 207], "stack": [49, 136, 196, 207], "exchang": [49, 207], "1024": [49, 207, 212], "preferencedataset": [49, 207], "stackexchangepair": 49, "allenai": [50, 207, 212], "c4": [50, 207, 212], "data_dir": [50, 207], "realnewslik": [50, 207], "wikitext_document_level": 51, "wikitext": [51, 212], "103": [51, 208], "wikipedia": 51, "clip": [52, 53, 54, 55, 136, 141], "max_num_til": [52, 53, 55, 136, 163], "tile": [52, 53, 54, 55, 136, 162, 163, 166], "patch": [52, 53, 54, 55, 136, 162], "document": [52, 53, 54, 55, 128, 140, 167, 178, 186, 205, 207], "vision_transform": [52, 53, 54, 55], "visiontransform": [52, 53, 54, 55], "divid": [52, 53, 54, 55, 136, 162, 163, 166], "dimension": [52, 53, 54, 55, 136], "aspect_ratio": [52, 53, 136], "bsz": [52, 53, 136, 175], "n_img": [52, 53, 136], "n_tile": [52, 53, 136], "n_token": [52, 53, 54, 136], "aspect": [52, 53, 203], "ratio": [52, 53, 139, 140, 141], "crop": [52, 53, 54, 55, 136, 166], "tile_s": [53, 54, 55, 136, 162, 163, 166], "patch_siz": [53, 54, 55, 136, 162], "local_token_positional_embed": 53, "_position_embed": [53, 136], "tokenpositionalembed": [53, 136], "gate": [53, 171, 205, 209], "global_token_positional_embed": 53, "advanc": [53, 54, 55, 136, 207], "40": [53, 54, 55, 136, 162, 213], "400": [53, 54, 55, 136, 162, 166], "10x10": [53, 54, 55, 136, 162], "grid": [53, 54, 55, 136, 162], "k": [53, 128, 211], "th": 53, "cls_output_dim": [55, 136], "out_indic": [55, 136], "output_cls_project": 55, "in_channel": [55, 136], "transformerencoderlay": 55, "cl": [55, 136, 207], "head": [55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 133, 134, 171, 210], "intermedi": [55, 66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 136, 170, 193, 210, 213], "fourth": [55, 136], "determin": [55, 149, 164], "channel": [55, 136, 212], "code_llama2": [56, 57, 58, 59, 60, 61, 62, 63, 64, 205], "transformerdecod": [56, 57, 58, 59, 60, 61, 62, 63, 64, 76, 77, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 122, 123, 124, 125, 127, 175, 211], "w": [56, 57, 58, 67, 68, 77, 78, 79, 80, 93, 94, 102, 103, 115, 117, 136, 165, 189, 190, 206, 208, 211, 213], "arxiv": [56, 57, 58, 62, 63, 64, 73, 74, 77, 78, 79, 80, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 128, 132, 133, 136, 139, 140, 141, 142, 150, 162], "org": [56, 57, 58, 62, 63, 64, 73, 74, 75, 77, 78, 79, 80, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 128, 132, 133, 136, 139, 140, 141, 142, 150, 162, 167, 173, 179, 184, 189, 193, 195, 202], "pdf": [56, 57, 58, 150, 162], "2308": [56, 57, 58], "12950": [56, 57, 58], "lora_attn_modul": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 148, 149, 211, 213], "q_proj": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 128, 148, 149, 211, 212, 213], "k_proj": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 128, 148, 149, 211, 212, 213], "v_proj": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 128, 148, 149, 211, 212, 213], "output_proj": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 128, 148, 149, 211, 212, 213], "apply_lora_to_mlp": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 148, 149, 211], "apply_lora_to_output": [59, 60, 61, 62, 63, 64, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 148, 149, 211], "lora_rank": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 211], "lora_alpha": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 211], "float": [59, 60, 61, 62, 63, 64, 66, 70, 71, 72, 73, 74, 76, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 116, 119, 120, 122, 123, 124, 127, 128, 132, 138, 139, 140, 141, 142, 144, 150, 151, 175, 180, 185, 187, 188, 189, 190, 211, 212, 213], "16": [59, 60, 61, 62, 63, 64, 71, 72, 73, 74, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 105, 106, 107, 108, 111, 113, 119, 120, 123, 127, 136, 153, 211, 213], "lora_dropout": [59, 60, 61, 62, 63, 64, 70, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 104, 110, 112, 122], "05": [59, 60, 61, 62, 63, 64, 76, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "quantize_bas": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 144, 213], "lora": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 144, 145, 148, 149, 168, 186, 201, 203, 206, 209, 210], "code_llama2_13b": 59, "tloen": [59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123], "8bb8579e403dc78e37fe81ffbb253c413007323f": [59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123], "l41": [59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123], "l43": [59, 60, 61, 71, 72, 83, 84, 85, 86, 97, 98, 105, 106, 123], "linear": [59, 60, 61, 62, 63, 64, 70, 71, 72, 73, 74, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 119, 120, 122, 123, 127, 134, 143, 144, 148, 149, 211, 212, 213], "mlp": [59, 60, 61, 66, 70, 71, 72, 76, 82, 83, 84, 85, 86, 92, 96, 97, 98, 101, 104, 105, 106, 110, 111, 112, 113, 114, 116, 122, 123, 124, 134, 135, 148, 149, 210, 211], "low": [59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 110, 111, 112, 113, 122, 123, 144, 208, 211, 213], "approxim": [59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 110, 111, 112, 113, 122, 123, 144, 211], "factor": [59, 60, 61, 70, 71, 72, 82, 83, 84, 85, 86, 96, 97, 98, 104, 105, 106, 110, 111, 112, 113, 122, 123, 144, 150, 164, 208], "dropout": [59, 60, 61, 66, 70, 76, 82, 83, 84, 85, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 144, 211, 213], "probabl": [59, 60, 61, 70, 82, 83, 84, 85, 96, 104, 110, 112, 122, 139, 140, 141, 142, 144, 175, 208], "code_llama2_70b": 60, "code_llama2_7b": 61, "qlora": [62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 137, 201, 203, 210, 211], "per": [62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 131, 136, 137, 139, 151, 162, 163, 205, 212, 213], "paper": [62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 139, 140, 142, 162, 211, 213], "ab": [62, 63, 64, 73, 74, 77, 78, 79, 80, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 128, 132, 133, 136, 139, 140, 141, 142], "2305": [62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127, 128, 139, 142], "14314": [62, 63, 64, 73, 74, 87, 88, 89, 90, 99, 100, 107, 108, 119, 120, 127], "lora_code_llama2_13b": 62, "lora_code_llama2_70b": 63, "lora_code_llama2_7b": 64, "gemma": [65, 67, 68, 69, 70, 71, 72, 73, 74, 171], "sentencepiec": [65, 75, 109, 121, 157, 210], "pretrain": [65, 75, 91, 109, 121, 157, 158, 205, 206, 209, 211, 213], "spm_model": [65, 75, 109, 121, 157, 206], "tokenized_text": [65, 75, 91, 109, 121, 157, 158], "add_bo": [65, 75, 91, 109, 121, 157, 158, 206], "31587": [65, 75, 91, 109, 121, 157, 158], "29644": [65, 75, 91, 109, 121, 157, 158], "102": [65, 75, 91, 109, 121, 157, 158], "tokenizer_path": [65, 75, 109, 121], "separ": [65, 75, 109, 121, 160, 168, 206, 209, 210, 211, 213], "concat": [65, 75, 109, 121, 160], "1788": [65, 75, 109, 121, 160], "2643": [65, 75, 109, 121, 160], "13": [65, 75, 109, 121, 136, 153, 154, 160, 213], "1792": [65, 75, 109, 121, 160], "9508": [65, 75, 109, 121, 160], "465": [65, 75, 109, 121, 160], "22137": [65, 75, 109, 121, 160], "2933": [65, 75, 109, 121, 160], "join": [65, 75, 109, 121, 160], "attribut": [65, 75, 109, 121, 145, 160, 174], "head_dim": [66, 70, 128, 131, 134], "intermediate_dim": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "attn_dropout": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 134], "norm_ep": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "1e": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 132], "06": [66, 70, 132, 211], "rope_bas": [66, 70, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "10000": [66, 70, 110, 112, 114, 116, 122, 124, 133], "norm_embed": [66, 70], "gemmatransformerdecod": [66, 67, 68, 70, 71, 72, 73, 74], "transformerdecoderlay": [66, 76, 92, 101, 114, 134], "rm": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124], "norm": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 134, 135], "space": [66, 76, 92, 101, 114, 134], "slide": [66, 114, 125], "window": [66, 114, 125, 207], "vocabulari": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 211], "queri": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 134, 135, 210], "mha": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 134], "dimens": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128, 131, 133, 134, 136, 144, 210, 211, 213], "onto": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128], "scaled_dot_product_attent": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128], "epsilon": [66, 70, 76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 141], "rotari": [66, 70, 110, 112, 114, 116, 122, 124, 133, 210], "10_000": [66, 70, 110, 112, 114, 116, 124], "blog": [67, 68], "technolog": [67, 68], "develop": [67, 68, 213], "gemmatoken": 69, "gemma_2b": 71, "gemma_7b": 72, "lora_gemma_2b": 73, "lora_gemma_7b": 74, "card": [75, 91], "regist": [75, 91, 95, 121, 126, 129, 137, 192, 213], "strongli": 75, "beforehand": 75, "html": [75, 167, 173, 179, 184, 189, 193, 195, 201], "problem": [75, 109], "due": [75, 109, 157, 211, 213], "whitespac": [75, 109, 157], "slice": [75, 109], "gqa": [76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128], "mqa": [76, 82, 92, 96, 101, 104, 110, 112, 114, 116, 122, 124, 128], "kvcach": [76, 82, 92, 96, 101, 104, 122, 128, 134], "scale_hidden_dim_for_mlp": [76, 82, 92, 96, 101, 104], "2307": [77, 78, 79, 80], "09288": [77, 78, 79, 80], "classif": [80, 112, 116, 117, 171], "reward": [80, 86, 90, 113, 117, 120, 139, 140, 142, 150, 151, 171], "llama2_70b": 84, "llama2_7b": [85, 211], "classifi": [86, 112, 116, 117, 207], "llama2_reward_7b": [86, 171], "lora_llama2_13b": 87, "lora_llama2_70b": 88, "lora_llama2_7b": [89, 211], "lora_llama2_reward_7b": 90, "special_token": [91, 121, 158, 206], "tiktoken": [91, 158, 210], "left": [91, 121, 152, 211], "canon": [91, 95, 121, 126], "tt_model": [91, 158], "token_id": [91, 109, 155, 158], "truncate_at_eo": [91, 158], "skip_special_token": [91, 158], "show": [91, 158, 162, 202, 205, 206, 211], "skip": [91, 158], "tokenize_head": 91, "tokenize_end": 91, "header": [91, 206], "eom": 91, "wether": 91, "500000": [92, 96, 101, 104], "special_tokens_path": [95, 126], "llama3token": [95, 206], "similarli": [95, 126, 207, 212], "llama3_70b": 97, "llama3_8b": [98, 175, 210, 212], "lora_llama3_70b": 99, "lora_llama3_8b": 100, "llama3_1": [102, 103, 104, 105, 106, 107, 108], "llama3_1_70b": 105, "llama3_1_8b": 106, "lora_llama3_1_70b": 107, "lora_llama3_1_8b": 108, "trim_leading_whitespac": [109, 157], "unbatch": [109, 157], "bo": [109, 126, 157, 160, 206, 207], "trim": [109, 157], "num_class": [112, 116], "announc": 115, "ray2333": 117, "feedback": [117, 139], "mistraltoken": [118, 206], "lora_mistral_7b": 119, "lora_mistral_reward_7b": 120, "ignore_system_prompt": 121, "phi3_mini": [123, 171], "phi": [125, 126, 171], "128k": 125, "nor": 125, "phi3minitoken": 126, "tokenizer_config": 126, "spm": 126, "lm": [126, 141], "unk": 126, "augment": [126, 213], "endoftext": 126, "phi3minisentencepiecebasetoken": 126, "lora_phi3_mini": 127, "pos_embed": [128, 211, 212], "kv_cach": 128, "introduc": [128, 132, 144, 206, 207, 211, 212, 213], "13245v1": 128, "multihead": 128, "extrem": 128, "share": [128, 207, 208], "credit": 128, "lightn": 128, "lit": 128, "lit_gpt": 128, "v": [128, 134, 211], "q": [128, 211], "n_kv_head": 128, "rotarypositionalembed": [128, 211, 212], "cach": [128, 131, 133, 134, 202, 205], "rope": [128, 133], "seq_length": [128, 135, 175], "boolean": [128, 134, 135, 167], "softmax": [128, 134, 135], "j": [128, 134, 135], "seq_len": 128, "bigger": 128, "n_h": [128, 133], "num": [128, 133], "n_kv": 128, "kv": [128, 131, 134, 212], "emb": [128, 134], "h_d": [128, 133], "gate_proj": 129, "down_proj": 129, "up_proj": 129, "silu": 129, "feed": [129, 135], "network": [129, 145, 211, 213], "fed": [129, 206], "multipli": 129, "subclass": [129, 173], "although": [129, 211, 212], "afterward": 129, "former": 129, "hook": [129, 137, 192, 213], "latter": 129, "layernorm": 130, "standalon": 131, "past": 131, "expand": 131, "dpython": [131, 134, 137, 194, 198], "reset": [131, 134, 180], "k_val": 131, "v_val": 131, "assert": 131, "longer": [131, 207], "h": [131, 136, 165, 202, 205], "ep": 132, "root": [132, 189, 190], "squar": 132, "1910": 132, "07467": 132, "verfic": [132, 133], "small": [132, 208], "divis": [132, 166], "propos": 133, "2104": 133, "09864": 133, "l80": 133, "upto": 133, "init": [133, 180, 190, 213], "exceed": 133, "freq": 133, "recomput": 133, "geometr": 133, "progress": [133, 209], "rotat": 133, "angl": 133, "todo": 133, "effici": [133, 148, 178, 201, 203, 208, 209, 211, 212], "belong": [134, 174], "reduc": [134, 139, 203, 207, 211, 212, 213], "statement": 134, "improv": [134, 142, 158, 178, 210, 211], "readabl": [134, 208], "caches_are_en": 134, "arang": 134, "prompt_length": 134, "causal_mask": 134, "m_": 134, "seq": 134, "reset_cach": 134, "setup_cach": 134, "attn": [135, 211, 212, 213], "causalselfattent": [135, 211, 212], "sa_norm": 135, "mlp_norm": 135, "ff": 135, "token_pos_embed": 136, "pre_tile_pos_emb": 136, "post_tile_pos_emb": 136, "cls_project": 136, "vit": 136, "11929": 136, "convolut": 136, "flatten": 136, "downscal": [136, 164, 165], "800x400": 136, "400x400": 136, "clipimagetransform": 136, "broken": 136, "down": [136, 170, 207, 211, 213], "whole": 136, "num_til": [136, 166], "101": 136, "pool": 136, "tiledtokenpositionalembed": 136, "tilepositionalembed": 136, "tile_pos_emb": 136, "even": [136, 202, 205, 206, 207, 210, 211, 213], "8x8": 136, "14": [136, 153, 212, 213], "15": [136, 153, 178, 206, 208, 211, 213], "17": [136, 153, 211], "18": [136, 153, 210], "19": [136, 153, 213], "20": [136, 153, 154, 212], "21": 136, "22": 136, "23": [136, 138], "24": [136, 166, 209, 210], "25": [136, 208], "26": 136, "27": [136, 208], "28": [136, 208], "29": [136, 213], "30": [136, 154, 212], "31": [136, 210], "33": 136, "34": 136, "35": [136, 213], "36": 136, "37": 136, "38": [136, 208], "39": 136, "41": 136, "42": 136, "43": 136, "44": 136, "45": 136, "46": 136, "47": 136, "48": [136, 208, 213], "49": 136, "50": [136, 154, 166, 208], "51": 136, "52": [136, 209], "53": 136, "54": 136, "55": [136, 209], "56": 136, "57": [136, 211, 213], "58": 136, "59": [136, 213], "60": 136, "61": [136, 208], "62": 136, "63": 136, "64": [136, 211], "num_patches_per_til": 136, "emb_dim": 136, "greater": [136, 197], "constain": 136, "anim": [136, 207], "max_n_img": 136, "n_channel": 136, "hidden_st": 136, "vision_util": 136, "tile_crop": 136, "num_channel": 136, "image_s": [136, 165], "800": [136, 165], "patch_grid_s": 136, "random": [136, 195, 209], "rand": [136, 164, 165, 166], "nch": 136, "tile_cropped_imag": 136, "batch_imag": 136, "unsqueez": 136, "batch_aspect_ratio": 136, "clip_vision_encod": 136, "common_util": 137, "bfloat16": [137, 194, 208, 209, 210, 211, 212], "offload_to_cpu": 137, "nf4": [137, 213], "restor": 137, "higher": [137, 140, 210, 212, 213], "offload": [137, 213], "increas": [137, 138, 139, 210, 211, 212], "peak": [137, 180, 185, 208, 210, 211, 213], "gpu": [137, 205, 208, 209, 210, 211, 212, 213], "_register_state_dict_hook": 137, "m": [137, 175, 206, 212], "mymodul": 137, "_after_": 137, "nf4tensor": [137, 213], "unquant": [137, 212, 213], "unus": 137, "num_warmup_step": 138, "num_training_step": 138, "num_cycl": [138, 196], "last_epoch": 138, "lambdalr": 138, "rate": [138, 203, 209], "schedul": [138, 196, 209], "linearli": 138, "decreas": [138, 207, 211, 212, 213], "cosin": 138, "v4": 138, "src": 138, "l104": 138, "warmup": [138, 196], "phase": 138, "wave": 138, "half": 138, "lr_schedul": 138, "beta": 139, "label_smooth": 139, "dpo": [139, 140, 142, 145, 153], "18290": 139, "intuit": [139, 140, 142], "dispref": 139, "dynam": [139, 212], "degener": 139, "occur": 139, "naiv": 139, "trl": [139, 140, 142], "librari": [139, 140, 142, 173, 177, 179, 195, 201, 203, 205, 207, 213], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 139, "dpo_train": [139, 140, 142], "l844": 139, "retain": [139, 213], "2009": 139, "01325": 139, "polici": [139, 140, 141, 142, 145, 151, 167, 178, 186, 193], "align": [139, 206], "regular": [139, 140, 212, 213], "baselin": [139, 141, 208, 211], "rather": 139, "overhead": [139, 212], "temperatur": [139, 140, 142, 175, 208], "uncertainti": 139, "policy_chosen_logp": [139, 140, 142], "policy_rejected_logp": [139, 140, 142], "reference_chosen_logp": [139, 140, 142], "reference_rejected_logp": [139, 140, 142], "chosen": [139, 140, 142, 196, 207], "reject": [139, 140, 142, 207], "chosen_reward": [139, 140, 142], "rejected_reward": [139, 140, 142], "tau": 140, "optimis": 140, "ipo": 140, "2310": 140, "12036": 140, "pi": 140, "pi_ref": 140, "regress": [140, 142], "gap": 140, "likelihood": 140, "he": 140, "weaker": 140, "regularis": 140, "logprob": [140, 151], "word": [140, 212], "unlik": [140, 148], "toward": 140, "4dce042a3863db1d375358e8c8092b874b02934b": [140, 142], "l1143": 140, "reciproc": 140, "larger": [140, 170, 208, 210], "value_clip_rang": 141, "value_coeff": 141, "proxim": 141, "1707": 141, "06347": 141, "eqn": 141, "vwxyzjn": 141, "ccc19538e817e98a60d3253242ac15e2a562cb49": 141, "lm_human_preference_detail": 141, "train_policy_acceler": 141, "l719": 141, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 141, "ppo2": 141, "l68": 141, "l75": 141, "coeffici": [141, 151], "pi_old_logprob": 141, "pi_logprob": 141, "phi_old_valu": 141, "phi_valu": 141, "padding_mask": [141, 154], "value_padding_mask": 141, "old": 141, "predict": [141, 150, 151, 175], "participag": 141, "five": 141, "policy_loss": 141, "value_loss": 141, "clipfrac": 141, "fraction": 141, "gamma": [142, 150], "statist": 142, "rso": 142, "hing": 142, "2309": 142, "06657": 142, "logist": 142, "slic": 142, "10425": 142, "almost": [142, 211], "vector": [142, 206], "svm": 142, "counter": 142, "l1141": 142, "peft": [143, 144, 145, 146, 147, 148, 149, 168, 211, 213], "protocol": 143, "adapter_param": [143, 144, 145, 146, 147], "proj": 143, "in_dim": [143, 144, 211, 213], "out_dim": [143, 144, 211, 213], "bia": [143, 144, 211, 212, 213], "loralinear": [143, 211, 213], "alpha": [144, 211, 213], "use_bia": 144, "perturb": 144, "decomposit": [144, 211], "matric": [144, 186, 211, 213], "trainabl": [144, 147, 186, 211, 213], "mapsto": 144, "w_0x": 144, "r": [144, 211], "bax": 144, "lora_a": [144, 211, 213], "lora_b": [144, 211, 213], "temporarili": 145, "neural": [145, 211, 213], "caller": 145, "whose": [145, 192], "yield": 145, "get_adapter_param": [147, 211], "base_miss": 148, "base_unexpect": 148, "lora_miss": 148, "lora_unexpect": 148, "validate_state_dict_for_lora": [148, 211], "reli": [148, 160, 208, 210], "unexpect": 148, "strict": [148, 211], "pull": [148, 205], "120600": 148, "assertionerror": [148, 149, 153], "nonempti": 148, "full_model_state_dict_kei": 149, "lora_state_dict_kei": 149, "base_model_state_dict_kei": 149, "confirm": [149, 202], "lora_modul": 149, "complement": 149, "disjoint": 149, "overlap": 149, "rlhf": [150, 151, 152, 153, 154, 207], "lmbda": 150, "estim": [150, 151], "1506": 150, "02438": 150, "reponse_len": [150, 151], "receiv": [150, 206], "discount": 150, "gae": 150, "lambda": 150, "particip": [150, 162], "score": 151, "ref_logprob": 151, "kl_coeff": 151, "valid_score_idx": 151, "kl": 151, "response_len": 151, "total_reward": 151, "combin": [151, 163], "diverg": 151, "kl_reward": 151, "ignore_idx": [153, 191], "input_id": 153, "chosen_input_id": [153, 207], "rejected_input_id": [153, 207], "chosen_label": [153, 207], "rejected_label": [153, 207], "stop_token": [154, 175], "fill_valu": 154, "stop": [154, 175], "sequence_length": 154, "pad_id": 154, "been": [154, 178, 206, 212], "stop_token_id": 154, "869": 154, "eos_mask": 154, "truncated_sequ": 154, "light": 157, "sentencepieceprocessor": 157, "prefix": 157, "bos_id": [158, 160], "lightweight": [158, 206], "break": 158, "substr": 158, "repetit": 158, "speed": [158, 196, 210, 212, 213], "identif": 158, "regex": 158, "chunk": 158, "present": [158, 170], "absent": 158, "tokenizer_json_path": 159, "heavili": 160, "beggin": 160, "runtimeerror": [160, 172, 177, 183], "satisfi": [160, 164, 208], "loos": 161, "image_token_id": 162, "laid": 162, "fig": 162, "flamingo": 162, "2204": 162, "14198": 162, "immedi": 162, "until": 162, "img1": 162, "img2": 162, "img3": 162, "dog": 162, "cat": 162, "text_seq_len": 162, "image_seq_len": 162, "equal": [162, 166, 197], "resolut": [163, 164, 165], "1x1": 163, "1x2": 163, "2x1": 163, "side": [163, 164, 165], "height": [163, 164, 165], "width": [163, 164, 165, 212], "224": [163, 164, 165], "896": 163, "448": [163, 164, 165], "672": [163, 164], "possible_resolut": 164, "resize_to_max_canva": 164, "canva": 164, "resiz": [164, 165], "distort": [164, 165], "select": 164, "smallest": 164, "upscal": [164, 165], "2x": 164, "5x": 164, "canvas": 164, "condit": [164, 175, 184, 205, 207], "pick": 164, "lowest": [164, 211], "area": [164, 208], "minim": [164, 207, 209, 211, 212, 213], "200": [164, 166, 213], "300": [164, 165, 166, 208], "scale_height": 164, "1200": 164, "3600": 164, "2400": 164, "scale_width": 164, "7467": 164, "4933": 164, "scaling_factor": 164, "upscaling_opt": 164, "selected_scal": 164, "150528": 164, "100352": 164, "optimal_canva": 164, "target_s": 165, "resampl": 165, "interpolationmod": 165, "max_upscaling_s": 165, "exce": 165, "torchvis": 165, "nearest": 165, "nearest_exact": 165, "bilinear": 165, "bicub": 165, "1194": 165, "1344": 165, "stai": 165, "600": [165, 166], "500": [165, 211], "1000": [165, 167, 212], "488": 165, "channel_s": 166, "4x6": 166, "2x3": 166, "datatyp": [167, 213], "denot": 167, "integ": [167, 191, 195], "auto_wrap_polici": [167, 178, 193], "submodul": [167, 186], "obei": 167, "contract": 167, "get_fsdp_polici": 167, "modules_to_wrap": [167, 178, 186], "min_num_param": 167, "my_fsdp_polici": 167, "recurs": [167, 186, 189], "isinst": [167, 207], "sum": [167, 211], "p": [167, 172, 211, 212, 213], "numel": [167, 211], "stabl": [167, 184, 189, 195, 202], "safe_seri": 168, "from_pretrain": 168, "0001_of_0003": 168, "0002_of_0003": 168, "preserv": [168, 213], "weight_map": [168, 208], "convert_weight": 168, "_model_typ": [168, 171], "intermediate_checkpoint": [168, 169, 170], "adapter_onli": [168, 169, 170], "_weight_map": 168, "shard": [169, 210], "wip": 169, "qualnam": 171, "boundari": 171, "distinguish": 171, "mistral_reward_7b": 171, "qwen2": 171, "my_new_model": 171, "my_custom_state_dict_map": 171, "optim_map": 172, "bare": 172, "bone": 172, "distribut": [172, 176, 183, 184, 193, 195, 203, 205, 209, 210], "optim_dict": [172, 174, 192], "cfg_optim": 172, "ckpt": 172, "optim_ckpt": 172, "placeholder_optim_dict": 172, "optiminbackwardwrapp": 172, "get_optim_kei": 172, "arbitrari": [172, 211], "hyperparamet": [172, 203, 209, 211, 213], "optim_ckpt_map": 172, "loadabl": 172, "argpars": 173, "argumentpars": 173, "builtin": 173, "said": 173, "noth": 173, "consult": 173, "info": [173, 209], "parse_known_arg": 173, "namespac": 173, "act": 173, "precid": 173, "parse_arg": 173, "too": [173, 210], "optimizerinbackwardwrapp": 174, "top": [174, 213], "named_paramet": 174, "max_generated_token": 175, "top_k": [175, 208], "custom_generate_next_token": 175, "prune": [175, 213], "compil": [175, 208, 210, 213], "generate_next_token": 175, "hi": [175, 206], "my": [175, 205, 206, 207, 208, 210], "jeremi": 175, "float32": 177, "bf16": [177, 213], "inde": [177, 208], "kernel": 177, "isn": [177, 205], "hardwar": [177, 203, 207, 208, 211], "memory_efficient_fsdp_wrap": [178, 212], "maxim": [178, 186, 201, 203], "workload": [178, 212], "alongsid": 178, "ac": 178, "fullyshardeddataparallel": [178, 186], "fsdppolicytyp": [178, 186], "handler": 179, "reset_stat": 180, "track": 180, "alloc": [180, 185, 186, 210, 213], "reserv": [180, 185, 206, 213], "stat": [180, 185, 213], "int4": [181, 212], "4w": 181, "recogn": 181, "int8dynactint4weightquant": [181, 212], "8da4w": [181, 212], "int8dynactint4weightqatquant": [181, 212], "qat": [181, 201], "mode": [181, 208], "aka": 182, "master": 184, "port": [184, 205], "address": 184, "hold": [184, 209], "peak_memory_act": 185, "peak_memory_alloc": 185, "peak_memory_reserv": 185, "get_memory_stat": 185, "hierarch": 186, "requires_grad": [186, 211, 213], "filenam": 187, "log_": 187, "unixtimestamp": 187, "thread": 187, "safe": 187, "flush": [187, 188, 189, 190], "ndarrai": [187, 188, 189, 190], "scalar": [187, 188, 189, 190], "record": [187, 188, 189, 190, 196], "payload": [187, 188, 189, 190], "organize_log": 189, "tensorboard": 189, "subdirectori": 189, "compar": [189, 197, 208, 210, 211, 212, 213], "logdir": 189, "startup": 189, "tree": [189, 207, 208, 210], "tfevent": 189, "encount": 189, "frontend": 189, "organ": [189, 205], "accordingli": [189, 212], "my_log_dir": 189, "view": [189, 209], "my_metr": [189, 190], "termin": [189, 190], "entiti": 190, "bias": [190, 211, 213], "sent": 190, "usernam": 190, "my_project": 190, "my_ent": 190, "my_group": 190, "importerror": 190, "account": [190, 211, 213], "log_config": 190, "link": [190, 208, 210], "capecap": 190, "6053ofw0": 190, "torchtune_config_j67sb73v": 190, "longest": 191, "token_pair": 191, "soon": 192, "readi": [192, 201, 206, 212], "grad": 192, "acwrappolicytyp": 193, "author": [193, 203, 209, 213], "fsdp_adavnced_tutori": 193, "insid": 194, "contextmanag": 194, "debug_mod": 195, "pseudo": 195, "commonli": [195, 211, 213], "numpi": 195, "determinist": 195, "global": [195, 207], "warn": 195, "nondeterminist": 195, "cudnn": 195, "set_deterministic_debug_mod": 195, "profile_memori": 196, "with_stack": 196, "record_shap": 196, "with_flop": 196, "wait_step": 196, "warmup_step": 196, "active_step": 196, "profil": 196, "layout": 196, "trace": 196, "profileract": 196, "gradient_accumul": 196, "sensibl": 196, "default_schedul": 196, "reduct": [196, 211], "iter": [196, 198, 213], "scope": 196, "flop": 196, "wait": 196, "cycl": 196, "repeat": 196, "__version__": 197, "named_param": 198, "generated_examples_python": 199, "zip": 199, "galleri": [199, 204], "sphinx": 199, "000": [200, 204, 210], "execut": [200, 204], "generated_exampl": 200, "mem": [200, 204], "mb": [200, 204], "topic": 201, "gentl": 201, "introduct": 201, "first_finetune_tutori": 201, "workflow": [201, 207, 209, 211], "requisit": 202, "proper": [202, 209], "host": [202, 205, 209], "latest": [202, 209, 213], "And": [202, 208], "ls": [202, 205, 208, 209, 210], "welcom": [202, 205], "greatest": [202, 209], "contributor": 202, "cd": [202, 208], "commit": 202, "branch": 202, "url": 202, "whl": 202, "therebi": [202, 212, 213], "forc": 202, "reinstal": 202, "opt": [202, 209], "suffix": 202, "cu121": 202, "On": [203, 211], "pointer": 203, "emphas": 203, "simplic": 203, "component": 203, "prove": 203, "democrat": 203, "box": [203, 213], "zoo": 203, "varieti": [203, 211], "techniqu": [203, 208, 209, 210, 211, 212], "integr": [203, 208, 209, 210, 211, 212, 213], "excit": 203, "checkout": 203, "quickstart": 203, "attain": 203, "better": [203, 206, 207, 208, 212], "chekckpoint": 203, "embodi": 203, "philosophi": 203, "usabl": 203, "composit": 203, "hard": [203, 207], "outlin": 203, "unecessari": 203, "never": 203, "thoroughli": 203, "short": 205, "subcommand": 205, "anytim": 205, "symlink": 205, "auto": 205, "wrote": 205, "readm": [205, 208, 210], "md": 205, "lot": [205, 208], "recent": 205, "releas": [205, 210], "agre": 205, "term": 205, "perman": 205, "eat": 205, "bandwith": 205, "storag": [205, 213], "00030": 205, "ootb": 205, "full_finetune_single_devic": [205, 207, 208, 209], "7b_full_low_memori": [205, 208, 209], "8b_full_single_devic": [205, 207], "mini_full_low_memori": 205, "7b_full": [205, 208, 209], "13b_full": [205, 208, 209], "70b_full": 205, "edit": 205, "clobber": 205, "destin": 205, "lora_finetune_distribut": [205, 210, 211], "torchrun": 205, "8b_lora_single_devic": [205, 206, 210], "launch": [205, 206, 209], "nproc": 205, "node": 205, "worker": 205, "nnode": [205, 211, 212], "minimum_nod": 205, "maximum_nod": 205, "fail": 205, "rdzv": 205, "rendezv": 205, "endpoint": 205, "8b_lora": [205, 210], "bypass": 205, "vice": 205, "versa": 205, "fancy_lora": 205, "8b_fancy_lora": 205, "sai": [205, 206, 209], "know": [206, 207, 208, 211], "intend": 206, "nice": 206, "meet": 206, "overhaul": 206, "begin_of_text": 206, "start_header_id": 206, "end_header_id": 206, "eot_id": 206, "yet": [206, 208], "untrain": 206, "accompani": 206, "who": 206, "influenti": 206, "hip": 206, "hop": 206, "artist": 206, "2pac": 206, "rakim": 206, "c": 206, "na": 206, "flavor": [206, 207], "msg": 206, "formatted_messag": [206, 207], "nyou": [206, 207], "nwho": 206, "why": [206, 209, 211], "518": 206, "25580": 206, "29962": 206, "3532": 206, "14816": 206, "29903": 206, "6778": 206, "_spm_model": 206, "piece_to_id": 206, "place": [206, 207], "manual": [206, 213], "529": 206, "29879": 206, "29958": 206, "nhere": 206, "128000": [206, 212], "128009": 206, "pure": 206, "That": 206, "won": 206, "mess": 206, "govern": 206, "prime": 206, "strictli": 206, "ask": 206, "untouch": 206, "though": 206, "robust": 206, "onlin": 206, "forum": 206, "panda": 206, "pd": 206, "df": 206, "read_csv": 206, "your_fil": 206, "nrow": 206, "tolist": 206, "iloc": 206, "gp": 206, "satellit": 206, "thing": [206, 213], "message_convert": 206, "input_msg": 206, "output_msg": 206, "But": [206, 208, 211], "mistralchatformat": 206, "custom_dataset": 206, "2048": 206, "honor": 206, "copi": [206, 208, 209, 210, 212, 213], "custom_8b_lora_single_devic": 206, "steer": 207, "wheel": 207, "publicli": 207, "great": [207, 208], "hood": [207, 213], "text_completion_dataset": [207, 212], "padded_col": 207, "upper": 207, "constraint": [207, 211], "slow": [207, 213], "signific": [207, 212], "speedup": [207, 208, 210], "my_data": 207, "fix": [207, 212], "goal": [207, 212], "respond": 207, "plant": 207, "miner": 207, "oak": 207, "copper": 207, "ore": 207, "eleph": 207, "customtempl": 207, "importlib": 207, "import_modul": 207, "mechan": 207, "search": 207, "often": [207, 211], "interpret": 207, "site": [207, 208], "runtim": 207, "pythonpath": 207, "chat_dataset": 207, "quit": [207, 213], "customchatformat": 207, "concatdataset": 207, "drive": 207, "rajpurkar": 207, "io": 207, "squad": 207, "explor": 207, "few": [207, 210, 211, 213], "adjust": [207, 212], "chosen_messag": 207, "transformed_sampl": 207, "key_chosen": 207, "rejected_messag": 207, "key_reject": 207, "c_mask": 207, "np": 207, "cross_entropy_ignore_idx": 207, "r_mask": 207, "stack_exchanged_paired_dataset": 207, "had": 207, "stackexchangedpairedtempl": 207, "response_j": 207, "response_k": 207, "rl": 207, "favorit": [208, 211], "seemlessli": 208, "beyond": [208, 213], "connect": [208, 212], "amount": 208, "natur": 208, "export": 208, "mobil": 208, "phone": 208, "leverag": [208, 210, 213], "plai": 208, "freez": [208, 211], "percentag": 208, "learnabl": 208, "16gb": [208, 211], "rtx": 208, "3090": 208, "4090": 208, "hour": 208, "7b_qlora_single_devic": [208, 209, 213], "473": 208, "98": [208, 213], "gb": [208, 210, 211, 212, 213], "484": 208, "01": [208, 209], "fact": [208, 210, 211], "third": 208, "realli": 208, "eleuther_ev": [208, 210, 212], "eleuther_evalu": [208, 210, 212], "lm_eval": [208, 210], "plan": 208, "custom_eval_config": [208, 210], "truthfulqa_mc2": [208, 210, 211], "measur": [208, 210], "propens": [208, 210], "shot": [208, 210, 212], "accuraci": [208, 210, 211, 212, 213], "324": 208, "loglikelihood": 208, "195": 208, "121": 208, "second": [208, 211, 213], "197": 208, "acc": [208, 212], "388": 208, "shown": [208, 212], "489": 208, "seem": 208, "custom_generation_config": [208, 210], "kick": 208, "interest": 208, "visit": 208, "bai": 208, "92": 208, "exploratorium": 208, "san": 208, "francisco": 208, "magazin": 208, "awesom": 208, "bridg": 208, "pretti": 208, "cool": 208, "96": [208, 213], "sec": [208, 210], "83": 208, "99": [208, 211], "72": 208, "littl": 208, "torchao": [208, 210, 212, 213], "int8_weight_onli": [208, 210], "int8_dynamic_activation_int8_weight": [208, 210], "ao": [208, 210], "quant_api": [208, 210], "_": [208, 210], "int4_weight_onli": [208, 210], "previous": [208, 210, 211], "benefit": 208, "doesn": 208, "fast": 208, "clone": [208, 211, 212, 213], "assumpt": 208, "new_dir": 208, "output_dict": 208, "sd_1": 208, "sd_2": 208, "dump": 208, "convert_hf_checkpoint": 208, "checkpoint_path": 208, "justin": 208, "school": 208, "math": 208, "teacher": 208, "ws": 208, "94": [208, 210], "bandwidth": [208, 210], "1391": 208, "84": 208, "thats": 208, "seamlessli": 208, "authent": [208, 209], "hopefulli": 208, "gave": 208, "grant": 209, "minut": 209, "agreement": 209, "altern": 209, "hackabl": 209, "singularli": 209, "technic": 209, "purpos": [209, 210], "depth": 209, "principl": 209, "boilerpl": 209, "substanti": [209, 211], "custom_config": 209, "replic": 209, "lorafinetunerecipesingledevic": 209, "lora_finetune_output": 209, "log_1713194212": 209, "3697006702423096": 209, "25880": [209, 213], "83it": 209, "monitor": 209, "tqdm": 209, "interv": 209, "e2": 209, "focu": 210, "128": [210, 211], "256": [210, 212], "theta": 210, "gain": 210, "basic": 210, "observ": [210, 212], "consum": [210, 213], "vram": [210, 211, 212], "overal": 210, "8b_qlora_single_devic": 210, "least": [210, 211, 212], "coupl": [210, 211, 213], "meta_model_0": [210, 212], "did": [210, 213], "122": 210, "sarah": 210, "busi": 210, "mum": 210, "young": 210, "children": 210, "live": 210, "north": 210, "east": 210, "england": 210, "135": 210, "88": 210, "138": 210, "346": 210, "09": 210, "139": 210, "broader": 210, "teach": 211, "straight": 211, "jump": 211, "unfamiliar": 211, "oppos": [211, 213], "momentum": 211, "relat": 211, "aghajanyan": 211, "et": 211, "al": 211, "hypothes": 211, "intrins": 211, "four": 211, "eight": 211, "practic": 211, "blue": 211, "rememb": 211, "approx": 211, "15m": 211, "8192": [211, 212], "65k": 211, "frozen_out": [211, 213], "lora_out": [211, 213], "omit": 211, "base_model": 211, "lora_model": 211, "lora_llama_2_7b": [211, 213], "alon": 211, "bit": [211, 212, 213], "in_featur": [211, 212], "out_featur": [211, 212], "inplac": 211, "feel": 211, "free": 211, "validate_missing_and_unexpected_for_lora": 211, "peft_util": 211, "set_trainable_param": 211, "fetch": 211, "lora_param": 211, "total_param": 211, "trainable_param": 211, "2f": 211, "6742609920": 211, "4194304": 211, "7b_lora": 211, "my_model_checkpoint_path": [211, 212, 213], "tokenizer_checkpoint": [211, 212, 213], "my_tokenizer_checkpoint_path": [211, 212, 213], "factori": 211, "benefici": 211, "impact": 211, "minor": 211, "good": 211, "lora_experiment_1": 211, "smooth": [211, 213], "curv": [211, 213], "ran": 211, "footprint": [211, 212], "commod": 211, "cogniz": 211, "ax": 211, "parallel": 211, "truthfulqa": 211, "475": 211, "87": 211, "508": 211, "86": 211, "504": 211, "04": 211, "514": 211, "absolut": 211, "4gb": 211, "tradeoff": 211, "potenti": 211, "awar": 212, "incur": [212, 213], "degrad": [212, 213], "perplex": 212, "simul": 212, "ultim": 212, "ptq": 212, "fake": 212, "kept": 212, "cast": 212, "nois": 212, "henc": 212, "x_q": 212, "int8": 212, "zp": 212, "x_float": 212, "qmin": 212, "qmax": 212, "clamp": 212, "x_fq": 212, "dequant": 212, "insert": 212, "proce": 212, "prepared_model": 212, "swap": 212, "int8dynactint4weightqatlinear": 212, "int8dynactint4weightlinear": 212, "train_loop": 212, "converted_model": 212, "demonstr": 212, "recov": 212, "modif": 212, "8b_qat_ful": 212, "custom_8b_qat_ful": 212, "2000": 212, "fake_quant_after_n_step": 212, "issu": 212, "futur": 212, "empir": 212, "led": 212, "presum": 212, "80gb": 212, "qat_distribut": 212, "op": 212, "mutat": 212, "5gb": 212, "custom_quant": 212, "groupsiz": 212, "poorli": 212, "custom_eleuther_evalu": 212, "fullmodeltorchtunecheckpoint": 212, "hellaswag": 212, "max_seq_length": 212, "my_eleuther_evalu": 212, "stderr": 212, "word_perplex": 212, "9148": 212, "byte_perplex": 212, "5357": 212, "bits_per_byt": 212, "6189": 212, "5687": 212, "0049": 212, "acc_norm": 212, "7536": 212, "0043": 212, "portion": [212, 213], "drop": 212, "74": 212, "048": 212, "190": 212, "7735": 212, "5598": 212, "6413": 212, "5481": 212, "0050": 212, "7390": 212, "0044": 212, "7251": 212, "4994": 212, "5844": 212, "5740": 212, "7610": 212, "outperform": 212, "importantli": 212, "characterist": 212, "187": 212, "958": 212, "halv": 212, "int4weightonlyquant": 212, "motiv": 212, "constrain": 212, "edg": 212, "smartphon": 212, "executorch": 212, "xnnpack": 212, "export_llama": 212, "use_sdpa_with_kv_cach": 212, "qmode": 212, "group_siz": 212, "get_bos_id": 212, "get_eos_id": 212, "128001": 212, "output_nam": 212, "llama3_8da4w": 212, "pte": 212, "881": 212, "oneplu": 212, "709": 212, "tok": 212, "815": 212, "316": 212, "364": 212, "highli": 213, "vanilla": 213, "held": 213, "therefor": 213, "bespok": 213, "normalfloat": 213, "8x": 213, "vast": 213, "major": 213, "normatfloat": 213, "doubl": 213, "themselv": 213, "deepdiv": 213, "idea": 213, "distinct": 213, "de": 213, "counterpart": 213, "set_default_devic": 213, "qlora_linear": 213, "memory_alloc": 213, "177": 213, "152": 213, "del": 213, "empty_cach": 213, "lora_linear": 213, "081": 213, "344": 213, "qlora_llama2_7b": 213, "qlora_model": 213, "essenti": 213, "reparametrize_as_dtype_state_dict_post_hook": 213, "slower": 213, "149": 213, "9157477021217346": 213, "02": 213, "08": 213, "15it": 213, "nightli": 213, "hundr": 213, "228": 213, "8158286809921265": 213, "95it": 213, "exercis": 213, "linear_nf4": 213, "to_nf4": 213, "linear_weight": 213, "autograd": 213, "incom": 213}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 3, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InputOutputToMessages"], [19, 1, 1, "", "InstructTemplate"], [20, 1, 1, "", "JSONToMessages"], [21, 1, 1, "", "Llama2ChatFormat"], [22, 1, 1, "", "Message"], [23, 1, 1, "", "MistralChatFormat"], [24, 1, 1, "", "PromptTemplate"], [25, 1, 1, "", "PromptTemplateInterface"], [26, 3, 1, "", "Role"], [27, 1, 1, "", "ShareGPTToMessages"], [28, 1, 1, "", "StackExchangedPairedTemplate"], [29, 3, 1, "", "SummarizeTemplate"], [30, 0, 1, "", "get_openai_messages"], [31, 0, 1, "", "get_sharegpt_messages"], [32, 0, 1, "", "truncate"], [33, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[19, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.Message": [[22, 4, 1, "", "contains_media"], [22, 2, 1, "", "from_dict"], [22, 4, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[23, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[28, 2, 1, "", "format"]], "torchtune.datasets": [[34, 1, 1, "", "ChatDataset"], [35, 1, 1, "", "ConcatDataset"], [36, 1, 1, "", "InstructDataset"], [37, 1, 1, "", "PackedDataset"], [38, 1, 1, "", "PreferenceDataset"], [39, 1, 1, "", "SFTDataset"], [40, 1, 1, "", "TextCompletionDataset"], [41, 0, 1, "", "alpaca_cleaned_dataset"], [42, 0, 1, "", "alpaca_dataset"], [43, 0, 1, "", "chat_dataset"], [44, 0, 1, "", "cnn_dailymail_articles_dataset"], [45, 0, 1, "", "grammar_dataset"], [46, 0, 1, "", "instruct_dataset"], [47, 0, 1, "", "samsum_dataset"], [48, 0, 1, "", "slimorca_dataset"], [49, 0, 1, "", "stack_exchanged_paired_dataset"], [50, 0, 1, "", "text_completion_dataset"], [51, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[52, 1, 1, "", "TilePositionalEmbedding"], [53, 1, 1, "", "TiledTokenPositionalEmbedding"], [54, 1, 1, "", "TokenPositionalEmbedding"], [55, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[52, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[53, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[54, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[56, 0, 1, "", "code_llama2_13b"], [57, 0, 1, "", "code_llama2_70b"], [58, 0, 1, "", "code_llama2_7b"], [59, 0, 1, "", "lora_code_llama2_13b"], [60, 0, 1, "", "lora_code_llama2_70b"], [61, 0, 1, "", "lora_code_llama2_7b"], [62, 0, 1, "", "qlora_code_llama2_13b"], [63, 0, 1, "", "qlora_code_llama2_70b"], [64, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[65, 1, 1, "", "GemmaTokenizer"], [66, 0, 1, "", "gemma"], [67, 0, 1, "", "gemma_2b"], [68, 0, 1, "", "gemma_7b"], [69, 0, 1, "", "gemma_tokenizer"], [70, 0, 1, "", "lora_gemma"], [71, 0, 1, "", "lora_gemma_2b"], [72, 0, 1, "", "lora_gemma_7b"], [73, 0, 1, "", "qlora_gemma_2b"], [74, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[65, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[75, 1, 1, "", "Llama2Tokenizer"], [76, 0, 1, "", "llama2"], [77, 0, 1, "", "llama2_13b"], [78, 0, 1, "", "llama2_70b"], [79, 0, 1, "", "llama2_7b"], [80, 0, 1, "", "llama2_reward_7b"], [81, 0, 1, "", "llama2_tokenizer"], [82, 0, 1, "", "lora_llama2"], [83, 0, 1, "", "lora_llama2_13b"], [84, 0, 1, "", "lora_llama2_70b"], [85, 0, 1, "", "lora_llama2_7b"], [86, 0, 1, "", "lora_llama2_reward_7b"], [87, 0, 1, "", "qlora_llama2_13b"], [88, 0, 1, "", "qlora_llama2_70b"], [89, 0, 1, "", "qlora_llama2_7b"], [90, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[75, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[91, 1, 1, "", "Llama3Tokenizer"], [92, 0, 1, "", "llama3"], [93, 0, 1, "", "llama3_70b"], [94, 0, 1, "", "llama3_8b"], [95, 0, 1, "", "llama3_tokenizer"], [96, 0, 1, "", "lora_llama3"], [97, 0, 1, "", "lora_llama3_70b"], [98, 0, 1, "", "lora_llama3_8b"], [99, 0, 1, "", "qlora_llama3_70b"], [100, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[91, 2, 1, "", "decode"], [91, 2, 1, "", "tokenize_message"], [91, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[101, 0, 1, "", "llama3_1"], [102, 0, 1, "", "llama3_1_70b"], [103, 0, 1, "", "llama3_1_8b"], [104, 0, 1, "", "lora_llama3_1"], [105, 0, 1, "", "lora_llama3_1_70b"], [106, 0, 1, "", "lora_llama3_1_8b"], [107, 0, 1, "", "qlora_llama3_1_70b"], [108, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[109, 1, 1, "", "MistralTokenizer"], [110, 0, 1, "", "lora_mistral"], [111, 0, 1, "", "lora_mistral_7b"], [112, 0, 1, "", "lora_mistral_classifier"], [113, 0, 1, "", "lora_mistral_reward_7b"], [114, 0, 1, "", "mistral"], [115, 0, 1, "", "mistral_7b"], [116, 0, 1, "", "mistral_classifier"], [117, 0, 1, "", "mistral_reward_7b"], [118, 0, 1, "", "mistral_tokenizer"], [119, 0, 1, "", "qlora_mistral_7b"], [120, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[109, 2, 1, "", "decode"], [109, 2, 1, "", "encode"], [109, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[121, 1, 1, "", "Phi3MiniTokenizer"], [122, 0, 1, "", "lora_phi3"], [123, 0, 1, "", "lora_phi3_mini"], [124, 0, 1, "", "phi3"], [125, 0, 1, "", "phi3_mini"], [126, 0, 1, "", "phi3_mini_tokenizer"], [127, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[121, 2, 1, "", "decode"], [121, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[128, 1, 1, "", "CausalSelfAttention"], [129, 1, 1, "", "FeedForward"], [130, 1, 1, "", "Fp32LayerNorm"], [131, 1, 1, "", "KVCache"], [132, 1, 1, "", "RMSNorm"], [133, 1, 1, "", "RotaryPositionalEmbeddings"], [134, 1, 1, "", "TransformerDecoder"], [135, 1, 1, "", "TransformerDecoderLayer"], [136, 1, 1, "", "VisionTransformer"], [138, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[128, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[129, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[130, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[131, 2, 1, "", "reset"], [131, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[132, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[133, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[134, 2, 1, "", "caches_are_enabled"], [134, 2, 1, "", "forward"], [134, 2, 1, "", "reset_caches"], [134, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[135, 2, 1, "", "forward"]], "torchtune.modules.VisionTransformer": [[136, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[137, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[139, 1, 1, "", "DPOLoss"], [140, 1, 1, "", "IPOLoss"], [141, 1, 1, "", "PPOLoss"], [142, 1, 1, "", "RSOLoss"]], "torchtune.modules.loss.DPOLoss": [[139, 2, 1, "", "forward"]], "torchtune.modules.loss.IPOLoss": [[140, 2, 1, "", "forward"]], "torchtune.modules.loss.PPOLoss": [[141, 2, 1, "", "forward"]], "torchtune.modules.loss.RSOLoss": [[142, 2, 1, "", "forward"]], "torchtune.modules.peft": [[143, 1, 1, "", "AdapterModule"], [144, 1, 1, "", "LoRALinear"], [145, 0, 1, "", "disable_adapter"], [146, 0, 1, "", "get_adapter_params"], [147, 0, 1, "", "set_trainable_params"], [148, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [149, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[143, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[144, 2, 1, "", "adapter_params"], [144, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[150, 0, 1, "", "estimate_advantages"], [151, 0, 1, "", "get_rewards_ppo"], [152, 0, 1, "", "left_padded_collate"], [153, 0, 1, "", "padded_collate_dpo"], [154, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.tokenizers": [[155, 1, 1, "", "BaseTokenizer"], [156, 1, 1, "", "ModelTokenizer"], [157, 1, 1, "", "SentencePieceBaseTokenizer"], [158, 1, 1, "", "TikTokenBaseTokenizer"], [159, 0, 1, "", "parse_hf_tokenizer_json"], [160, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[155, 2, 1, "", "decode"], [155, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[156, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[157, 2, 1, "", "decode"], [157, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[158, 2, 1, "", "decode"], [158, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[161, 1, 1, "", "Transform"], [162, 1, 1, "", "VisionCrossAttentionMask"], [163, 0, 1, "", "find_supported_resolutions"], [164, 0, 1, "", "get_canvas_best_fit"], [165, 0, 1, "", "resize_with_pad"], [166, 0, 1, "", "tile_crop"]], "torchtune.utils": [[167, 3, 1, "", "FSDPPolicyType"], [168, 1, 1, "", "FullModelHFCheckpointer"], [169, 1, 1, "", "FullModelMetaCheckpointer"], [170, 1, 1, "", "FullModelTorchTuneCheckpointer"], [171, 1, 1, "", "ModelType"], [172, 1, 1, "", "OptimizerInBackwardWrapper"], [173, 1, 1, "", "TuneRecipeArgumentParser"], [174, 0, 1, "", "create_optim_in_bwd_wrapper"], [175, 0, 1, "", "generate"], [176, 0, 1, "", "get_device"], [177, 0, 1, "", "get_dtype"], [178, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [179, 0, 1, "", "get_logger"], [180, 0, 1, "", "get_memory_stats"], [181, 0, 1, "", "get_quantizer_mode"], [182, 0, 1, "", "get_world_size_and_rank"], [183, 0, 1, "", "init_distributed"], [184, 0, 1, "", "is_distributed"], [185, 0, 1, "", "log_memory_stats"], [186, 0, 1, "", "lora_fsdp_wrap_policy"], [191, 0, 1, "", "padded_collate"], [192, 0, 1, "", "register_optim_in_bwd_hooks"], [193, 0, 1, "", "set_activation_checkpointing"], [194, 0, 1, "", "set_default_dtype"], [195, 0, 1, "", "set_seed"], [196, 0, 1, "", "setup_torch_profiler"], [197, 0, 1, "", "torch_version_ge"], [198, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[168, 2, 1, "", "load_checkpoint"], [168, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[169, 2, 1, "", "load_checkpoint"], [169, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[170, 2, 1, "", "load_checkpoint"], [170, 2, 1, "", "save_checkpoint"]], "torchtune.utils.OptimizerInBackwardWrapper": [[172, 2, 1, "", "get_optim_key"], [172, 2, 1, "", "load_state_dict"], [172, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[173, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[187, 1, 1, "", "DiskLogger"], [188, 1, 1, "", "StdoutLogger"], [189, 1, 1, "", "TensorBoardLogger"], [190, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[187, 2, 1, "", "close"], [187, 2, 1, "", "log"], [187, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[188, 2, 1, "", "close"], [188, 2, 1, "", "log"], [188, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[189, 2, 1, "", "close"], [189, 2, 1, "", "log"], [189, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[190, 2, 1, "", "close"], [190, 2, 1, "", "log"], [190, 2, 1, "", "log_config"], [190, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 17, 26, 29, 167, 201, 203, 205, 208, 210, 211, 212, 213], "config": [0, 7, 8, 205, 209], "data": [1, 5, 17, 26, 29, 206], "text": [1, 207, 210], "templat": [1, 206, 207], "type": 1, "convert": 1, "messag": [1, 22], "transform": [1, 4, 161], "helper": 1, "function": 1, "dataset": [2, 206, 207], "exampl": 2, "gener": [2, 175, 208, 210], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 205, 208, 209, 210, 211, 212], "llama3": [3, 92, 206, 210, 212], "1": 3, "llama2": [3, 76, 206, 208, 211, 213], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 114], "gemma": [3, 66], "clip": 3, "modul": 4, "compon": [4, 7], "build": [4, 202, 213], "block": 4, "base": 4, "token": [4, 206], "util": [4, 5, 167], "peft": 4, "loss": 4, "vision": 4, "reinforc": 4, "learn": 4, "from": [4, 206, 213], "human": 4, "feedback": 4, "rlhf": 4, "checkpoint": [5, 6, 9, 208], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 207, 211, 213], "manag": 5, "perform": [5, 211], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 203, 208], "format": [6, 207], "handl": 6, "differ": 6, "hfcheckpoint": 6, "metacheckpoint": 6, "torchtunecheckpoint": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 208, 211, 213], "put": [6, 213], "thi": 6, "all": [6, 7, 213], "togeth": [6, 213], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 207], "us": [7, 8, 206, 208, 213], "instanti": [7, 10], "referenc": 7, "other": [7, 208], "field": 7, "interpol": 7, "valid": [7, 13, 205], "your": [7, 8, 208, 209], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 203, 211, 212, 213], "ar": 8, "recip": [8, 205, 209, 211, 212], "script": 8, "run": [8, 205, 208], "cli": [8, 205], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "inputoutputtomessag": 18, "instructtempl": 19, "jsontomessag": 20, "llama2chatformat": 21, "mistralchatformat": 23, "prompttempl": 24, "prompttemplateinterfac": 25, "role": 26, "sharegpttomessag": 27, "stackexchangedpairedtempl": 28, "summarizetempl": 29, "get_openai_messag": 30, "get_sharegpt_messag": 31, "truncat": 32, "validate_messag": 33, "chatdataset": 34, "concatdataset": 35, "instructdataset": 36, "packeddataset": 37, "preferencedataset": 38, "sftdataset": 39, "textcompletiondataset": 40, "alpaca_cleaned_dataset": 41, "alpaca_dataset": 42, "chat_dataset": 43, "cnn_dailymail_articles_dataset": 44, "grammar_dataset": 45, "instruct_dataset": 46, "samsum_dataset": 47, "slimorca_dataset": 48, "stack_exchanged_paired_dataset": 49, "text_completion_dataset": 50, "wikitext_dataset": 51, "tilepositionalembed": 52, "tiledtokenpositionalembed": 53, "tokenpositionalembed": 54, "clip_vision_encod": 55, "code_llama2_13b": 56, "code_llama2_70b": 57, "code_llama2_7b": 58, "lora_code_llama2_13b": 59, "lora_code_llama2_70b": 60, "lora_code_llama2_7b": 61, "qlora_code_llama2_13b": 62, "qlora_code_llama2_70b": 63, "qlora_code_llama2_7b": 64, "gemmatoken": 65, "gemma_2b": 67, "gemma_7b": 68, "gemma_token": 69, "lora_gemma": 70, "lora_gemma_2b": 71, "lora_gemma_7b": 72, "qlora_gemma_2b": 73, "qlora_gemma_7b": 74, "llama2token": 75, "llama2_13b": 77, "llama2_70b": 78, "llama2_7b": 79, "llama2_reward_7b": 80, "llama2_token": 81, "lora_llama2": 82, "lora_llama2_13b": 83, "lora_llama2_70b": 84, "lora_llama2_7b": 85, "lora_llama2_reward_7b": 86, "qlora_llama2_13b": 87, "qlora_llama2_70b": 88, "qlora_llama2_7b": 89, "qlora_llama2_reward_7b": 90, "llama3token": 91, "llama3_70b": 93, "llama3_8b": 94, "llama3_token": 95, "lora_llama3": 96, "lora_llama3_70b": 97, "lora_llama3_8b": 98, "qlora_llama3_70b": 99, "qlora_llama3_8b": 100, "llama3_1": 101, "llama3_1_70b": 102, "llama3_1_8b": 103, "lora_llama3_1": 104, "lora_llama3_1_70b": 105, "lora_llama3_1_8b": 106, "qlora_llama3_1_70b": 107, "qlora_llama3_1_8b": 108, "mistraltoken": 109, "lora_mistr": 110, "lora_mistral_7b": 111, "lora_mistral_classifi": 112, "lora_mistral_reward_7b": 113, "mistral_7b": 115, "mistral_classifi": 116, "mistral_reward_7b": 117, "mistral_token": 118, "qlora_mistral_7b": 119, "qlora_mistral_reward_7b": 120, "phi3minitoken": 121, "lora_phi3": 122, "lora_phi3_mini": 123, "phi3": 124, "phi3_mini": 125, "phi3_mini_token": 126, "qlora_phi3_mini": 127, "causalselfattent": 128, "todo": [128, 135], "feedforward": 129, "fp32layernorm": 130, "kvcach": 131, "rmsnorm": 132, "rotarypositionalembed": 133, "transformerdecod": 134, "transformerdecoderlay": 135, "visiontransform": 136, "reparametrize_as_dtype_state_dict_post_hook": 137, "get_cosine_schedule_with_warmup": 138, "dpoloss": 139, "ipoloss": 140, "ppoloss": 141, "rsoloss": 142, "adaptermodul": 143, "loralinear": 144, "disable_adapt": 145, "get_adapter_param": 146, "set_trainable_param": 147, "validate_missing_and_unexpected_for_lora": 148, "validate_state_dict_for_lora": 149, "estimate_advantag": 150, "get_rewards_ppo": 151, "left_padded_col": 152, "padded_collate_dpo": 153, "truncate_sequence_at_first_stop_token": 154, "basetoken": 155, "modeltoken": 156, "sentencepiecebasetoken": 157, "tiktokenbasetoken": 158, "parse_hf_tokenizer_json": 159, "tokenize_messages_no_special_token": 160, "visioncrossattentionmask": 162, "find_supported_resolut": 163, "get_canvas_best_fit": 164, "resize_with_pad": 165, "tile_crop": 166, "fsdppolicytyp": 167, "fullmodelhfcheckpoint": 168, "fullmodelmetacheckpoint": 169, "fullmodeltorchtunecheckpoint": 170, "modeltyp": 171, "optimizerinbackwardwrapp": 172, "tunerecipeargumentpars": 173, "create_optim_in_bwd_wrapp": 174, "get_devic": 176, "get_dtyp": 177, "get_full_finetune_fsdp_wrap_polici": 178, "get_logg": 179, "get_memory_stat": 180, "get_quantizer_mod": 181, "get_world_size_and_rank": 182, "init_distribut": 183, "is_distribut": 184, "log_memory_stat": 185, "lora_fsdp_wrap_polici": 186, "disklogg": 187, "stdoutlogg": 188, "tensorboardlogg": 189, "wandblogg": 190, "padded_col": 191, "register_optim_in_bwd_hook": 192, "set_activation_checkpoint": 193, "set_default_dtyp": 194, "set_se": 195, "setup_torch_profil": 196, "torch_version_g": 197, "validate_expected_param_dtyp": 198, "comput": [200, 204], "time": [200, 204], "welcom": 201, "document": 201, "get": [201, 205, 210], "start": [201, 205], "tutori": 201, "instal": 202, "instruct": [202, 207, 210], "via": [202, 210], "pypi": 202, "git": 202, "clone": 202, "nightli": 202, "kei": 203, "concept": 203, "design": 203, "principl": 203, "download": [205, 208, 209], "list": 205, "built": [205, 207], "copi": 205, "fine": [206, 207, 209, 210], "tune": [206, 207, 209, 210], "chat": [206, 207], "chang": 206, "prompt": 206, "special": 206, "when": 206, "should": 206, "i": 206, "custom": [206, 207], "hug": [207, 208], "face": [207, 208], "set": 207, "max": 207, "sequenc": 207, "length": 207, "sampl": 207, "pack": 207, "unstructur": 207, "corpu": 207, "multipl": 207, "local": 207, "remot": 207, "fulli": 207, "end": 208, "workflow": 208, "7b": 208, "finetun": [208, 211, 212, 213], "evalu": [208, 210, 212], "eleutherai": [208, 210], "s": [208, 210], "eval": [208, 210], "har": [208, 210], "speed": 208, "up": 208, "quantiz": [208, 210, 212], "librari": 208, "upload": 208, "hub": 208, "first": 209, "llm": 209, "select": 209, "modifi": 209, "train": 209, "next": 209, "step": 209, "meta": 210, "8b": 210, "access": 210, "our": 210, "faster": 210, "how": 211, "doe": 211, "work": 211, "appli": [211, 212], "trade": 211, "off": 211, "qat": 212, "lower": 212, "devic": 212, "option": 212, "qlora": 213, "save": 213, "deep": 213, "dive": 213}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})