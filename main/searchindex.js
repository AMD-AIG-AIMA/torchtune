Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "Llama3Tokenizer", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "MistralTokenizer", "lora_mistral_7b", "lora_mistral_classifier_7b", "mistral_7b", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "Phi3MiniTokenizer", "lora_phi3_mini", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 20, 21, 22, 30, 31, 32, 34, 35, 39, 43, 73, 85, 91, 92, 150, 154, 155, 158, 160, 161], "prompt": [1, 14, 15, 17, 18, 19, 21, 22, 23, 24, 25, 28, 30, 32, 34, 35, 36, 38, 39, 40, 41, 43, 54, 62, 73, 81, 89, 99, 114, 123, 156, 157, 159], "chat": [1, 2, 15, 16, 19, 20, 24, 25, 28, 36, 41, 62, 92], "includ": [1, 6, 7, 8, 15, 18, 62, 92, 105, 116, 117, 121, 152, 154, 155, 156, 157, 158, 159, 160, 161], "some": [1, 6, 7, 16, 107, 108, 150, 152, 154, 155, 156, 157, 158, 160, 161], "specif": [1, 4, 7, 8, 10, 126, 155, 156, 157, 161], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 28, 30, 32, 34, 35, 36, 39, 41, 62, 73, 116, 117, 118, 119, 154, 155, 157, 158, 159, 160], "differ": [1, 7, 9, 28, 29, 30, 32, 81, 111, 119, 140, 147, 152, 154, 155, 157, 159, 160, 161], "dataset": [1, 5, 7, 14, 17, 18, 20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 152, 158, 159], "model": [1, 2, 6, 7, 8, 10, 16, 21, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 116, 117, 118, 119, 122, 123, 126, 128, 134, 141, 142, 150, 152, 155, 156, 161], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 55, 56, 63, 64, 65, 76, 81, 84, 85, 92, 95, 99, 100, 102, 104, 107, 110, 111, 113, 116, 117, 118, 120, 121, 122, 123, 137, 138, 141, 149, 151, 153, 154, 156, 157, 158, 159, 160], "common": [1, 2, 4, 7, 114, 154, 155, 156, 159, 160], "json": [1, 6, 24, 25, 76, 92, 113, 116, 154, 156, 157], "messag": [1, 15, 16, 19, 21, 24, 25, 27, 28, 36, 54, 62, 73, 81, 89, 114, 151, 154, 155, 156], "miscellan": 1, "function": [1, 7, 8, 10, 12, 28, 94, 95, 101, 103, 106, 109, 110, 115, 116, 123, 124, 130, 134, 140, 144, 152, 155, 156, 161], "us": [1, 2, 4, 6, 9, 10, 12, 16, 19, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 62, 76, 92, 94, 95, 97, 98, 99, 100, 101, 103, 106, 109, 111, 112, 115, 116, 117, 119, 120, 121, 123, 124, 125, 126, 128, 134, 135, 136, 137, 138, 144, 150, 151, 152, 154, 156, 158, 159, 160], "modifi": [1, 7, 8, 9, 101, 152, 157, 159, 160, 161], "For": [2, 5, 6, 7, 8, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 43, 44, 62, 94, 99, 116, 121, 122, 129, 138, 142, 144, 151, 154, 155, 156, 157, 158, 159, 160, 161], "detail": [2, 6, 36, 41, 62, 96, 115, 126, 134, 144, 154, 157, 158, 159, 160, 161], "usag": [2, 101, 119, 120, 145, 151, 154, 156, 157, 158, 159, 161], "guid": [2, 7, 9, 152, 155, 156, 158, 160], "pleas": [2, 5, 51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 115, 126, 134, 142, 151, 161], "see": [2, 5, 6, 9, 19, 21, 36, 41, 44, 51, 52, 53, 60, 61, 62, 70, 71, 72, 79, 80, 87, 88, 93, 96, 104, 115, 119, 121, 126, 127, 134, 138, 142, 144, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "our": [2, 6, 8, 152, 155, 156, 157, 158, 160, 161], "tutori": [2, 6, 62, 142, 152, 155, 156, 157, 158, 159, 160, 161], "support": [2, 6, 8, 9, 10, 21, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 91, 92, 94, 105, 117, 118, 120, 125, 128, 152, 154, 155, 156, 157, 158, 159, 160, 161], "sever": 2, "wide": 2, "help": [2, 6, 19, 99, 116, 121, 150, 151, 152, 154, 155, 156, 157, 158, 159, 161], "quickli": [2, 7, 33, 155, 156], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 22, 23, 28, 33, 62, 137, 138, 150, 151, 152, 154, 155, 156, 159, 160, 161], "fine": [2, 6, 8, 9, 31, 62, 150, 152, 157, 160], "tune": [2, 3, 6, 7, 8, 9, 12, 31, 62, 150, 151, 152, 154, 157, 160, 161], "also": [2, 6, 7, 8, 9, 10, 36, 39, 43, 92, 94, 99, 105, 124, 126, 128, 134, 138, 151, 154, 155, 156, 157, 158, 159, 160, 161], "like": [2, 6, 7, 8, 9, 28, 92, 118, 151, 154, 155, 156, 157, 158, 160], "These": [2, 4, 6, 7, 8, 10, 31, 121, 155, 156, 157, 158, 159, 160, 161], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 23, 27, 30, 31, 32, 34, 35, 36, 38, 39, 40, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 99, 105, 106, 109, 110, 115, 116, 117, 119, 120, 122, 123, 125, 128, 132, 134, 140, 145, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "especi": [2, 152, 154, 157], "specifi": [2, 6, 7, 8, 10, 36, 94, 99, 100, 103, 115, 123, 126, 129, 134, 138, 142, 145, 154, 155, 156, 157, 158, 159, 161], "yaml": [2, 7, 8, 10, 11, 12, 36, 39, 43, 121, 138, 152, 154, 155, 156, 157, 158, 159, 160, 161], "config": [2, 6, 9, 10, 11, 12, 13, 36, 39, 43, 94, 109, 116, 120, 121, 138, 145, 152, 155, 156, 157, 159, 160, 161], "represent": [2, 160, 161], "abov": [2, 6, 101, 132, 151, 157, 159, 160, 161], "all": [3, 4, 8, 13, 28, 29, 31, 36, 76, 92, 94, 95, 99, 101, 106, 116, 120, 121, 122, 132, 141, 147, 148, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160], "famili": [3, 8, 34, 35, 37, 41, 42, 44, 119, 152, 154, 159], "download": [3, 6, 148, 151, 155, 156, 159, 160, 161], "meta": [3, 6, 19, 62, 73, 116, 117, 154, 155, 157, 158], "8b": [3, 75, 78, 80, 90, 154, 155], "hf": [3, 6, 89, 103, 116, 154, 155, 157, 158, 159], "token": [3, 6, 7, 8, 20, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 57, 62, 66, 73, 76, 81, 86, 89, 92, 94, 98, 99, 100, 111, 112, 113, 114, 123, 126, 139, 154, 156, 157, 158, 159, 160, 161], "access_token": 3, "pre": [3, 19, 31, 62, 151, 155, 156], "train": [3, 5, 6, 8, 9, 19, 28, 29, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 62, 94, 98, 99, 100, 101, 102, 116, 117, 118, 125, 128, 134, 145, 150, 152, 154, 155, 156, 157, 159, 160, 161], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 43, 44, 62, 81, 97, 98, 106, 111, 112, 115, 116, 119, 121, 126, 134, 137, 138, 142, 145, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "hug": [3, 6, 16, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 76, 92, 102, 113, 152, 154, 158, 159], "face": [3, 6, 16, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 76, 92, 102, 113, 152, 154, 158, 159], "hub": [3, 6, 154, 156, 158], "follow": [3, 6, 8, 24, 25, 28, 31, 94, 102, 118, 119, 120, 132, 138, 145, 150, 151, 154, 156, 157, 158, 159, 160, 161], "command": [3, 8, 9, 121, 151, 154, 155, 156, 157, 158, 159, 160, 161], "2": [3, 6, 9, 27, 31, 41, 54, 62, 73, 81, 89, 94, 111, 112, 114, 116, 117, 139, 140, 143, 144, 145, 146, 155, 157, 158, 159, 160], "7b": [3, 6, 30, 32, 33, 34, 35, 37, 39, 43, 44, 47, 50, 53, 56, 59, 65, 69, 72, 82, 83, 84, 85, 116, 117, 155, 158, 159, 160, 161], "codellama": 3, "mini": [3, 89, 90, 91, 92, 93], "microsoft": [3, 91, 92], "4k": [3, 91, 92], "hf_token": 3, "ignor": [3, 6, 89, 94, 95, 154], "pattern": [3, 112, 154], "ai": [3, 84, 94, 138, 155, 159], "mistralai": [3, 154], "v0": 3, "1": [3, 6, 8, 31, 41, 54, 62, 73, 81, 89, 94, 99, 102, 103, 111, 112, 114, 117, 119, 123, 132, 137, 138, 139, 140, 143, 144, 154, 155, 157, 158, 159, 160, 161], "size": [3, 6, 8, 10, 34, 35, 38, 40, 94, 96, 97, 98, 99, 130, 132, 152, 154, 156, 157, 158, 159, 160], "2b": [3, 55, 58], "googl": [3, 55, 56], "perform": [4, 6, 31, 62, 95, 106, 123, 152, 155, 157, 159, 161], "direct": [4, 8, 103, 140, 151], "encod": [4, 54, 62, 73, 81, 89, 103, 111, 112, 114, 155], "text": [4, 28, 31, 33, 36, 37, 43, 44, 62, 73, 81, 89, 111, 112, 155, 157], "id": [4, 6, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 62, 73, 81, 89, 94, 98, 99, 100, 111, 112, 113, 116, 118, 123, 139, 140, 155, 156, 157], "decod": [4, 73, 81, 89, 99, 111, 112, 123, 155], "typic": [4, 7, 31, 33, 43, 92, 103, 156, 161], "byte": [4, 112, 161], "pair": [4, 7, 14, 42, 112, 139, 140, 156], "underli": [4, 81, 111, 161], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 101, 104, 107, 109, 120, 121, 129, 151, 152, 156, 157, 159, 160, 161], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 25, 26, 28, 30, 32, 33, 36, 37, 39, 43, 44, 62, 81, 101, 107, 108, 109, 110, 111, 114, 116, 117, 118, 120, 123, 131, 134, 144, 147, 154, 155, 156, 157, 158, 159, 160], "offer": 5, "allow": [5, 29, 109, 137, 154, 161], "seamless": 5, "transit": 5, "between": [5, 6, 116, 119, 156, 157, 159, 160, 161], "interoper": [5, 6, 8, 152, 157, 161], "rest": [5, 155, 161], "ecosystem": [5, 6, 8, 152, 157, 159, 161], "comprehens": 5, "overview": [5, 7, 9, 150, 158, 160, 161], "deep": [5, 6, 7, 8, 9, 152, 158, 159], "dive": [5, 6, 7, 8, 9, 152, 158, 159], "enabl": [5, 7, 8, 9, 29, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 105, 144, 145, 159, 160, 161], "work": [5, 6, 8, 121, 152, 154, 157, 159, 161], "set": [5, 6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 73, 89, 94, 98, 99, 106, 108, 115, 124, 126, 132, 134, 142, 143, 144, 145, 152, 154, 155, 157, 158, 159, 160], "consumpt": [5, 29], "dure": [5, 6, 29, 30, 31, 34, 35, 38, 40, 94, 96, 98, 99, 100, 101, 128, 155, 157, 159, 160, 161], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 26, 28, 29, 30, 31, 32, 41, 99, 106, 118, 121, 124, 126, 138, 145, 152, 154, 155, 156, 157, 158, 159], "debug": [5, 6, 7, 8, 154], "finetun": [5, 6, 7, 8, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 150, 152, 158, 159], "job": [5, 9, 144, 158], "variou": [5, 18], "walk": [6, 8, 137, 152, 155, 156, 157, 158, 161], "you": [6, 7, 8, 9, 10, 18, 19, 23, 28, 30, 32, 33, 34, 35, 37, 39, 43, 44, 119, 121, 123, 137, 138, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "through": [6, 7, 8, 9, 95, 106, 152, 154, 155, 156, 157, 158, 161], "design": [6, 8], "behavior": [6, 134, 155, 156], "associ": [6, 7, 8, 123, 157, 160], "util": [6, 7, 8, 9, 10, 29, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 152, 157, 158, 159, 161], "what": [6, 7, 9, 19, 21, 22, 33, 38, 40, 150, 155, 156, 157, 158, 159], "cover": [6, 7, 8, 9, 155, 157, 161], "how": [6, 7, 8, 9, 23, 115, 142, 150, 154, 155, 156, 157, 158, 159, 161], "we": [6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 62, 81, 94, 96, 98, 99, 105, 116, 117, 118, 123, 125, 129, 134, 141, 152, 154, 155, 156, 157, 158, 159, 160, 161], "them": [6, 7, 28, 29, 30, 32, 39, 54, 62, 81, 89, 95, 101, 114, 154, 155, 156, 157, 160, 161], "scenario": [6, 29], "full": [6, 7, 8, 36, 39, 51, 52, 53, 54, 60, 61, 62, 70, 71, 72, 79, 80, 81, 87, 88, 89, 93, 109, 110, 114, 152, 154, 156, 159, 160], "compos": 6, "compon": [6, 8, 13, 140, 152, 156, 158, 160, 161], "which": [6, 7, 8, 29, 30, 31, 33, 34, 35, 38, 40, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 81, 82, 83, 90, 94, 98, 99, 100, 102, 109, 110, 111, 116, 117, 118, 120, 125, 135, 138, 142, 152, 154, 155, 156, 157, 158, 159, 160, 161], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 95, 109, 116, 117, 118, 152, 155, 156, 157, 159, 161], "evalu": [6, 8, 150, 152, 158, 160, 161], "gener": [6, 8, 14, 17, 22, 23, 28, 30, 31, 32, 37, 41, 62, 81, 106, 143, 144, 145, 148, 150, 155, 156, 160, 161], "each": [6, 8, 15, 18, 29, 31, 48, 49, 50, 54, 58, 59, 62, 67, 68, 69, 73, 77, 78, 81, 82, 83, 89, 90, 94, 98, 99, 100, 103, 109, 110, 114, 140, 144, 145, 152, 154, 156, 157, 158, 159, 160], "make": [6, 7, 8, 9, 94, 100, 152, 154, 157, 158, 159, 160, 161], "easi": [6, 8, 152, 156, 160], "understand": [6, 7, 8, 150, 152, 155, 156, 160, 161], "extend": [6, 8, 152], "befor": [6, 27, 30, 31, 32, 94, 99, 100, 105, 112, 116, 154, 157], "let": [6, 7, 9, 154, 155, 156, 157, 158, 159, 160, 161], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 24, 25, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 49, 50, 54, 62, 67, 68, 69, 73, 77, 78, 81, 82, 83, 89, 90, 92, 94, 96, 98, 99, 100, 101, 103, 104, 107, 109, 110, 115, 116, 117, 120, 124, 126, 128, 134, 137, 142, 143, 152, 154, 155, 156, 158, 160, 161], "defin": [6, 7, 8, 95, 104, 105, 107, 156, 158, 160], "concept": [6, 157, 158], "In": [6, 7, 8, 28, 98, 105, 115, 134, 137, 138, 155, 157, 159, 160, 161], "ll": [6, 7, 8, 123, 129, 152, 155, 156, 157, 158, 159, 161], "talk": 6, "about": [6, 8, 103, 138, 152, 154, 155, 157, 158, 159, 160, 161], "take": [6, 7, 8, 10, 95, 96, 101, 116, 118, 121, 124, 140, 155, 156, 157, 158, 159, 160, 161], "close": [6, 8, 135, 136, 137, 138, 160], "look": [6, 7, 8, 122, 137, 151, 155, 156, 157, 158, 159, 160], "veri": [6, 29, 99, 154, 157], "simpli": [6, 7, 31, 154, 155, 156, 157, 159, 161], "dictat": 6, "state_dict": [6, 101, 109, 116, 117, 118, 119, 120, 160, 161], "store": [6, 29, 135, 138, 160, 161], "file": [6, 7, 8, 9, 10, 11, 12, 54, 62, 73, 76, 81, 89, 92, 111, 112, 113, 116, 117, 118, 121, 135, 138, 145, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161], "disk": [6, 33, 135], "weight": [6, 8, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 101, 104, 105, 109, 111, 116, 117, 118, 119, 129, 134, 138, 150, 154, 155, 157, 158, 159, 160, 161], "string": [6, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 62, 73, 81, 89, 104, 111, 112, 114, 124, 125, 129, 154, 156], "kei": [6, 7, 9, 14, 17, 22, 23, 24, 28, 30, 32, 39, 94, 96, 99, 100, 108, 109, 110, 116, 118, 120, 140, 145, 154, 157, 158, 160, 161], "identifi": 6, "state": [6, 8, 101, 107, 108, 109, 110, 116, 117, 118, 120, 122, 157, 159, 160, 161], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 36, 37, 39, 43, 44, 73, 89, 101, 107, 108, 109, 110, 112, 113, 116, 117, 118, 120, 122, 128, 131, 133, 139, 140, 141, 156], "If": [6, 7, 13, 14, 17, 18, 21, 22, 23, 24, 26, 27, 28, 30, 32, 34, 35, 38, 39, 40, 41, 73, 89, 94, 98, 99, 100, 101, 103, 105, 110, 116, 117, 118, 119, 120, 123, 124, 125, 126, 128, 129, 131, 137, 138, 144, 147, 151, 154, 155, 156, 157, 158, 159, 160], "don": [6, 7, 8, 138, 144, 154, 155, 156, 157, 158, 159, 161], "t": [6, 7, 8, 125, 138, 144, 154, 155, 156, 157, 158, 159, 161], "match": [6, 28, 30, 32, 39, 89, 110, 151, 154, 156, 157, 159, 160], "up": [6, 8, 9, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 112, 122, 145, 154, 155, 156, 158, 159, 160, 161], "exactli": [6, 110], "those": [6, 119, 160], "definit": [6, 160], "either": [6, 110, 116, 123, 142, 154, 160, 161], "run": [6, 7, 9, 12, 95, 96, 99, 101, 116, 117, 118, 120, 122, 132, 137, 138, 141, 151, 152, 155, 156, 158, 159, 160, 161], "explicit": 6, "error": [6, 7, 27, 116, 144, 154], "load": [6, 8, 28, 29, 30, 31, 32, 33, 109, 116, 117, 118, 120, 121, 137, 155, 156, 157, 159, 160], "rais": [6, 10, 13, 21, 24, 27, 36, 41, 89, 94, 96, 99, 103, 109, 110, 116, 117, 118, 120, 125, 128, 131, 138, 140, 144, 147], "an": [6, 7, 8, 9, 10, 14, 20, 27, 29, 38, 40, 44, 94, 99, 103, 104, 106, 107, 108, 115, 116, 117, 118, 120, 124, 126, 138, 145, 152, 154, 155, 156, 157, 158, 159, 160, 161], "except": [6, 20, 21, 114, 156], "wors": 6, "silent": [6, 95], "succe": 6, "infer": [6, 19, 28, 62, 94, 96, 98, 99, 100, 124, 150, 155, 157, 158, 159, 161], "expect": [6, 7, 10, 14, 17, 18, 22, 23, 28, 30, 32, 36, 39, 98, 110, 120, 138, 147, 155, 156, 160], "addit": [6, 7, 8, 10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 62, 109, 115, 116, 117, 118, 125, 126, 131, 134, 135, 137, 138, 142, 145, 152, 155, 158, 160], "line": [6, 8, 14, 121, 154, 156, 158, 159], "need": [6, 7, 8, 9, 18, 28, 31, 41, 94, 95, 99, 134, 137, 138, 141, 151, 154, 155, 156, 157, 158, 159, 160, 161], "shape": [6, 94, 96, 98, 99, 100, 103, 105, 123, 145], "valu": [6, 7, 25, 41, 45, 46, 47, 55, 56, 63, 64, 65, 74, 75, 84, 85, 94, 96, 97, 99, 100, 102, 109, 116, 119, 120, 121, 123, 135, 136, 137, 138, 140, 144, 154, 156, 158, 159, 160], "two": [6, 7, 27, 152, 157, 158, 159, 160, 161], "popular": [6, 152, 156, 157], "llama2": [6, 7, 8, 10, 19, 28, 30, 32, 33, 34, 35, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 95, 99, 100, 119, 150, 152, 154, 158, 159], "offici": [6, 19, 155, 158, 159], "implement": [6, 8, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 62, 81, 89, 95, 97, 98, 102, 103, 104, 105, 116, 129, 137, 152, 156, 160, 161], "when": [6, 7, 8, 12, 20, 29, 31, 33, 62, 94, 98, 99, 100, 101, 102, 109, 123, 126, 137, 141, 154, 157, 159, 160, 161], "llama": [6, 19, 28, 62, 73, 97, 98, 116, 117, 154, 155, 157, 158, 159, 160], "websit": 6, "get": [6, 7, 8, 9, 28, 62, 81, 125, 127, 128, 130, 151, 152, 155, 156, 157, 158, 160], "access": [6, 7, 8, 29, 116, 122, 154, 157, 158], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 33, 94, 109, 116, 117, 118, 120, 122, 154, 155, 156, 157, 158, 159, 160, 161], "pth": [6, 157], "inspect": [6, 157, 160, 161], "content": [6, 20, 24, 25, 28, 54, 62, 81, 89, 114, 155, 156], "easili": [6, 7, 152, 156, 160, 161], "torch": [6, 7, 29, 96, 99, 101, 102, 103, 118, 120, 122, 123, 124, 125, 128, 131, 132, 140, 141, 142, 143, 144, 145, 146, 147, 157, 158, 159, 160, 161], "import": [6, 7, 10, 36, 39, 43, 137, 138, 155, 156, 157, 158, 160, 161], "consolid": [6, 159], "00": [6, 149, 153, 158], "mmap": [6, 157], "true": [6, 7, 20, 30, 31, 34, 35, 36, 38, 39, 40, 43, 51, 52, 53, 54, 60, 61, 62, 70, 71, 72, 73, 79, 80, 81, 87, 88, 89, 93, 94, 99, 100, 101, 106, 111, 112, 114, 115, 116, 117, 118, 126, 128, 131, 132, 134, 137, 145, 146, 154, 155, 156, 157, 159, 160, 161], "weights_onli": [6, 118], "map_loc": [6, 157], "cpu": [6, 8, 101, 125, 145, 151, 154, 157, 161], "tensor": [6, 94, 95, 96, 97, 98, 99, 100, 101, 103, 105, 116, 123, 135, 136, 137, 138, 139, 140, 143, 160, 161], "item": 6, "print": [6, 9, 29, 34, 35, 38, 40, 41, 54, 62, 73, 81, 89, 111, 112, 114, 123, 146, 155, 156, 158, 160, 161], "f": [6, 9, 34, 35, 38, 40, 155, 157, 160, 161], "tok_embed": [6, 99], "32000": [6, 10, 160], "4096": [6, 10, 30, 32, 33, 34, 35, 37, 39, 43, 44, 94, 98, 156, 160], "len": [6, 29, 34, 35, 38, 40, 99], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 28, 29, 30, 31, 32, 38, 40, 41, 42, 48, 49, 50, 54, 58, 59, 62, 67, 68, 69, 73, 77, 78, 81, 89, 90, 97, 98, 101, 102, 103, 106, 111, 112, 113, 114, 115, 116, 118, 121, 124, 125, 127, 129, 138, 143, 145, 146, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "contain": [6, 20, 24, 29, 31, 33, 43, 54, 62, 73, 76, 81, 89, 92, 94, 96, 98, 99, 100, 104, 107, 108, 109, 112, 114, 116, 117, 118, 120, 121, 122, 128, 133, 137, 139, 140, 145, 155, 157, 159, 160], "input": [6, 14, 15, 17, 18, 22, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 81, 89, 94, 95, 97, 98, 99, 100, 105, 111, 112, 116, 118, 139, 140, 144, 147, 155, 156, 160, 161], "embed": [6, 94, 96, 97, 98, 99, 126, 155, 159], "tabl": [6, 155, 161], "call": [6, 10, 20, 95, 101, 109, 121, 135, 136, 137, 138, 141, 145, 155, 156, 160, 161], "layer": [6, 8, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 99, 100, 105, 109, 110, 115, 126, 152, 159, 160, 161], "have": [6, 7, 10, 94, 96, 104, 110, 118, 120, 121, 126, 134, 137, 147, 151, 155, 156, 157, 158, 159, 160, 161], "dim": [6, 94, 95, 97, 98, 99], "most": [6, 7, 155, 158, 160, 161], "within": [6, 7, 10, 28, 31, 41, 95, 123, 137, 144, 145, 154, 156, 157, 159, 160, 161], "default": [6, 7, 16, 20, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 58, 59, 62, 63, 64, 65, 67, 68, 69, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 89, 90, 92, 94, 95, 97, 98, 99, 100, 101, 102, 103, 105, 109, 111, 112, 114, 116, 117, 118, 121, 123, 125, 130, 134, 135, 138, 139, 140, 143, 144, 145, 151, 154, 155, 156, 157, 159, 160, 161], "everi": [6, 8, 95, 137, 145, 151, 154, 161], "repo": [6, 116, 117, 119, 154, 157], "first": [6, 7, 10, 27, 31, 96, 99, 116, 121, 150, 152, 155, 156, 157, 159, 160, 161], "big": [6, 157], "split": [6, 31, 112, 155, 156, 157], "across": [6, 8, 29, 116, 137, 144, 157, 159], "bin": [6, 154, 157], "To": [6, 7, 8, 9, 31, 116, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "correctli": [6, 8, 13, 109, 116, 151, 155, 158, 161], "piec": 6, "one": [6, 8, 27, 54, 62, 81, 89, 95, 103, 114, 118, 155, 156, 157, 158, 159, 161], "pytorch_model": [6, 157], "00001": [6, 154], "00002": [6, 154], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 31, 37, 105, 107, 109, 111, 117, 118, 120, 121, 123, 125, 126, 128, 129, 134, 154, 156, 157, 158, 159, 160, 161], "doe": [6, 21, 24, 28, 31, 62, 91, 94, 99, 100, 104, 114, 116, 118, 120, 121, 154, 155, 157], "fewer": [6, 94], "sinc": [6, 7, 10, 95, 116, 118, 155, 157, 159], "instead": [6, 8, 31, 36, 39, 43, 95, 96, 105, 154, 157, 159, 160], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 41, 43, 44, 104, 108, 110, 112, 116, 117, 118, 119, 120, 121, 122, 123, 124, 135, 136, 137, 138, 147, 154, 155, 157, 159], "caus": [6, 81, 111], "try": [6, 7, 155, 157, 158, 159, 161], "same": [6, 7, 48, 49, 50, 54, 58, 59, 62, 67, 68, 69, 77, 78, 81, 89, 90, 96, 100, 114, 120, 121, 126, 138, 154, 155, 157, 159, 160, 161], "As": [6, 7, 8, 9, 105, 152, 157, 159, 161], "re": [6, 7, 118, 152, 155, 157, 158, 159, 160], "care": [6, 95, 116, 118, 157, 159, 160], "end": [6, 8, 20, 29, 73, 81, 112, 150, 152, 155, 159, 160], "number": [6, 8, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 94, 96, 99, 102, 116, 117, 118, 123, 130, 144, 145, 154, 158, 160], "just": [6, 14, 152, 154, 155, 156, 158, 159, 160], "save": [6, 8, 9, 101, 116, 117, 118, 120, 126, 134, 138, 150, 154, 155, 156, 157, 159, 160], "less": [6, 41, 157, 158, 159, 161], "prone": 6, "manag": [6, 29, 106, 143, 155], "invari": 6, "accept": [6, 7, 41, 115, 156, 158, 161], "multipl": [6, 7, 8, 20, 28, 29, 94, 99, 100, 105, 135, 136, 137, 138, 140, 145, 158, 159], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 155, 156, 157], "worri": [6, 155, 158], "explicitli": [6, 104, 152, 160], "convert": [6, 24, 25, 28, 116, 139, 155, 157, 161], "time": [6, 54, 62, 81, 89, 114, 135, 137, 145, 154, 155, 156, 157, 159, 161], "produc": [6, 120, 161], "back": [6, 27, 106, 116, 156, 160, 161], "origin": [6, 34, 35, 101, 105, 155, 157, 159, 160, 161], "form": [6, 7, 8, 27, 154], "One": [6, 157], "advantag": [6, 160], "being": [6, 116, 117, 118, 122, 124, 161], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 24, 25, 31, 36, 39, 43, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 94, 95, 103, 104, 109, 110, 115, 121, 133, 135, 136, 137, 138, 151, 152, 156, 157, 158, 159, 160, 161], "abl": [6, 8, 157, 158, 159], "post": [6, 141, 145, 161], "tool": [6, 156, 157, 158], "quantiz": [6, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 105, 118, 129, 150, 158, 161], "eval": [6, 150, 152], "without": [6, 7, 9, 14, 109, 151, 152, 155, 157, 160], "code": [6, 8, 45, 46, 47, 48, 49, 50, 51, 52, 53, 99, 148, 152, 156, 158], "chang": [6, 7, 9, 14, 118, 151, 157, 158, 159, 160, 161], "OR": [6, 24], "convers": [6, 15, 16, 19, 21, 24, 25, 27, 28, 36, 41, 116, 118, 119, 152, 155, 156, 157, 159, 160, 161], "script": [6, 9, 154, 157, 158, 159], "wai": [6, 7, 28, 109, 154, 155, 156, 157, 158, 159], "surround": [6, 8, 152], "load_checkpoint": [6, 8, 116, 117, 118, 119], "save_checkpoint": [6, 8, 9, 116, 117, 118], "convertor": 6, "avail": [6, 8, 44, 121, 124, 125, 132, 152, 154, 157, 159, 160], "here": [6, 7, 9, 14, 16, 17, 22, 23, 38, 97, 98, 154, 155, 156, 157, 158, 159, 160, 161], "three": [6, 8, 103, 158], "hfcheckpoint": 6, "read": [6, 116, 117, 118, 152], "write": [6, 8, 14, 116, 117, 118, 135, 155, 156, 158], "compat": [6, 116, 118], "transform": [6, 8, 28, 30, 32, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 99, 100, 102, 142, 160], "framework": [6, 8, 152], "mention": [6, 157, 161], "assum": [6, 14, 17, 18, 22, 23, 30, 32, 39, 94, 98, 99, 100, 102, 107, 112, 120, 122, 125, 134, 157, 160], "checkpoint_dir": [6, 7, 116, 117, 118, 157, 159], "necessari": [6, 41, 135, 136, 137, 138, 155, 160], "easiest": [6, 157, 158], "sure": [6, 7, 157, 158, 159, 160, 161], "everyth": [6, 8, 121, 152, 158], "flow": [6, 28, 30, 31, 32, 161], "By": [6, 154, 159, 160, 161], "safetensor": [6, 116, 154], "output": [6, 18, 34, 35, 38, 41, 48, 49, 50, 67, 68, 69, 77, 78, 82, 83, 90, 94, 95, 97, 98, 99, 100, 105, 108, 109, 110, 118, 123, 126, 136, 145, 151, 154, 155, 156, 157, 158, 159, 160, 161], "dir": [6, 138, 151, 154, 157, 158, 159], "output_dir": [6, 7, 116, 117, 118, 145, 157, 159, 160, 161], "argument": [6, 7, 10, 18, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 94, 115, 121, 126, 131, 135, 137, 138, 142, 154, 155, 156, 159, 160], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 99, 145, 154, 156, 157, 160, 161], "_component_": [6, 7, 9, 10, 36, 39, 43, 155, 156, 157, 159, 160], "fullmodelhfcheckpoint": [6, 157], "directori": [6, 7, 116, 117, 118, 135, 137, 138, 145, 154, 157, 158, 159], "sort": [6, 116, 118], "so": [6, 7, 31, 116, 121, 151, 152, 155, 157, 158, 159, 160, 161], "order": [6, 8, 116, 118, 137, 138, 158], "matter": [6, 116, 118, 154, 160], "checkpoint_fil": [6, 7, 9, 116, 117, 118, 157, 159, 160, 161], "restart": [6, 154], "previou": [6, 31, 116, 117, 118], "more": [6, 7, 8, 36, 41, 62, 96, 98, 109, 115, 118, 121, 138, 142, 144, 152, 154, 156, 157, 158, 159, 160, 161], "next": [6, 31, 123, 159, 161], "section": [6, 8, 128, 150, 157, 159, 161], "recipe_checkpoint": [6, 116, 117, 118], "null": [6, 7], "usual": [6, 98, 116, 138, 154, 157, 160], "model_typ": [6, 116, 117, 118, 157, 159], "resume_from_checkpoint": [6, 116, 117, 118], "fals": [6, 7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 93, 94, 99, 100, 105, 106, 109, 111, 116, 117, 118, 132, 145, 154, 155, 156, 157, 159, 160, 161], "requir": [6, 7, 29, 33, 41, 43, 62, 116, 118, 120, 131, 132, 134, 137, 138, 140, 144, 145, 151, 154, 155, 156, 158, 161], "param": [6, 8, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 105, 107, 108, 110, 116, 134, 160, 161], "directli": [6, 7, 8, 10, 36, 39, 43, 115, 116, 154, 157, 158, 159, 160, 161], "ensur": [6, 7, 13, 27, 41, 94, 116, 118, 125, 152, 156, 158], "out": [6, 7, 8, 28, 30, 34, 35, 36, 38, 40, 116, 117, 150, 152, 154, 155, 157, 158, 159, 160, 161], "case": [6, 8, 9, 20, 116, 120, 125, 129, 134, 135, 142, 152, 154, 155, 156, 157, 159, 160, 161], "discrep": [6, 116], "along": [6, 159, 160], "found": [6, 7, 9, 97, 98, 154, 160, 161], "metacheckpoint": 6, "github": [6, 10, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 94, 97, 98, 102, 103, 109, 151, 156, 158], "repositori": [6, 19, 157, 158], "fullmodelmetacheckpoint": [6, 159], "torchtunecheckpoint": 6, "current": [6, 31, 91, 94, 96, 98, 99, 100, 117, 118, 126, 130, 135, 137, 141, 144, 157, 158, 159], "test": [6, 7, 8, 152, 155], "complet": [6, 8, 14, 31, 37, 92, 155, 156, 157, 158, 159], "written": [6, 7, 8, 116, 117, 135, 136, 137, 138, 152], "begin": [6, 31, 62, 81, 112, 155, 159, 161], "partit": [6, 116, 161], "ha": [6, 62, 81, 104, 106, 107, 110, 118, 120, 147, 156, 157, 158, 159, 160, 161], "standard": [6, 17, 24, 94, 136, 152, 155, 157, 159], "key_1": [6, 118], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 157], "inform": [6, 20, 138, 142, 152, 154, 157, 158, 159], "subsequ": [6, 8], "recipe_st": [6, 116, 117, 118], "pt": [6, 9, 116, 117, 118, 157, 159], "epoch": [6, 8, 9, 102, 116, 117, 118, 154, 155, 157, 158, 159], "optim": [6, 7, 8, 29, 62, 91, 102, 103, 118, 120, 122, 128, 140, 141, 145, 155, 157, 158, 159, 160, 161], "etc": [6, 8, 116, 128, 158], "prevent": [6, 31, 154], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 62, 81, 99, 104, 120, 141, 144, 145, 155, 156, 157, 160, 161], "updat": [6, 7, 8, 96, 120, 145, 151, 155, 157, 158, 159, 160, 161], "hf_model_0001_0": [6, 157], "hf_model_0002_0": [6, 157], "both": [6, 29, 110, 154, 157, 160, 161], "adapt": [6, 104, 105, 106, 107, 108, 116, 117, 118, 155, 157, 160, 161], "merg": [6, 10, 11, 116, 157, 159, 161], "would": [6, 7, 9, 31, 99, 151, 155, 156, 157, 160, 161], "primari": [6, 7, 8, 158], "want": [6, 7, 8, 9, 10, 28, 123, 151, 154, 155, 156, 157, 158, 159, 160], "resum": [6, 8, 102, 116, 117, 118, 161], "initi": [6, 8, 12, 29, 31, 45, 46, 47, 55, 56, 63, 64, 65, 74, 75, 84, 85, 120, 131, 132, 158, 160, 161], "frozen": [6, 160, 161], "base": [6, 10, 30, 32, 41, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 98, 102, 103, 105, 106, 108, 109, 110, 116, 121, 124, 126, 134, 135, 150, 155, 157, 158, 159, 160, 161], "well": [6, 7, 8, 152, 154, 156, 157, 159, 161], "learnt": [6, 155, 157], "someth": [6, 8, 9, 155, 157], "NOT": 6, "refer": [6, 7, 8, 97, 98, 103, 106, 152, 160], "adapter_checkpoint": [6, 116, 117, 118], "adapter_0": [6, 157], "now": [6, 120, 122, 155, 156, 157, 158, 159, 160, 161], "knowledg": 6, "creat": [6, 7, 10, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 93, 96, 102, 115, 116, 117, 118, 122, 135, 137, 154, 155, 156, 157, 159, 161], "simpl": [6, 8, 14, 17, 22, 23, 150, 156, 158, 160, 161], "forward": [6, 8, 94, 95, 97, 98, 99, 100, 103, 105, 128, 145, 159, 160, 161], "13b": [6, 45, 48, 51, 63, 67, 70], "modeltyp": [6, 116, 117, 118], "llama2_13b": [6, 67], "right": [6, 116, 157, 159, 160], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 109, 120, 160], "successfulli": [6, 154, 158], "vocab": [6, 10, 99, 159], "70": [6, 74], "x": [6, 94, 95, 97, 98, 99, 100, 105, 123, 143, 160, 161], "randint": 6, "0": [6, 8, 31, 48, 49, 50, 51, 52, 53, 54, 62, 67, 68, 69, 70, 71, 72, 81, 89, 94, 99, 102, 103, 105, 114, 123, 137, 138, 139, 140, 144, 146, 149, 153, 155, 156, 157, 158, 159, 160, 161], "no_grad": 6, "6": [6, 31, 97, 139, 140, 157, 161], "3989": 6, "9": [6, 140, 157, 161], "0531": 6, "3": [6, 31, 73, 90, 91, 92, 119, 121, 127, 139, 140, 143, 154, 155, 157, 158, 159, 161], "2375": 6, "5": [6, 7, 14, 102, 103, 139, 140, 157, 158, 159], "2822": 6, "4": [6, 7, 41, 94, 139, 140, 146, 152, 154, 156, 157, 159, 160, 161], "4872": 6, "7469": 6, "8": [6, 34, 35, 38, 40, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 140, 157, 160, 161], "6737": 6, "11": [6, 140, 157, 159, 161], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 139, 140], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 28, 30, 32, 39, 41, 109, 114, 138, 154, 155, 156, 157, 158, 159, 160], "find": [6, 8, 9, 154, 157, 158, 160], "list": [6, 7, 15, 16, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 93, 104, 105, 109, 110, 111, 112, 114, 116, 117, 118, 121, 123, 127, 139, 140, 155, 156, 158, 159], "builder": [6, 37, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 93, 155, 156, 161], "hope": 6, "deeper": [6, 158], "insight": [6, 157], "happi": [6, 157], "thi": [7, 8, 9, 10, 17, 20, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 43, 44, 62, 73, 81, 89, 91, 92, 94, 95, 98, 99, 100, 101, 102, 104, 106, 109, 110, 111, 112, 114, 115, 116, 117, 118, 120, 121, 123, 124, 125, 128, 132, 134, 135, 137, 138, 140, 141, 142, 144, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161], "pars": [7, 10, 11, 113, 121, 155, 158], "effect": 7, "cli": [7, 9, 11, 12, 151, 157, 158], "prerequisit": [7, 155, 156, 157, 158, 159, 160, 161], "Be": [7, 155, 157, 158, 159, 160, 161], "familiar": [7, 155, 157, 158, 159, 160, 161], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 151, 155, 156, 158], "instal": [7, 9, 132, 137, 138, 150, 154, 157, 158, 159, 160, 161], "fundament": 7, "There": [7, 15, 27, 155, 157, 158, 159, 160], "entri": [7, 8, 158], "point": [7, 8, 24, 25, 114, 156, 157, 158, 159, 160, 161], "locat": [7, 154, 159, 160, 161], "thei": [7, 8, 20, 29, 99, 110, 121, 126, 154, 155, 156, 160], "truth": [7, 157, 159], "reproduc": 7, "overridden": [7, 95, 121, 145], "quick": [7, 29], "experiment": 7, "serv": [7, 114, 115, 156, 160], "particular": [7, 28, 29, 41, 115, 156, 160, 161], "seed": [7, 8, 9, 144, 158], "shuffl": [7, 31], "devic": [7, 8, 109, 120, 124, 125, 128, 154, 155, 157, 158, 159, 160], "cuda": [7, 124, 125, 128, 145, 151, 157, 161], "dtype": [7, 8, 96, 99, 101, 125, 143, 147, 157, 161], "fp32": [7, 161], "enable_fsdp": 7, "mani": [7, 31, 156, 157], "object": [7, 10, 11, 15, 16, 19, 21, 94, 115, 129, 155], "keyword": [7, 10, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 101, 155, 156], "loss": [7, 8, 30, 34, 35, 38, 40, 103, 158, 160, 161], "exampl": [7, 8, 9, 10, 12, 14, 17, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 54, 62, 73, 81, 89, 94, 103, 104, 106, 111, 112, 114, 115, 116, 117, 119, 120, 123, 129, 137, 138, 139, 140, 143, 146, 148, 149, 151, 153, 154, 155, 156, 157, 159, 160, 161], "subfield": 7, "dotpath": 7, "wish": [7, 156], "exact": [7, 10, 157], "path": [7, 8, 9, 10, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 57, 62, 66, 73, 76, 81, 86, 89, 92, 111, 112, 113, 116, 117, 118, 121, 145, 154, 155, 156, 157, 159, 160], "normal": [7, 28, 31, 62, 81, 97, 99, 100, 111, 155, 156, 160, 161], "python": [7, 121, 127, 138, 144, 148, 154, 157], "alpaca_dataset": [7, 34, 156], "custom": [7, 8, 28, 30, 32, 36, 39, 43, 142, 152, 154, 157, 158, 159, 160], "train_on_input": [7, 24, 25, 28, 30, 34, 35, 36, 38, 39, 40, 41, 155, 156], "onc": [7, 106, 157, 158, 159, 160, 161], "ve": [7, 96, 155, 156, 157, 159, 160], "instanc": [7, 10, 29, 95, 101, 107, 108, 160], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 36, 154, 157, 161], "under": [7, 145, 156, 157, 159, 161], "preced": [7, 10, 154, 159, 160], "actual": [7, 9, 14, 17, 22, 23, 28, 155], "throw": 7, "notic": [7, 155, 156, 160], "miss": [7, 109, 110, 145, 160], "posit": [7, 10, 31, 94, 96, 98, 99, 100, 159], "anoth": [7, 157], "handl": [7, 12, 20, 29, 62, 81, 111, 112, 155, 157, 160, 161], "def": [7, 8, 9, 12, 115, 119, 155, 156, 160, 161], "dictconfig": [7, 8, 10, 11, 12, 13, 138, 145], "arg": [7, 10, 99, 101, 104, 121, 136, 145], "tupl": [7, 10, 29, 41, 54, 62, 73, 81, 89, 96, 101, 103, 114, 115, 121, 130, 139, 140, 145, 147], "kwarg": [7, 10, 101, 104, 121, 131, 135, 136, 137, 138, 142, 145, 156], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 57, 62, 66, 73, 76, 81, 86, 89, 92, 101, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 116, 117, 118, 120, 121, 124, 125, 127, 128, 129, 131, 133, 135, 136, 137, 138, 139, 140, 144, 145, 146, 147, 155, 156], "mean": [7, 94, 97, 99, 100, 134, 154, 155, 156, 158, 160], "pass": [7, 10, 28, 29, 30, 32, 33, 36, 37, 39, 43, 44, 94, 95, 101, 106, 110, 112, 115, 118, 125, 126, 128, 131, 134, 137, 138, 142, 145, 154, 155, 156, 160, 161], "add": [7, 9, 28, 31, 62, 114, 118, 119, 121, 156, 157, 159, 160, 161], "d": [7, 20, 94, 96, 99, 154, 155, 160], "llama2_token": [7, 155, 157], "tmp": [7, 120, 155, 158, 159], "llama2token": [7, 66], "option": [7, 8, 14, 17, 18, 22, 23, 26, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 48, 49, 50, 54, 58, 59, 62, 67, 68, 69, 73, 76, 77, 78, 81, 82, 83, 89, 90, 92, 94, 98, 99, 100, 101, 109, 110, 111, 114, 116, 117, 118, 123, 124, 125, 127, 129, 135, 138, 144, 145, 151, 152, 154, 156, 157], "modeltoken": [7, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 114, 155, 156], "bool": [7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 93, 101, 105, 109, 110, 111, 112, 114, 115, 116, 117, 118, 126, 128, 131, 132, 134, 137, 142, 145, 146, 155, 161], "max_seq_len": [7, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 62, 73, 81, 89, 94, 96, 98, 99, 114, 155, 156], "int": [7, 9, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 93, 94, 96, 97, 98, 99, 102, 105, 111, 112, 113, 114, 115, 116, 117, 118, 123, 126, 130, 134, 135, 136, 137, 138, 139, 140, 142, 144, 145, 154, 155, 156, 160, 161], "512": [7, 34, 35, 156, 161], "instructdataset": [7, 34, 35, 38, 39, 40, 156], "alreadi": [7, 119, 131, 134, 151, 154, 156, 157, 160], "overwrit": [7, 118, 151], "duplic": [7, 8, 152, 154], "sometim": 7, "than": [7, 27, 41, 94, 96, 115, 118, 119, 146, 147, 155, 156, 157, 158, 159, 160, 161], "resolv": [7, 11, 158], "alpaca": [7, 14, 34, 35, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 156], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 135, 136, 137, 138], "disklogg": 7, "log_dir": [7, 135, 137, 138], "conveni": [7, 8, 154], "verifi": [7, 124, 125, 126, 155, 158, 160], "properli": [7, 109, 132, 154], "experi": [7, 138, 150, 152, 155, 159, 160], "wa": [7, 109, 155, 157, 159, 160, 161], "cp": [7, 151, 154, 155, 157, 158, 159], "7b_lora_single_devic": [7, 157, 158, 160, 161], "my_config": 7, "discuss": [7, 158, 160], "guidelin": 7, "while": [7, 8, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 95, 152, 157, 161], "mai": [7, 9, 126, 155, 156, 158, 160], "tempt": 7, "put": [7, 8, 158, 160], "much": [7, 157, 159, 160, 161], "give": [7, 156, 160], "maximum": [7, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 73, 94, 96, 98, 99, 154], "flexibl": [7, 29, 156], "switch": 7, "encourag": [7, 62, 160], "clariti": 7, "significantli": 7, "easier": [7, 157, 158], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 118, 155, 158], "parent": [7, 154], "modul": [7, 10, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 122, 126, 134, 141, 142, 144, 158, 160, 161], "__init__": [7, 8, 160, 161], "py": [7, 10, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 94, 96, 97, 98, 102, 103, 154, 157, 159], "guarante": 7, "stabil": [7, 152, 161], "underscor": 7, "_alpaca": 7, "collect": [7, 123, 158], "itself": 7, "via": [7, 9, 36, 39, 43, 105, 116, 160, 161], "k1": [7, 8], "v1": [7, 8, 44], "k2": [7, 8], "v2": [7, 8, 156], "lora_finetune_single_devic": [7, 154, 155, 157, 158, 159, 160, 161], "checkpoint": [7, 8, 101, 112, 116, 117, 118, 119, 120, 138, 142, 152, 154, 159, 160, 161], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 36, 39, 54, 62, 73, 81, 89, 94, 95, 96, 97, 98, 99, 100, 103, 104, 105, 107, 108, 111, 112, 116, 117, 118, 119, 120, 121, 135, 136, 137, 138, 155, 156, 158, 160, 161], "assign": [7, 33], "nest": 7, "dot": 7, "notat": [7, 94, 98, 99], "certain": [7, 145, 155], "flag": [7, 8, 30, 34, 35, 38, 40, 115, 118, 126, 154, 161], "built": [7, 9, 42, 151, 155, 158, 161], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 28], "pytorch": [7, 8, 62, 99, 101, 109, 115, 132, 137, 142, 144, 145, 150, 151, 152, 159, 160, 161], "llama3": [7, 28, 41, 73, 74, 75, 76, 77, 78, 79, 80, 119, 123, 126, 150, 154, 156], "8b_full": [7, 154, 156], "adamw": [7, 160], "lr": [7, 102], "2e": 7, "fuse": [7, 141], "nproc_per_nod": [7, 156, 159, 160], "full_finetune_distribut": [7, 154, 156, 157, 158], "core": [8, 152, 156, 158, 161], "i": [8, 19, 21, 94, 99, 100, 101, 108, 120, 123, 156, 157, 159, 161], "structur": [8, 15, 16, 19, 21, 24, 25, 28, 36, 76, 92, 155, 156, 157], "new": [8, 37, 84, 96, 119, 135, 137, 155, 157, 158, 159, 160, 161], "user": [8, 15, 16, 19, 20, 21, 24, 25, 27, 28, 54, 62, 81, 89, 94, 114, 155, 156, 158], "thought": [8, 152, 158, 161], "target": [8, 152], "pipelin": [8, 152], "llm": [8, 150, 152, 156, 157, 160], "eg": [8, 99, 116, 152], "meaning": [8, 152, 157], "featur": [8, 9, 151, 152, 157, 158], "fsdp": [8, 115, 120, 126, 134, 152, 158, 159], "activ": [8, 95, 128, 133, 142, 145, 152, 161], "gradient": [8, 134, 141, 145, 152, 157, 159, 160, 161], "accumul": [8, 141, 145, 152], "mix": [8, 154, 156, 157], "precis": [8, 101, 125, 152, 158, 161], "appli": [8, 28, 30, 32, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 97, 98, 99, 100, 109, 110, 142, 152, 161], "given": [8, 10, 14, 17, 18, 22, 23, 27, 105, 106, 123, 124, 125, 129, 134, 141, 146, 152, 160], "complex": 8, "becom": [8, 151, 156], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 99, 119, 154, 156], "methodolog": 8, "reason": [8, 123, 157], "possibl": [8, 28, 31, 36, 154, 156], "trade": 8, "off": [8, 62, 81, 157], "memori": [8, 29, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 101, 109, 126, 128, 133, 134, 145, 150, 152, 157, 158, 159], "vs": [8, 158], "qualiti": [8, 157, 160], "believ": 8, "best": [8, 155], "suit": [8, 158], "b": [8, 94, 96, 98, 99, 100, 105, 134, 138, 160, 161], "fit": [8, 28, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 156], "solut": 8, "result": [8, 54, 62, 81, 89, 114, 145, 157, 159, 160, 161], "meant": [8, 101, 120], "depend": [8, 9, 14, 116, 145, 154, 156, 157, 160, 161], "level": [8, 122, 127, 134, 152, 161], "expertis": 8, "routin": 8, "yourself": [8, 154, 159, 160], "exist": [8, 151, 154, 157, 158, 159, 161], "ad": [8, 81, 111, 118, 119, 155, 160, 161], "ones": 8, "modular": [8, 152], "build": [8, 36, 39, 43, 152, 159, 160], "block": [8, 31, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 109, 110, 152], "wandb": [8, 9, 138, 158], "log": [8, 11, 103, 127, 128, 133, 135, 136, 137, 138, 157, 158, 159, 161], "fulli": [8, 29], "nativ": [8, 150, 152, 160, 161], "correct": [8, 17, 38, 97, 98, 99, 124, 152, 155, 156], "numer": [8, 152], "pariti": [8, 152], "verif": 8, "extens": [8, 118, 152], "comparison": [8, 160, 161], "benchmark": [8, 144, 152, 157, 159, 160], "limit": [8, 120, 156], "hidden": [8, 95], "behind": 8, "100": [8, 30, 34, 35, 38, 40, 41, 123, 139, 140, 160, 161], "prefer": [8, 22, 42, 103, 140, 152, 154, 156], "over": [8, 102, 121, 152, 157, 159, 160, 161], "unnecessari": 8, "abstract": [8, 15, 18, 152, 158, 161], "No": [8, 118, 152], "inherit": [8, 121, 152, 156], "go": [8, 19, 21, 54, 62, 81, 89, 114, 152, 156, 157, 158, 161], "upon": [8, 29, 159], "figur": [8, 160, 161], "spectrum": 8, "decid": 8, "interact": [8, 150, 158], "start": [8, 9, 29, 114, 119, 151, 152, 155, 156, 157, 158], "paradigm": 8, "consist": [8, 44, 158], "configur": [8, 30, 32, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 73, 89, 100, 152, 155, 158, 159, 160, 161], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 152, 154, 155, 156, 157, 158, 159, 160, 161], "overrid": [8, 11, 12, 154, 157, 158, 159, 161], "togeth": [8, 31, 138, 158, 160], "valid": [8, 27, 109, 110, 147, 151, 157, 158], "environ": [8, 124, 132, 151, 154, 157, 158], "logic": [8, 119, 152, 158, 160], "api": [8, 9, 24, 51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 109, 154, 155, 157, 158, 159, 161], "closer": [8, 160], "monolith": [8, 152], "trainer": [8, 103], "A": [8, 9, 24, 25, 29, 31, 54, 62, 81, 89, 94, 99, 100, 101, 103, 105, 109, 111, 112, 114, 115, 120, 121, 128, 129, 133, 134, 139, 140, 149, 150, 153, 154, 155, 157, 160, 161], "wrapper": [8, 111, 112, 120, 122, 154, 160], "around": [8, 28, 62, 81, 111, 112, 128, 154, 155, 157, 160, 161], "extern": [8, 156], "primarili": [8, 29, 160], "eleutherai": [8, 152, 160], "har": [8, 152, 160], "control": [8, 30, 34, 35, 38, 40, 106, 144, 157], "multi": [8, 28, 94, 109, 159], "stage": 8, "distil": 8, "oper": [8, 29, 106, 144], "turn": [8, 20, 27, 28, 155], "dataload": [8, 31, 34, 35, 38, 40], "applic": [8, 94, 116, 117, 138], "clean": [8, 9, 34], "after": [8, 89, 94, 96, 97, 99, 100, 109, 134, 135, 136, 137, 138, 155, 161], "process": [8, 9, 101, 130, 131, 144, 156, 158, 161], "group": [8, 94, 130, 131, 135, 136, 137, 138, 154, 159], "init_process_group": [8, 131], "backend": [8, 154], "gloo": 8, "els": [8, 121, 138, 152, 161], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 29, 118, 121, 126, 145, 156, 158, 159, 160], "stuff": 8, "carri": 8, "relev": [8, 20, 154, 157, 160], "interfac": [8, 15, 18, 29, 104], "metric": [8, 158], "logger": [8, 127, 133, 135, 136, 137, 138, 158], "self": [8, 9, 31, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 94, 99, 100, 104, 109, 110, 116, 119, 120, 156, 160, 161], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 115, 126, 134, 142, 155], "_model": [8, 120], "_setup_model": 8, "_token": [8, 156], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 120, 122, 141, 145, 161], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 103, 144, 154, 159], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 31], "batch": [8, 31, 34, 35, 38, 40, 94, 96, 98, 99, 103, 139, 140, 145, 152, 156, 158, 159, 160], "enumer": 8, "_autocast": 8, "logit": [8, 123], "label": [8, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 103, 139, 140], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 135, 136, 137, 138], "step": [8, 31, 99, 102, 122, 135, 136, 137, 138, 141, 145, 150, 157, 160, 161], "learn": [8, 29, 102, 152, 155, 156, 158, 159, 160, 161], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 23, 26, 27, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 54, 62, 73, 76, 81, 89, 92, 94, 96, 98, 99, 100, 106, 108, 109, 110, 111, 114, 116, 117, 118, 119, 123, 124, 125, 127, 129, 133, 135, 136, 137, 138, 141, 142, 143, 144, 145, 147, 155, 156, 157], "fullfinetunerecip": 8, "wandblogg": [9, 160, 161], "workspac": 9, "seen": [9, 160, 161], "screenshot": 9, "below": [9, 14, 98, 115, 156, 159, 160, 161], "packag": [9, 137, 138, 151], "pip": [9, 137, 138, 151, 157, 159], "Then": [9, 106, 158], "login": [9, 138, 154, 157], "project": [9, 48, 49, 50, 67, 68, 69, 77, 78, 82, 83, 90, 94, 95, 109, 110, 126, 138, 150, 160, 161], "grab": [9, 159], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 132, 155], "exit": [9, 151, 154], "resourc": [9, 135, 136, 137, 138], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 39, 41, 43, 94, 98, 99, 100, 123, 155, 157], "desir": [9, 28, 143, 155], "suggest": 9, "approach": [9, 29, 156], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 157], "_output_dir": [9, 116, 117, 118], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 145], "type": [9, 10, 12, 20, 24, 25, 26, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 101, 103, 105, 107, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 140, 142, 143, 144, 145, 146, 156, 157, 160, 161], "descript": [9, 36, 41, 154], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 24, 25, 28, 31, 34, 35, 38, 40, 133, 156], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 84, 85, 87, 88, 90, 91, 92, 93, 94, 97, 98, 102, 103, 109, 115, 116, 117, 121, 127, 132, 137, 138, 142, 144, 151, 156, 157], "com": [10, 48, 49, 50, 58, 59, 62, 67, 68, 69, 73, 77, 78, 90, 94, 97, 98, 102, 103, 109, 151], "facebookresearch": [10, 97, 98], "blob": [10, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90, 92, 94, 97, 98, 102, 103], "main": [10, 12, 62, 92, 94, 97, 98, 151, 157, 159], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 99], "32": [10, 159, 160, 161], "num_head": [10, 94, 96, 98, 99], "num_kv_head": [10, 94, 96], "vocab_s": 10, "must": [10, 29, 104, 121, 161], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 89, 90, 91, 92, 94, 96, 97, 98, 99, 100, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 139, 140, 143, 144, 145, 146, 155, 156, 160, 161], "nn": [10, 94, 95, 96, 99, 100, 101, 104, 106, 107, 108, 115, 122, 134, 141, 142, 147, 160, 161], "parsed_yaml": 10, "embed_dim": [10, 94, 98, 100, 160], "valueerror": [10, 21, 24, 27, 36, 41, 89, 94, 96, 99, 103, 116, 117, 118, 125, 128, 144, 147], "recipe_nam": 11, "rank": [11, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 105, 130, 132, 144, 158, 160, 161], "zero": [11, 96, 97, 157, 159], "displai": 11, "callabl": [12, 28, 30, 32, 99, 106, 115, 123, 126, 129, 134, 142], "With": [12, 157, 160, 161], "my_recip": 12, "foo": 12, "bar": [12, 152, 158], "instanti": [13, 45, 46, 47, 48, 49, 50, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68, 69, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 90, 91, 92, 120], "configerror": 13, "cannot": [13, 118, 159], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 62, 128, 135, 136, 137, 138, 156, 157, 161], "templat": [14, 15, 17, 18, 22, 23, 28, 30, 32, 34, 35, 36, 38, 39, 40, 41, 62], "style": [14, 31, 34, 35, 36, 41, 161], "slightli": 14, "describ": [14, 62, 73, 142, 156], "task": [14, 23, 29, 37, 155, 156, 157, 159, 160, 161], "further": [14, 154, 156, 160, 161], "context": [14, 16, 91, 106, 143, 145, 156], "respons": [14, 16, 54, 62, 81, 89, 103, 114, 156, 157, 158, 159], "appropri": [14, 16, 19, 21, 29, 102, 116, 156, 161], "request": [14, 125, 156, 157], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 156], "map": [14, 17, 18, 22, 23, 24, 25, 28, 29, 30, 31, 32, 39, 73, 89, 108, 112, 113, 116, 120, 122, 135, 136, 137, 138, 141, 145, 155, 156, 157, 160], "column_map": [14, 17, 18, 22, 23, 28, 30, 32, 39, 156], "placehold": [14, 15, 17, 18, 22, 23, 28, 30, 32, 39, 156], "column": [14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 43, 94, 99, 100, 155, 156], "ident": [14, 17, 18, 21, 22, 23, 30, 31, 32, 39, 157], "poem": 14, "n": [14, 22, 54, 62, 81, 89, 94, 114, 149, 153, 155, 156], "nwrite": 14, "long": [14, 31, 112, 155, 160], "where": [14, 17, 22, 23, 28, 29, 34, 35, 38, 40, 62, 81, 94, 99, 105, 111, 126, 134, 140, 156], "me": 14, "role": [15, 20, 24, 25, 28, 54, 62, 81, 89, 114, 155, 156], "system": [15, 16, 19, 20, 21, 24, 25, 27, 28, 54, 62, 81, 89, 114, 155, 156], "assist": [15, 16, 19, 20, 24, 25, 27, 28, 54, 62, 81, 89, 92, 114, 123, 155, 156], "accord": [15, 21, 155], "openai": [16, 24, 36, 156], "markup": 16, "languag": [16, 105, 123, 160], "It": [16, 21, 154, 155, 156, 161], "im_start": 16, "im_end": 16, "goe": [16, 106], "tag": [16, 19, 21, 28, 135, 136, 137, 138, 155], "grammar": [17, 38, 156], "english": 17, "sentenc": [17, 31, 81], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 160], "lazi": 17, "dog": 17, "alwai": [18, 121], "human": [19, 25, 155], "taken": [19, 160, 161], "inst": [19, 21, 28, 62, 155, 156], "sy": [19, 62, 155, 156], "respect": [19, 29, 108, 145, 155, 156], "honest": [19, 155, 156], "am": [19, 21, 155, 156, 157, 159], "pari": [19, 21, 23, 156], "capit": [19, 21, 22, 23, 156], "franc": [19, 21, 22, 23, 156], "known": [19, 21, 62, 81, 129, 156], "its": [19, 21, 31, 94, 98, 99, 100, 141, 144, 154, 156, 157, 159, 160], "stun": [19, 21, 156], "liter": [20, 48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 109, 110], "mask": [20, 30, 31, 34, 35, 38, 40, 54, 62, 73, 81, 89, 94, 99, 100, 114, 155, 156], "ipython": 20, "eot": 20, "dataclass": [20, 155], "repres": [20, 140, 155], "individu": [20, 31, 128, 138, 142, 155, 156], "tiktoken": [20, 73, 112, 159], "special": [20, 28, 62, 73, 76, 81, 89, 92, 112, 113, 114, 120, 156], "variabl": [20, 28, 29, 30, 32, 39, 132, 161], "writer": 20, "whether": [20, 24, 25, 28, 30, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 58, 59, 67, 68, 69, 73, 77, 78, 81, 82, 83, 89, 90, 101, 105, 109, 110, 111, 112, 115, 125, 128, 155, 156], "correspond": [20, 104, 107, 125, 140, 158, 159], "consecut": [20, 27], "from_dict": [20, 155], "construct": [20, 160], "dictionari": [20, 31, 128, 133, 135, 136, 137, 138, 140, 157], "mistral": [21, 28, 41, 81, 82, 83, 84, 85, 86, 87, 88, 119, 154, 155, 157, 158], "llama2chatformat": [21, 62, 155, 156], "similar": [22, 37, 42, 44, 109, 156, 157, 159, 160, 161], "stackexchangedpair": 22, "question": [22, 155, 156, 157, 159], "answer": [22, 155, 157, 159], "nanswer": 22, "summar": [23, 40, 155, 156], "dialogu": [23, 40, 155], "summari": [23, 40, 128, 156], "dialog": 23, "hello": [23, 54, 62, 73, 81, 89, 111, 112, 155, 157, 159], "did": [23, 157, 159, 161], "know": [23, 155, 156, 157, 159, 160], "adher": [24, 25], "could": [24, 160], "remain": [24, 25, 102, 160], "unmask": [24, 25], "sharegpt": [25, 36], "gpt": [25, 94, 157], "eos_id": [26, 112, 114], "length": [26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 41, 43, 44, 54, 62, 73, 81, 89, 91, 94, 96, 98, 99, 112, 114, 117, 139, 140], "last": [26, 31, 102, 156], "replac": [26, 30, 34, 35, 38, 40, 101, 160], "forth": [27, 156], "come": [27, 104, 160], "empti": [27, 154], "shorter": 27, "min": [27, 160], "invalid": 27, "convert_to_messag": [28, 155], "chat_format": [28, 36, 41, 155, 156], "chatformat": [28, 36, 156], "load_dataset_kwarg": [28, 30, 32, 33, 36, 37, 39, 43, 44], "multiturn": [28, 155], "prepar": [28, 155], "truncat": [28, 30, 31, 32, 33, 37, 39, 41, 43, 44, 54, 62, 73, 81, 89, 112, 114, 156], "tokenize_messag": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 54, 62, 73, 81, 89, 114, 155, 156], "anyth": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "load_dataset": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 155, 156], "huggingfac": [28, 30, 32, 33, 36, 37, 39, 43, 44, 85, 91, 92, 102, 103, 116, 117, 154, 157], "co": [28, 30, 32, 33, 36, 37, 39, 43, 44, 85, 91, 92, 116, 117, 157], "doc": [28, 30, 32, 33, 36, 37, 39, 43, 44, 62, 73, 115, 121, 127, 132, 137, 138, 144, 154, 157], "en": [28, 30, 32, 33, 36, 37, 39, 43, 44], "package_refer": [28, 30, 32, 33, 36, 37, 39, 43, 44], "loading_method": [28, 30, 32, 33, 36, 37, 39, 43, 44], "extra": [28, 151, 160, 161], "still": [28, 121, 160, 161], "unless": 28, "check": [28, 36, 99, 109, 125, 132, 146, 150, 155, 157, 158, 160], "concaten": [29, 54, 62, 81, 89, 114, 140], "sub": [29, 137], "unifi": [29, 85], "were": [29, 106, 155, 158], "simplifi": [29, 154, 160], "simultan": 29, "intern": [29, 121], "aggreg": 29, "transpar": 29, "index": [29, 31, 94, 98, 99, 100, 102, 139, 140, 151, 155, 157], "howev": [29, 92, 151], "constitu": 29, "might": [29, 154, 157], "larg": [29, 105, 145, 154, 161], "comput": [29, 94, 95, 98, 99, 103, 128, 144, 157, 161], "cumul": 29, "maintain": [29, 161], "indic": [29, 31, 94, 98, 99, 100, 115, 132, 155], "deleg": 29, "retriev": [29, 126], "lead": [29, 81, 111], "high": [29, 152, 160], "scale": [29, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 105, 123, 160, 161], "consid": 29, "strategi": 29, "stream": [29, 127], "demand": 29, "deriv": [29, 95, 99, 100], "_dataset": 29, "_len": 29, "total": [29, 102, 130, 149, 153, 157, 159, 160], "combin": 29, "_index": 29, "lookup": 29, "dataset1": 29, "mycustomdataset": 29, "params1": 29, "dataset2": 29, "params2": 29, "concat_dataset": 29, "data_point": 29, "1500": 29, "element": [29, 157], "focus": [29, 158], "enhanc": [29, 161], "divers": 29, "machin": [29, 124, 154, 157], "instructtempl": [30, 32, 156], "contribut": [30, 34, 35, 38, 40], "disabl": [30, 32, 33, 37, 39, 43, 44, 106, 144], "recommend": [30, 32, 33, 34, 35, 37, 39, 43, 44, 137, 155, 157, 161], "highest": [30, 32, 33, 34, 35, 37, 39, 43, 44], "sequenc": [30, 31, 32, 33, 34, 35, 37, 39, 41, 43, 44, 54, 62, 73, 81, 89, 94, 96, 98, 99, 112, 114, 139, 140, 155], "ds": [31, 41], "padding_idx": [31, 139, 140], "max_pack": 31, "split_across_pack": 31, "greedi": 31, "pack": [31, 34, 35, 36, 38, 39, 40, 41, 43, 94, 98, 99, 100], "done": [31, 109, 125, 134, 160, 161], "preprocess": 31, "outsid": [31, 144, 145, 157, 159, 160], "sampler": [31, 158], "part": [31, 155, 161], "buffer": 31, "enough": [31, 155], "attent": [31, 48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 91, 94, 96, 98, 99, 100, 109, 110, 159, 160, 161], "lower": [31, 160], "triangular": 31, "cross": 31, "attend": [31, 94, 99, 100], "rel": [31, 94, 98, 99, 100, 128, 160], "pad": [31, 123, 139, 140, 156], "max": [31, 41, 54, 62, 81, 89, 99, 102, 112, 114, 154, 160], "wise": 31, "collat": [31, 139, 156], "made": [31, 36, 39, 43, 98, 157], "smaller": [31, 157, 159, 160, 161], "jam": 31, "vari": 31, "s1": [31, 62, 81, 111], "s2": [31, 62, 81, 111], "s3": 31, "s4": 31, "contamin": 31, "input_po": [31, 94, 96, 98, 99, 100], "matrix": 31, "causal": [31, 94, 99, 100], "continu": [31, 156], "increment": 31, "move": [31, 99], "entir": [31, 134, 155, 161], "avoid": [31, 97, 101, 144, 154, 161], "freeform": [33, 43], "unstructur": [33, 44], "corpu": [33, 37, 44], "local": [33, 43, 76, 92, 138, 144, 151, 154, 155, 157, 158], "tabular": [33, 43], "yahma": [34, 39], "codebas": [34, 35, 38, 40, 157], "prior": [34, 35, 36, 38, 39, 40, 41, 43], "alpaca_d": [34, 35], "batch_siz": [34, 35, 38, 40, 94, 96, 99, 100, 103, 157], "tatsu": 35, "lab": 35, "conversation_styl": [36, 156], "chatdataset": [36, 41, 155, 156], "friendli": [36, 39, 43, 123, 155], "huggingfaceh4": 36, "no_robot": 36, "chatmlformat": 36, "2096": [36, 39, 43], "accomplish": [36, 39, 43], "packeddataset": [36, 39, 43, 156], "ccdv": 37, "cnn_dailymail": 37, "textcompletiondataset": [37, 43, 44, 156], "cnn": 37, "dailymail": 37, "articl": [37, 44], "extract": [37, 113], "highlight": [37, 161], "liweili": 38, "c4_200m": 38, "variant": [38, 40], "mirror": [38, 40], "llama_recip": [38, 40], "grammar_d": 38, "alpaca_clean": 39, "alpacainstructtempl": [39, 156], "samsum": [40, 156], "samsum_d": 40, "open": [41, 55, 56, 156, 157], "orca": 41, "slimorca": 41, "dedup": 41, "1024": [41, 42, 156], "prescrib": 41, "least": [41, 159, 160], "though": [41, 155], "10": [41, 139, 140, 157, 159, 161], "351": 41, "82": [41, 157], "391": 41, "221": 41, "220": 41, "193": 41, "12": [41, 140, 151], "471": 41, "lvwerra": [42, 156], "stack": [42, 145, 156], "exchang": [42, 156], "preferencedataset": [42, 156], "stackexchangepair": 42, "textdataset": 43, "omit": [43, 160], "allenai": [43, 156], "c4": [43, 156], "data_dir": [43, 156], "realnewslik": [43, 156], "wikitext": 44, "subset": [44, 107], "103": [44, 157], "raw": 44, "wikipedia": 44, "page": [44, 151, 152, 154, 158, 159], "code_llama2": [45, 46, 47, 48, 49, 50, 51, 52, 53, 154], "transformerdecod": [45, 46, 47, 48, 49, 50, 51, 52, 53, 63, 64, 65, 67, 68, 69, 70, 71, 72, 74, 75, 77, 78, 79, 80, 82, 83, 84, 85, 87, 88, 90, 91, 93, 123, 160], "w": [45, 46, 47, 55, 56, 63, 64, 65, 74, 75, 84, 85, 137, 138, 155, 157, 160, 161], "arxiv": [45, 46, 47, 51, 52, 53, 60, 61, 63, 64, 65, 70, 71, 72, 79, 80, 87, 88, 93, 94, 97, 98, 103], "org": [45, 46, 47, 51, 52, 53, 60, 61, 62, 63, 64, 65, 70, 71, 72, 79, 80, 87, 88, 93, 94, 97, 98, 103, 115, 121, 127, 132, 137, 142, 144, 151], "pdf": [45, 46, 47, 94, 97], "2308": [45, 46, 47], "12950": [45, 46, 47], "70b": [46, 49, 52, 64, 68, 71, 74, 77, 79, 159], "lora_attn_modul": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 109, 110, 160, 161], "q_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 109, 110, 160, 161], "k_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 109, 110, 160, 161], "v_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 109, 110, 160, 161], "output_proj": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 109, 110, 160, 161], "apply_lora_to_mlp": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 109, 110, 160], "apply_lora_to_output": [48, 49, 50, 51, 52, 53, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 109, 110, 160], "lora_rank": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 160], "lora_alpha": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 160], "float": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 94, 97, 102, 103, 105, 123, 128, 133, 135, 136, 137, 138, 160, 161], "16": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 140, 160, 161], "lora_dropout": [48, 49, 50, 51, 52, 53, 67, 68, 69, 70, 71, 72], "05": [48, 49, 50, 51, 52, 53, 67, 68, 69, 70, 71, 72], "quantize_bas": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 105, 161], "lora": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 105, 106, 109, 110, 116, 134, 150, 152, 155, 158, 159], "code_llama2_13b": 48, "tloen": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90], "8bb8579e403dc78e37fe81ffbb253c413007323f": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90], "l41": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90], "l43": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 90], "linear": [48, 49, 50, 51, 52, 53, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 82, 83, 87, 88, 90, 93, 99, 104, 105, 109, 110, 160, 161], "mlp": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 99, 100, 109, 110, 159, 160], "final": [48, 49, 50, 67, 68, 69, 77, 78, 82, 83, 90, 95, 99, 106, 109, 110, 157, 159, 160, 161], "low": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 105, 157, 160, 161], "approxim": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 105, 160], "factor": [48, 49, 50, 58, 59, 67, 68, 69, 77, 78, 82, 83, 90, 105, 157], "code_llama2_70b": 49, "code_llama2_7b": 50, "qlora": [51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 101, 150, 152, 159, 160], "per": [51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 96, 101, 154, 159, 161], "paper": [51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 160, 161], "ab": [51, 52, 53, 60, 61, 63, 64, 65, 70, 71, 72, 79, 80, 87, 88, 93, 98, 103], "2305": [51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93, 94, 103], "14314": [51, 52, 53, 60, 61, 70, 71, 72, 79, 80, 87, 88, 93], "lora_code_llama2_13b": 51, "lora_code_llama2_70b": 52, "lora_code_llama2_7b": 53, "gemma": [54, 55, 56, 57, 58, 59, 60, 61, 119], "sentencepiec": [54, 62, 81, 89, 111, 159], "pretrain": [54, 62, 73, 81, 89, 111, 112, 154, 155, 158, 160, 161], "spm_model": [54, 62, 81, 89, 111, 155], "tokenized_text": [54, 62, 73, 81, 89, 111, 112], "world": [54, 62, 73, 81, 89, 111, 112, 130, 132, 157], "add_bo": [54, 62, 73, 81, 89, 111, 112, 155], "add_eo": [54, 62, 73, 81, 89, 111, 112, 155], "31587": [54, 62, 73, 81, 89, 111, 112], "29644": [54, 62, 73, 81, 89, 111, 112], "102": [54, 62, 73, 81, 89, 111, 112], "tokenizer_path": [54, 62, 81, 89], "separ": [54, 62, 81, 89, 114, 116, 155, 158, 159, 160, 161], "concat": [54, 62, 81, 89, 114], "1788": [54, 62, 81, 89, 114], "2643": [54, 62, 81, 89, 114], "13": [54, 62, 81, 89, 114, 140, 157, 159, 161], "1792": [54, 62, 81, 89, 114], "9508": [54, 62, 81, 89, 114], "465": [54, 62, 81, 89, 114], "22137": [54, 62, 81, 89, 114], "2933": [54, 62, 81, 89, 114], "join": [54, 62, 81, 89, 114], "attribut": [54, 62, 81, 89, 106, 114, 122], "gemmatransformerdecod": [55, 56, 58, 59, 60, 61], "blog": [55, 56], "technolog": [55, 56], "develop": [55, 56, 161], "gemmatoken": 57, "gemma_2b": 58, "gemma_7b": 59, "lora_gemma_2b": 60, "lora_gemma_7b": 61, "card": [62, 73], "regist": [62, 73, 76, 89, 92, 95, 101, 141, 161], "uniqu": [62, 119], "strongli": 62, "beforehand": 62, "html": [62, 115, 121, 127, 132, 137, 142, 144, 150], "problem": [62, 81], "due": [62, 81, 111, 160, 161], "whitespac": [62, 81, 111], "prepend": [62, 73, 81, 111], "slice": [62, 81], "2307": [63, 64, 65], "09288": [63, 64, 65], "llama2_70b": 68, "llama2_7b": [69, 160], "lora_llama2_13b": 70, "lora_llama2_70b": 71, "lora_llama2_7b": [72, 160], "special_token": [73, 89, 112], "left": [73, 89, 160], "canon": [73, 76, 89, 92], "tt_model": [73, 112], "token_id": [73, 81, 112], "truncate_at_eo": [73, 112], "tokenize_head": 73, "header": [73, 155], "special_tokens_path": [76, 92], "llama3token": [76, 155], "similarli": [76, 92, 145, 156], "llama3_70b": 77, "llama3_8b": [78, 123, 159], "lora_llama3_70b": 79, "lora_llama3_8b": 80, "trim_leading_whitespac": [81, 111], "unbatch": [81, 111], "bo": [81, 92, 111, 114, 155, 156], "append": [81, 89, 111, 151], "eo": [81, 89, 92, 111, 114, 155, 156], "trim": [81, 111], "classifi": [83, 85, 88, 156], "announc": 84, "ray2333": 85, "reward": [85, 103], "feedback": 85, "transformerclassifi": 85, "mistraltoken": [86, 155], "lora_mistral_7b": 87, "lora_mistral_classifier_7b": 88, "phi3": [89, 90, 91, 92, 93, 119, 154], "ignore_system_prompt": 89, "phi3_mini": [90, 119], "ref": [91, 92, 138], "phi": [91, 92, 119], "128k": 91, "nor": 91, "slide": 91, "window": [91, 156], "phi3minitoken": 92, "tokenizer_config": 92, "spm": 92, "lm": 92, "unk": 92, "augment": [92, 161], "endoftext": 92, "phi3minisentencepiecebasetoken": 92, "lora_phi3_mini": 93, "head_dim": [94, 96, 99], "pos_embed": [94, 160], "kv_cach": 94, "kvcach": [94, 99], "attn_dropout": [94, 99], "head": [94, 96, 98, 99, 119, 159], "queri": [94, 96, 99, 100, 159], "gqa": 94, "introduc": [94, 97, 105, 155, 156, 160, 161], "13245v1": 94, "version": [94, 123, 146, 151, 155, 159, 161], "multihead": 94, "mha": [94, 99], "extrem": 94, "share": [94, 156, 157], "mqa": 94, "credit": 94, "document": [94, 115, 126, 134, 154, 156], "lightn": 94, "lit": 94, "lit_gpt": 94, "v": [94, 99, 160], "k": [94, 160], "q": [94, 160], "n_kv_head": 94, "dimens": [94, 96, 98, 99, 105, 159, 160, 161], "calcul": [94, 99, 159], "e": [94, 101, 104, 108, 116, 120, 128, 145, 151, 157, 159, 160, 161], "g": [94, 104, 116, 128, 145, 159, 160, 161], "rotarypositionalembed": [94, 160], "cach": [94, 96, 98, 99, 151, 154], "rope": [94, 98], "dropout": [94, 105, 160, 161], "onto": 94, "scaled_dot_product_attent": 94, "seq_length": [94, 100, 123], "boolean": [94, 99, 100, 115], "softmax": [94, 99, 100], "row": [94, 99, 100, 155], "j": [94, 99, 100], "seq_len": 94, "bigger": 94, "n_h": [94, 98], "num": [94, 98], "n_kv": 94, "kv": [94, 96, 99], "emb": [94, 99], "h_d": [94, 98], "gate_proj": 95, "down_proj": 95, "up_proj": 95, "silu": 95, "feed": [95, 100], "network": [95, 106, 160, 161], "fed": [95, 155], "multipli": 95, "subclass": [95, 121], "although": [95, 160], "afterward": 95, "former": 95, "hook": [95, 101, 141, 161], "latter": 95, "standalon": 96, "past": 96, "becaus": [96, 99, 118, 154, 155, 157, 159], "expand": 96, "dpython": [96, 99, 101], "reset": [96, 99, 128], "k_val": 96, "v_val": 96, "h": [96, 151, 154], "longer": [96, 156], "ep": 97, "1e": 97, "06": [97, 160], "root": [97, 137, 138], "squar": 97, "1910": 97, "07467": 97, "verfic": [97, 98], "small": [97, 157], "divis": 97, "10000": 98, "rotari": [98, 159], "propos": 98, "2104": 98, "09864": 98, "l450": 98, "upto": 98, "init": [98, 128, 138, 161], "exceed": 98, "freq": 98, "recomput": 98, "geometr": 98, "progress": [98, 158], "rotat": 98, "angl": 98, "todo": 98, "effici": [98, 109, 126, 150, 152, 157, 158, 160], "transformerdecoderlay": 99, "norm": [99, 100], "space": 99, "belong": [99, 122], "reduc": [99, 152, 156, 160, 161], "statement": 99, "improv": [99, 112, 126, 157, 159, 160], "readabl": [99, 157], "At": 99, "arang": 99, "prompt_length": 99, "causal_mask": 99, "m_": 99, "seq": 99, "reset_cach": 99, "setup_cach": 99, "attn": [100, 160, 161], "causalselfattent": [100, 160], "sa_norm": 100, "mlp_norm": 100, "ff": 100, "common_util": 101, "bfloat16": [101, 143, 157, 158, 159, 160], "offload_to_cpu": 101, "nf4": [101, 161], "restor": 101, "higher": [101, 159, 161], "offload": [101, 161], "increas": [101, 102, 159, 160], "peak": [101, 128, 133, 157, 159, 160, 161], "gpu": [101, 154, 157, 158, 159, 160, 161], "_register_state_dict_hook": 101, "m": [101, 123, 155], "mymodul": 101, "_after_": 101, "nf4tensor": [101, 161], "unquant": [101, 157, 161], "unus": 101, "num_warmup_step": 102, "num_training_step": 102, "num_cycl": [102, 145], "last_epoch": 102, "lambdalr": 102, "rate": [102, 152, 158], "schedul": [102, 145, 158], "linearli": 102, "decreas": [102, 156, 160, 161], "cosin": 102, "v4": 102, "23": [102, 159], "src": 102, "l104": 102, "warmup": [102, 145], "phase": 102, "wave": 102, "half": 102, "lr_schedul": 102, "beta": 103, "label_smooth": 103, "loss_typ": 103, "sigmoid": 103, "dpo": [103, 106, 140], "18290": 103, "trl": 103, "librari": [103, 121, 125, 127, 144, 150, 152, 154, 156, 161], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 103, "dpo_train": 103, "l844": 103, "temperatur": [103, 123, 157], "uncertainti": 103, "hing": 103, "ipo": 103, "kto_pair": 103, "policy_chosen_logp": 103, "policy_rejected_logp": 103, "reference_chosen_logp": 103, "reference_rejected_logp": 103, "polici": [103, 106, 115, 126, 134, 142], "probabl": [103, 105, 123, 157], "chosen": [103, 145, 156], "reject": [103, 156], "chosen_reward": 103, "rejected_reward": 103, "unknown": 103, "peft": [104, 105, 106, 107, 108, 109, 110, 116, 160, 161], "protocol": 104, "adapter_param": [104, 105, 106, 107, 108], "proj": 104, "in_dim": [104, 105, 160, 161], "out_dim": [104, 105, 160, 161], "bia": [104, 105, 160, 161], "loralinear": [104, 160, 161], "alpha": [105, 160, 161], "use_bia": 105, "perturb": 105, "decomposit": [105, 160], "matric": [105, 134, 160, 161], "trainabl": [105, 108, 134, 160, 161], "mapsto": 105, "w_0x": 105, "r": [105, 160], "bax": 105, "lora_a": [105, 160, 161], "lora_b": [105, 160, 161], "temporarili": 106, "neural": [106, 160, 161], "treat": [106, 121, 155], "caller": 106, "whose": [106, 141], "yield": 106, "get_adapter_param": [108, 160], "base_miss": 109, "base_unexpect": 109, "lora_miss": 109, "lora_unexpect": 109, "validate_state_dict_for_lora": [109, 160], "unlik": [109, 157, 159], "reli": [109, 114], "unexpect": 109, "strict": [109, 160], "pull": [109, 154], "120600": 109, "assertionerror": [109, 110, 140], "nonempti": 109, "full_model_state_dict_kei": 110, "lora_state_dict_kei": 110, "base_model_state_dict_kei": 110, "confirm": [110, 151], "determin": 110, "lora_modul": 110, "complement": 110, "disjoint": 110, "union": [110, 135, 136, 137, 138, 142, 144], "non": 110, "overlap": 110, "light": 111, "sentencepieceprocessor": 111, "addition": [111, 112, 144, 156, 160], "prefix": 111, "bos_id": [112, 114], "lightweight": [112, 155], "break": 112, "substr": 112, "repetit": 112, "speed": [112, 145, 159, 161], "identif": 112, "regex": 112, "chunk": 112, "present": [112, 118], "absent": 112, "tokenizer_json_path": 113, "heavili": 114, "datatyp": [115, 161], "denot": 115, "integ": [115, 139, 144], "auto_wrap_polici": [115, 126, 142], "submodul": [115, 134], "obei": 115, "contract": 115, "get_fsdp_polici": 115, "modules_to_wrap": [115, 126, 134], "min_num_param": 115, "my_fsdp_polici": 115, "recurs": [115, 134, 137], "isinst": [115, 156], "sum": [115, 160], "p": [115, 120, 160, 161], "numel": [115, 160], "1000": 115, "functool": 115, "partial": 115, "stabl": [115, 132, 137, 144, 151], "alia": 115, "safe_seri": 116, "from_pretrain": 116, "0001_of_0003": 116, "0002_of_0003": 116, "preserv": [116, 161], "weight_map": [116, 157], "convert_weight": 116, "_model_typ": [116, 119], "intermediate_checkpoint": [116, 117, 118], "_weight_map": 116, "shard": [117, 159], "wip": 117, "larger": [118, 157, 159], "down": [118, 156, 160, 161], "intermedi": [118, 142, 159, 161], "qualnam": 119, "boundari": 119, "distinguish": 119, "gate": [119, 154, 158], "my_new_model": 119, "my_custom_state_dict_map": 119, "mistral_reward": 119, "classif": 119, "mistral_classifi": 119, "optim_map": 120, "bare": 120, "bone": 120, "distribut": [120, 124, 131, 132, 142, 144, 152, 154, 158, 159], "optim_dict": [120, 122, 141], "cfg_optim": 120, "ckpt": 120, "optim_ckpt": 120, "placeholder_optim_dict": 120, "optiminbackwardwrapp": 120, "get_optim_kei": 120, "arbitrari": [120, 160], "hyperparamet": [120, 152, 158, 160, 161], "optim_ckpt_map": 120, "runtimeerror": [120, 125, 131], "loadabl": 120, "argpars": 121, "argumentpars": 121, "builtin": 121, "said": 121, "noth": 121, "consult": 121, "info": [121, 158], "parse_known_arg": 121, "namespac": 121, "act": 121, "precid": 121, "parse_arg": 121, "properti": [121, 160], "too": [121, 159], "optimizerinbackwardwrapp": 122, "top": [122, 157, 161], "named_paramet": 122, "max_generated_token": 123, "pad_id": 123, "top_k": [123, 157], "stop_token": 123, "custom_generate_next_token": 123, "condit": [123, 132, 154, 156], "bsz": 123, "predict": 123, "prune": [123, 161], "stop": 123, "compil": [123, 157, 159, 161], "generate_next_token": 123, "llama3_token": [123, 155, 159], "hi": [123, 155], "my": [123, 154, 155, 156, 157, 159], "jeremi": 123, "float32": 125, "bf16": [125, 161], "inde": [125, 157], "kernel": 125, "isn": [125, 154], "hardwar": [125, 152, 156, 157, 160], "memory_efficient_fsdp_wrap": 126, "maxim": [126, 134, 150, 152], "been": [126, 159], "workload": 126, "15": [126, 140, 155, 157, 160, 161], "alongsid": 126, "ac": 126, "fullyshardeddataparallel": [126, 134], "fsdppolicytyp": [126, 134], "handler": 127, "reset_stat": 128, "track": 128, "alloc": [128, 133, 134, 159, 161], "reserv": [128, 133, 155, 161], "stat": [128, 133, 161], "int4": 129, "4w": [129, 157, 159], "recogn": 129, "mode": [129, 157], "aka": 130, "master": 132, "port": [132, 154], "address": 132, "hold": [132, 158], "peak_memory_act": 133, "peak_memory_alloc": 133, "peak_memory_reserv": 133, "get_memory_stat": 133, "own": [134, 144, 154, 155, 156, 157, 160], "unit": [134, 152], "hierarch": 134, "requires_grad": [134, 160, 161], "filenam": 135, "log_": 135, "unixtimestamp": 135, "txt": [135, 156, 158], "thread": 135, "safe": 135, "flush": [135, 136, 137, 138], "ndarrai": [135, 136, 137, 138], "scalar": [135, 136, 137, 138], "record": [135, 136, 137, 138, 145], "payload": [135, 136, 137, 138], "organize_log": 137, "tensorboard": 137, "subdirectori": 137, "compar": [137, 146, 157, 160, 161], "logdir": 137, "startup": 137, "tree": [137, 156, 157], "tfevent": 137, "encount": 137, "frontend": 137, "organ": [137, 154], "accordingli": 137, "my_log_dir": 137, "view": [137, 157, 158], "my_metr": [137, 138], "termin": [137, 138], "entiti": 138, "bias": 138, "sent": 138, "usernam": 138, "my_project": 138, "my_ent": 138, "my_group": 138, "importerror": 138, "account": [138, 160, 161], "log_config": 138, "link": [138, 157], "capecap": 138, "6053ofw0": 138, "torchtune_config_j67sb73v": 138, "ignore_idx": [139, 140], "longest": 139, "token_pair": 139, "input_id": 140, "chosen_input_id": [140, 156], "chosen_label": [140, 156], "rejected_input_id": [140, 156], "rejected_label": [140, 156], "14": [140, 161], "17": [140, 157, 160], "18": [140, 159], "19": [140, 157, 159, 161], "20": 140, "soon": 141, "readi": [141, 150, 155], "grad": 141, "achiev": [141, 157, 159, 160, 161], "acwrappolicytyp": 142, "author": [142, 152, 158, 161], "fsdp_adavnced_tutori": 142, "insid": 143, "contextmanag": 143, "debug_mod": 144, "pseudo": 144, "random": [144, 158], "commonli": [144, 157, 160, 161], "numpi": 144, "determinist": 144, "global": [144, 156], "warn": 144, "nondeterminist": 144, "cudnn": 144, "set_deterministic_debug_mod": 144, "algorithm": 144, "profile_memori": 145, "with_stack": 145, "record_shap": 145, "with_flop": 145, "wait_step": 145, "warmup_step": 145, "active_step": 145, "profil": 145, "layout": 145, "trace": 145, "profileract": 145, "flop": 145, "wait": 145, "cycl": 145, "repeat": 145, "reduct": [145, 160], "iter": [145, 147, 161], "scope": 145, "gradient_accumul": 145, "sensibl": 145, "default_schedul": 145, "greater": 146, "equal": 146, "against": [146, 161], "__version__": 146, "named_param": 147, "generated_examples_python": 148, "zip": 148, "galleri": [148, 153], "sphinx": 148, "000": [149, 153, 159], "execut": [149, 153], "generated_exampl": 149, "mem": [149, 153], "mb": [149, 153], "topic": 150, "gentl": 150, "introduct": 150, "first_finetune_tutori": 150, "workflow": [150, 156, 158, 160], "requisit": 151, "proper": [151, 158], "host": [151, 154, 158], "latest": [151, 158, 161], "And": [151, 157, 159], "ls": [151, 154, 157, 158, 159], "welcom": [151, 154], "show": [151, 154, 155, 160], "greatest": [151, 158], "contributor": 151, "cd": [151, 157], "even": [151, 154, 155, 156, 159, 160, 161], "commit": 151, "branch": 151, "url": 151, "whl": 151, "therebi": [151, 161], "forc": 151, "reinstal": 151, "opt": [151, 158], "suffix": 151, "cu121": 151, "On": [152, 160], "pointer": 152, "emphas": 152, "aspect": 152, "simplic": 152, "component": 152, "reus": 152, "prove": 152, "democrat": 152, "box": [152, 161], "zoo": 152, "varieti": [152, 160], "techniqu": [152, 157, 158, 160], "integr": [152, 157, 158, 159, 160, 161], "excit": 152, "checkout": 152, "quickstart": 152, "attain": 152, "better": [152, 155, 156, 157], "chekckpoint": 152, "embodi": 152, "philosophi": 152, "usabl": 152, "composit": 152, "hard": [152, 156], "outlin": 152, "unecessari": 152, "never": 152, "thoroughli": 152, "short": 154, "subcommand": 154, "anytim": 154, "symlink": 154, "auto": 154, "wrote": 154, "readm": 154, "md": 154, "lot": [154, 157], "recent": 154, "releas": [154, 159], "agre": 154, "term": 154, "perman": 154, "eat": 154, "bandwith": 154, "storag": [154, 161], "00030": 154, "ootb": 154, "full_finetune_single_devic": [154, 156, 157, 158], "7b_full_low_memori": [154, 157, 158], "8b_full_single_devic": [154, 156], "mini_full_low_memori": 154, "7b_full": [154, 157, 158], "13b_full": [154, 157, 158], "70b_full": 154, "edit": 154, "destin": 154, "lora_finetune_distribut": [154, 159, 160], "torchrun": 154, "8b_lora_single_devic": [154, 155, 159], "launch": [154, 155, 158], "nproc": 154, "node": 154, "worker": 154, "nnode": [154, 160], "minimum_nod": 154, "maximum_nod": 154, "fail": 154, "rdzv": 154, "rendezv": 154, "endpoint": 154, "8b_lora": [154, 159], "bypass": 154, "vice": 154, "versa": 154, "fancy_lora": 154, "8b_fancy_lora": 154, "sai": [154, 155, 158], "align": 155, "intend": 155, "nice": 155, "meet": 155, "overhaul": 155, "begin_of_text": 155, "start_header_id": 155, "end_header_id": 155, "eot_id": 155, "accompani": 155, "who": 155, "influenti": 155, "hip": 155, "hop": 155, "artist": [155, 159], "2pac": 155, "rakim": 155, "c": 155, "na": 155, "flavor": [155, 156], "msg": 155, "formatted_messag": [155, 156], "nyou": [155, 156], "nwho": 155, "why": [155, 158, 160], "user_messag": 155, "518": 155, "25580": 155, "29962": 155, "3532": 155, "14816": 155, "29903": 155, "6778": 155, "piece_to_id": 155, "vector": 155, "place": 155, "manual": [155, 161], "529": 155, "29879": 155, "29958": 155, "nhere": 155, "_encode_special_token": 155, "128000": 155, "128009": 155, "pure": 155, "That": 155, "won": [155, 157, 159], "mess": 155, "govern": 155, "prime": 155, "strictli": 155, "summarizetempl": [155, 156], "ask": 155, "untouch": 155, "nsummari": 155, "robust": 155, "csv": [155, 156], "onlin": 155, "forum": 155, "panda": 155, "pd": 155, "df": 155, "read_csv": 155, "your_fil": 155, "nrow": 155, "tolist": 155, "iloc": 155, "gp": 155, "receiv": 155, "commun": [155, 156, 157], "satellit": 155, "thing": [155, 161], "message_convert": 155, "input_msg": 155, "output_msg": 155, "assistant_messag": 155, "But": [155, 157, 159, 160], "mistralchatformat": 155, "custom_dataset": 155, "2048": 155, "data_fil": [155, 156], "honor": 155, "copi": [155, 157, 158, 159, 161], "custom_8b_lora_single_devic": 155, "steer": 156, "wheel": 156, "publicli": 156, "great": [156, 157], "hood": [156, 157, 161], "text_completion_dataset": 156, "padded_col": 156, "upper": 156, "constraint": [156, 160], "slow": [156, 161], "signific": 156, "speedup": [156, 159], "minim": [156, 158, 160, 161], "my_data": 156, "instruct_dataset": 156, "fix": 156, "goal": 156, "agnost": 156, "respond": 156, "anim": 156, "plant": 156, "miner": 156, "oak": 156, "copper": 156, "ore": 156, "eleph": 156, "customtempl": 156, "cl": 156, "chat_dataset": 156, "quit": [156, 161], "incorpor": 156, "advanc": 156, "customchatformat": 156, "vicgal": 156, "gpt4": 156, "drive": 156, "rajpurkar": 156, "io": 156, "squad": 156, "explor": 156, "rlhf": 156, "few": [156, 159, 160, 161], "adjust": 156, "chosen_messag": 156, "transformed_sampl": 156, "key_chosen": 156, "rejected_messag": 156, "key_reject": 156, "c_mask": 156, "np": 156, "cross_entropy_ignore_idx": 156, "r_mask": 156, "stack_exchanged_paired_dataset": 156, "had": 156, "stackexchangedpairedtempl": 156, "response_j": 156, "response_k": 156, "rl": 156, "favorit": [157, 159, 160], "seemlessli": 157, "beyond": [157, 161], "connect": 157, "amount": 157, "natur": 157, "export": 157, "mobil": 157, "phone": 157, "leverag": [157, 159, 161], "plai": 157, "freez": [157, 160], "percentag": 157, "learnabl": 157, "keep": [157, 160], "16gb": [157, 160], "rtx": 157, "3090": 157, "4090": 157, "hour": 157, "7b_qlora_single_devic": [157, 158, 161], "473": 157, "98": [157, 161], "gb": [157, 159, 160, 161], "50": 157, "484": 157, "01": [157, 158], "fact": [157, 159, 160], "third": 157, "realli": 157, "eleuther_ev": [157, 159], "eleuther_evalu": [157, 159], "lm_eval": [157, 159], "plan": 157, "custom_eval_config": [157, 159], "truthfulqa_mc2": [157, 159, 160], "measur": [157, 159], "propens": [157, 159], "shot": [157, 159], "accuraci": [157, 159, 160, 161], "baselin": [157, 160], "324": 157, "loglikelihood": 157, "195": 157, "121": 157, "27": 157, "second": [157, 159, 160, 161], "197": 157, "acc": 157, "388": 157, "38": 157, "shown": 157, "489": 157, "48": [157, 161], "seem": 157, "custom_generation_config": [157, 159], "kick": 157, "300": 157, "interest": 157, "site": 157, "visit": 157, "bai": 157, "area": 157, "92": [157, 159], "exploratorium": 157, "san": 157, "francisco": 157, "magazin": 157, "awesom": 157, "bridg": 157, "pretti": 157, "cool": 157, "96": [157, 161], "61": 157, "sec": [157, 159], "25": 157, "83": 157, "99": [157, 160], "72": 157, "littl": 157, "saw": 157, "took": [157, 159], "torchao": [157, 159, 161], "bit": [157, 159, 160, 161], "custom_quantization_config": [157, 159], "68": 157, "76": 157, "69": 157, "95": [157, 159], "67": 157, "engin": [157, 159], "fullmodeltorchtunecheckpoint": [157, 159], "int4weightonlyquant": [157, 159], "groupsiz": [157, 159], "256": [157, 159], "park": 157, "sit": 157, "hill": 157, "beauti": 157, "62": [157, 159], "85": 157, "sped": 157, "almost": [157, 159, 160], "3x": [157, 159], "benefit": 157, "doesn": 157, "yet": 157, "fast": 157, "clone": [157, 160, 161], "assumpt": 157, "satisfi": 157, "new_dir": 157, "output_dict": 157, "sd_1": 157, "sd_2": 157, "dump": 157, "convert_hf_checkpoint": 157, "checkpoint_path": 157, "justin": 157, "school": 157, "math": 157, "teacher": 157, "ws": 157, "94": [157, 159], "28": 157, "bandwidth": [157, 159], "1391": 157, "84": 157, "thats": 157, "seamlessli": 157, "authent": [157, 158], "hopefulli": 157, "gave": 157, "grant": 158, "minut": 158, "agreement": 158, "altern": 158, "hackabl": 158, "singularli": 158, "technic": 158, "purpos": [158, 159], "depth": 158, "principl": 158, "boilerpl": 158, "substanti": [158, 160], "custom_config": 158, "replic": 158, "lorafinetunerecipesingledevic": 158, "lora_finetune_output": 158, "log_1713194212": 158, "52": 158, "3697006702423096": 158, "25880": [158, 161], "24": [158, 159], "55": 158, "83it": 158, "monitor": 158, "tqdm": 158, "interv": 158, "e2": 158, "focu": 159, "128": [159, 160], "theta": 159, "gain": 159, "illustr": 159, "basic": 159, "observ": 159, "consum": [159, 161], "vram": [159, 160], "overal": 159, "8b_qlora_single_devic": 159, "coupl": [159, 160, 161], "meta_model_0": 159, "122": 159, "sarah": 159, "busi": 159, "mum": 159, "young": 159, "children": 159, "live": 159, "north": 159, "east": 159, "england": 159, "135": 159, "88": 159, "138": 159, "346": 159, "09": 159, "139": 159, "31": 159, "far": 159, "drill": 159, "90": 159, "93": 159, "91": 159, "104": 159, "four": [159, 160], "again": 159, "jake": 159, "disciplin": 159, "passion": 159, "draw": 159, "paint": 159, "57": [159, 160, 161], "broader": 159, "teach": 160, "straight": 160, "unfamiliar": 160, "oppos": [160, 161], "momentum": 160, "relat": 160, "aghajanyan": 160, "et": 160, "al": 160, "hypothes": 160, "intrins": 160, "often": 160, "eight": 160, "practic": 160, "imag": 160, "blue": 160, "rememb": 160, "approx": 160, "15m": 160, "8192": 160, "65k": 160, "frozen_out": [160, 161], "lora_out": [160, 161], "base_model": 160, "choos": 160, "lora_model": 160, "lora_llama_2_7b": [160, 161], "alon": 160, "in_featur": 160, "out_featur": 160, "inplac": 160, "feel": 160, "free": 160, "whenev": 160, "peft_util": 160, "set_trainable_param": 160, "fetch": 160, "lora_param": 160, "total_param": 160, "trainable_param": 160, "2f": 160, "6742609920": 160, "4194304": 160, "7b_lora": 160, "my_model_checkpoint_path": [160, 161], "tokenizer_checkpoint": [160, 161], "my_tokenizer_checkpoint_path": [160, 161], "factori": 160, "benefici": 160, "impact": 160, "minor": 160, "good": 160, "64": 160, "lora_experiment_1": 160, "smooth": [160, 161], "curv": [160, 161], "500": 160, "ran": 160, "footprint": 160, "commod": 160, "cogniz": 160, "ax": 160, "parallel": 160, "truthfulqa": 160, "previous": 160, "475": 160, "87": 160, "508": 160, "86": 160, "504": 160, "04": 160, "514": 160, "lowest": 160, "absolut": 160, "4gb": 160, "tradeoff": 160, "potenti": 160, "highli": 161, "vanilla": 161, "held": 161, "therefor": 161, "bespok": 161, "normalfloat": 161, "8x": 161, "retain": 161, "vast": 161, "major": 161, "degrad": 161, "normatfloat": 161, "doubl": 161, "themselv": 161, "deepdiv": 161, "idea": 161, "distinct": 161, "de": 161, "incur": 161, "counterpart": 161, "set_default_devic": 161, "qlora_linear": 161, "memory_alloc": 161, "177": 161, "152": 161, "del": 161, "empty_cach": 161, "lora_linear": 161, "081": 161, "344": 161, "qlora_llama2_7b": 161, "qlora_model": 161, "essenti": 161, "reparametrize_as_dtype_state_dict_post_hook": 161, "35": 161, "40": 161, "29": 161, "slower": 161, "149": 161, "9157477021217346": 161, "02": 161, "08": 161, "15it": 161, "nightli": 161, "200": 161, "hundr": 161, "228": 161, "8158286809921265": 161, "59": 161, "95it": 161, "exercis": 161, "portion": 161, "linear_nf4": 161, "to_nf4": 161, "linear_weight": 161, "autograd": 161, "regular": 161, "incom": 161}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "StackExchangedPairedTemplate"], [23, 1, 1, "", "SummarizeTemplate"], [24, 0, 1, "", "get_openai_messages"], [25, 0, 1, "", "get_sharegpt_messages"], [26, 0, 1, "", "truncate"], [27, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.StackExchangedPairedTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[23, 2, 1, "", "format"]], "torchtune.datasets": [[28, 1, 1, "", "ChatDataset"], [29, 1, 1, "", "ConcatDataset"], [30, 1, 1, "", "InstructDataset"], [31, 1, 1, "", "PackedDataset"], [32, 1, 1, "", "PreferenceDataset"], [33, 1, 1, "", "TextCompletionDataset"], [34, 0, 1, "", "alpaca_cleaned_dataset"], [35, 0, 1, "", "alpaca_dataset"], [36, 0, 1, "", "chat_dataset"], [37, 0, 1, "", "cnn_dailymail_articles_dataset"], [38, 0, 1, "", "grammar_dataset"], [39, 0, 1, "", "instruct_dataset"], [40, 0, 1, "", "samsum_dataset"], [41, 0, 1, "", "slimorca_dataset"], [42, 0, 1, "", "stack_exchanged_paired_dataset"], [43, 0, 1, "", "text_completion_dataset"], [44, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[45, 0, 1, "", "code_llama2_13b"], [46, 0, 1, "", "code_llama2_70b"], [47, 0, 1, "", "code_llama2_7b"], [48, 0, 1, "", "lora_code_llama2_13b"], [49, 0, 1, "", "lora_code_llama2_70b"], [50, 0, 1, "", "lora_code_llama2_7b"], [51, 0, 1, "", "qlora_code_llama2_13b"], [52, 0, 1, "", "qlora_code_llama2_70b"], [53, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[54, 1, 1, "", "GemmaTokenizer"], [55, 0, 1, "", "gemma_2b"], [56, 0, 1, "", "gemma_7b"], [57, 0, 1, "", "gemma_tokenizer"], [58, 0, 1, "", "lora_gemma_2b"], [59, 0, 1, "", "lora_gemma_7b"], [60, 0, 1, "", "qlora_gemma_2b"], [61, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[54, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[62, 1, 1, "", "Llama2Tokenizer"], [63, 0, 1, "", "llama2_13b"], [64, 0, 1, "", "llama2_70b"], [65, 0, 1, "", "llama2_7b"], [66, 0, 1, "", "llama2_tokenizer"], [67, 0, 1, "", "lora_llama2_13b"], [68, 0, 1, "", "lora_llama2_70b"], [69, 0, 1, "", "lora_llama2_7b"], [70, 0, 1, "", "qlora_llama2_13b"], [71, 0, 1, "", "qlora_llama2_70b"], [72, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[62, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[73, 1, 1, "", "Llama3Tokenizer"], [74, 0, 1, "", "llama3_70b"], [75, 0, 1, "", "llama3_8b"], [76, 0, 1, "", "llama3_tokenizer"], [77, 0, 1, "", "lora_llama3_70b"], [78, 0, 1, "", "lora_llama3_8b"], [79, 0, 1, "", "qlora_llama3_70b"], [80, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[73, 2, 1, "", "decode"], [73, 2, 1, "", "tokenize_message"], [73, 2, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[81, 1, 1, "", "MistralTokenizer"], [82, 0, 1, "", "lora_mistral_7b"], [83, 0, 1, "", "lora_mistral_classifier_7b"], [84, 0, 1, "", "mistral_7b"], [85, 0, 1, "", "mistral_classifier_7b"], [86, 0, 1, "", "mistral_tokenizer"], [87, 0, 1, "", "qlora_mistral_7b"], [88, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.mistral.MistralTokenizer": [[81, 2, 1, "", "decode"], [81, 2, 1, "", "encode"], [81, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[89, 1, 1, "", "Phi3MiniTokenizer"], [90, 0, 1, "", "lora_phi3_mini"], [91, 0, 1, "", "phi3_mini"], [92, 0, 1, "", "phi3_mini_tokenizer"], [93, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[89, 2, 1, "", "decode"], [89, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[94, 1, 1, "", "CausalSelfAttention"], [95, 1, 1, "", "FeedForward"], [96, 1, 1, "", "KVCache"], [97, 1, 1, "", "RMSNorm"], [98, 1, 1, "", "RotaryPositionalEmbeddings"], [99, 1, 1, "", "TransformerDecoder"], [100, 1, 1, "", "TransformerDecoderLayer"], [102, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[94, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[95, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[96, 2, 1, "", "reset"], [96, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[97, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[98, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[99, 2, 1, "", "forward"], [99, 2, 1, "", "reset_caches"], [99, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[100, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[101, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[103, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[103, 2, 1, "", "forward"]], "torchtune.modules.peft": [[104, 1, 1, "", "AdapterModule"], [105, 1, 1, "", "LoRALinear"], [106, 0, 1, "", "disable_adapter"], [107, 0, 1, "", "get_adapter_params"], [108, 0, 1, "", "set_trainable_params"], [109, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [110, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[104, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[105, 2, 1, "", "adapter_params"], [105, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[111, 1, 1, "", "SentencePieceBaseTokenizer"], [112, 1, 1, "", "TikTokenBaseTokenizer"], [113, 0, 1, "", "parse_hf_tokenizer_json"], [114, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[111, 2, 1, "", "decode"], [111, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[112, 2, 1, "", "decode"], [112, 2, 1, "", "encode"]], "torchtune.utils": [[115, 4, 1, "", "FSDPPolicyType"], [116, 1, 1, "", "FullModelHFCheckpointer"], [117, 1, 1, "", "FullModelMetaCheckpointer"], [118, 1, 1, "", "FullModelTorchTuneCheckpointer"], [119, 1, 1, "", "ModelType"], [120, 1, 1, "", "OptimizerInBackwardWrapper"], [121, 1, 1, "", "TuneRecipeArgumentParser"], [122, 0, 1, "", "create_optim_in_bwd_wrapper"], [123, 0, 1, "", "generate"], [124, 0, 1, "", "get_device"], [125, 0, 1, "", "get_dtype"], [126, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [127, 0, 1, "", "get_logger"], [128, 0, 1, "", "get_memory_stats"], [129, 0, 1, "", "get_quantizer_mode"], [130, 0, 1, "", "get_world_size_and_rank"], [131, 0, 1, "", "init_distributed"], [132, 0, 1, "", "is_distributed"], [133, 0, 1, "", "log_memory_stats"], [134, 0, 1, "", "lora_fsdp_wrap_policy"], [139, 0, 1, "", "padded_collate"], [140, 0, 1, "", "padded_collate_dpo"], [141, 0, 1, "", "register_optim_in_bwd_hooks"], [142, 0, 1, "", "set_activation_checkpointing"], [143, 0, 1, "", "set_default_dtype"], [144, 0, 1, "", "set_seed"], [145, 0, 1, "", "setup_torch_profiler"], [146, 0, 1, "", "torch_version_ge"], [147, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[116, 2, 1, "", "load_checkpoint"], [116, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[117, 2, 1, "", "load_checkpoint"], [117, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[118, 2, 1, "", "load_checkpoint"], [118, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[119, 3, 1, "", "GEMMA"], [119, 3, 1, "", "LLAMA2"], [119, 3, 1, "", "LLAMA3"], [119, 3, 1, "", "MISTRAL"], [119, 3, 1, "", "MISTRAL_REWARD"], [119, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[120, 2, 1, "", "get_optim_key"], [120, 2, 1, "", "load_state_dict"], [120, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[121, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[135, 1, 1, "", "DiskLogger"], [136, 1, 1, "", "StdoutLogger"], [137, 1, 1, "", "TensorBoardLogger"], [138, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[135, 2, 1, "", "close"], [135, 2, 1, "", "log"], [135, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[136, 2, 1, "", "close"], [136, 2, 1, "", "log"], [136, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[137, 2, 1, "", "close"], [137, 2, 1, "", "log"], [137, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[138, 2, 1, "", "close"], [138, 2, 1, "", "log"], [138, 2, 1, "", "log_config"], [138, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 115, 150, 152, 154, 157, 159, 160, 161], "config": [0, 7, 8, 154, 158], "data": [1, 5, 155], "text": [1, 156, 159], "templat": [1, 155, 156], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 155, 156], "exampl": 2, "gener": [2, 123, 157, 159], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 154, 157, 158, 159, 160], "llama3": [3, 155, 159], "llama2": [3, 155, 157, 160, 161], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 151, 161], "block": 4, "base": 4, "token": [4, 155], "util": [4, 5, 115], "peft": 4, "loss": 4, "checkpoint": [5, 6, 9, 157], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 156, 160, 161], "manag": 5, "perform": [5, 160], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 152, 157], "format": [6, 156], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 157, 160, 161], "put": [6, 161], "thi": 6, "all": [6, 7, 161], "togeth": [6, 161], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 156], "us": [7, 8, 155, 157, 161], "instanti": [7, 10], "referenc": 7, "other": [7, 157], "field": 7, "interpol": 7, "valid": [7, 13, 154], "your": [7, 8, 157, 158], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 152, 160, 161], "ar": 8, "recip": [8, 154, 158, 160], "script": 8, "run": [8, 154, 157], "cli": [8, 154], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "stackexchangedpairedtempl": 22, "summarizetempl": 23, "get_openai_messag": 24, "get_sharegpt_messag": 25, "truncat": 26, "validate_messag": 27, "chatdataset": 28, "concatdataset": 29, "instructdataset": 30, "packeddataset": 31, "preferencedataset": 32, "textcompletiondataset": 33, "alpaca_cleaned_dataset": 34, "alpaca_dataset": 35, "chat_dataset": 36, "cnn_dailymail_articles_dataset": 37, "grammar_dataset": 38, "instruct_dataset": 39, "samsum_dataset": 40, "slimorca_dataset": 41, "stack_exchanged_paired_dataset": 42, "text_completion_dataset": 43, "wikitext_dataset": 44, "code_llama2_13b": 45, "code_llama2_70b": 46, "code_llama2_7b": 47, "lora_code_llama2_13b": 48, "lora_code_llama2_70b": 49, "lora_code_llama2_7b": 50, "qlora_code_llama2_13b": 51, "qlora_code_llama2_70b": 52, "qlora_code_llama2_7b": 53, "gemmatoken": 54, "gemma_2b": 55, "gemma_7b": 56, "gemma_token": 57, "lora_gemma_2b": 58, "lora_gemma_7b": 59, "qlora_gemma_2b": 60, "qlora_gemma_7b": 61, "llama2token": 62, "llama2_13b": 63, "llama2_70b": 64, "llama2_7b": 65, "llama2_token": 66, "lora_llama2_13b": 67, "lora_llama2_70b": 68, "lora_llama2_7b": 69, "qlora_llama2_13b": 70, "qlora_llama2_70b": 71, "qlora_llama2_7b": 72, "llama3token": 73, "llama3_70b": 74, "llama3_8b": 75, "llama3_token": 76, "lora_llama3_70b": 77, "lora_llama3_8b": 78, "qlora_llama3_70b": 79, "qlora_llama3_8b": 80, "mistraltoken": 81, "lora_mistral_7b": 82, "lora_mistral_classifier_7b": 83, "mistral_7b": 84, "mistral_classifier_7b": 85, "mistral_token": 86, "qlora_mistral_7b": 87, "qlora_mistral_classifier_7b": 88, "phi3minitoken": 89, "lora_phi3_mini": 90, "phi3_mini": 91, "phi3_mini_token": 92, "qlora_phi3_mini": 93, "causalselfattent": 94, "todo": [94, 100], "feedforward": 95, "kvcach": 96, "rmsnorm": 97, "rotarypositionalembed": 98, "transformerdecod": 99, "transformerdecoderlay": 100, "reparametrize_as_dtype_state_dict_post_hook": 101, "get_cosine_schedule_with_warmup": 102, "dpoloss": 103, "adaptermodul": 104, "loralinear": 105, "disable_adapt": 106, "get_adapter_param": 107, "set_trainable_param": 108, "validate_missing_and_unexpected_for_lora": 109, "validate_state_dict_for_lora": 110, "sentencepiecebasetoken": 111, "tiktokenbasetoken": 112, "parse_hf_tokenizer_json": 113, "tokenize_messages_no_special_token": 114, "fsdppolicytyp": 115, "fullmodelhfcheckpoint": 116, "fullmodelmetacheckpoint": 117, "fullmodeltorchtunecheckpoint": 118, "modeltyp": 119, "optimizerinbackwardwrapp": 120, "tunerecipeargumentpars": 121, "create_optim_in_bwd_wrapp": 122, "get_devic": 124, "get_dtyp": 125, "get_full_finetune_fsdp_wrap_polici": 126, "get_logg": 127, "get_memory_stat": 128, "get_quantizer_mod": 129, "get_world_size_and_rank": 130, "init_distribut": 131, "is_distribut": 132, "log_memory_stat": 133, "lora_fsdp_wrap_polici": 134, "disklogg": 135, "stdoutlogg": 136, "tensorboardlogg": 137, "wandblogg": 138, "padded_col": 139, "padded_collate_dpo": 140, "register_optim_in_bwd_hook": 141, "set_activation_checkpoint": 142, "set_default_dtyp": 143, "set_se": 144, "setup_torch_profil": 145, "torch_version_g": 146, "validate_expected_param_dtyp": 147, "comput": [149, 153], "time": [149, 153], "welcom": 150, "document": 150, "get": [150, 154, 159], "start": [150, 154], "tutori": 150, "instal": 151, "instruct": [151, 156, 159], "via": [151, 159], "pypi": 151, "git": 151, "clone": 151, "nightli": 151, "kei": 152, "concept": 152, "design": 152, "principl": 152, "download": [154, 157, 158], "list": 154, "built": [154, 156], "copi": 154, "fine": [155, 156, 158, 159], "tune": [155, 156, 158, 159], "chat": [155, 156], "chang": 155, "from": [155, 161], "prompt": 155, "special": 155, "when": 155, "should": 155, "i": 155, "custom": [155, 156], "hug": [156, 157], "face": [156, 157], "set": 156, "max": 156, "sequenc": 156, "length": 156, "sampl": 156, "pack": 156, "unstructur": 156, "corpu": 156, "multipl": 156, "local": 156, "remot": 156, "fulli": 156, "end": 157, "workflow": 157, "7b": 157, "finetun": [157, 160, 161], "evalu": [157, 159], "eleutherai": [157, 159], "s": [157, 159], "eval": [157, 159], "har": [157, 159], "speed": 157, "up": 157, "quantiz": [157, 159], "librari": 157, "upload": 157, "hub": 157, "first": 158, "llm": 158, "select": 158, "modifi": 158, "train": 158, "next": 158, "step": 158, "meta": 159, "8b": 159, "access": 159, "our": 159, "faster": 159, "how": 160, "doe": 160, "work": 160, "appli": 160, "trade": 160, "off": 160, "qlora": 161, "save": 161, "deep": 161, "dive": 161}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})