Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "MistralChatFormat", "SummarizeTemplate", "validate_messages", "alpaca_cleaned_dataset", "alpaca_dataset", "grammar_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"all": [3, 4, 8, 12, 44, 45, 49, 51, 58, 59, 61, 76, 78, 80, 81, 83, 84, 85, 86], "from": [3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 42, 45, 49, 50, 52, 53, 55, 57, 59, 60, 61, 70, 71, 77, 79, 81, 82, 83, 84, 85, 86], "famili": [3, 8, 22, 23, 26, 80, 85], "tune": [3, 6, 7, 8, 9, 11, 78, 79, 80, 83, 86, 87], "download": [3, 6, 76, 79, 85, 86, 87], "meta": [3, 6, 18, 59, 60, 83, 84, 85], "llama": [3, 6, 18, 47, 48, 59, 60, 83, 84, 85, 86], "3": [3, 6, 58, 61, 64, 72, 83, 84, 85, 87], "8b": [3, 37, 38, 39], "hf": [3, 6, 59, 83, 84, 85], "token": [3, 6, 7, 8, 22, 23, 24, 25, 26, 44, 48, 49, 50, 57, 58, 82, 83, 84, 85, 86, 87], "access_token": 3, "pre": [3, 18, 79], "train": [3, 5, 6, 8, 9, 18, 22, 23, 24, 25, 26, 44, 51, 52, 59, 60, 63, 73, 78, 80, 82, 83, 85, 86, 87], "can": [3, 4, 6, 7, 8, 9, 10, 12, 22, 23, 47, 48, 57, 59, 61, 70, 71, 78, 79, 80, 82, 83, 84, 85, 86, 87], "hug": [3, 6, 22, 23, 24, 25, 26, 52, 80, 82, 84, 85], "face": [3, 6, 22, 23, 24, 25, 26, 52, 80, 82, 84, 85], "hub": [3, 6, 84], "follow": [3, 6, 8, 44, 52, 71, 78, 79, 82, 83, 84, 85, 86, 87], "command": [3, 8, 9, 61, 79, 83, 84, 85, 86, 87], "2": [3, 6, 9, 21, 26, 44, 57, 59, 60, 72, 75, 83, 84, 85, 86], "7b": [3, 6, 22, 23, 30, 33, 41, 42, 59, 60, 84, 85, 86, 87], "ai": [3, 42, 44, 71, 85], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 26, 44, 49, 52, 57, 58, 60, 70, 71, 72, 75, 83, 84, 85, 86, 87], "googl": [3, 27], "2b": [3, 27], "ignor": [3, 6, 44, 45], "pattern": [3, 58], "These": [4, 6, 7, 8, 10, 61, 82, 83, 84, 85, 86, 87], "ar": [4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 49, 54, 59, 60, 63, 79, 80, 82, 83, 84, 85, 86, 87], "common": [4, 7, 82, 85, 86], "us": [4, 6, 9, 10, 11, 15, 18, 22, 23, 24, 25, 26, 44, 45, 47, 48, 49, 50, 51, 57, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 75, 78, 79, 80, 82, 84, 85, 86], "offer": 5, "allow": [5, 70, 87], "seamless": 5, "transit": 5, "between": [5, 6, 59, 83, 85, 86, 87], "format": [5, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 26, 59, 60, 83, 84, 85, 86], "interoper": [5, 6, 8, 80, 83, 87], "rest": [5, 87], "ecosystem": [5, 6, 8, 80, 83, 85, 87], "For": [5, 6, 7, 8, 22, 23, 44, 49, 61, 71, 75, 79, 82, 83, 84, 85, 86, 87], "comprehens": 5, "overview": [5, 7, 9, 84, 86, 87], "pleas": [5, 35, 40, 43, 79, 87], "see": [5, 6, 9, 18, 19, 26, 35, 40, 43, 46, 53, 61, 64, 71, 73, 75, 79, 80, 82, 83, 84, 85, 86, 87], "deep": [5, 6, 7, 8, 9, 80, 84, 85], "dive": [5, 6, 7, 8, 9, 80, 84, 85], "enabl": [5, 7, 8, 9, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 54, 73, 75, 85, 86, 87], "work": [5, 6, 8, 61, 80, 83, 85, 87], "set": [5, 6, 7, 8, 9, 22, 23, 24, 25, 26, 48, 49, 56, 62, 74, 75, 80, 82, 83, 84, 85, 86], "consumpt": 5, "dure": [5, 6, 22, 23, 24, 25, 44, 46, 48, 49, 50, 51, 83, 85, 86, 87], "provid": [5, 6, 7, 8, 10, 15, 19, 26, 49, 61, 71, 80, 82, 83, 84, 85], "debug": [5, 6, 7, 8], "your": [5, 9, 10, 70, 71, 78, 79, 80, 82, 85, 86, 87], "finetun": [5, 6, 7, 8, 31, 32, 33, 34, 38, 39, 67, 78, 80, 84, 85], "job": [5, 9, 75, 84], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 20, 22, 23, 24, 25, 26, 80, 84, 85], "walk": [6, 8, 70, 80, 82, 83, 84, 87], "you": [6, 7, 8, 9, 10, 17, 18, 22, 23, 61, 70, 71, 78, 79, 80, 82, 83, 84, 85, 86, 87], "through": [6, 7, 8, 9, 45, 80, 82, 83, 84, 87], "design": [6, 8], "behavior": [6, 82], "associ": [6, 7, 8, 83, 86], "util": [6, 7, 8, 9, 10, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 80, 83, 84, 85, 87], "what": [6, 7, 9, 18, 19, 24, 25, 78, 82, 83, 84, 85], "cover": [6, 7, 8, 9, 83, 87], "how": [6, 7, 8, 9, 78, 82, 83, 84, 85, 87], "we": [6, 7, 8, 9, 22, 23, 44, 46, 48, 49, 54, 57, 59, 60, 63, 80, 82, 83, 84, 85, 86, 87], "them": [6, 7, 26, 45, 51, 57, 82, 83, 86, 87], "scenario": 6, "full": [6, 7, 8, 35, 40, 43, 57, 80, 85, 86], "compos": 6, "compon": [6, 8, 12, 73, 80, 82, 84, 86, 87], "which": [6, 8, 22, 23, 24, 25, 31, 32, 33, 34, 38, 39, 41, 44, 48, 49, 50, 52, 57, 59, 60, 63, 68, 71, 80, 82, 83, 84, 85, 86, 87], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 20, 51, 55, 56, 57, 59, 60, 75, 82, 83, 84, 85, 86], "recip": [6, 7, 9, 10, 11, 45, 59, 60, 80, 82, 83, 85, 87], "evalu": [6, 8, 78, 80, 84, 86, 87], "gener": [6, 8, 13, 16, 20, 26, 57, 75, 76, 78, 82, 86, 87], "each": [6, 8, 14, 17, 31, 32, 33, 34, 38, 39, 41, 44, 48, 49, 57, 58, 75, 80, 82, 83, 84, 85, 86], "support": [6, 8, 9, 10, 19, 22, 23, 24, 25, 26, 44, 54, 60, 63, 67, 80, 82, 83, 84, 85, 86, 87], "model": [6, 7, 8, 10, 15, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 59, 60, 73, 74, 78, 80, 82, 87], "make": [6, 7, 8, 9, 44, 50, 80, 83, 84, 85, 86, 87], "easi": [6, 8, 80, 86], "understand": [6, 7, 8, 78, 80, 82, 86, 87], "extend": [6, 8, 80], "befor": [6, 21, 49, 50, 54, 59, 83], "let": [6, 7, 9, 82, 83, 84, 85, 86, 87], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 19, 21, 22, 23, 24, 25, 26, 31, 32, 33, 34, 38, 39, 41, 44, 48, 49, 50, 51, 53, 55, 58, 59, 60, 62, 70, 73, 80, 82, 84, 86, 87], "defin": [6, 7, 8, 45, 53, 54, 55, 84, 86], "some": [6, 7, 15, 55, 56, 78, 80, 83, 84, 86, 87], "concept": [6, 83, 84], "In": [6, 7, 8, 48, 54, 70, 71, 83, 85, 86, 87], "ll": [6, 7, 8, 58, 80, 82, 83, 84, 85, 87], "talk": 6, "about": [6, 8, 59, 71, 80, 83, 84, 85, 86, 87], "take": [6, 7, 8, 10, 45, 46, 51, 59, 61, 62, 82, 83, 84, 85, 86, 87], "close": [6, 8, 68, 69, 70, 71, 86], "look": [6, 7, 8, 70, 79, 82, 83, 84, 85, 86], "veri": [6, 49, 83], "simpli": [6, 7, 82, 83, 85, 87], "dictat": 6, "state_dict": [6, 51, 59, 60, 86, 87], "store": [6, 68, 71, 86, 87], "file": [6, 7, 8, 9, 10, 11, 57, 58, 59, 60, 61, 68, 71, 73, 77, 80, 81, 82, 83, 84, 85, 86, 87], "disk": [6, 68], "weight": [6, 8, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 51, 53, 54, 59, 60, 71, 78, 83, 84, 85, 86, 87], "string": [6, 22, 23, 24, 25, 26, 53, 57, 58, 62, 63], "kei": [6, 7, 9, 26, 44, 46, 49, 56, 59, 83, 84, 86, 87], "identifi": 6, "state": [6, 8, 51, 55, 56, 59, 60, 83, 85, 86, 87], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 20, 51, 55, 56, 59, 60, 66], "If": [6, 7, 12, 13, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 44, 51, 54, 59, 60, 62, 63, 66, 70, 71, 75, 79, 82, 83, 84, 85, 86], "don": [6, 7, 8, 71, 75, 83, 84, 85, 87], "t": [6, 7, 8, 26, 58, 63, 71, 75, 83, 84, 85, 87], "match": [6, 26, 79, 83, 85, 86], "up": [6, 8, 9, 22, 23, 82, 84, 85, 86, 87], "exactli": 6, "those": [6, 86], "definit": [6, 86], "either": [6, 59, 86, 87], "run": [6, 7, 9, 11, 45, 46, 49, 51, 59, 60, 70, 71, 79, 80, 84, 85, 86, 87], "explicit": 6, "error": [6, 7, 21, 59, 75], "load": [6, 8, 59, 60, 61, 70, 83, 85, 86], "rais": [6, 10, 12, 19, 21, 26, 44, 49, 59, 60, 63, 66, 71, 75], "an": [6, 7, 8, 9, 10, 13, 21, 22, 23, 24, 25, 26, 44, 49, 53, 55, 56, 59, 60, 71, 80, 82, 83, 84, 85, 86, 87], "except": [6, 19, 82], "wors": 6, "silent": [6, 45], "succe": 6, "infer": [6, 18, 44, 46, 48, 49, 50, 78, 83, 84, 85, 87], "expect": [6, 7, 10, 13, 16, 17, 20, 48, 71, 82, 86], "addit": [6, 7, 8, 10, 59, 60, 63, 66, 68, 70, 71, 74, 80, 84, 86], "line": [6, 8, 61, 84, 85], "also": [6, 7, 8, 9, 10, 44, 49, 54, 62, 71, 79, 82, 83, 84, 85, 86, 87], "need": [6, 7, 8, 9, 17, 26, 44, 45, 49, 70, 71, 79, 82, 83, 84, 85, 86, 87], "shape": [6, 44, 46, 48, 49, 50, 54], "valu": [6, 7, 26, 27, 28, 29, 30, 36, 37, 42, 44, 46, 47, 49, 52, 59, 61, 68, 69, 70, 71, 75, 84, 85, 86], "two": [6, 7, 21, 80, 83, 84, 85, 86, 87], "popular": [6, 80, 82, 83], "llama2": [6, 7, 8, 10, 18, 22, 23, 26, 28, 29, 30, 31, 32, 33, 34, 35, 45, 49, 50, 57, 78, 80, 84, 85], "offici": [6, 18, 84, 85], "implement": [6, 8, 22, 23, 24, 25, 26, 45, 47, 48, 52, 53, 54, 59, 70, 80, 86, 87], "when": [6, 7, 8, 11, 49, 51, 52, 70, 83, 85, 86, 87], "websit": 6, "get": [6, 7, 8, 9, 57, 63, 64, 65, 79, 80, 82, 83, 84, 86], "access": [6, 7, 8, 59, 83, 84], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 19, 20, 44, 59, 60, 83, 84, 85, 86, 87], "pth": [6, 83, 85], "inspect": [6, 83, 86, 87], "content": [6, 57, 82], "easili": [6, 7, 80, 82, 86, 87], "torch": [6, 46, 49, 51, 52, 62, 63, 66, 73, 74, 75, 83, 84, 85, 86, 87], "import": [6, 7, 10, 70, 71, 82, 83, 84, 86, 87], "consolid": [6, 85], "00": [6, 77, 81, 84, 85], "mmap": [6, 83], "true": [6, 7, 22, 23, 24, 25, 34, 35, 40, 43, 51, 57, 58, 59, 60, 66, 70, 82, 83, 85, 86, 87], "weights_onli": 6, "map_loc": [6, 83], "cpu": [6, 8, 51, 63, 79, 83, 87], "tensor": [6, 44, 45, 46, 47, 48, 49, 50, 51, 54, 59, 68, 69, 70, 71, 72, 86, 87], "item": 6, "print": [6, 9, 22, 23, 24, 25, 26, 57, 82, 84, 86, 87], "f": [6, 9, 22, 23, 24, 25, 83, 86, 87], "tok_embed": [6, 49], "size": [6, 8, 10, 22, 23, 24, 25, 44, 46, 47, 48, 49, 50, 65, 80, 82, 83, 84, 85, 86], "32000": [6, 10, 86], "4096": [6, 10, 22, 23, 44, 48, 86], "len": [6, 22, 23, 24, 25, 49], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 31, 32, 33, 34, 38, 39, 47, 48, 51, 52, 57, 58, 59, 61, 62, 63, 64, 71, 73, 75, 79, 80, 83, 84, 85, 86, 87], "contain": [6, 44, 46, 48, 49, 50, 53, 55, 56, 57, 58, 59, 60, 61, 70, 72, 83, 85, 86], "includ": [6, 7, 8, 14, 17, 54, 59, 60, 61, 80, 83, 84, 85, 86, 87], "input": [6, 13, 14, 17, 22, 23, 24, 26, 44, 45, 47, 48, 49, 50, 54, 57, 59, 72, 75, 82, 86, 87], "embed": [6, 44, 46, 47, 48, 49, 85], "tabl": [6, 87], "call": [6, 10, 45, 51, 61, 68, 69, 70, 71, 86, 87], "layer": [6, 8, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 49, 50, 54, 80, 85, 86, 87], "have": [6, 7, 10, 44, 46, 53, 61, 70, 73, 79, 82, 83, 84, 85, 86, 87], "dim": [6, 44, 45, 47, 48, 49, 50], "most": [6, 7, 58, 84, 86, 87], "within": [6, 7, 10, 26, 45, 70, 75, 83, 85, 86, 87], "default": [6, 7, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 44, 45, 47, 48, 49, 50, 51, 52, 54, 57, 58, 59, 60, 61, 63, 68, 71, 72, 73, 75, 79, 83, 85, 86, 87], "everi": [6, 8, 45, 70, 79, 87], "config": [6, 9, 10, 11, 12, 44, 59, 61, 71, 80, 82, 83, 85, 86, 87], "repo": [6, 59, 60, 83], "first": [6, 7, 10, 21, 49, 58, 59, 61, 78, 80, 83, 85, 86, 87], "big": [6, 83], "split": [6, 82, 83], "across": [6, 8, 59, 70, 75, 83, 85], "bin": [6, 83], "To": [6, 7, 8, 9, 59, 79, 80, 82, 83, 84, 85, 86, 87], "correctli": [6, 8, 12, 59, 79, 84, 87], "piec": 6, "one": [6, 8, 21, 45, 57, 82, 83, 84, 85, 87], "pytorch_model": [6, 83], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 44, 48, 49, 50, 54, 55, 57, 60, 61, 63, 83, 84, 85, 86, 87], "doe": [6, 19, 53, 59, 61, 83], "fewer": [6, 44], "sinc": [6, 7, 10, 45, 59, 83, 85], "instead": [6, 8, 45, 46, 54, 83, 85, 86], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 20, 26, 53, 56, 58, 59, 60, 61, 62, 68, 69, 70, 71, 83, 85], "caus": [6, 57], "try": [6, 7, 83, 84, 85, 87], "same": [6, 7, 31, 32, 33, 34, 38, 39, 44, 46, 50, 57, 61, 71, 83, 85, 86, 87], "As": [6, 7, 8, 9, 54, 80, 83, 85, 87], "re": [6, 7, 58, 80, 83, 84, 85, 86], "care": [6, 45, 59, 83, 85, 86], "like": [6, 7, 8, 9, 79, 82, 83, 84, 86], "end": [6, 8, 58, 78, 80, 85, 86], "number": [6, 8, 22, 23, 26, 44, 46, 49, 52, 59, 60, 65, 75, 84, 86], "just": [6, 13, 80, 82, 84, 85, 86], "save": [6, 8, 9, 51, 59, 60, 71, 78, 83, 85, 86], "less": [6, 26, 83, 84, 85, 87], "prone": 6, "manag": [6, 73], "invari": 6, "accept": [6, 7, 26, 57, 84, 87], "multipl": [6, 7, 8, 54, 68, 69, 70, 71, 84, 85], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 82, 83], "worri": [6, 84], "explicitli": [6, 53, 80, 86], "convert": [6, 59, 72, 83, 87], "time": [6, 57, 68, 70, 83, 85, 87], "produc": [6, 87], "back": [6, 21, 59, 86, 87], "origin": [6, 22, 23, 51, 54, 82, 83, 85, 86, 87], "form": [6, 7, 8, 21], "One": [6, 83], "advantag": [6, 86], "being": [6, 59, 60, 62, 87], "should": [6, 7, 8, 14, 17, 18, 19, 31, 32, 33, 34, 38, 39, 41, 44, 45, 53, 61, 68, 69, 70, 71, 79, 80, 82, 83, 84, 85, 86, 87], "abl": [6, 8, 82, 83, 84, 85], "fine": [6, 8, 9, 78, 80, 83, 86], "post": [6, 87], "tool": [6, 83, 84], "quantiz": [6, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 54, 78, 84, 87], "eval": [6, 78, 80], "without": [6, 7, 9, 79, 80, 82, 83, 86], "code": [6, 8, 49, 76, 80, 82, 84], "chang": [6, 7, 9, 13, 79, 82, 83, 84, 85, 86, 87], "OR": 6, "convers": [6, 14, 15, 18, 19, 21, 26, 59, 80, 82, 83, 85, 86, 87], "script": [6, 9, 83, 84, 85], "wai": [6, 7, 82, 83, 84, 85], "surround": [6, 8, 80], "load_checkpoint": [6, 8, 59, 60], "save_checkpoint": [6, 8, 9, 59, 60], "method": [6, 7, 8, 9, 11, 22, 23, 24, 25, 26, 51, 53, 55, 61, 79, 80, 83, 85, 86, 87], "convertor": 6, "avail": [6, 8, 61, 62, 63, 80, 83, 85, 86], "here": [6, 7, 9, 15, 24, 47, 48, 82, 83, 84, 85, 86, 87], "three": [6, 8, 84], "hfcheckpoint": 6, "read": [6, 59, 60, 80], "write": [6, 8, 59, 60, 68, 82, 84], "compat": [6, 59], "transform": [6, 8, 31, 32, 33, 34, 38, 39, 41, 49, 50, 52, 86], "framework": [6, 8, 80], "mention": [6, 83, 87], "abov": [6, 51, 79, 83, 85, 86, 87], "assum": [6, 13, 16, 17, 20, 52, 55, 58, 63, 83, 86], "checkpoint_dir": [6, 7, 59, 60, 83, 85], "necessari": [6, 26, 68, 69, 70, 71, 86], "json": [6, 59, 73, 83], "easiest": [6, 83, 84], "sure": [6, 7, 83, 84, 85, 86, 87], "everyth": [6, 8, 61, 80, 84], "flow": [6, 87], "By": [6, 85, 86, 87], "safetensor": 6, "output": [6, 17, 22, 23, 24, 26, 31, 32, 33, 34, 38, 39, 41, 44, 45, 47, 48, 49, 50, 54, 56, 69, 73, 79, 82, 83, 84, 85, 86, 87], "dir": [6, 71, 79, 83, 84, 85], "output_dir": [6, 7, 59, 60, 73, 83, 85, 86, 87], "specifi": [6, 7, 8, 10, 44, 71, 82, 83, 84, 85, 87], "argument": [6, 7, 10, 17, 26, 35, 40, 43, 44, 61, 66, 68, 70, 71, 74, 85, 86], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 49, 74, 83, 86, 87], "_component_": [6, 7, 9, 10, 82, 83, 85, 86], "fullmodelhfcheckpoint": [6, 83], "directori": [6, 7, 59, 60, 68, 70, 71, 83, 84, 85], "sort": [6, 59], "id": [6, 22, 23, 26, 57, 58, 59, 72, 83], "so": [6, 7, 59, 61, 79, 80, 83, 84, 85, 86, 87], "order": [6, 8, 59, 70, 71, 84], "matter": [6, 59, 86], "checkpoint_fil": [6, 7, 9, 59, 60, 83, 85, 86, 87], "restart": 6, "previou": [6, 59, 60], "more": [6, 7, 8, 26, 46, 48, 61, 71, 73, 75, 80, 82, 83, 84, 85, 86, 87], "next": [6, 85, 87], "section": [6, 8, 78, 83, 85, 87], "recipe_checkpoint": [6, 59, 60], "null": [6, 7], "usual": [6, 48, 59, 71, 83, 86], "model_typ": [6, 59, 60, 83, 85], "resume_from_checkpoint": [6, 59, 60], "fals": [6, 7, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 54, 57, 58, 59, 60, 73, 83, 85, 86, 87], "requir": [6, 7, 26, 59, 70, 71, 75, 79, 82, 84, 87], "param": [6, 8, 31, 32, 33, 34, 38, 39, 54, 55, 56, 59, 86, 87], "directli": [6, 7, 8, 10, 59, 82, 83, 84, 85, 86, 87], "help": [6, 18, 49, 59, 61, 78, 79, 80, 82, 83, 84, 85, 87], "ensur": [6, 7, 12, 21, 26, 44, 59, 63, 80, 84], "out": [6, 7, 8, 22, 23, 24, 25, 59, 60, 78, 80, 83, 84, 85, 86, 87], "case": [6, 8, 9, 44, 59, 63, 68, 80, 82, 83, 85, 86, 87], "discrep": [6, 59], "along": [6, 85, 86], "detail": [6, 26, 46, 73, 75, 83, 84, 85, 86, 87], "found": [6, 7, 9, 47, 48, 86, 87], "metacheckpoint": 6, "github": [6, 10, 31, 32, 33, 34, 38, 39, 44, 47, 48, 52, 79, 84], "repositori": [6, 18, 83, 84], "fullmodelmetacheckpoint": [6, 85], "torchtunecheckpoint": 6, "perform": [6, 45, 80, 83, 85, 87], "current": [6, 44, 48, 49, 50, 60, 65, 68, 70, 75, 83, 84, 85], "test": [6, 7, 8, 79, 80], "complet": [6, 8, 82, 83, 84, 85], "written": [6, 7, 8, 59, 60, 68, 69, 70, 71, 80], "begin": [6, 57, 58, 85, 87], "partit": [6, 87], "ha": [6, 53, 55, 57, 82, 83, 84, 85, 86, 87], "standard": [6, 69, 80, 83, 85], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 83], "inform": [6, 71, 80, 83, 84, 85], "subsequ": [6, 8], "recipe_st": [6, 59, 60], "pt": [6, 9, 59, 60, 83, 85], "epoch": [6, 8, 9, 52, 59, 60, 83, 84, 85], "optim": [6, 7, 8, 52, 83, 84, 85, 86, 87], "etc": [6, 8, 59, 84], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 49, 53, 57, 59, 73, 75, 82, 83, 86, 87], "updat": [6, 7, 8, 79, 83, 84, 85, 86, 87], "hf_model_0001_0": [6, 83], "hf_model_0002_0": [6, 83], "both": [6, 83, 86, 87], "adapt": [6, 53, 54, 55, 56, 59, 60, 83, 86, 87], "merg": [6, 10, 59, 83, 85, 87], "would": [6, 7, 9, 49, 79, 82, 83, 86, 87], "our": [6, 8, 80, 82, 83, 84, 86, 87], "tutori": [6, 80, 82, 83, 84, 85, 86, 87], "primari": [6, 7, 8, 84], "want": [6, 7, 8, 9, 10, 79, 83, 84, 85, 86], "resum": [6, 8, 52, 59, 60, 87], "initi": [6, 8, 11, 27, 28, 29, 30, 36, 37, 42, 66, 84, 86, 87], "frozen": [6, 86, 87], "base": [6, 10, 26, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 48, 52, 54, 56, 59, 61, 68, 78, 83, 84, 85, 86, 87], "well": [6, 7, 8, 80, 82, 83, 85, 87], "learnt": [6, 83], "someth": [6, 8, 9, 83], "NOT": 6, "refer": [6, 7, 8, 47, 48, 80, 86], "adapter_checkpoint": [6, 59, 60], "adapter_0": [6, 83], "now": [6, 57, 82, 83, 84, 85, 86, 87], "knowledg": 6, "creat": [6, 7, 10, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 52, 59, 60, 68, 70, 82, 83, 85, 87], "simpl": [6, 8, 78, 84, 86, 87], "forward": [6, 8, 44, 45, 47, 48, 49, 50, 54, 85, 86, 87], "13b": [6, 28, 31, 34], "modeltyp": [6, 59, 60], "llama2_13b": [6, 31, 34], "right": [6, 59, 83, 85, 86], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 86], "successfulli": [6, 84], "vocab": [6, 10, 49, 85], "70": [6, 29, 36], "x": [6, 44, 45, 47, 48, 49, 50, 54, 86, 87], "randint": 6, "0": [6, 8, 32, 33, 35, 44, 49, 52, 54, 57, 70, 71, 72, 75, 77, 81, 83, 84, 85, 86, 87], "no_grad": 6, "6": [6, 47, 72, 83, 87], "3989": 6, "9": [6, 83, 87], "0531": 6, "2375": 6, "5": [6, 52, 72, 73, 83, 84, 85], "2822": 6, "4": [6, 26, 44, 72, 80, 83, 85, 86, 87], "4872": 6, "7469": 6, "8": [6, 22, 23, 24, 25, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 83, 86, 87], "6737": 6, "11": [6, 83, 85, 87], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 72], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 26, 71, 82, 83, 84, 85, 86], "find": [6, 8, 9, 83, 84, 86], "list": [6, 7, 14, 15, 18, 19, 21, 22, 23, 26, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 53, 54, 57, 58, 59, 60, 61, 64, 67, 72, 82, 84, 85], "builder": [6, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 82, 87], "hope": 6, "deeper": [6, 84], "insight": [6, 83], "happi": [6, 83], "thi": [7, 8, 9, 10, 22, 23, 26, 44, 45, 48, 49, 50, 51, 52, 53, 57, 59, 60, 61, 62, 63, 68, 70, 71, 73, 75, 78, 79, 80, 82, 83, 84, 85, 86, 87], "guid": [7, 9, 80, 82, 84, 86], "yaml": [7, 8, 10, 11, 61, 71, 80, 83, 84, 85, 86, 87], "pars": [7, 10, 58, 61, 84], "effect": 7, "cli": [7, 9, 11, 79, 83, 84], "prerequisit": [7, 82, 83, 84, 85, 86, 87], "Be": [7, 83, 84, 85, 86, 87], "familiar": [7, 83, 84, 85, 86, 87], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 79, 82, 84], "instal": [7, 9, 70, 71, 78, 83, 84, 85, 86, 87], "fundament": 7, "There": [7, 14, 21, 83, 84, 85, 86], "entri": [7, 8, 84], "point": [7, 8, 82, 83, 84, 85, 86, 87], "locat": [7, 85, 86, 87], "thei": [7, 8, 49, 61, 82, 86], "truth": [7, 83, 85], "reproduc": 7, "overridden": [7, 45, 61], "quick": 7, "experiment": 7, "modifi": [7, 8, 9, 51, 80, 83, 85, 86, 87], "serv": [7, 82, 86], "particular": [7, 26, 82, 86, 87], "seed": [7, 8, 9, 75, 84], "shuffl": 7, "devic": [7, 8, 62, 63, 83, 84, 85, 86], "cuda": [7, 62, 63, 79, 83, 87], "dtype": [7, 8, 46, 51, 63, 67, 83, 87], "fp32": [7, 87], "enable_fsdp": 7, "mani": [7, 82, 83], "object": [7, 10, 14, 15, 18, 19, 44, 82], "keyword": [7, 10, 26, 51], "loss": [7, 8, 22, 23, 24, 25, 84, 86, 87], "function": [7, 8, 10, 11, 44, 45, 51, 62, 65, 75, 80, 82, 87], "exampl": [7, 8, 9, 10, 11, 15, 18, 19, 22, 23, 24, 25, 26, 44, 53, 57, 59, 60, 70, 71, 72, 76, 77, 79, 81, 82, 83, 85, 86, 87], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 83], "path": [7, 8, 9, 10, 22, 23, 24, 25, 26, 57, 58, 59, 60, 61, 73, 83, 85, 86], "normal": [7, 47, 49, 50, 57, 82, 86, 87], "python": [7, 58, 61, 64, 71, 75, 76, 83], "alpaca_dataset": [7, 22, 82], "custom": [7, 8, 80, 83, 84, 85, 86], "train_on_input": [7, 22, 23, 24, 25, 26, 82], "onc": [7, 83, 84, 85, 86, 87], "ve": [7, 46, 58, 82, 83, 85, 86], "instanc": [7, 10, 45, 51, 55, 56, 86], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 83, 87], "under": [7, 83, 85, 87], "preced": [7, 10, 85, 86], "actual": [7, 9], "throw": 7, "notic": [7, 82, 86], "miss": [7, 86], "posit": [7, 10, 44, 48, 49, 50, 85], "anoth": [7, 83], "handl": [7, 11, 57, 83, 86, 87], "def": [7, 8, 9, 11, 82, 86, 87], "dictconfig": [7, 8, 10, 11, 12, 71], "arg": [7, 10, 49, 51, 53, 61, 69], "tupl": [7, 10, 26, 51, 57, 58, 61, 65, 72], "kwarg": [7, 10, 51, 53, 61, 66, 68, 69, 70, 71, 74], "str": [7, 10, 13, 16, 17, 20, 22, 23, 24, 25, 26, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 73, 75], "mean": [7, 47, 84, 86], "pass": [7, 10, 44, 45, 51, 63, 66, 70, 71, 74, 86, 87], "add": [7, 9, 58, 61, 82, 83, 85, 86, 87], "d": [7, 44, 49, 50, 58, 82, 86], "llama2_token": [7, 83], "tmp": [7, 84, 85], "option": [7, 8, 13, 16, 17, 20, 26, 31, 32, 33, 34, 38, 39, 41, 44, 48, 49, 50, 51, 57, 58, 59, 60, 62, 63, 64, 68, 71, 73, 74, 75, 79, 80, 83], "bool": [7, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 51, 54, 57, 58, 59, 60, 66, 70, 73, 87], "max_seq_len": [7, 10, 22, 23, 26, 44, 46, 48, 49, 57, 58, 82], "int": [7, 9, 22, 23, 26, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 46, 47, 48, 49, 52, 54, 57, 58, 59, 60, 65, 68, 69, 70, 71, 72, 75, 82, 86, 87], "512": [7, 22, 23, 82, 87], "instructdataset": [7, 22, 23, 24, 25, 82], "alreadi": [7, 66, 79, 83, 86], "overwrit": [7, 79], "duplic": [7, 8, 80], "sometim": 7, "than": [7, 21, 26, 44, 83, 84, 85, 86, 87], "resolv": [7, 84], "alpaca": [7, 13, 22, 23, 31, 32, 33, 34, 38, 39, 82], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 68, 69, 70, 71], "disklogg": 7, "log_dir": [7, 68, 70, 71], "conveni": [7, 8], "quickli": [7, 82], "verifi": [7, 62, 63, 84, 86], "properli": 7, "experi": [7, 71, 78, 80, 85, 86], "wa": [7, 83, 85, 86, 87], "cp": [7, 79, 83, 84, 85], "7b_lora_single_devic": [7, 83, 84, 86, 87], "my_config": 7, "discuss": [7, 84, 86], "guidelin": 7, "while": [7, 8, 31, 32, 33, 34, 38, 39, 45, 80, 83, 87], "mai": [7, 9, 73, 82, 84, 86], "tempt": 7, "put": [7, 8, 84, 86], "much": [7, 83, 85, 86, 87], "give": [7, 86], "maximum": [7, 22, 23, 26, 44, 46, 48, 49, 58], "flexibl": [7, 82], "switch": 7, "encourag": [7, 86], "clariti": 7, "significantli": 7, "easier": [7, 83, 84], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 87], "expos": [7, 8, 82, 84], "parent": 7, "modul": [7, 10, 26, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 74, 75, 84, 86, 87], "__init__": [7, 8, 86, 87], "py": [7, 10, 31, 32, 33, 34, 38, 39, 44, 46, 47, 48, 52, 83, 85], "guarante": 7, "stabil": [7, 80, 87], "underscor": 7, "_alpaca": 7, "collect": [7, 84], "differ": [7, 9, 57, 80, 83, 85, 86, 87], "itself": 7, "via": [7, 9, 54, 86, 87], "pair": [7, 72, 82], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 83, 84, 85, 86, 87], "checkpoint": [7, 8, 51, 58, 59, 60, 71, 74, 80, 85, 86, 87], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 26, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, 60, 61, 68, 69, 70, 71, 82, 84, 86, 87], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 44, 48, 49, 50], "core": [8, 80, 82, 84, 87], "i": [8, 18, 19, 51, 56, 58, 82, 83, 85, 87], "structur": [8, 14, 15, 18, 19, 82, 83], "new": [8, 42, 68, 70, 83, 84, 85, 86, 87], "user": [8, 14, 15, 18, 19, 21, 44, 57, 82, 84], "thought": [8, 80, 84, 87], "target": [8, 80], "pipelin": [8, 80], "llm": [8, 78, 80, 82, 83, 86], "eg": [8, 49, 59, 80], "meaning": [8, 80, 83], "featur": [8, 9, 79, 80, 83, 84], "fsdp": [8, 80, 84, 85], "activ": [8, 45, 74, 80, 87], "gradient": [8, 80, 83, 85, 86, 87], "accumul": [8, 80], "mix": [8, 83], "precis": [8, 51, 63, 80, 84, 87], "appli": [8, 31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 47, 48, 49, 50, 80, 87], "given": [8, 10, 17, 21, 54, 62, 63, 80, 86], "complex": 8, "becom": [8, 79, 82], "harder": 8, "anticip": 8, "architectur": [8, 18, 19, 49, 82], "methodolog": 8, "reason": [8, 83], "possibl": 8, "trade": 8, "off": [8, 57, 83], "memori": [8, 22, 23, 51, 78, 80, 82, 83, 84, 85], "vs": [8, 84], "qualiti": [8, 83, 86], "believ": 8, "best": 8, "suit": [8, 84], "specif": [8, 10, 82, 83, 87], "b": [8, 44, 48, 49, 50, 54, 71, 86, 87], "fit": [8, 22, 23], "solut": 8, "result": [8, 57, 83, 85, 86, 87], "meant": [8, 51], "depend": [8, 9, 13, 83, 86, 87], "level": [8, 64, 80, 87], "expertis": 8, "routin": 8, "yourself": [8, 85, 86], "exist": [8, 79, 82, 83, 84, 85, 87], "ad": [8, 57, 86, 87], "ones": 8, "modular": [8, 80], "build": [8, 80, 85, 86], "block": [8, 31, 32, 33, 34, 38, 39, 41, 80], "wandb": [8, 9, 71, 84], "log": [8, 64, 68, 69, 70, 71, 83, 84, 85, 87], "fulli": 8, "nativ": [8, 78, 80, 86, 87], "pytorch": [8, 49, 51, 70, 73, 75, 78, 79, 80, 85, 86, 87], "correct": [8, 16, 24, 47, 48, 49, 62, 80, 82], "numer": [8, 80], "pariti": [8, 80], "verif": 8, "extens": [8, 80], "comparison": [8, 86, 87], "benchmark": [8, 75, 80, 83, 85, 86], "limit": 8, "hidden": [8, 45], "behind": 8, "100": [8, 22, 23, 24, 25, 26, 72, 73, 86, 87], "flag": [8, 22, 23, 24, 25, 87], "prefer": [8, 80, 82], "over": [8, 52, 61, 80, 82, 83, 85, 86, 87], "unnecessari": 8, "abstract": [8, 14, 17, 80, 84, 87], "No": [8, 80], "inherit": [8, 61, 80], "go": [8, 18, 19, 57, 80, 82, 83, 84, 87], "upon": [8, 85], "figur": [8, 86, 87], "spectrum": 8, "decid": 8, "interact": [8, 78, 84], "start": [8, 9, 58, 79, 80, 82, 83, 84], "paradigm": 8, "consist": [8, 84], "configur": [8, 22, 23, 24, 25, 26, 50, 80, 84, 85, 86, 87], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 66, 68, 69, 70, 71, 72, 73, 74, 75, 78, 80, 83, 84, 85, 86, 87], "overrid": [8, 11, 83, 84, 85, 87], "togeth": [8, 71, 84, 86], "valid": [8, 21, 79, 83, 84], "environ": [8, 79, 83, 84], "logic": [8, 59, 80, 84, 86], "api": [8, 9, 35, 40, 43, 83, 84, 85, 87], "closer": [8, 86], "monolith": [8, 80], "trainer": [8, 65], "A": [8, 9, 51, 54, 57, 58, 59, 61, 72, 77, 78, 81, 83, 86, 87], "wrapper": [8, 57, 58, 86], "around": [8, 57, 58, 73, 83, 86, 87], "extern": 8, "primarili": [8, 86], "eleutherai": [8, 80, 86], "har": [8, 80, 86], "control": [8, 22, 23, 24, 25, 75, 83], "multi": [8, 44, 85], "stage": 8, "distil": 8, "oper": [8, 73, 75], "turn": [8, 21, 58], "dataload": [8, 22, 23, 24, 25], "applic": [8, 44, 59, 60, 71], "clean": [8, 9, 22], "after": [8, 46, 47, 68, 69, 70, 71, 87], "process": [8, 9, 51, 75, 84, 87], "group": [8, 44, 68, 69, 70, 71, 85], "init_process_group": [8, 66], "backend": 8, "gloo": 8, "els": [8, 61, 71, 80, 87], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 61, 82, 84, 85, 86], "stuff": 8, "carri": 8, "relev": [8, 83, 86], "interfac": [8, 14, 17, 53], "metric": [8, 84], "logger": [8, 64, 68, 69, 70, 71, 84], "self": [8, 9, 31, 32, 33, 34, 38, 39, 41, 44, 49, 50, 53, 82, 86, 87], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 73, 74], "_model": 8, "_setup_model": 8, "_token": [8, 82], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 87], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 75, 85], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 22, 23, 24, 25, 44, 46, 48, 49, 50, 57, 72, 80, 82, 84, 85, 86], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 22, 23, 26, 72], "total_training_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 68, 69, 70, 71], "step": [8, 49, 52, 58, 68, 69, 70, 71, 73, 78, 83, 86, 87], "learn": [8, 52, 80, 82, 84, 85, 86, 87], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 19, 20, 21, 44, 48, 49, 50, 56, 57, 58, 59, 60, 62, 63, 64, 68, 69, 70, 71, 74, 75, 83], "fullfinetunerecip": 8, "direct": [8, 79], "wandblogg": [9, 86, 87], "workspac": 9, "seen": [9, 86, 87], "screenshot": 9, "below": [9, 48, 82, 85, 86, 87], "packag": [9, 70, 71, 79], "pip": [9, 70, 71, 79, 83, 85], "Then": [9, 84], "login": [9, 71, 83], "built": [9, 79, 82, 84, 87], "project": [9, 31, 32, 33, 34, 38, 39, 41, 44, 45, 71, 78, 86, 87], "grab": [9, 85], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": 9, "exit": [9, 79], "resourc": [9, 68, 69, 70, 71], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 26, 82, 83], "desir": 9, "suggest": 9, "approach": [9, 82], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 83], "_output_dir": [9, 59, 60], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 44, 46, 47, 48, 49, 50, 51, 54, 55, 57, 58, 59, 60, 63, 64, 65, 66, 73, 83, 86, 87], "descript": [9, 26], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 22, 23, 24, 25], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 39, 40, 42, 43, 44, 47, 48, 52, 59, 60, 61, 64, 70, 71, 73, 75, 79, 83], "com": [10, 31, 32, 33, 34, 38, 39, 44, 47, 48, 52, 79], "facebookresearch": [10, 47, 48], "blob": [10, 31, 32, 33, 34, 38, 39, 44, 47, 48, 52], "main": [10, 11, 44, 47, 48, 79, 83, 85], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 49], "32": [10, 85, 86, 87], "num_head": [10, 44, 46, 48, 49], "num_kv_head": [10, 44, 46], "vocab_s": 10, "must": [10, 22, 23, 24, 25, 26, 53, 58, 61, 87], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 44, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 64, 65, 66, 67, 72, 73, 75, 82, 86, 87], "nn": [10, 44, 45, 46, 49, 50, 51, 53, 55, 56, 74, 86, 87], "parsed_yaml": 10, "embed_dim": [10, 44, 48, 50, 86], "valueerror": [10, 19, 21, 26, 44, 49, 59, 60, 63, 75], "callabl": [11, 49], "With": [11, 83, 86, 87], "my_recip": 11, "foo": 11, "bar": [11, 80, 84], "instanti": [12, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42], "configerror": 12, "cannot": [12, 85], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 68, 69, 70, 71, 82, 83, 87], "prompt": [13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 49, 57, 82, 83, 85], "templat": [13, 14, 16, 17, 20, 22, 23, 24, 25, 26], "style": [13, 22, 23, 26, 87], "slightli": 13, "instruct": [13, 15, 17, 19, 22, 23, 78, 84, 85, 86, 87], "classmethod": [13, 14, 15, 16, 17, 18, 19, 20], "map": [13, 16, 17, 20, 26, 56, 59, 68, 69, 70, 71, 83, 86], "column_map": [13, 16, 17, 20, 26, 82], "placehold": [13, 14, 16, 17, 20, 26], "column": [13, 16, 17, 20, 26], "ident": [13, 16, 17, 19, 20, 83], "chat": [14, 15, 18, 26], "role": [14, 57, 82], "system": [14, 15, 18, 19, 21, 57, 82], "assist": [14, 15, 18, 21, 57, 82], "messag": [14, 15, 18, 19, 21, 57, 58, 79, 82], "accord": [14, 19], "openai": 15, "markup": 15, "languag": [15, 54, 86], "It": [15, 19, 82, 87], "huggingfac": [15, 52, 59, 60, 82, 83], "im_start": 15, "context": [15, 73, 82], "im_end": 15, "goe": 15, "respons": [15, 57, 82, 83, 84, 85], "appropri": [15, 18, 19, 52, 82, 87], "tag": [15, 18, 19, 58, 68, 69, 70, 71], "grammar": [16, 24, 82], "sentenc": 16, "alwai": [17, 61], "human": 18, "taken": [18, 86, 87], "inst": [18, 19, 82], "sy": [18, 82], "respect": [18, 56, 82], "honest": [18, 82], "am": [18, 19, 82, 83, 85], "pari": [18, 19, 82], "capit": [18, 19, 82], "franc": [18, 19, 82], "known": [18, 19, 57, 82], "its": [18, 19, 75, 82, 83, 85, 86], "stun": [18, 19, 82], "mistral": [19, 41, 42, 43, 83, 84], "llama2chatformat": [19, 26, 82], "summar": [20, 25, 82], "task": [20, 82, 83, 85, 86, 87], "dialogu": [20, 25], "dialog": 20, "forth": 21, "consecut": 21, "come": [21, 53, 86], "empti": 21, "shorter": 21, "length": [21, 22, 23, 26, 44, 46, 48, 49, 50, 57, 58, 60, 72], "min": [21, 86], "invalid": 21, "yahma": 22, "codebas": [22, 23, 24, 25, 83], "where": [22, 23, 24, 25, 44, 49, 54, 57, 82], "mask": [22, 23, 24, 25, 44, 50, 57, 58, 82], "contribut": [22, 23, 24, 25], "replac": [22, 23, 24, 25, 51, 86], "encod": [22, 23, 24, 25, 26, 57, 58], "decod": [22, 23, 24, 25, 26, 49, 57, 58], "anyth": [22, 23, 24, 25, 26], "load_dataset": [22, 23, 24, 25, 26], "whether": [22, 23, 24, 25, 26, 31, 32, 33, 34, 38, 39, 41, 51, 54, 57, 58, 63], "recommend": [22, 23, 70, 83, 87], "highest": [22, 23], "sequenc": [22, 23, 26, 44, 46, 48, 49, 50, 57, 58, 72], "alpaca_d": [22, 23], "batch_siz": [22, 23, 24, 25, 44, 50, 83], "tatsu": [23, 82], "lab": [23, 82], "liweili": 24, "c4_200m": 24, "variant": [24, 25], "mirror": [24, 25], "llama_recip": [24, 25], "grammar_d": 24, "samsum": 25, "summari": 25, "samsum_d": 25, "_util": 26, "open": [26, 27, 82, 83], "orca": [26, 82], "slimorca": [26, 82], "dedup": [26, 82], "chat_format": [26, 82], "_chat_format": 26, "chatformat": [26, 82], "1024": [26, 82], "chatdataset": 26, "adher": 26, "doesn": [26, 83], "prescrib": 26, "truncat": [26, 57, 58], "variabl": [26, 87], "least": [26, 85, 86], "though": 26, "max": [26, 49, 52, 57, 86], "ds": 26, "10": [26, 72, 83, 85, 87], "351": 26, "82": [26, 83], "391": 26, "221": 26, "220": 26, "193": 26, "12": [26, 79], "471": 26, "gemma": 27, "transformerdecod": [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 86], "w": [27, 28, 29, 30, 36, 37, 42, 70, 71, 83, 86, 87], "blog": 27, "technolog": 27, "develop": [27, 87], "arxiv": [28, 29, 30, 35, 40, 43, 44, 47, 48], "org": [28, 29, 30, 35, 40, 43, 44, 47, 48, 61, 64, 70, 73, 75, 79], "ab": [28, 29, 30, 35, 40, 43, 48], "2307": [28, 29, 30], "09288": [28, 29, 30], "lora_attn_modul": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86, 87], "liter": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43], "q_proj": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 86, 87], "k_proj": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 86, 87], "v_proj": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 86, 87], "output_proj": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 86, 87], "apply_lora_to_mlp": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86], "apply_lora_to_output": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86], "lora_rank": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86], "lora_alpha": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86], "float": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 44, 47, 52, 54, 68, 69, 70, 71, 86, 87], "16": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 86, 87], "quantize_bas": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 54, 87], "lora": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 54, 78, 80, 84, 85], "tloen": [31, 32, 33, 34, 38, 39], "8bb8579e403dc78e37fe81ffbb253c413007323f": [31, 32, 33, 34, 38, 39], "l41": [31, 32, 33, 34, 38, 39], "l43": [31, 32, 33, 34, 38, 39], "linear": [31, 32, 33, 34, 35, 38, 39, 40, 41, 43, 49, 53, 54, 86, 87], "attent": [31, 32, 33, 34, 38, 39, 41, 44, 46, 48, 49, 50, 85, 86, 87], "mlp": [31, 32, 33, 34, 38, 39, 41, 49, 50, 85, 86], "final": [31, 32, 33, 34, 38, 39, 41, 45, 49, 58, 83, 85, 86, 87], "rank": [31, 32, 33, 34, 38, 39, 41, 54, 65, 75, 84, 86, 87], "low": [31, 32, 33, 34, 38, 39, 41, 54, 83, 86, 87], "approxim": [31, 32, 33, 34, 38, 39, 41, 54, 86], "scale": [31, 32, 33, 34, 38, 39, 41, 54, 86, 87], "factor": [31, 32, 33, 34, 38, 39, 41, 54, 83], "lora_dropout": [32, 33, 35], "05": [32, 33, 35], "70b": [32, 36, 38], "llama2_7b": [32, 33, 86], "qlora": [35, 40, 43, 51, 78, 80, 85, 86], "per": [35, 40, 43, 46, 51, 85, 87], "paper": [35, 40, 43, 86, 87], "2305": [35, 40, 43, 44], "14314": [35, 40, 43], "lora_llama2_7b": [35, 86], "llama3": [36, 37, 38, 39, 40, 78], "llama3_70b": 38, "llama3_8b": [39, 85], "lora_llama3_8b": 40, "announc": 42, "lora_mistral_7b": 43, "head_dim": [44, 46, 48, 49], "pos_embed": [44, 86], "kv_cach": 44, "kvcach": [44, 49], "attn_dropout": [44, 49], "head": [44, 46, 48, 49, 85], "queri": [44, 46, 49, 85], "gqa": 44, "introduc": [44, 47, 54, 86, 87], "pdf": [44, 47], "13245v1": 44, "version": [44, 79, 85, 87], "multihead": 44, "mha": [44, 49], "n": [44, 57, 58, 77, 81, 82], "extrem": 44, "share": [44, 82, 83], "mqa": 44, "credit": 44, "document": 44, "lightn": 44, "lit": 44, "gpt": [44, 83], "lit_gpt": 44, "v": [44, 49, 86], "k": [44, 86], "q": [44, 86], "n_kv_head": 44, "dimens": [44, 46, 48, 49, 54, 85, 86, 87], "calcul": [44, 85], "e": [44, 51, 53, 56, 79, 83, 85, 86, 87], "g": [44, 53, 85, 86, 87], "rotarypositionalembed": [44, 86], "cach": [44, 46, 48, 79], "comput": [44, 45, 48, 49, 75, 83, 87], "rope": [44, 48], "dropout": [44, 54, 86, 87], "onto": 44, "scaled_dot_product_attent": 44, "input_po": [44, 48, 49, 50], "seq_length": [44, 50], "seq_len": [44, 48], "bigger": 44, "n_h": [44, 48], "num": [44, 48], "n_kv": 44, "kv": [44, 46, 49], "emb": [44, 49, 50], "h_d": [44, 48], "gate_proj": 45, "down_proj": 45, "up_proj": 45, "silu": 45, "feed": [45, 50], "network": [45, 86, 87], "deriv": [45, 49, 50], "fed": 45, "multipli": 45, "subclass": [45, 61], "although": [45, 86], "afterward": 45, "former": 45, "regist": [45, 51, 87], "hook": [45, 51, 87], "latter": 45, "max_batch_s": 46, "standalon": 46, "past": 46, "becaus": [46, 49, 83, 85], "expand": 46, "dpython": [46, 51], "ep": 47, "1e": 47, "06": [47, 86], "root": [47, 70, 71], "squar": 47, "1910": 47, "07467": 47, "verfic": [47, 48], "small": [47, 83], "avoid": [47, 51, 75, 87], "divis": 47, "zero": [47, 83, 85], "10000": 48, "rotari": [48, 85], "propos": 48, "2104": 48, "09864": 48, "l450": 48, "upto": 48, "init": [48, 71, 87], "exceed": 48, "freq": 48, "recomput": 48, "geometr": 48, "progress": [48, 84], "rotat": 48, "angl": 48, "bsz": 48, "todo": 48, "made": [48, 83], "effici": [48, 78, 80, 83, 84, 86], "transformerdecoderlay": 49, "norm": [49, 50], "move": 49, "space": 49, "check": [49, 63, 78, 83, 84, 86], "belong": 49, "reduc": [49, 80, 82, 86, 87], "statement": 49, "improv": [49, 83, 85, 86], "readabl": [49, 83], "At": 49, "arang": 49, "prompt_length": 49, "causal_mask": 49, "m_": 49, "seq": 49, "attn": [50, 86, 87], "causalselfattent": [50, 86], "sa_norm": 50, "mlp_norm": 50, "ff": 50, "common_util": 51, "bfloat16": [51, 83, 84, 85, 86], "offload_to_cpu": 51, "nf4": [51, 87], "restor": 51, "higher": [51, 85, 87], "offload": [51, 87], "increas": [51, 52, 85, 86], "peak": [51, 83, 85, 86, 87], "gpu": [51, 83, 84, 85, 86, 87], "usag": [51, 79, 83, 84, 85, 87], "_register_state_dict_hook": 51, "m": [51, 58], "mymodul": 51, "_after_": 51, "nf4tensor": [51, 87], "unquant": [51, 83, 87], "unus": 51, "num_warmup_step": 52, "num_training_step": 52, "num_cycl": 52, "last_epoch": 52, "lambdalr": 52, "rate": [52, 80, 84], "schedul": [52, 73, 84], "linearli": 52, "lr": 52, "decreas": [52, 86, 87], "cosin": 52, "remain": [52, 86], "v4": 52, "23": [52, 85], "src": 52, "l104": 52, "warmup": [52, 73], "phase": 52, "total": [52, 65, 77, 81, 83, 85, 86], "wave": 52, "half": 52, "index": [52, 72, 79, 83], "last": 52, "lr_schedul": 52, "peft": [53, 54, 55, 56, 86, 87], "protocol": 53, "adapter_param": [53, 54, 55, 56], "correspond": [53, 55, 63, 84, 85], "proj": 53, "in_dim": [53, 54, 86, 87], "out_dim": [53, 54, 86, 87], "bia": [53, 54, 86, 87], "loralinear": [53, 86, 87], "alpha": [54, 86, 87], "use_bia": 54, "larg": [54, 87], "perturb": 54, "decomposit": [54, 86], "matric": [54, 86, 87], "trainabl": [54, 56, 86, 87], "mapsto": 54, "w_0x": 54, "r": [54, 58, 86], "bax": 54, "probabl": [54, 83], "lora_a": [54, 86, 87], "lora_b": [54, 86, 87], "subset": 55, "get_adapter_param": [56, 86], "sentencepieceprocessor": 57, "pretrain": [57, 58, 84, 86, 87], "non": 57, "spm_model": 57, "tokenized_text": 57, "hello": [57, 83, 85], "world": [57, 65, 83], "add_bo": [57, 58], "add_eo": [57, 58], "31587": 57, "29644": 57, "102": 57, "text": [57, 58, 82, 83], "trim_leading_whitespac": 57, "prefix": 57, "unbatch": 57, "prepend": [57, 58], "bo": [57, 58], "append": [57, 79], "eo": [57, 58], "trim": 57, "lead": 57, "whitespac": 57, "underli": [57, 87], "sentencepiec": [57, 85], "s1": 57, "s2": 57, "due": [57, 86, 87], "tokenize_messag": [57, 58, 82], "concaten": 57, "problem": 57, "slice": 57, "tokenizer_path": 57, "separ": [57, 59, 84, 85, 86, 87], "concat": 57, "1788": 57, "2643": 57, "13": [57, 83, 85, 87], "1792": 57, "9508": 57, "465": 57, "22137": 57, "2933": 57, "join": 57, "attribut": 57, "llama3_tiktoken": 58, "p": [58, 86, 87], "l": 58, "all_special_token": 58, "bos_token": 58, "begin_of_text": 58, "eos_token": 58, "end_of_text": 58, "start_header_id": 58, "end_header_id": 58, "step_id": 58, "eom_id": 58, "eot_id": 58, "python_tag": 58, "tiktoken": [58, 85], "identif": 58, "regex": 58, "special": 58, "element": [58, 83], "second": [58, 83, 85, 86, 87], "uniqu": 58, "256": [58, 82, 83, 85], "header": 58, "token_id": 58, "truncate_at_eo": 58, "tokenize_head": 58, "co": [59, 60, 83], "few": [59, 82, 85, 86, 87], "0001_of_0003": 59, "0002_of_0003": 59, "preserv": [59, 87], "weight_map": [59, 83], "intermediate_checkpoint": [59, 60], "parit": 59, "_weight_map": 59, "shard": [60, 85], "wip": 60, "argpars": 61, "argumentpars": 61, "builtin": 61, "said": 61, "noth": 61, "treat": 61, "still": [61, 86, 87], "consult": 61, "doc": [61, 64, 70, 71, 73, 75, 83], "info": [61, 84], "librari": [61, 64, 75, 78, 80, 87], "html": [61, 64, 70, 73, 75], "parse_known_arg": 61, "namespac": 61, "act": 61, "precid": 61, "parse_arg": 61, "intern": 61, "properti": [61, 86], "too": [61, 85], "availab": 62, "machin": [62, 83], "distribut": [62, 66, 74, 75, 80, 84, 85], "bf16": [63, 87], "request": [63, 82, 83], "inde": [63, 83], "kernel": 63, "runtimeerror": [63, 66], "float32": 63, "done": [63, 86, 87], "isn": 63, "hardwar": [63, 80, 83, 86], "stream": 64, "handler": 64, "aka": 65, "filenam": 68, "log_": 68, "unixtimestamp": 68, "txt": [68, 84], "thread": 68, "safe": 68, "flush": [68, 69, 70, 71], "union": [68, 69, 70, 71, 75], "ndarrai": [68, 69, 70, 71], "scalar": [68, 69, 70, 71], "record": [68, 69, 70, 71], "payload": [68, 69, 70, 71], "dictionari": [68, 69, 70, 71, 83], "organize_log": 70, "tensorboard": 70, "stabl": [70, 73, 75, 79], "subdirectori": 70, "sub": 70, "compar": [70, 83, 86, 87], "logdir": 70, "startup": 70, "recurs": 70, "tree": [70, 82, 83], "tfevent": 70, "encount": 70, "frontend": 70, "organ": 70, "accordingli": 70, "my_log_dir": 70, "view": [70, 83, 84], "my_metr": [70, 71], "termin": [70, 71], "entiti": 71, "bias": 71, "ref": 71, "sent": 71, "usernam": 71, "individu": 71, "my_project": 71, "my_ent": 71, "my_group": 71, "importerror": 71, "account": [71, 86, 87], "log_config": 71, "local": [71, 75, 79, 83, 84], "link": [71, 83], "capecap": 71, "6053ofw0": 71, "torchtune_config_j67sb73v": 71, "padding_idx": 72, "ignore_idx": 72, "pad": 72, "longest": 72, "integ": [72, 75], "tokenpair": 72, "collat": 72, "token_pair": 72, "torchtune_perf_trac": 73, "contextmanag": 73, "wait": 73, "trace": 73, "speed": [73, 85, 87], "reduct": [73, 86], "auto_wrap_polici": 74, "polici": 74, "debug_mod": 75, "pseudo": 75, "random": [75, 84], "commonli": [75, 83, 86, 87], "numpi": 75, "own": [75, 82, 83, 86], "determinist": 75, "global": 75, "warn": 75, "nondeterminist": 75, "addition": [75, 86], "cudnn": 75, "disabl": 75, "set_deterministic_debug_mod": 75, "algorithm": 75, "outsid": [75, 83, 85, 86], "generated_examples_python": 76, "zip": 76, "galleri": [76, 81], "sphinx": 76, "000": [77, 81, 85], "execut": [77, 81], "generated_exampl": 77, "mem": [77, 81], "mb": [77, 81], "topic": 78, "gentl": 78, "introduct": 78, "readi": 78, "maxim": [78, 80], "workflow": [78, 82, 84, 86], "requisit": 79, "proper": [79, 84], "host": [79, 84], "page": [79, 80, 84, 85], "latest": [79, 84, 87], "confirm": 79, "And": [79, 83, 85], "h": 79, "ls": [79, 83, 84, 85], "welcom": 79, "show": [79, 86], "greatest": [79, 84], "contributor": 79, "cd": [79, 83], "even": [79, 85, 86, 87], "commit": 79, "branch": 79, "extra": [79, 86, 87], "url": 79, "whl": 79, "therebi": [79, 87], "howev": 79, "forc": 79, "reinstal": 79, "opt": [79, 84], "suffix": 79, "cu121": 79, "On": [80, 86], "pointer": 80, "author": [80, 84, 87], "emphas": 80, "aspect": 80, "simplic": 80, "component": 80, "reus": 80, "high": [80, 86], "prove": 80, "democrat": 80, "box": [80, 87], "zoo": 80, "varieti": [80, 86], "techniqu": [80, 83, 84, 86], "integr": [80, 83, 84, 85, 86, 87], "excit": 80, "checkout": 80, "quickstart": 80, "attain": 80, "better": [80, 82, 83], "chekckpoint": 80, "hyperparamet": [80, 84, 86, 87], "embodi": 80, "philosophi": 80, "especi": [80, 83], "usabl": 80, "composit": 80, "hard": 80, "outlin": 80, "unecessari": 80, "never": 80, "thoroughli": 80, "unit": 80, "know": [82, 83, 85, 86], "steer": 82, "wheel": 82, "publicli": 82, "great": [82, 83], "sever": 82, "wide": 82, "bootstrap": 82, "indic": 82, "iter": [82, 87], "knob": 82, "tweak": 82, "sai": [82, 84], "footprint": [82, 86], "could": [82, 86], "achiev": [82, 83, 85, 86, 87], "fix": 82, "goal": 82, "flavor": 82, "agnost": 82, "condit": 82, "respond": 82, "alpacainstructtempl": 82, "describ": 82, "further": [82, 86, 87], "classifi": 82, "anim": 82, "plant": 82, "miner": 82, "oak": 82, "copper": 82, "ore": 82, "eleph": 82, "instructtempl": 82, "instruct_dataset": 82, "mydataset": 82, "onthehub": 82, "customtempl": 82, "similar": [82, 83, 85, 86, 87], "quit": [82, 87], "similarli": 82, "chat_dataset": 82, "conversation_styl": 82, "sharegpt": 82, "formatted_messag": 82, "nyou": 82, "incorpor": 82, "advanc": 82, "preferencedataset": 82, "rlhf": 82, "adjust": 82, "chosen": 82, "reject": 82, "chosen_messag": 82, "transformed_sampl": 82, "key_chosen": 82, "rejected_messag": 82, "key_reject": 82, "chosen_input_id": 82, "c_mask": 82, "chosen_label": 82, "np": 82, "cross_entropy_ignore_idx": 82, "rejected_input_id": 82, "r_mask": 82, "rejected_label": 82, "purpos": [82, 84, 85], "stack_exchanged_paired_dataset": 82, "had": 82, "lvwerra": 82, "stack": 82, "exchang": 82, "stackexchangedpairedtempl": 82, "question": [82, 83, 85], "response_j": 82, "response_k": 82, "data_dir": 82, "rl": 82, "favorit": [83, 85, 86], "commun": 83, "seemlessli": 83, "beyond": [83, 87], "connect": 83, "larger": [83, 85], "might": 83, "amount": 83, "natur": 83, "export": 83, "mobil": 83, "phone": 83, "leverag": [83, 85, 87], "mode": 83, "lot": 83, "plai": 83, "freez": [83, 86], "percentag": 83, "learnabl": 83, "keep": [83, 86], "16gb": [83, 86], "rtx": 83, "3090": 83, "4090": 83, "hour": 83, "full_finetune_single_devic": [83, 84], "7b_full_low_memori": [83, 84], "full_finetune_distribut": [83, 84], "7b_full": [83, 84], "13b_full": [83, 84], "7b_qlora_single_devic": [83, 84, 87], "473": 83, "98": [83, 87], "gb": [83, 85, 86, 87], "50": 83, "484": 83, "01": [83, 84], "fact": [83, 85, 86], "third": 83, "smaller": [83, 85, 86, 87], "But": [83, 85, 86], "realli": 83, "eleuther_ev": [83, 85], "eleuther_evalu": [83, 85], "lm_eval": [83, 85], "plan": 83, "copi": [83, 84, 85, 87], "custom_eval_config": [83, 85], "truthfulqa_mc2": [83, 85, 86], "measur": [83, 85], "propens": [83, 85], "answer": [83, 85], "shot": [83, 85], "accuraci": [83, 85, 86, 87], "baselin": [83, 86], "324": 83, "loglikelihood": 83, "195": 83, "121": 83, "27": 83, "197": 83, "acc": 83, "388": 83, "38": 83, "shown": 83, "489": 83, "48": [83, 87], "seem": 83, "custom_generation_config": [83, 85], "kick": 83, "top_k": 83, "300": 83, "temperatur": 83, "interest": 83, "site": 83, "visit": 83, "bai": 83, "area": 83, "92": [83, 85], "exploratorium": 83, "san": 83, "francisco": 83, "magazin": 83, "awesom": 83, "bridg": 83, "pretti": 83, "cool": 83, "96": [83, 87], "61": 83, "sec": [83, 85], "25": 83, "83": 83, "99": [83, 86], "15": [83, 86, 87], "72": 83, "littl": 83, "saw": 83, "took": [83, 85], "torchao": [83, 85, 87], "bit": [83, 85, 86, 87], "custom_quantization_config": [83, 85], "68": 83, "19": [83, 85, 87], "76": 83, "69": 83, "95": [83, 85], "67": 83, "4w": [83, 85], "unlik": [83, 85], "won": [83, 85], "engin": [83, 85], "fullmodeltorchtunecheckpoint": [83, 85], "int4weightonlyquant": [83, 85], "groupsiz": [83, 85], "did": [83, 85, 87], "park": 83, "sit": 83, "top": [83, 87], "hill": 83, "beauti": 83, "62": [83, 85], "17": [83, 86], "85": 83, "compil": [83, 85, 87], "hood": [83, 87], "sped": 83, "almost": [83, 85, 86], "3x": [83, 85], "benefit": 83, "yet": 83, "fast": 83, "clone": [83, 86, 87], "assumpt": 83, "satisfi": 83, "new_dir": 83, "output_dict": 83, "sd_1": 83, "sd_2": 83, "dump": 83, "convert_hf_checkpoint": 83, "checkpoint_path": 83, "my": [83, 85], "justin": 83, "school": 83, "math": 83, "teacher": 83, "ws": 83, "94": [83, 85], "103": 83, "28": 83, "bandwidth": [83, 85], "1391": 83, "84": 83, "thats": 83, "seamlessli": 83, "authent": [83, 84], "hopefulli": 83, "gave": 83, "launch": 84, "gate": 84, "grant": 84, "minut": 84, "agreement": 84, "altern": 84, "hackabl": 84, "singularli": 84, "focus": 84, "technic": 84, "depth": 84, "why": [84, 86], "principl": 84, "minim": [84, 86, 87], "boilerpl": 84, "hold": 84, "substanti": [84, 86], "custom_config": 84, "replic": 84, "lorafinetunerecipesingledevic": 84, "lora_finetune_output": 84, "log_1713194212": 84, "sampler": 84, "52": 84, "3697006702423096": 84, "25880": [84, 87], "24": [84, 85], "55": 84, "83it": 84, "were": 84, "monitor": 84, "tqdm": 84, "interv": 84, "e2": 84, "releas": 85, "128": [85, 86], "intermedi": [85, 87], "theta": 85, "gain": 85, "illustr": 85, "basic": 85, "8b_lora_single_devic": 85, "observ": 85, "18": 85, "consum": [85, 87], "vram": [85, 86], "overal": 85, "nproc_per_nod": [85, 86], "lora_finetune_distribut": [85, 86], "8b_lora": 85, "8b_qlora_single_devic": 85, "alloc": [85, 87], "coupl": [85, 86, 87], "llama3_token": 85, "122": 85, "sarah": 85, "busi": 85, "mum": 85, "young": 85, "children": 85, "live": 85, "north": 85, "east": 85, "england": 85, "135": 85, "88": 85, "138": 85, "346": 85, "09": 85, "139": 85, "31": 85, "been": 85, "far": 85, "drill": 85, "90": 85, "93": 85, "91": 85, "104": 85, "four": [85, 86], "again": 85, "jake": 85, "disciplin": 85, "artist": 85, "passion": 85, "draw": 85, "paint": 85, "57": [85, 86, 87], "speedup": 85, "broader": 85, "teach": 86, "straight": 86, "jump": 86, "neural": [86, 87], "unfamiliar": 86, "oppos": [86, 87], "momentum": 86, "adamw": 86, "arbitrari": 86, "relat": 86, "aghajanyan": 86, "et": 86, "al": 86, "hypothes": 86, "intrins": 86, "lower": 86, "down": [86, 87], "often": 86, "eight": 86, "practic": 86, "imag": 86, "simplifi": 86, "represent": [86, 87], "left": 86, "blue": 86, "rememb": 86, "approx": 86, "15m": 86, "8192": 86, "65k": 86, "requires_grad": [86, 87], "frozen_out": [86, 87], "lora_out": [86, 87], "omit": 86, "construct": 86, "base_model": 86, "choos": 86, "lora_model": 86, "lora_llama_2_7b": [86, 87], "alon": 86, "in_featur": 86, "out_featur": 86, "inplac": 86, "feel": 86, "free": 86, "strict": 86, "whenev": 86, "validate_state_dict_for_lora": 86, "peft_util": 86, "set_trainable_param": 86, "fetch": 86, "lora_param": 86, "total_param": 86, "sum": 86, "numel": 86, "trainable_param": 86, "2f": 86, "6742609920": 86, "4194304": 86, "nnode": 86, "7b_lora": 86, "my_model_checkpoint_path": [86, 87], "tokenizer_checkpoint": [86, 87], "my_tokenizer_checkpoint_path": [86, 87], "constraint": 86, "factori": 86, "benefici": 86, "long": 86, "impact": 86, "rel": 86, "minor": 86, "good": 86, "64": 86, "lora_experiment_1": 86, "smooth": [86, 87], "curv": [86, 87], "500": 86, "ran": 86, "commod": 86, "cogniz": 86, "ax": 86, "parallel": 86, "truthfulqa": 86, "previous": 86, "475": 86, "87": 86, "508": 86, "86": 86, "504": 86, "04": 86, "514": 86, "lowest": 86, "absolut": 86, "4gb": 86, "tradeoff": 86, "potenti": 86, "enhanc": 87, "maintain": 87, "highli": 87, "part": 87, "vanilla": 87, "held": 87, "therefor": 87, "bespok": 87, "normalfloat": 87, "8x": 87, "retain": 87, "vast": 87, "major": 87, "highlight": 87, "degrad": 87, "normatfloat": 87, "doubl": 87, "themselv": 87, "prune": 87, "deepdiv": 87, "idea": 87, "distinct": 87, "storag": 87, "datatyp": 87, "de": 87, "incur": 87, "counterpart": 87, "set_default_devic": 87, "qlora_linear": 87, "memory_alloc": 87, "177": 87, "152": 87, "byte": 87, "del": 87, "empty_cach": 87, "lora_linear": 87, "081": 87, "344": 87, "qlora_llama2_7b": 87, "qlora_model": 87, "essenti": 87, "reparametrize_as_dtype_state_dict_post_hook": 87, "entir": 87, "stat": 87, "reserv": 87, "against": 87, "35": 87, "40": 87, "29": 87, "slow": 87, "slower": 87, "149": 87, "9157477021217346": 87, "02": 87, "08": 87, "14": 87, "15it": 87, "thing": 87, "nightli": 87, "200": 87, "hundr": 87, "228": 87, "8158286809921265": 87, "59": 87, "95it": 87, "exercis": 87, "manual": 87, "portion": 87, "augment": 87, "linear_nf4": 87, "to_nf4": 87, "linear_weight": 87, "autograd": 87, "regular": 87, "incom": 87}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "MistralChatFormat"], [20, 1, 1, "", "SummarizeTemplate"], [21, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.MistralChatFormat": [[19, 2, 1, "", "format"], [19, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[20, 2, 1, "", "format"]], "torchtune.datasets": [[22, 0, 1, "", "alpaca_cleaned_dataset"], [23, 0, 1, "", "alpaca_dataset"], [24, 0, 1, "", "grammar_dataset"], [25, 0, 1, "", "samsum_dataset"], [26, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[27, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[28, 0, 1, "", "llama2_13b"], [29, 0, 1, "", "llama2_70b"], [30, 0, 1, "", "llama2_7b"], [31, 0, 1, "", "lora_llama2_13b"], [32, 0, 1, "", "lora_llama2_70b"], [33, 0, 1, "", "lora_llama2_7b"], [34, 0, 1, "", "qlora_llama2_13b"], [35, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[36, 0, 1, "", "llama3_70b"], [37, 0, 1, "", "llama3_8b"], [38, 0, 1, "", "lora_llama3_70b"], [39, 0, 1, "", "lora_llama3_8b"], [40, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[41, 0, 1, "", "lora_mistral_7b"], [42, 0, 1, "", "mistral_7b"], [43, 0, 1, "", "qlora_mistral_7b"]], "torchtune.modules": [[44, 1, 1, "", "CausalSelfAttention"], [45, 1, 1, "", "FeedForward"], [46, 1, 1, "", "KVCache"], [47, 1, 1, "", "RMSNorm"], [48, 1, 1, "", "RotaryPositionalEmbeddings"], [49, 1, 1, "", "TransformerDecoder"], [50, 1, 1, "", "TransformerDecoderLayer"], [52, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[44, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[45, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[47, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[48, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[49, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[50, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[51, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[53, 1, 1, "", "AdapterModule"], [54, 1, 1, "", "LoRALinear"], [55, 0, 1, "", "get_adapter_params"], [56, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[53, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[54, 2, 1, "", "adapter_params"], [54, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[57, 1, 1, "", "SentencePieceTokenizer"], [58, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[57, 2, 1, "", "decode"], [57, 2, 1, "", "encode"], [57, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[58, 2, 1, "", "decode"], [58, 2, 1, "", "encode"], [58, 2, 1, "", "tokenize_message"], [58, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[59, 1, 1, "", "FullModelHFCheckpointer"], [60, 1, 1, "", "FullModelMetaCheckpointer"], [61, 1, 1, "", "TuneRecipeArgumentParser"], [62, 0, 1, "", "get_device"], [63, 0, 1, "", "get_dtype"], [64, 0, 1, "", "get_logger"], [65, 0, 1, "", "get_world_size_and_rank"], [66, 0, 1, "", "init_distributed"], [67, 0, 1, "", "list_dtypes"], [72, 0, 1, "", "padded_collate"], [73, 0, 1, "", "profiler"], [74, 0, 1, "", "set_activation_checkpointing"], [75, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[59, 2, 1, "", "load_checkpoint"], [59, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[60, 2, 1, "", "load_checkpoint"], [60, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[61, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[68, 1, 1, "", "DiskLogger"], [69, 1, 1, "", "StdoutLogger"], [70, 1, 1, "", "TensorBoardLogger"], [71, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[68, 2, 1, "", "close"], [68, 2, 1, "", "log"], [68, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[69, 2, 1, "", "close"], [69, 2, 1, "", "log"], [69, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[70, 2, 1, "", "close"], [70, 2, 1, "", "log"], [70, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[71, 2, 1, "", "close"], [71, 2, 1, "", "log"], [71, 2, 1, "", "log_config"], [71, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 78, 80, 83, 85, 86, 87], "config": [0, 7, 8, 84], "data": [1, 5], "dataset": [2, 82], "model": [3, 4, 9, 83, 84, 85, 86], "llama3": [3, 85], "llama2": [3, 83, 86, 87], "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 79, 87], "block": 4, "token": 4, "peft": 4, "util": [4, 5], "checkpoint": [5, 6, 9, 83], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 86, 87], "manag": 5, "perform": [5, 86], "profil": [5, 73], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 80, 83], "format": [6, 82], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 83, 86, 87], "put": [6, 87], "thi": 6, "all": [6, 7, 87], "togeth": [6, 87], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 82], "us": [7, 8, 83, 87], "instanti": [7, 10], "referenc": 7, "other": [7, 83], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 83, 84], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 80, 86, 87], "ar": 8, "recip": [8, 84, 86], "script": 8, "class": 8, "run": [8, 83], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "mistralchatformat": 19, "summarizetempl": 20, "validate_messag": 21, "alpaca_cleaned_dataset": 22, "alpaca_dataset": 23, "grammar_dataset": 24, "samsum_dataset": 25, "slimorca_dataset": 26, "gemma_2b": 27, "llama2_13b": 28, "llama2_70b": 29, "llama2_7b": 30, "lora_llama2_13b": 31, "lora_llama2_70b": 32, "lora_llama2_7b": 33, "qlora_llama2_13b": 34, "qlora_llama2_7b": 35, "llama3_70b": 36, "llama3_8b": 37, "lora_llama3_70b": 38, "lora_llama3_8b": 39, "qlora_llama3_8b": 40, "lora_mistral_7b": 41, "mistral_7b": 42, "qlora_mistral_7b": 43, "causalselfattent": 44, "todo": [44, 50], "feedforward": 45, "kvcach": 46, "rmsnorm": 47, "rotarypositionalembed": 48, "transformerdecod": 49, "transformerdecoderlay": 50, "reparametrize_as_dtype_state_dict_post_hook": 51, "get_cosine_schedule_with_warmup": 52, "adaptermodul": 53, "loralinear": 54, "get_adapter_param": 55, "set_trainable_param": 56, "sentencepiecetoken": 57, "tiktokentoken": 58, "fullmodelhfcheckpoint": 59, "fullmodelmetacheckpoint": 60, "tunerecipeargumentpars": 61, "get_devic": 62, "get_dtyp": 63, "get_logg": 64, "get_world_size_and_rank": 65, "init_distribut": 66, "list_dtyp": 67, "disklogg": 68, "stdoutlogg": 69, "tensorboardlogg": 70, "wandblogg": 71, "padded_col": 72, "set_activation_checkpoint": 74, "set_se": 75, "comput": [77, 81], "time": [77, 81], "welcom": 78, "document": 78, "get": [78, 85], "start": 78, "tutori": 78, "instal": 79, "instruct": [79, 82], "via": [79, 85], "pypi": 79, "git": 79, "clone": 79, "nightli": 79, "kei": 80, "concept": 80, "design": 80, "principl": 80, "fine": [82, 84, 85], "tune": [82, 84, 85], "custom": 82, "templat": 82, "chat": 82, "fulli": 82, "end": 83, "workflow": 83, "download": [83, 84], "7b": 83, "finetun": [83, 86, 87], "evalu": [83, 85], "eleutherai": [83, 85], "s": [83, 85], "eval": [83, 85], "har": [83, 85], "gener": [83, 85], "speed": 83, "up": 83, "quantiz": [83, 85], "librari": 83, "upload": 83, "hug": 83, "face": 83, "hub": 83, "first": 84, "llm": 84, "select": 84, "modifi": 84, "train": 84, "next": 84, "step": 84, "8b": 85, "access": 85, "text": 85, "our": 85, "faster": 85, "how": 86, "doe": 86, "work": 86, "appli": 86, "trade": 86, "off": 86, "qlora": 87, "save": 87, "deep": 87, "dive": 87, "from": 87}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})