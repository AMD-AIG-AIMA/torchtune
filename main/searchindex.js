Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.Role", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.loss.IPOLoss", "generated/torchtune.modules.loss.PPOLoss", "generated/torchtune.modules.loss.RSOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.left_padded_collate", "generated/torchtune.modules.rlhf.padded_collate_dpo", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.loss.IPOLoss.rst", "generated/torchtune.modules.loss.PPOLoss.rst", "generated/torchtune.modules.loss.RSOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.left_padded_collate.rst", "generated/torchtune.modules.rlhf.padded_collate_dpo.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "torchtune.data.GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.Role", "StackExchangedPairedTemplate", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "left_padded_collate", "padded_collate_dpo", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 21, 25, 33, 34, 35, 36, 39, 43, 47, 88, 114, 121, 122, 123, 198, 202, 203, 206, 208, 209, 210], "prompt": [1, 14, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 31, 33, 35, 36, 39, 40, 42, 43, 44, 45, 62, 72, 88, 106, 118, 131, 157, 172, 204, 205, 207], "chat": [1, 2, 15, 16, 19, 27, 28, 31, 36, 40, 45, 72, 123], "includ": [1, 6, 7, 8, 15, 18, 22, 23, 36, 52, 63, 72, 73, 89, 98, 111, 123, 141, 152, 161, 165, 166, 170, 200, 202, 203, 204, 205, 206, 207, 208, 210], "some": [1, 6, 7, 16, 109, 143, 144, 198, 200, 202, 203, 204, 205, 206, 208, 209, 210], "specif": [1, 4, 7, 8, 10, 36, 42, 44, 153, 175, 203, 204, 205, 209, 210], "format": [1, 2, 5, 14, 15, 16, 18, 19, 20, 21, 25, 27, 31, 33, 35, 36, 39, 40, 42, 43, 44, 45, 72, 88, 153, 162, 165, 166, 167, 168, 202, 203, 205, 206, 207, 208], "differ": [1, 7, 9, 32, 33, 35, 49, 50, 51, 106, 133, 136, 150, 154, 168, 195, 200, 202, 203, 205, 207, 208, 209, 210], "dataset": [1, 5, 7, 14, 18, 20, 25, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 136, 137, 200, 206, 207, 209], "model": [1, 2, 6, 7, 8, 10, 16, 20, 21, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 152, 153, 156, 157, 158, 165, 166, 167, 168, 171, 172, 175, 177, 183, 189, 190, 198, 200, 203, 204, 210], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 18, 19, 20, 25, 28, 31, 32, 33, 34, 35, 36, 37, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 64, 65, 74, 75, 76, 77, 92, 106, 112, 114, 123, 126, 131, 132, 133, 135, 136, 137, 139, 140, 143, 146, 154, 156, 159, 161, 162, 165, 166, 167, 169, 170, 171, 172, 186, 187, 189, 197, 199, 201, 202, 204, 205, 206, 207, 208, 209], "common": [1, 2, 4, 7, 157, 202, 203, 204, 207, 208, 209], "json": [1, 6, 27, 28, 31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 92, 123, 156, 165, 202, 204, 205, 209], "messag": [1, 15, 16, 19, 21, 22, 23, 27, 28, 30, 31, 36, 40, 42, 44, 62, 72, 88, 106, 118, 153, 157, 199, 202, 203, 204], "miscellan": 1, "function": [1, 4, 6, 7, 8, 10, 12, 31, 50, 51, 52, 125, 126, 133, 134, 136, 138, 142, 145, 146, 150, 164, 165, 172, 173, 179, 183, 192, 200, 203, 204, 210], "us": [1, 2, 3, 4, 6, 9, 10, 12, 16, 19, 20, 22, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 72, 73, 79, 89, 92, 93, 98, 101, 119, 123, 125, 126, 129, 130, 131, 132, 133, 134, 136, 138, 142, 145, 147, 148, 154, 155, 159, 160, 162, 164, 165, 166, 168, 169, 170, 172, 173, 174, 175, 177, 183, 184, 185, 186, 187, 192, 198, 199, 200, 202, 204, 206, 207, 208, 209], "modifi": [1, 7, 8, 9, 134, 200, 205, 207, 208, 209, 210], "For": [2, 5, 6, 7, 8, 22, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 63, 67, 72, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 131, 133, 160, 161, 165, 170, 171, 178, 187, 190, 192, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210], "detail": [2, 6, 31, 33, 35, 36, 37, 38, 40, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 72, 113, 128, 133, 138, 164, 175, 183, 192, 202, 204, 205, 206, 207, 208, 209, 210], "usag": [2, 134, 168, 169, 193, 199, 202, 204, 205, 206, 207, 209, 210], "guid": [2, 7, 9, 200, 203, 204, 206, 208], "pleas": [2, 5, 17, 26, 49, 50, 51, 52, 59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 133, 164, 175, 183, 190, 199, 205, 207, 210], "see": [2, 5, 6, 9, 17, 19, 21, 26, 31, 33, 35, 36, 37, 38, 40, 42, 43, 44, 45, 47, 48, 59, 60, 61, 70, 71, 72, 84, 85, 86, 87, 96, 97, 104, 105, 113, 116, 117, 124, 128, 133, 140, 164, 168, 170, 175, 176, 183, 187, 190, 192, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "our": [2, 6, 8, 200, 203, 204, 205, 206, 208, 209, 210], "tutori": [2, 6, 72, 190, 200, 203, 204, 205, 206, 207, 208, 209, 210], "support": [2, 3, 6, 8, 9, 10, 20, 21, 31, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 67, 79, 93, 101, 107, 109, 119, 122, 123, 125, 127, 133, 139, 141, 160, 162, 166, 167, 169, 174, 177, 178, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "sever": 2, "wide": 2, "help": [2, 6, 19, 131, 133, 165, 170, 198, 199, 200, 202, 203, 204, 205, 206, 209, 210], "quickli": [2, 7, 22, 37, 203, 204], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 22, 25, 31, 37, 50, 51, 52, 72, 133, 186, 187, 198, 199, 200, 202, 203, 204, 207, 208, 209, 210], "fine": [2, 6, 8, 9, 20, 34, 36, 47, 72, 198, 200, 205, 208, 209], "tune": [2, 3, 6, 7, 8, 9, 12, 20, 34, 36, 47, 72, 198, 199, 200, 202, 205, 208, 209, 210], "also": [2, 6, 7, 8, 9, 10, 32, 40, 43, 47, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 123, 125, 131, 141, 173, 175, 177, 183, 187, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210], "like": [2, 4, 6, 7, 8, 9, 31, 123, 133, 167, 199, 202, 203, 204, 205, 206, 208, 209], "These": [2, 4, 6, 7, 8, 10, 34, 133, 159, 170, 203, 204, 205, 206, 207, 208, 209, 210], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 18, 19, 21, 22, 23, 25, 30, 33, 34, 35, 36, 39, 40, 42, 43, 44, 50, 56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 131, 133, 141, 142, 145, 146, 148, 150, 159, 161, 164, 165, 166, 168, 169, 171, 172, 174, 177, 181, 183, 193, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "especi": [2, 200, 202, 205], "specifi": [2, 6, 7, 8, 10, 36, 40, 73, 79, 89, 93, 98, 101, 125, 131, 132, 164, 172, 175, 178, 183, 187, 190, 193, 202, 203, 204, 205, 206, 207, 209, 210], "yaml": [2, 7, 8, 10, 11, 12, 32, 40, 43, 47, 170, 187, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "config": [2, 6, 9, 10, 11, 12, 13, 32, 40, 43, 47, 125, 145, 165, 169, 170, 187, 193, 200, 203, 204, 205, 207, 208, 209, 210], "represent": [2, 208, 209, 210], "abov": [2, 3, 6, 134, 161, 181, 199, 205, 207, 208, 209, 210], "all": [3, 4, 8, 13, 22, 32, 34, 36, 40, 52, 92, 123, 125, 126, 131, 133, 134, 142, 158, 160, 161, 165, 169, 170, 171, 181, 189, 195, 196, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209], "famili": [3, 6, 8, 39, 41, 45, 46, 48, 168, 200, 202, 207], "request": [3, 14, 174, 204, 205], "access": [3, 6, 7, 8, 32, 165, 171, 202, 204, 205, 206], "hug": [3, 6, 16, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 92, 123, 135, 156, 200, 202, 206, 207], "face": [3, 6, 16, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 92, 123, 135, 156, 200, 202, 206, 207], "To": [3, 6, 7, 8, 9, 34, 133, 165, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "download": [3, 6, 196, 199, 203, 204, 207, 208, 209, 210], "8b": [3, 91, 95, 97, 100, 103, 105, 120, 202, 203, 209], "meta": [3, 6, 19, 72, 88, 130, 165, 166, 202, 203, 205, 206], "hf": [3, 6, 118, 136, 137, 139, 165, 202, 203, 205, 206, 207], "token": [3, 6, 7, 8, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 62, 63, 66, 67, 72, 73, 78, 79, 88, 89, 92, 93, 98, 101, 106, 107, 109, 111, 113, 115, 118, 119, 121, 123, 125, 130, 131, 132, 133, 138, 148, 149, 151, 152, 153, 154, 155, 156, 157, 159, 172, 175, 188, 202, 204, 205, 206, 207, 208, 209, 210], "hf_token": 3, "70b": [3, 54, 57, 60, 75, 81, 85, 90, 94, 96, 99, 102, 104, 207], "ignor": [3, 6, 47, 118, 125, 126, 202], "pattern": [3, 155, 202], "origin": [3, 6, 38, 39, 134, 141, 203, 205, 207, 208, 209, 210], "consolid": [3, 6], "weight": [3, 6, 8, 56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 125, 134, 136, 140, 141, 145, 154, 165, 166, 167, 168, 178, 183, 187, 198, 202, 203, 205, 206, 207, 208, 209, 210], "you": [3, 6, 7, 8, 9, 10, 18, 19, 20, 22, 31, 33, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 133, 161, 168, 170, 172, 186, 187, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 22, 23, 32, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 72, 106, 129, 130, 133, 142, 154, 155, 160, 162, 164, 165, 168, 170, 175, 183, 186, 187, 190, 193, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "instead": [3, 6, 8, 34, 40, 43, 47, 52, 126, 128, 133, 141, 202, 207, 208, 209], "The": [3, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 19, 20, 21, 25, 30, 31, 32, 33, 34, 35, 36, 42, 44, 45, 46, 49, 50, 51, 52, 56, 57, 58, 62, 67, 68, 69, 72, 79, 80, 81, 82, 83, 88, 93, 94, 95, 101, 102, 103, 106, 107, 109, 118, 119, 120, 127, 129, 130, 133, 134, 135, 136, 137, 138, 139, 142, 147, 149, 152, 153, 154, 155, 156, 157, 159, 161, 162, 163, 164, 165, 167, 170, 173, 174, 176, 178, 187, 191, 193, 194, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "reus": [3, 200], "llama3_token": [3, 172, 203, 207], "builder": [3, 6, 38, 41, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 68, 69, 70, 71, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 108, 110, 112, 114, 116, 117, 120, 122, 124, 203, 204, 210], "class": [3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 31, 32, 33, 34, 35, 36, 37, 40, 43, 49, 50, 51, 62, 72, 77, 88, 106, 109, 113, 114, 118, 125, 126, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 143, 144, 152, 153, 154, 155, 158, 159, 165, 166, 167, 168, 169, 170, 184, 185, 186, 187, 203, 204, 206, 208, 210], "7b": [3, 6, 33, 35, 37, 39, 41, 43, 47, 48, 55, 58, 61, 65, 69, 76, 77, 82, 83, 86, 87, 108, 110, 112, 114, 117, 165, 166, 203, 206, 207, 208, 210], "2": [3, 6, 9, 30, 34, 45, 49, 50, 62, 72, 88, 106, 118, 125, 133, 137, 138, 149, 150, 151, 154, 155, 157, 160, 161, 162, 165, 166, 178, 188, 191, 192, 193, 194, 203, 205, 206, 207, 208, 209], "13b": [3, 6, 53, 56, 59, 74, 80, 84], "codellama": 3, "mini": [3, 118, 120, 121, 122, 123, 124], "4k": [3, 121, 122, 123], "microsoft": [3, 122, 123], "ai": [3, 36, 112, 125, 187, 203, 207], "v0": 3, "mistralai": [3, 202], "size": [3, 6, 8, 10, 39, 42, 44, 50, 51, 52, 125, 128, 129, 130, 131, 133, 147, 148, 159, 160, 162, 163, 179, 181, 200, 202, 204, 205, 206, 207, 208, 209], "2b": [3, 64, 68], "googl": [3, 64, 65], "gguf": 3, "vision": [3, 36, 52], "compon": [3, 6, 8, 13, 36, 150, 200, 204, 206, 208, 210], "multimod": [3, 20, 36], "encod": [3, 4, 36, 52, 62, 72, 88, 106, 118, 136, 152, 154, 155, 157, 159, 203], "perform": [4, 6, 34, 72, 126, 133, 142, 158, 172, 200, 203, 205, 207, 209, 210], "direct": [4, 8, 136, 150, 199], "text": [4, 20, 22, 23, 31, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 47, 48, 72, 88, 106, 118, 152, 154, 155, 157, 159, 203, 205, 209], "id": [4, 6, 31, 33, 34, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 72, 88, 106, 118, 125, 130, 131, 132, 149, 150, 152, 153, 154, 155, 156, 157, 159, 165, 167, 172, 188, 203, 204, 205], "decod": [4, 63, 67, 73, 79, 88, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 131, 152, 154, 155, 172, 203], "typic": [4, 7, 34, 36, 37, 47, 123, 136, 204, 209, 210], "byte": [4, 155, 210], "pair": [4, 7, 14, 46, 137, 150, 155, 188, 204], "underli": [4, 106, 154, 210], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 31, 33, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 134, 140, 143, 145, 152, 153, 162, 169, 170, 178, 199, 200, 204, 208, 210], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 18, 22, 25, 27, 28, 29, 31, 33, 35, 36, 37, 40, 41, 43, 47, 48, 51, 72, 106, 127, 134, 143, 144, 145, 146, 152, 153, 154, 157, 165, 166, 167, 169, 172, 180, 183, 192, 195, 202, 203, 204, 206, 208, 209], "preprocess": [4, 34, 133], "imag": [4, 20, 36, 49, 50, 51, 52, 133, 159, 160, 161, 162, 163, 208], "algorithm": [4, 147, 192], "ppo": [4, 136, 138, 147, 148], "offer": 5, "allow": [5, 32, 145, 161, 186, 202, 209, 210], "seamless": 5, "transit": 5, "between": [5, 6, 137, 138, 148, 165, 168, 204, 205, 207, 208, 209, 210], "train": [5, 6, 8, 9, 19, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 72, 125, 127, 130, 131, 132, 134, 135, 136, 165, 166, 167, 174, 177, 183, 193, 198, 200, 202, 203, 204, 205, 207, 208, 209, 210], "interoper": [5, 6, 8, 200, 205, 210], "rest": [5, 203, 210], "ecosystem": [5, 6, 8, 200, 205, 207, 210], "comprehens": 5, "overview": [5, 7, 9, 198, 206, 208, 210], "deep": [5, 6, 7, 8, 9, 200, 206, 207], "dive": [5, 6, 7, 8, 9, 200, 206, 207], "enabl": [5, 7, 8, 9, 32, 56, 57, 58, 59, 60, 61, 68, 69, 70, 71, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 102, 103, 104, 105, 108, 110, 116, 117, 120, 124, 141, 192, 193, 207, 208, 210], "work": [5, 6, 8, 170, 200, 202, 205, 207, 210], "set": [5, 6, 7, 8, 9, 20, 33, 34, 35, 37, 39, 41, 42, 43, 44, 45, 47, 48, 73, 79, 88, 89, 93, 98, 101, 107, 109, 111, 113, 118, 119, 121, 125, 130, 131, 142, 144, 162, 164, 173, 175, 181, 183, 190, 191, 192, 193, 200, 202, 203, 205, 206, 207, 208, 209], "consumpt": [5, 32], "dure": [5, 6, 33, 34, 39, 42, 44, 125, 128, 130, 131, 132, 133, 134, 177, 203, 205, 207, 208, 209, 210], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 29, 32, 33, 34, 35, 45, 52, 131, 133, 136, 142, 167, 170, 173, 175, 187, 193, 200, 202, 203, 204, 205, 206, 207], "debug": [5, 6, 7, 8, 202], "finetun": [5, 6, 7, 8, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 198, 200, 206, 207], "job": [5, 9, 192, 206], "variou": [5, 18], "walk": [6, 8, 186, 200, 203, 204, 205, 206, 209, 210], "through": [6, 7, 8, 9, 52, 126, 133, 142, 200, 202, 203, 204, 205, 206, 209, 210], "design": [6, 8], "behavior": [6, 183, 203, 204], "associ": [6, 7, 8, 52, 63, 73, 89, 98, 111, 172, 205, 208], "util": [6, 7, 8, 9, 10, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 200, 205, 206, 207, 209, 210], "what": [6, 7, 9, 19, 21, 25, 36, 42, 44, 133, 198, 203, 204, 205, 206, 207], "cover": [6, 7, 8, 9, 203, 205, 210], "how": [6, 7, 8, 9, 133, 164, 190, 198, 202, 203, 204, 205, 206, 207, 209, 210], "we": [6, 7, 8, 9, 33, 34, 35, 36, 37, 39, 41, 43, 47, 48, 72, 106, 125, 128, 130, 131, 133, 136, 141, 160, 161, 165, 166, 167, 172, 174, 178, 183, 189, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "them": [6, 7, 32, 33, 35, 43, 62, 72, 106, 118, 126, 133, 134, 157, 202, 203, 204, 205, 208, 209, 210], "scenario": [6, 32, 36], "full": [6, 7, 8, 17, 26, 40, 43, 48, 59, 60, 61, 62, 70, 71, 72, 84, 85, 86, 87, 96, 97, 104, 105, 106, 116, 117, 118, 124, 145, 146, 157, 200, 202, 204, 205, 207, 208, 209], "compos": [6, 133], "which": [6, 7, 8, 32, 33, 34, 37, 39, 42, 44, 47, 56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 88, 93, 94, 95, 101, 102, 103, 106, 107, 108, 109, 110, 119, 120, 125, 130, 131, 132, 133, 135, 145, 146, 154, 161, 165, 166, 167, 169, 174, 184, 187, 190, 200, 202, 203, 204, 205, 206, 208, 209, 210], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 126, 145, 165, 166, 167, 200, 203, 204, 205, 207, 210], "evalu": [6, 8, 198, 200, 206, 208, 210], "gener": [6, 8, 14, 25, 31, 33, 34, 35, 41, 45, 47, 72, 106, 142, 147, 191, 192, 193, 196, 198, 203, 204, 208, 209, 210], "each": [6, 8, 15, 18, 22, 23, 32, 34, 36, 49, 50, 51, 52, 56, 57, 58, 62, 67, 68, 69, 72, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 106, 107, 108, 109, 110, 118, 119, 120, 125, 130, 131, 132, 133, 136, 137, 139, 145, 146, 147, 148, 150, 157, 159, 161, 163, 192, 193, 200, 202, 204, 205, 206, 208, 209], "make": [6, 7, 8, 9, 125, 132, 133, 200, 202, 203, 205, 206, 207, 208, 209, 210], "easi": [6, 8, 200, 204, 208], "understand": [6, 7, 8, 198, 200, 203, 204, 208, 210], "extend": [6, 8, 200], "befor": [6, 22, 30, 33, 34, 35, 49, 50, 52, 63, 67, 125, 131, 132, 133, 141, 155, 165, 202, 205, 209], "let": [6, 7, 9, 202, 203, 204, 205, 206, 207, 208, 210], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 27, 28, 30, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 56, 57, 58, 62, 72, 79, 80, 81, 82, 83, 88, 93, 94, 95, 101, 102, 103, 106, 107, 108, 109, 110, 118, 119, 120, 123, 125, 128, 130, 131, 132, 133, 134, 136, 137, 139, 140, 143, 145, 146, 151, 155, 161, 164, 165, 166, 169, 173, 175, 177, 183, 186, 190, 191, 200, 202, 203, 204, 206, 208, 209, 210], "defin": [6, 7, 8, 22, 31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 126, 140, 141, 143, 148, 204, 206, 208], "concept": [6, 205, 206], "In": [6, 7, 8, 31, 50, 51, 52, 130, 133, 137, 141, 161, 164, 183, 186, 187, 203, 205, 207, 208, 209, 210], "ll": [6, 7, 8, 172, 178, 200, 203, 204, 205, 206, 207, 209, 210], "talk": 6, "about": [6, 8, 133, 136, 187, 200, 202, 203, 205, 206, 207, 208, 209, 210], "take": [6, 7, 8, 10, 36, 126, 128, 133, 134, 150, 165, 167, 170, 173, 203, 204, 205, 206, 207, 208, 210], "close": [6, 8, 184, 185, 186, 187, 208], "look": [6, 7, 8, 171, 186, 199, 203, 204, 205, 206, 207, 208, 209], "veri": [6, 32, 131, 202, 205], "simpli": [6, 7, 34, 36, 136, 137, 202, 203, 204, 205, 207, 210], "dictat": 6, "state_dict": [6, 134, 145, 165, 166, 167, 168, 169, 208, 210], "store": [6, 36, 184, 187, 208, 210], "file": [6, 7, 8, 9, 10, 11, 12, 31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 62, 72, 88, 92, 106, 118, 123, 154, 155, 156, 165, 166, 167, 170, 184, 187, 193, 197, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210], "disk": [6, 37, 184], "string": [6, 20, 22, 39, 40, 41, 45, 46, 62, 72, 88, 106, 118, 140, 152, 154, 155, 157, 173, 174, 178, 202, 204], "kei": [6, 7, 9, 14, 25, 27, 31, 33, 35, 36, 43, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 131, 132, 144, 145, 146, 150, 165, 167, 169, 193, 202, 205, 206, 208, 210], "identifi": 6, "state": [6, 8, 133, 134, 136, 143, 144, 145, 146, 147, 165, 166, 167, 169, 171, 205, 207, 208, 210], "dict": [6, 7, 8, 9, 10, 14, 18, 20, 22, 25, 27, 28, 31, 33, 35, 36, 37, 40, 41, 42, 43, 44, 47, 48, 88, 118, 134, 143, 144, 145, 146, 149, 150, 152, 153, 155, 156, 158, 165, 166, 167, 169, 171, 177, 180, 182, 188, 189, 204], "If": [6, 7, 13, 14, 18, 20, 21, 25, 27, 29, 30, 31, 33, 35, 36, 39, 42, 43, 44, 45, 52, 73, 79, 88, 89, 93, 98, 101, 118, 125, 130, 131, 132, 133, 134, 141, 146, 161, 162, 165, 166, 167, 168, 169, 172, 173, 174, 175, 177, 178, 180, 186, 187, 192, 193, 195, 199, 202, 203, 204, 205, 206, 207, 208, 209], "don": [6, 7, 8, 187, 192, 202, 203, 204, 205, 206, 210], "t": [6, 7, 8, 137, 174, 187, 192, 202, 203, 204, 205, 206, 210], "match": [6, 33, 35, 43, 118, 146, 161, 199, 202, 204, 205, 207, 208], "up": [6, 8, 9, 33, 34, 35, 37, 39, 41, 43, 47, 48, 155, 159, 160, 162, 171, 193, 202, 203, 204, 206, 207, 208, 210], "exactli": [6, 146, 209], "those": [6, 168, 205, 207, 208], "definit": [6, 208], "either": [6, 36, 146, 165, 172, 190, 202, 208, 209, 210], "run": [6, 7, 9, 12, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 126, 128, 131, 134, 165, 166, 167, 169, 171, 181, 186, 187, 189, 199, 200, 203, 204, 206, 207, 208, 209, 210], "explicit": 6, "error": [6, 7, 17, 30, 128, 165, 192, 202], "load": [6, 8, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 44, 45, 46, 47, 48, 145, 165, 166, 167, 169, 170, 186, 203, 204, 205, 207, 208], "rais": [6, 10, 13, 21, 27, 30, 31, 33, 40, 45, 118, 125, 128, 131, 133, 145, 146, 150, 157, 165, 166, 167, 169, 174, 177, 180, 187, 192, 195], "an": [6, 7, 8, 9, 10, 14, 30, 31, 32, 33, 37, 42, 44, 47, 48, 49, 50, 51, 79, 93, 101, 107, 109, 113, 119, 125, 128, 131, 133, 136, 140, 142, 143, 144, 159, 160, 161, 162, 164, 165, 166, 167, 169, 173, 175, 187, 193, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "except": [6, 20, 21, 157, 204], "wors": 6, "silent": [6, 126], "succe": 6, "infer": [6, 19, 31, 36, 63, 72, 111, 125, 128, 130, 131, 132, 173, 198, 203, 205, 206, 207, 209, 210], "expect": [6, 7, 10, 14, 18, 25, 31, 33, 35, 36, 40, 42, 43, 44, 130, 146, 169, 187, 195, 203, 204, 208, 209], "addit": [6, 7, 8, 10, 31, 33, 35, 36, 37, 40, 41, 43, 47, 48, 72, 136, 145, 164, 165, 166, 167, 174, 175, 180, 183, 184, 186, 187, 190, 200, 203, 206, 208], "line": [6, 8, 14, 170, 202, 204, 206, 207], "need": [6, 7, 8, 9, 18, 22, 31, 34, 36, 45, 125, 126, 131, 133, 183, 186, 187, 189, 199, 202, 203, 204, 205, 206, 207, 208, 210], "shape": [6, 49, 50, 51, 52, 125, 128, 130, 131, 132, 133, 136, 137, 138, 139, 141, 147, 148, 149, 151, 159, 161, 163, 172, 193], "valu": [6, 7, 28, 45, 53, 54, 55, 63, 64, 65, 67, 73, 74, 75, 76, 77, 79, 89, 90, 91, 93, 98, 99, 100, 101, 107, 109, 111, 112, 113, 114, 119, 121, 125, 128, 129, 131, 132, 135, 138, 145, 147, 148, 150, 151, 165, 168, 169, 170, 172, 184, 185, 186, 187, 192, 202, 204, 206, 207, 208, 209], "two": [6, 7, 30, 50, 133, 151, 159, 161, 200, 205, 206, 207, 208, 209, 210], "popular": [6, 200, 204, 205], "llama2": [6, 7, 8, 10, 19, 31, 33, 35, 36, 37, 39, 41, 43, 45, 47, 48, 53, 54, 55, 56, 57, 58, 59, 60, 61, 72, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 126, 131, 132, 168, 198, 200, 202, 206, 207, 209], "offici": [6, 19, 203, 206, 207], "implement": [6, 8, 31, 33, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 62, 72, 106, 118, 126, 129, 130, 133, 135, 136, 137, 138, 139, 140, 141, 152, 153, 165, 178, 186, 200, 204, 208, 209, 210], "when": [6, 7, 8, 12, 32, 34, 36, 37, 47, 72, 125, 130, 131, 132, 133, 134, 135, 145, 148, 160, 162, 172, 175, 186, 189, 202, 205, 207, 208, 209, 210], "llama": [6, 19, 31, 72, 88, 129, 130, 165, 166, 202, 203, 205, 206, 207, 208], "websit": 6, "get": [6, 7, 8, 9, 31, 36, 72, 106, 174, 176, 177, 179, 199, 200, 203, 204, 205, 206, 208, 209], "singl": [6, 7, 10, 14, 15, 16, 18, 19, 21, 25, 27, 28, 32, 34, 36, 37, 47, 50, 51, 52, 77, 88, 114, 125, 133, 145, 165, 166, 167, 168, 169, 171, 202, 203, 204, 205, 206, 207, 208, 210], "pth": [6, 205], "inspect": [6, 205, 208, 210], "content": [6, 15, 20, 22, 23, 27, 28, 31, 36, 62, 72, 106, 118, 157, 203, 204], "easili": [6, 7, 200, 204, 208, 209, 210], "torch": [6, 7, 49, 50, 51, 127, 128, 131, 133, 134, 135, 136, 137, 138, 139, 147, 148, 149, 150, 151, 161, 162, 163, 167, 169, 171, 172, 173, 174, 177, 178, 180, 181, 188, 189, 190, 191, 192, 193, 194, 195, 205, 206, 207, 208, 210], "import": [6, 7, 10, 40, 43, 47, 133, 136, 186, 187, 203, 204, 205, 206, 207, 208, 209, 210], "00": [6, 197, 201, 206], "mmap": [6, 205], "true": [6, 7, 20, 32, 33, 34, 37, 38, 39, 40, 42, 43, 44, 47, 48, 52, 59, 60, 61, 62, 63, 67, 70, 71, 72, 84, 85, 86, 87, 88, 96, 97, 104, 105, 106, 116, 117, 118, 124, 125, 131, 132, 134, 138, 142, 147, 151, 154, 155, 157, 159, 161, 164, 165, 166, 167, 175, 177, 180, 181, 183, 186, 193, 194, 202, 203, 204, 205, 207, 208, 209, 210], "weights_onli": [6, 167], "map_loc": [6, 205], "cpu": [6, 8, 134, 174, 193, 199, 202, 205, 210], "tensor": [6, 49, 50, 51, 52, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 141, 147, 148, 149, 150, 151, 161, 162, 163, 165, 172, 184, 185, 186, 187, 188, 191, 208, 210], "item": 6, "print": [6, 9, 32, 39, 42, 44, 45, 62, 72, 88, 106, 118, 133, 154, 155, 157, 172, 194, 203, 204, 206, 208, 209, 210], "f": [6, 9, 39, 42, 44, 203, 205, 208, 210], "tok_embed": [6, 131], "32000": [6, 10, 208], "4096": [6, 10, 33, 35, 37, 39, 41, 43, 47, 48, 125, 130, 204, 208, 209], "len": [6, 32, 39, 42, 44, 131, 133], "292": 6, "contain": [6, 20, 27, 34, 36, 37, 47, 62, 72, 88, 92, 106, 118, 123, 125, 128, 130, 131, 132, 140, 143, 144, 145, 147, 149, 150, 151, 155, 157, 160, 165, 166, 167, 169, 170, 171, 177, 182, 186, 188, 193, 203, 205, 207, 208], "input": [6, 14, 18, 25, 31, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 88, 106, 118, 125, 126, 127, 129, 130, 131, 132, 133, 141, 149, 150, 154, 155, 159, 162, 163, 165, 167, 188, 192, 195, 203, 204, 208, 210], "embed": [6, 49, 50, 51, 52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 129, 130, 131, 133, 175, 203, 207, 209], "tabl": [6, 203, 205, 207, 210], "call": [6, 10, 20, 22, 36, 126, 133, 134, 145, 170, 184, 185, 186, 187, 189, 193, 203, 204, 208, 210], "layer": [6, 8, 52, 56, 57, 58, 59, 60, 61, 63, 67, 68, 69, 70, 71, 73, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 114, 116, 117, 119, 120, 121, 124, 125, 131, 132, 133, 141, 145, 146, 164, 175, 200, 207, 208, 209, 210], "have": [6, 7, 10, 50, 51, 52, 125, 128, 133, 140, 146, 159, 161, 167, 169, 170, 175, 183, 186, 195, 199, 203, 204, 205, 206, 207, 208, 209, 210], "dim": [6, 125, 126, 129, 130, 131], "most": [6, 7, 22, 160, 203, 206, 208, 210], "within": [6, 7, 10, 31, 34, 45, 49, 67, 79, 93, 101, 107, 109, 119, 126, 133, 172, 186, 192, 193, 202, 204, 208, 210], "hub": [6, 36, 202, 204, 206], "default": [6, 7, 16, 20, 27, 28, 29, 31, 33, 34, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 67, 68, 69, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 123, 125, 126, 129, 130, 131, 132, 134, 135, 136, 141, 145, 147, 148, 149, 150, 154, 155, 157, 165, 166, 167, 170, 172, 174, 179, 183, 184, 187, 188, 191, 192, 193, 199, 202, 203, 204, 205, 207, 208, 209, 210], "everi": [6, 8, 49, 50, 51, 126, 133, 186, 193, 199, 202, 210], "repo": [6, 165, 166, 168, 202, 205], "first": [6, 7, 10, 30, 34, 52, 128, 131, 133, 151, 165, 170, 198, 200, 203, 204, 205, 207, 208, 209, 210], "big": 6, "split": [6, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 155, 203, 204, 205, 209], "across": [6, 8, 32, 165, 186, 192, 205, 207, 209], "bin": [6, 202, 205], "correctli": [6, 8, 13, 145, 165, 199, 203, 206, 210], "piec": 6, "one": [6, 8, 30, 62, 72, 106, 118, 126, 133, 157, 161, 167, 203, 204, 205, 206, 207, 210], "pytorch_model": [6, 205], "00001": [6, 202], "00002": [6, 202], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 34, 36, 41, 52, 67, 79, 93, 101, 107, 109, 119, 133, 141, 143, 145, 154, 165, 166, 167, 169, 170, 172, 174, 175, 177, 178, 183, 202, 204, 205, 206, 208, 209, 210], "doe": [6, 21, 27, 31, 34, 47, 63, 72, 111, 122, 125, 131, 132, 140, 157, 165, 167, 169, 170, 202, 203, 205, 209], "fewer": [6, 125], "sinc": [6, 7, 10, 36, 126, 161, 162, 165, 167, 203, 205, 207, 209], "mismatch": 6, "name": [6, 7, 9, 11, 14, 18, 25, 33, 35, 37, 42, 43, 44, 45, 47, 48, 140, 144, 146, 155, 165, 166, 167, 168, 169, 170, 171, 172, 173, 184, 185, 186, 187, 195, 202, 203, 205, 207, 209], "caus": [6, 106, 154, 162], "try": [6, 7, 203, 205, 206, 207, 210], "same": [6, 7, 22, 49, 50, 56, 57, 58, 62, 68, 69, 72, 80, 81, 82, 83, 94, 95, 102, 103, 106, 118, 120, 128, 132, 133, 138, 151, 157, 169, 170, 175, 187, 202, 203, 205, 207, 208, 209, 210], "As": [6, 7, 8, 9, 141, 200, 205, 210], "re": [6, 7, 167, 200, 203, 205, 206, 208], "care": [6, 126, 165, 167, 205, 207, 208], "end": [6, 8, 20, 37, 47, 88, 106, 155, 157, 198, 200, 203, 207, 208, 209], "number": [6, 8, 31, 33, 34, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 49, 50, 52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 131, 133, 135, 159, 160, 165, 166, 167, 172, 179, 192, 193, 202, 206, 208], "just": [6, 14, 200, 202, 203, 204, 206, 207, 208, 209], "save": [6, 8, 9, 134, 165, 166, 167, 169, 175, 183, 187, 198, 202, 203, 204, 205, 207, 208, 209], "less": [6, 45, 205, 206, 207, 210], "prone": 6, "manag": [6, 32, 142, 191, 203], "invari": 6, "accept": [6, 7, 45, 164, 204, 206, 210], "multipl": [6, 7, 8, 20, 31, 32, 36, 125, 131, 132, 133, 141, 150, 160, 161, 184, 185, 186, 187, 193, 206, 207], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 203, 204, 205, 209], "worri": [6, 203, 206], "explicitli": [6, 140, 200, 208], "convert": [6, 27, 28, 31, 36, 42, 44, 165, 188, 203, 205, 209, 210], "time": [6, 62, 63, 72, 106, 111, 118, 147, 157, 184, 186, 193, 202, 203, 204, 205, 207, 210], "produc": [6, 169, 209, 210], "back": [6, 30, 142, 165, 204, 208, 210], "form": [6, 7, 8, 30, 36, 202], "One": [6, 209], "advantag": [6, 138, 147, 208], "being": [6, 36, 165, 166, 167, 171, 173, 209, 210], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 22, 27, 28, 34, 40, 43, 47, 56, 57, 58, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 113, 119, 120, 121, 125, 126, 133, 138, 140, 145, 146, 147, 163, 164, 170, 182, 184, 185, 186, 187, 199, 200, 204, 205, 206, 207, 208, 209, 210], "abl": [6, 8, 205, 206, 209], "post": [6, 133, 189, 193, 205, 207, 209, 210], "tool": [6, 20, 22, 36, 204, 205, 206], "quantiz": [6, 56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 141, 167, 178, 198, 206, 210], "eval": [6, 198, 200, 209], "without": [6, 7, 9, 14, 145, 161, 162, 199, 200, 203, 205, 208, 209], "code": [6, 8, 53, 54, 55, 56, 57, 58, 59, 60, 61, 131, 196, 200, 204, 206], "chang": [6, 7, 9, 14, 167, 199, 202, 205, 206, 207, 208, 209, 210], "OR": [6, 27], "convers": [6, 15, 16, 19, 21, 27, 28, 30, 31, 36, 40, 45, 165, 167, 168, 200, 203, 204, 205, 208, 210], "script": [6, 9, 202, 204, 205, 206, 207], "wai": [6, 7, 31, 36, 145, 202, 203, 204, 205, 206, 207], "surround": [6, 8, 200], "load_checkpoint": [6, 8, 165, 166, 167, 168], "save_checkpoint": [6, 8, 9, 165, 166, 167], "map": [6, 14, 18, 22, 25, 27, 28, 31, 32, 33, 34, 35, 42, 43, 44, 88, 118, 144, 155, 156, 165, 169, 171, 184, 185, 186, 187, 189, 193, 203, 204, 205, 208], "exampl": [6, 7, 8, 9, 10, 12, 14, 22, 25, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 47, 48, 52, 62, 72, 88, 106, 118, 125, 133, 136, 137, 139, 140, 142, 149, 150, 151, 154, 155, 157, 160, 161, 162, 163, 164, 165, 166, 168, 169, 172, 178, 186, 187, 188, 191, 194, 196, 197, 199, 201, 202, 203, 204, 205, 207, 208, 209, 210], "appli": [6, 8, 31, 33, 35, 36, 56, 57, 58, 59, 60, 61, 63, 67, 68, 69, 70, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 116, 117, 119, 120, 124, 125, 129, 130, 131, 132, 145, 146, 190, 200, 210], "permut": 6, "certain": [6, 7, 193, 203], "ensur": [6, 7, 13, 30, 36, 45, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 165, 167, 174, 200, 204, 206], "behav": 6, "further": [6, 14, 133, 202, 204, 208, 209, 210], "illustr": [6, 207], "whilst": 6, "other": [6, 8, 10, 22, 32, 137, 167, 170, 175, 193, 204, 206, 207, 208, 209], "phi3": [6, 118, 119, 120, 122, 123, 124, 168, 202], "own": [6, 22, 183, 192, 202, 203, 204, 205, 207, 208], "found": [6, 7, 9, 129, 130, 165, 166, 167, 202, 208, 210], "folder": [6, 203], "three": [6, 8, 36, 136, 137, 139, 206], "read": [6, 165, 166, 167, 200], "write": [6, 8, 14, 165, 166, 167, 184, 203, 204, 206], "compat": [6, 165, 167, 209], "transform": [6, 8, 31, 33, 35, 36, 42, 44, 52, 56, 57, 58, 63, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 113, 119, 120, 121, 131, 132, 133, 135, 159, 160, 161, 162, 163, 190, 208, 209], "framework": [6, 8, 200], "mention": [6, 205, 210], "assum": [6, 14, 18, 25, 33, 35, 42, 43, 44, 125, 130, 131, 132, 135, 143, 155, 169, 171, 174, 183, 203, 205, 208], "checkpoint_dir": [6, 7, 165, 166, 167, 205, 207, 209], "necessari": [6, 36, 45, 184, 185, 186, 187, 203, 208], "easiest": [6, 205, 206], "sure": [6, 7, 203, 205, 206, 207, 208, 209, 210], "everyth": [6, 8, 170, 200, 206], "follow": [6, 8, 20, 22, 27, 28, 31, 34, 36, 125, 135, 138, 159, 160, 167, 168, 169, 181, 187, 193, 198, 199, 202, 204, 205, 206, 207, 208, 209, 210], "flow": [6, 31, 33, 34, 35, 209, 210], "By": [6, 202, 208, 209, 210], "safetensor": [6, 165, 202], "output": [6, 18, 32, 33, 36, 39, 42, 44, 45, 52, 56, 57, 58, 63, 67, 73, 77, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 114, 119, 120, 125, 126, 127, 129, 130, 131, 132, 133, 141, 144, 145, 146, 159, 162, 167, 172, 175, 185, 193, 199, 202, 203, 204, 205, 206, 207, 208, 210], "dir": [6, 187, 199, 202, 205, 206, 207, 209], "output_dir": [6, 7, 165, 166, 167, 193, 205, 207, 208, 209, 210], "here": [6, 7, 9, 14, 16, 25, 42, 129, 130, 202, 203, 204, 205, 206, 207, 208, 209, 210], "argument": [6, 7, 10, 17, 18, 26, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 125, 164, 170, 175, 180, 184, 186, 187, 190, 202, 203, 204, 208, 209], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 131, 193, 202, 204, 205, 208, 210], "_component_": [6, 7, 9, 10, 32, 40, 43, 47, 193, 203, 204, 205, 207, 208, 209], "fullmodelhfcheckpoint": [6, 205], "directori": [6, 7, 165, 166, 167, 184, 186, 187, 193, 202, 203, 204, 205, 206, 207], "sort": [6, 165, 167], "so": [6, 7, 34, 133, 161, 165, 170, 199, 200, 203, 205, 206, 207, 208, 209, 210], "order": [6, 8, 165, 167, 186, 187, 206], "matter": [6, 165, 167, 202, 208], "checkpoint_fil": [6, 7, 9, 165, 166, 167, 205, 207, 208, 209, 210], "restart": [6, 202], "previou": [6, 34, 165, 166, 167], "more": [6, 7, 8, 22, 31, 33, 35, 36, 37, 38, 40, 42, 43, 44, 45, 47, 48, 72, 128, 130, 133, 145, 164, 167, 170, 187, 190, 192, 200, 202, 204, 205, 206, 207, 208, 209, 210], "next": [6, 34, 47, 52, 133, 159, 172, 207, 210], "section": [6, 8, 177, 198, 205, 207, 210], "recipe_checkpoint": [6, 165, 166, 167, 209], "null": [6, 7, 209], "usual": [6, 130, 151, 165, 187, 202, 205, 208], "model_typ": [6, 165, 166, 167, 205, 207, 209], "resume_from_checkpoint": [6, 165, 166, 167], "fals": [6, 7, 20, 27, 28, 31, 32, 33, 34, 38, 39, 40, 42, 43, 44, 45, 47, 48, 52, 56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 116, 117, 118, 119, 120, 124, 125, 131, 132, 141, 142, 145, 151, 154, 161, 165, 166, 167, 181, 193, 202, 203, 204, 205, 207, 208, 209, 210], "requir": [6, 7, 32, 36, 37, 45, 47, 72, 150, 165, 167, 169, 178, 180, 181, 183, 186, 187, 192, 193, 199, 202, 203, 204, 206, 209, 210], "param": [6, 8, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 141, 143, 144, 146, 165, 183, 208, 209, 210], "directli": [6, 7, 8, 10, 36, 40, 43, 47, 136, 164, 165, 202, 205, 206, 207, 208, 209, 210], "out": [6, 7, 8, 33, 39, 40, 42, 44, 159, 165, 166, 198, 200, 202, 203, 205, 206, 207, 208, 210], "case": [6, 8, 9, 20, 22, 50, 51, 52, 133, 165, 169, 174, 178, 183, 184, 190, 200, 202, 203, 204, 205, 207, 208, 210], "discrep": [6, 165], "along": [6, 208], "github": [6, 10, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 125, 129, 130, 135, 136, 137, 138, 139, 145, 199, 204, 205, 206, 207], "repositori": [6, 19, 31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 205, 206], "fullmodelmetacheckpoint": [6, 207, 209], "current": [6, 34, 63, 67, 79, 93, 101, 107, 109, 111, 119, 122, 125, 128, 130, 131, 132, 138, 166, 167, 175, 178, 179, 184, 186, 189, 192, 204, 206, 207, 209], "test": [6, 7, 8, 200, 203], "complet": [6, 8, 14, 34, 41, 47, 123, 137, 203, 204, 205, 206, 207], "written": [6, 7, 8, 165, 166, 184, 185, 186, 187, 200], "begin": [6, 34, 47, 72, 106, 133, 155, 203, 207, 210], "partit": [6, 165, 210], "ha": [6, 72, 106, 133, 140, 142, 143, 146, 151, 167, 169, 195, 203, 204, 205, 206, 207, 208, 210], "standard": [6, 17, 27, 36, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 185, 200, 203, 205, 207], "key_1": [6, 167], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 205], "inform": [6, 187, 190, 200, 202, 205, 206], "subsequ": [6, 8, 133, 159], "recipe_st": [6, 165, 166, 167], "pt": [6, 9, 165, 166, 167, 205, 207, 209], "epoch": [6, 8, 9, 135, 165, 166, 167, 202, 203, 205, 206, 207, 209], "optim": [6, 7, 8, 32, 63, 72, 111, 122, 135, 136, 138, 139, 150, 167, 169, 171, 177, 189, 193, 203, 205, 206, 207, 208, 210], "etc": [6, 8, 165, 177, 206], "prevent": [6, 34, 136, 202], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 67, 131, 140, 169, 189, 192, 203, 204, 205, 208, 209, 210], "updat": [6, 7, 8, 22, 128, 136, 138, 158, 169, 193, 199, 203, 205, 206, 207, 208, 209, 210], "hf_model_0001_0": [6, 205], "hf_model_0002_0": [6, 205], "both": [6, 32, 146, 202, 205, 208, 209, 210], "adapt": [6, 140, 141, 142, 143, 144, 165, 166, 167, 203, 205, 208, 210], "merg": [6, 10, 11, 165, 205, 207, 210], "would": [6, 7, 9, 22, 34, 131, 133, 137, 199, 203, 204, 205, 208, 210], "addition": [6, 154, 155, 192, 204, 208], "option": [6, 7, 8, 14, 18, 25, 29, 31, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 47, 48, 51, 52, 56, 57, 58, 62, 67, 68, 69, 72, 73, 79, 80, 81, 82, 83, 88, 89, 92, 93, 94, 95, 98, 101, 102, 103, 106, 107, 108, 109, 110, 118, 119, 120, 123, 125, 130, 131, 132, 133, 134, 138, 145, 146, 147, 148, 152, 154, 157, 161, 162, 165, 166, 167, 172, 173, 174, 176, 178, 184, 187, 192, 193, 199, 200, 202, 204, 205], "save_adapter_weights_onli": 6, "choos": [6, 208], "primari": [6, 7, 8, 36, 206], "want": [6, 7, 8, 9, 10, 31, 36, 160, 161, 172, 199, 202, 203, 204, 205, 206, 207, 208], "resum": [6, 8, 135, 165, 166, 167, 210], "initi": [6, 8, 12, 32, 34, 53, 54, 55, 64, 65, 74, 75, 76, 77, 90, 91, 99, 100, 112, 114, 136, 169, 180, 181, 206, 208, 210], "frozen": [6, 136, 208, 210], "base": [6, 10, 20, 22, 33, 35, 36, 45, 56, 57, 58, 59, 60, 61, 63, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 116, 117, 119, 120, 121, 124, 130, 135, 136, 137, 139, 141, 142, 144, 145, 146, 148, 165, 170, 173, 175, 183, 184, 198, 203, 205, 206, 207, 208, 210], "well": [6, 7, 8, 200, 202, 204, 205, 207, 210], "learnt": [6, 203, 205], "someth": [6, 8, 9, 203, 205, 209], "NOT": [6, 63, 111], "refer": [6, 7, 8, 129, 130, 133, 136, 137, 138, 139, 142, 148, 200, 208, 209], "adapter_checkpoint": [6, 165, 166, 167], "adapter_0": [6, 205], "now": [6, 169, 171, 203, 204, 205, 206, 207, 208, 209, 210], "knowledg": 6, "creat": [6, 7, 10, 22, 34, 36, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 68, 69, 70, 71, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 94, 95, 96, 97, 99, 100, 102, 103, 104, 105, 108, 110, 112, 114, 116, 117, 120, 122, 124, 128, 133, 135, 164, 165, 166, 167, 171, 184, 186, 202, 203, 204, 205, 210], "simpl": [6, 8, 14, 25, 133, 198, 204, 206, 208, 209, 210], "forward": [6, 8, 49, 50, 51, 125, 126, 127, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 177, 193, 207, 208, 210], "modeltyp": [6, 165, 166, 167], "llama2_13b": [6, 80], "right": [6, 165, 205, 207, 208], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 145, 169, 208], "successfulli": [6, 202, 206], "vocab": [6, 10, 131, 207], "70": [6, 90], "x": [6, 49, 50, 51, 125, 126, 127, 129, 130, 131, 132, 133, 141, 172, 191, 208, 209, 210], "randint": 6, "0": [6, 8, 34, 52, 56, 57, 58, 59, 60, 61, 62, 63, 67, 72, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 125, 131, 133, 135, 136, 137, 138, 139, 141, 149, 150, 151, 157, 161, 172, 178, 186, 187, 188, 192, 194, 197, 201, 203, 204, 205, 206, 207, 208, 209, 210], "1": [6, 8, 34, 45, 49, 50, 62, 72, 73, 79, 88, 89, 93, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 111, 113, 118, 119, 121, 125, 131, 133, 135, 136, 137, 138, 139, 149, 150, 154, 155, 157, 161, 162, 166, 168, 172, 181, 186, 187, 188, 191, 192, 202, 203, 205, 206, 208, 209, 210], "no_grad": 6, "6": [6, 34, 63, 67, 129, 133, 149, 150, 163, 188, 209, 210], "3989": 6, "9": [6, 133, 150, 205, 209, 210], "0531": 6, "3": [6, 34, 52, 88, 120, 122, 123, 133, 149, 150, 161, 162, 163, 168, 170, 176, 178, 188, 191, 202, 203, 205, 206, 207, 209, 210], "2375": 6, "5": [6, 7, 14, 133, 135, 136, 149, 150, 151, 161, 188, 205, 206, 207], "2822": 6, "4": [6, 7, 45, 52, 125, 133, 149, 150, 160, 178, 188, 194, 200, 202, 204, 205, 207, 208, 209, 210], "4872": 6, "7469": 6, "8": [6, 39, 42, 44, 56, 57, 58, 59, 60, 61, 68, 69, 70, 71, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 102, 103, 104, 105, 108, 110, 116, 117, 120, 124, 133, 149, 150, 205, 208, 209, 210], "6737": 6, "11": [6, 133, 150, 205, 209, 210], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 133, 138, 149, 150, 159, 188], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 133], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 20, 33, 35, 43, 45, 145, 157, 187, 202, 203, 204, 205, 206, 207, 208, 209], "find": [6, 8, 9, 136, 202, 205, 206, 208], "list": [6, 7, 15, 16, 19, 20, 21, 22, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 56, 57, 58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 116, 117, 118, 119, 120, 124, 133, 140, 141, 145, 146, 149, 150, 152, 153, 154, 155, 157, 159, 160, 161, 165, 166, 167, 170, 172, 176, 188, 203, 204, 205, 206, 207, 209], "hope": 6, "deeper": [6, 206], "insight": [6, 205], "happi": [6, 205], "thi": [7, 8, 9, 10, 17, 20, 26, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 63, 67, 72, 73, 79, 88, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 122, 123, 125, 126, 130, 131, 132, 133, 134, 135, 136, 137, 138, 140, 142, 145, 146, 148, 150, 154, 155, 157, 159, 164, 165, 166, 167, 169, 170, 172, 173, 174, 177, 181, 183, 184, 186, 187, 189, 190, 192, 198, 199, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "pars": [7, 10, 11, 156, 170, 203, 206], "effect": [7, 209], "cli": [7, 9, 11, 12, 199, 205, 206], "prerequisit": [7, 203, 204, 205, 206, 207, 208, 209, 210], "Be": [7, 203, 205, 206, 207, 208, 209, 210], "familiar": [7, 203, 205, 206, 207, 208, 209, 210], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 203, 204, 206], "instal": [7, 9, 181, 186, 187, 198, 202, 204, 205, 206, 207, 208, 209, 210], "fundament": [7, 209], "There": [7, 30, 50, 161, 203, 206, 207, 208], "entri": [7, 8, 206], "point": [7, 8, 27, 28, 157, 204, 205, 206, 207, 208, 209, 210], "locat": [7, 202, 204, 207, 208, 209, 210], "thei": [7, 8, 32, 52, 131, 133, 146, 170, 175, 202, 203, 204, 208, 209], "truth": [7, 205, 207], "reproduc": 7, "overridden": [7, 126, 170, 193], "quick": 7, "experiment": 7, "serv": [7, 157, 164, 204, 208], "particular": [7, 31, 32, 36, 45, 164, 204, 208, 210], "seed": [7, 8, 9, 192, 206, 209], "shuffl": [7, 34, 209], "devic": [7, 8, 145, 169, 173, 174, 177, 202, 203, 205, 206, 207, 208], "cuda": [7, 173, 174, 177, 193, 199, 205, 210], "dtype": [7, 8, 128, 131, 134, 174, 191, 195, 205, 209, 210], "fp32": [7, 209, 210], "enable_fsdp": 7, "mani": [7, 34, 204, 205], "object": [7, 10, 11, 15, 16, 19, 21, 42, 44, 52, 125, 136, 164, 178, 203], "keyword": [7, 10, 31, 33, 35, 36, 37, 40, 41, 43, 45, 47, 48, 134, 203, 204], "loss": [7, 8, 20, 22, 33, 36, 39, 42, 44, 136, 137, 138, 139, 206, 208, 210], "subfield": 7, "dotpath": [7, 204], "wish": [7, 204], "exact": [7, 10, 205], "path": [7, 8, 9, 10, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 62, 66, 72, 78, 88, 92, 106, 115, 118, 123, 154, 155, 156, 165, 166, 167, 170, 193, 202, 203, 204, 205, 207, 208], "normal": [7, 31, 34, 36, 72, 106, 127, 129, 131, 132, 154, 203, 204, 208, 209, 210], "python": [7, 170, 176, 187, 192, 196, 202, 204, 205, 209], "alpaca_dataset": [7, 38, 204], "custom": [7, 8, 22, 31, 33, 35, 36, 40, 43, 47, 190, 200, 202, 206, 207, 208], "train_on_input": [7, 27, 28, 31, 32, 33, 38, 39, 40, 42, 43, 44, 45, 203, 204], "onc": [7, 22, 142, 205, 206, 207, 208, 210], "ve": [7, 128, 202, 203, 204, 205, 207, 208], "instanc": [7, 10, 31, 32, 33, 79, 93, 101, 107, 109, 119, 126, 134, 143, 144, 208], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 40, 202, 205, 210], "under": [7, 193, 204, 210], "preced": [7, 10, 202, 207, 208], "actual": [7, 9, 14, 25, 31, 36, 203, 209], "throw": 7, "notic": [7, 49, 50, 51, 133, 203, 204, 208], "miss": [7, 145, 146, 193, 208], "posit": [7, 10, 34, 49, 50, 51, 52, 63, 67, 107, 109, 111, 113, 119, 121, 125, 128, 130, 131, 132, 133, 207], "anoth": [7, 36, 205], "handl": [7, 12, 32, 36, 72, 106, 154, 155, 203, 205, 208, 210], "def": [7, 8, 9, 12, 164, 168, 203, 204, 208, 210], "dictconfig": [7, 8, 10, 11, 12, 13, 187, 193], "arg": [7, 10, 23, 51, 127, 131, 134, 140, 152, 153, 158, 170, 185, 193, 209], "tupl": [7, 10, 22, 45, 51, 62, 72, 88, 106, 118, 128, 133, 134, 136, 137, 138, 139, 147, 148, 150, 151, 153, 157, 160, 161, 162, 164, 170, 179, 193, 195], "kwarg": [7, 10, 23, 127, 134, 140, 152, 153, 158, 170, 180, 184, 185, 186, 187, 190, 193, 204], "str": [7, 10, 11, 14, 18, 20, 22, 25, 27, 28, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 62, 66, 72, 78, 88, 92, 106, 115, 118, 123, 134, 140, 141, 143, 144, 145, 146, 149, 150, 152, 153, 154, 155, 156, 165, 166, 167, 168, 169, 170, 173, 174, 176, 177, 178, 180, 182, 184, 185, 186, 187, 188, 192, 193, 194, 195, 203, 204], "mean": [7, 125, 129, 131, 132, 147, 183, 202, 203, 204, 206, 208, 209], "pass": [7, 10, 20, 22, 31, 32, 33, 35, 36, 37, 40, 41, 42, 43, 44, 47, 48, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 126, 134, 138, 142, 146, 155, 164, 167, 174, 175, 177, 180, 183, 186, 187, 190, 193, 202, 203, 204, 208, 209, 210], "add": [7, 9, 31, 34, 36, 37, 47, 72, 88, 133, 155, 157, 167, 168, 170, 204, 205, 207, 208, 210], "d": [7, 20, 125, 128, 131, 202, 203, 208, 209], "llama2_token": [7, 203, 205], "tmp": [7, 169, 203, 206], "llama2token": [7, 78], "modeltoken": [7, 20, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 157, 203, 204], "bool": [7, 20, 27, 28, 31, 33, 34, 37, 38, 39, 40, 42, 43, 44, 45, 47, 48, 52, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 116, 117, 118, 119, 120, 124, 131, 134, 141, 145, 146, 147, 151, 153, 154, 155, 157, 161, 164, 165, 166, 167, 175, 177, 180, 181, 183, 186, 190, 193, 194, 203, 210], "max_seq_len": [7, 10, 29, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 62, 63, 67, 72, 73, 79, 88, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 125, 128, 130, 131, 149, 157, 203, 204, 209], "int": [7, 9, 29, 31, 33, 34, 35, 37, 38, 39, 40, 41, 43, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 113, 116, 117, 118, 119, 120, 121, 124, 125, 128, 129, 130, 131, 133, 135, 141, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 172, 175, 179, 183, 184, 185, 186, 187, 188, 190, 192, 193, 202, 203, 204, 208, 210], "512": [7, 38, 39, 52, 204, 210], "instructdataset": [7, 38, 39, 43, 204], "alreadi": [7, 168, 180, 183, 199, 202, 204, 205, 208], "overwrit": [7, 167, 199, 202], "duplic": [7, 8, 200, 202], "sometim": 7, "than": [7, 30, 45, 125, 128, 133, 136, 164, 167, 168, 194, 195, 203, 204, 205, 206, 207, 208, 210], "resolv": [7, 11, 206], "alpaca": [7, 14, 32, 38, 39, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 204], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 184, 185, 186, 187], "disklogg": 7, "log_dir": [7, 184, 186, 187], "conveni": [7, 8, 202], "verifi": [7, 173, 174, 175, 203, 206, 208], "properli": [7, 145, 181, 202], "experi": [7, 187, 198, 200, 203, 207, 208], "wa": [7, 50, 51, 52, 133, 145, 203, 208, 209, 210], "cp": [7, 199, 202, 203, 205, 206, 207, 209], "7b_lora_single_devic": [7, 205, 206, 208, 210], "my_config": [7, 202], "discuss": [7, 205, 206, 207, 208], "guidelin": 7, "while": [7, 8, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 126, 200, 205, 209, 210], "mai": [7, 9, 133, 175, 203, 204, 206, 208], "tempt": 7, "put": [7, 8, 206, 208, 209], "much": [7, 205, 207, 208, 209, 210], "give": [7, 204, 208], "maximum": [7, 29, 31, 33, 34, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 49, 50, 52, 63, 67, 73, 79, 88, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 130, 131, 149, 160, 161, 162, 202], "flexibl": [7, 32, 204], "switch": 7, "encourag": [7, 72, 208], "clariti": 7, "significantli": [7, 136], "easier": [7, 205, 206], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 167, 203, 206], "parent": [7, 202], "modul": [7, 10, 42, 44, 49, 50, 51, 52, 109, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 168, 171, 175, 183, 189, 190, 192, 204, 206, 208, 210], "__init__": [7, 8, 208, 210], "py": [7, 10, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 125, 128, 129, 130, 135, 136, 137, 138, 139, 202, 205, 207], "guarante": 7, "stabil": [7, 200, 209, 210], "underscor": 7, "_alpaca": 7, "collect": [7, 172, 206], "itself": 7, "via": [7, 9, 40, 43, 47, 141, 165, 208, 210], "k1": [7, 8], "v1": [7, 8, 48], "k2": [7, 8], "v2": [7, 8, 204], "lora_finetune_single_devic": [7, 202, 203, 205, 206, 207, 208, 210], "checkpoint": [7, 8, 134, 155, 165, 166, 167, 168, 169, 187, 190, 200, 202, 207, 208, 209, 210], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "assign": [7, 36], "nest": 7, "dot": 7, "notat": [7, 125, 130, 131, 147, 148], "flag": [7, 8, 20, 33, 39, 42, 44, 164, 167, 175, 202, 210], "built": [7, 9, 46, 199, 203, 206, 210], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 31], "pytorch": [7, 8, 72, 131, 134, 145, 164, 181, 186, 190, 192, 193, 198, 199, 200, 205, 207, 208, 209, 210], "llama3": [7, 31, 45, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 119, 168, 172, 175, 198, 202, 204, 205], "8b_full": [7, 202, 204], "adamw": [7, 208], "lr": [7, 135], "2e": 7, "fuse": [7, 189, 209], "nproc_per_nod": [7, 204, 207, 208, 209], "full_finetune_distribut": [7, 202, 204, 205, 206], "core": [8, 36, 200, 204, 206, 210], "i": [8, 19, 20, 21, 125, 131, 132, 133, 134, 144, 169, 172, 204, 205, 207, 209, 210], "structur": [8, 15, 16, 19, 21, 23, 27, 28, 31, 36, 40, 92, 123, 159, 203, 204, 205, 209], "new": [8, 41, 42, 44, 112, 128, 168, 184, 186, 203, 205, 206, 207, 208, 210], "user": [8, 15, 16, 17, 19, 20, 21, 22, 23, 24, 26, 27, 28, 30, 31, 36, 62, 72, 73, 79, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 125, 157, 162, 203, 204, 206, 209], "thought": [8, 200, 206, 210], "target": [8, 200], "pipelin": [8, 200], "llm": [8, 198, 200, 204, 205, 207, 208], "eg": [8, 131, 165, 200], "meaning": [8, 200, 205], "featur": [8, 9, 199, 200, 205, 206], "fsdp": [8, 164, 169, 175, 183, 200, 206, 207], "activ": [8, 126, 177, 182, 190, 193, 200, 209, 210], "gradient": [8, 183, 189, 193, 200, 205, 207, 208, 210], "accumul": [8, 189, 193, 200], "mix": [8, 127, 202, 204, 205], "precis": [8, 127, 134, 174, 200, 206, 210], "given": [8, 10, 14, 18, 25, 30, 39, 41, 42, 44, 45, 46, 48, 137, 141, 142, 148, 152, 153, 172, 173, 174, 178, 183, 189, 194, 200, 208], "complex": 8, "becom": [8, 133, 137, 199, 204], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 131, 133, 168, 202, 204], "methodolog": 8, "reason": [8, 172, 205, 209], "possibl": [8, 34, 40, 160, 161, 202, 204], "trade": 8, "off": [8, 22, 72, 106, 205, 209], "memori": [8, 32, 33, 34, 35, 37, 39, 41, 43, 47, 48, 134, 145, 175, 177, 182, 183, 193, 198, 200, 205, 206, 207, 209], "vs": [8, 137, 206], "qualiti": [8, 205, 208, 209], "believ": 8, "best": [8, 161, 203], "suit": [8, 206], "b": [8, 125, 128, 130, 131, 132, 141, 147, 148, 183, 187, 208, 210], "fit": [8, 31, 33, 34, 35, 37, 39, 41, 43, 47, 48, 133, 136, 137, 161, 162, 204], "solut": [8, 137], "result": [8, 52, 62, 72, 106, 118, 133, 157, 159, 193, 205, 207, 208, 209, 210], "meant": [8, 134, 169], "depend": [8, 9, 14, 165, 193, 202, 204, 205, 208, 210], "level": [8, 36, 158, 171, 176, 183, 200, 210], "expertis": 8, "routin": 8, "yourself": [8, 202, 207, 208], "exist": [8, 199, 202, 205, 206, 207, 210], "ad": [8, 22, 49, 50, 51, 106, 113, 133, 154, 167, 168, 203, 204, 208, 209, 210], "ones": 8, "modular": [8, 200], "build": [8, 40, 43, 47, 52, 63, 73, 89, 98, 111, 113, 200, 207, 208], "block": [8, 34, 56, 57, 58, 63, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 119, 120, 145, 146, 200], "wandb": [8, 9, 187, 206], "log": [8, 11, 136, 137, 138, 139, 176, 177, 182, 184, 185, 186, 187, 205, 206, 207, 208, 210], "fulli": [8, 32], "nativ": [8, 198, 200, 208, 209, 210], "correct": [8, 17, 42, 129, 130, 131, 173, 200, 203, 204], "numer": [8, 200, 209], "pariti": [8, 200], "verif": 8, "extens": [8, 167, 200], "comparison": [8, 208, 210], "benchmark": [8, 192, 200, 205, 207, 208, 209], "limit": [8, 161, 162, 169, 204, 209], "hidden": [8, 52, 126, 133], "behind": 8, "100": [8, 33, 39, 42, 44, 45, 150, 172, 188, 208, 210], "prefer": [8, 25, 46, 136, 137, 138, 139, 150, 200, 202, 204], "over": [8, 36, 135, 136, 137, 170, 200, 202, 205, 208, 210], "unnecessari": 8, "abstract": [8, 15, 18, 152, 153, 200, 206, 210], "No": [8, 167, 200], "inherit": [8, 170, 200, 204], "go": [8, 19, 21, 52, 62, 72, 106, 118, 133, 157, 200, 204, 205, 206, 210], "upon": [8, 32, 207], "figur": [8, 208, 210], "spectrum": 8, "decid": 8, "interact": [8, 198, 206], "start": [8, 9, 157, 168, 199, 200, 203, 204, 205, 206, 209], "avail": [8, 48, 170, 173, 174, 181, 200, 202, 205, 207, 208], "paradigm": 8, "consist": [8, 48, 206], "configur": [8, 33, 35, 36, 39, 40, 41, 42, 43, 44, 45, 47, 48, 67, 79, 88, 93, 101, 107, 118, 119, 132, 200, 203, 206, 207, 208, 209, 210], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 121, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 171, 172, 173, 174, 175, 176, 177, 178, 180, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210], "command": [8, 9, 170, 199, 202, 203, 204, 205, 206, 207, 208, 209, 210], "overrid": [8, 11, 12, 202, 205, 206, 207, 210], "togeth": [8, 34, 187, 206, 208, 209], "valid": [8, 30, 145, 146, 148, 195, 199, 205, 206], "environ": [8, 173, 181, 199, 202, 204, 205, 206, 209], "logic": [8, 36, 153, 168, 200, 206, 208], "api": [8, 9, 17, 26, 27, 36, 59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 145, 202, 203, 206, 207, 210], "closer": [8, 208], "monolith": [8, 200], "trainer": [8, 136, 137, 139], "A": [8, 9, 17, 26, 27, 28, 32, 34, 52, 62, 72, 106, 118, 125, 131, 132, 133, 134, 136, 137, 138, 139, 141, 145, 147, 148, 149, 150, 151, 154, 155, 157, 159, 161, 164, 168, 169, 170, 177, 178, 182, 183, 188, 197, 198, 201, 202, 203, 208, 209, 210], "wrapper": [8, 127, 154, 155, 169, 171, 202, 208], "around": [8, 31, 36, 72, 106, 127, 154, 155, 177, 202, 203, 205, 208, 209, 210], "extern": [8, 204], "primarili": [8, 32, 208], "eleutherai": [8, 48, 200, 208, 209], "har": [8, 200, 208, 209], "control": [8, 33, 39, 42, 44, 137, 142, 192, 205], "multi": [8, 31, 125, 145, 207], "stage": [8, 133], "distil": 8, "oper": [8, 133, 142, 158, 192, 209], "turn": [8, 20, 30, 31, 203], "dataload": [8, 34, 39, 42, 44], "applic": [8, 125, 165, 166, 187], "clean": [8, 9, 38], "after": [8, 22, 36, 88, 118, 125, 128, 129, 131, 132, 145, 151, 183, 184, 185, 186, 187, 203, 205, 207, 209, 210], "process": [8, 9, 36, 52, 133, 134, 179, 180, 192, 204, 206, 209, 210], "group": [8, 125, 179, 180, 184, 185, 186, 187, 202, 207, 209], "init_process_group": [8, 180], "backend": [8, 202, 209], "gloo": 8, "els": [8, 170, 187, 200, 210], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "stuff": 8, "carri": [8, 36], "relev": [8, 202, 205, 208], "interfac": [8, 15, 18, 22, 23, 32, 140, 158, 204], "metric": [8, 206, 209], "logger": [8, 176, 182, 184, 185, 186, 187, 206], "self": [8, 9, 34, 56, 57, 58, 63, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 113, 119, 120, 121, 125, 131, 132, 140, 145, 146, 165, 168, 169, 204, 208, 210], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 164, 175, 183, 190, 203], "_model": [8, 169], "_setup_model": 8, "_token": [8, 204], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 169, 171, 189, 193, 210], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 136, 138, 192, 202, 207, 209], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 34], "batch": [8, 34, 39, 42, 44, 50, 125, 128, 130, 131, 133, 136, 137, 139, 147, 148, 149, 150, 188, 193, 200, 204, 206, 207, 208], "enumer": 8, "_autocast": 8, "logit": [8, 172], "label": [8, 31, 33, 34, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 136, 150, 188], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 184, 185, 186, 187], "step": [8, 34, 36, 131, 135, 147, 171, 184, 185, 186, 187, 189, 193, 198, 205, 208, 209, 210], "learn": [8, 32, 135, 137, 200, 203, 204, 206, 207, 208, 209, 210], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 18, 25, 29, 30, 31, 33, 34, 35, 36, 37, 40, 41, 42, 43, 44, 45, 47, 48, 52, 62, 72, 73, 79, 88, 89, 92, 93, 98, 101, 106, 118, 123, 125, 128, 130, 131, 132, 133, 138, 142, 144, 145, 146, 147, 148, 154, 157, 162, 165, 166, 167, 168, 172, 173, 174, 176, 178, 182, 184, 185, 186, 187, 189, 190, 191, 192, 193, 195, 203, 204, 205, 209], "fullfinetunerecip": 8, "wandblogg": [9, 208, 210], "workspac": 9, "seen": [9, 208, 210], "screenshot": 9, "below": [9, 14, 130, 164, 204, 207, 208, 210], "packag": [9, 186, 187, 199, 204], "pip": [9, 186, 187, 199, 205, 207], "Then": [9, 142, 206], "login": [9, 187, 202, 205], "project": [9, 52, 56, 57, 58, 63, 67, 73, 77, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 114, 119, 120, 125, 126, 133, 145, 146, 168, 175, 187, 198, 203, 208, 210], "grab": [9, 207], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 50, 51, 52, 133, 181, 203, 209], "exit": [9, 199, 202], "resourc": [9, 184, 185, 186, 187, 209], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 18, 19, 20, 21, 25, 27, 28, 31, 33, 34, 35, 36, 37, 43, 45, 47, 125, 130, 131, 132, 133, 139, 158, 159, 172, 203, 205], "desir": [9, 31, 36, 162, 191, 203], "suggest": 9, "approach": [9, 32, 204], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 205], "_output_dir": [9, 165, 166, 167], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 193], "type": [9, 10, 12, 20, 27, 28, 29, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 141, 143, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 188, 190, 191, 192, 193, 194, 195, 204, 205, 208, 209, 210], "descript": [9, 40, 45, 202], "whatev": 9, "metadata": [9, 209], "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": [9, 209], "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 27, 28, 31, 34, 36, 39, 42, 44, 182, 204], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 31, 33, 35, 37, 40, 41, 43, 47, 48, 53, 54, 55, 56, 57, 58, 59, 60, 61, 64, 65, 68, 69, 70, 71, 72, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 102, 103, 104, 105, 112, 114, 116, 117, 120, 122, 123, 124, 125, 129, 130, 133, 135, 136, 137, 138, 139, 145, 147, 159, 164, 165, 166, 170, 176, 181, 186, 187, 190, 192, 199, 204, 205, 207], "com": [10, 56, 57, 58, 68, 69, 72, 80, 81, 82, 83, 88, 94, 95, 102, 103, 120, 125, 129, 130, 135, 136, 137, 138, 139, 145, 199, 205, 207], "facebookresearch": [10, 129], "blob": [10, 56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120, 123, 125, 129, 130, 135, 136, 137, 138, 139], "main": [10, 12, 72, 123, 125, 129, 130, 199, 205, 207], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 131, 133], "32": [10, 133, 207, 208, 209, 210], "num_head": [10, 52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 130, 131], "num_kv_head": [10, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128], "vocab_s": [10, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "must": [10, 22, 32, 140, 170, 210], "return": [10, 12, 14, 15, 16, 18, 19, 20, 21, 22, 25, 27, 28, 29, 31, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 167, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 188, 191, 192, 193, 194, 203, 204, 208, 210], "nn": [10, 125, 126, 128, 131, 132, 133, 134, 140, 142, 143, 144, 164, 171, 183, 189, 190, 195, 208, 210], "parsed_yaml": 10, "embed_dim": [10, 49, 50, 51, 52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 130, 132, 133, 208], "valueerror": [10, 21, 27, 30, 31, 33, 40, 45, 118, 125, 131, 133, 165, 166, 167, 174, 177, 192, 195], "recipe_nam": 11, "rank": [11, 56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 107, 108, 109, 110, 119, 120, 141, 179, 181, 192, 206, 208, 210], "zero": [11, 128, 129, 205, 207, 209], "displai": 11, "callabl": [12, 31, 33, 35, 36, 131, 142, 164, 172, 175, 178, 183, 190], "With": [12, 205, 208, 209, 210], "my_recip": 12, "foo": 12, "bar": [12, 200, 206], "instanti": [13, 22, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 169], "configerror": 13, "cannot": [13, 167, 207], "data": [14, 15, 16, 18, 19, 20, 21, 22, 23, 25, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 72, 133, 136, 139, 158, 177, 184, 185, 186, 187, 204, 205, 209, 210], "templat": [14, 17, 18, 22, 23, 25, 26, 31, 32, 33, 35, 36, 39, 40, 42, 43, 44, 45, 72], "style": [14, 34, 38, 39, 40, 45, 210], "slightli": 14, "describ": [14, 72, 88, 190, 204], "task": [14, 17, 26, 32, 36, 41, 203, 204, 205, 207, 208, 209, 210], "context": [14, 16, 122, 142, 191, 193, 204], "respons": [14, 16, 20, 36, 62, 72, 106, 118, 136, 137, 139, 147, 148, 157, 204, 205, 206, 207], "appropri": [14, 16, 19, 20, 21, 32, 135, 165, 204, 210], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 18, 19, 20, 21, 25, 204], "column_map": [14, 18, 25, 32, 33, 35, 42, 43, 44, 204], "placehold": [14, 18, 25, 33, 35, 43, 204], "column": [14, 18, 25, 33, 35, 36, 37, 42, 43, 44, 47, 125, 131, 132, 203, 204, 209], "ident": [14, 18, 21, 25, 33, 34, 35, 42, 43, 44, 137, 205, 209], "poem": 14, "n": [14, 17, 22, 25, 26, 62, 72, 106, 118, 125, 133, 157, 161, 197, 201, 202, 203, 204, 209], "nwrite": 14, "long": [14, 34, 155, 203, 204, 208], "where": [14, 22, 25, 31, 39, 42, 44, 50, 72, 77, 106, 114, 125, 131, 133, 136, 137, 138, 141, 147, 150, 151, 154, 159, 161, 175, 183, 204], "me": 14, "tag": [15, 16, 19, 21, 22, 31, 36, 184, 185, 186, 187, 203], "system": [15, 16, 19, 20, 21, 22, 23, 24, 27, 28, 30, 31, 36, 62, 72, 106, 118, 157, 203, 204], "assist": [15, 16, 19, 20, 22, 23, 24, 27, 28, 30, 31, 36, 62, 72, 106, 118, 123, 157, 172, 203, 204], "role": [15, 20, 22, 23, 27, 28, 31, 36, 62, 72, 106, 118, 157, 203, 204], "prepend": [15, 22, 23, 72, 88, 106, 154], "append": [15, 22, 23, 88, 106, 118, 154, 199, 204], "accord": [15, 21, 203], "openai": [16, 27, 40, 138, 204], "markup": 16, "languag": [16, 136, 141, 172, 208], "It": [16, 20, 21, 22, 36, 133, 136, 202, 203, 204, 210], "im_start": 16, "im_end": 16, "goe": [16, 142], "functool": [17, 26, 164], "partial": [17, 26, 164], "_prompt_templ": [17, 26, 42, 44], "prompttempl": [17, 26, 36, 42, 44], "english": 17, "ncorrect": 17, "grammar": [17, 42, 204], "user_messag": [17, 26, 203], "assistant_messag": [17, 26, 203], "alwai": [18, 42, 44, 137, 170], "human": [19, 20, 28, 136, 138, 139, 203], "pre": [19, 34, 36, 47, 72, 133, 199, 203, 204], "taken": [19, 208, 210], "inst": [19, 21, 31, 36, 72, 203, 204], "sy": [19, 72, 203, 204], "respect": [19, 32, 144, 161, 193, 203, 204], "honest": [19, 203, 204], "am": [19, 21, 203, 204, 205, 207], "pari": [19, 21, 204], "capit": [19, 21, 25, 204], "franc": [19, 21, 25, 204], "known": [19, 21, 72, 106, 178, 204, 209], "its": [19, 21, 34, 109, 125, 130, 131, 132, 137, 189, 192, 202, 203, 204, 205, 207, 208], "stun": [19, 21, 204], "liter": [20, 22, 24, 56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 145, 146], "ipython": [20, 22, 24, 36], "union": [20, 42, 44, 47, 48, 146, 184, 185, 186, 187, 190, 192], "mask": [20, 22, 33, 34, 36, 39, 42, 44, 62, 72, 88, 106, 118, 125, 131, 132, 138, 147, 153, 157, 159, 203, 204], "eot": [20, 88], "repres": [20, 49, 50, 133, 150, 161, 203, 209], "individu": [20, 34, 177, 187, 190, 203, 204], "interleav": [20, 159], "tokenize_messag": [20, 31, 33, 35, 37, 39, 40, 41, 43, 45, 46, 47, 48, 62, 72, 88, 106, 118, 153, 157, 203, 204], "attach": 20, "special": [20, 31, 36, 72, 88, 92, 106, 118, 123, 133, 152, 153, 155, 156, 157, 159, 169, 204], "writer": 20, "dictionari": [20, 22, 34, 36, 149, 150, 177, 182, 184, 185, 186, 187, 188, 205], "hello": [20, 62, 72, 88, 106, 118, 154, 155, 203, 205, 207], "world": [20, 62, 72, 88, 106, 118, 154, 155, 179, 181, 205], "whether": [20, 27, 28, 31, 33, 37, 39, 40, 42, 43, 44, 45, 47, 48, 56, 57, 58, 63, 67, 68, 69, 79, 80, 81, 82, 83, 88, 93, 94, 95, 101, 102, 103, 106, 107, 108, 109, 110, 118, 119, 120, 134, 141, 145, 146, 154, 155, 164, 174, 177, 203, 204], "calcul": [20, 22, 125, 131, 133, 138, 147, 148, 161, 207], "correspond": [20, 138, 140, 143, 147, 150, 174, 206, 207, 209], "consecut": [20, 30, 159], "e": [20, 31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 125, 133, 134, 140, 144, 159, 161, 165, 169, 177, 193, 199, 205, 207, 208, 209, 210], "properti": [20, 170, 208], "contains_media": 20, "non": [20, 146, 148], "from_dict": [20, 203], "construct": [20, 159, 208], "text_cont": [20, 203], "mistral": [21, 31, 36, 45, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 168, 202, 203, 205, 206], "llama2chatformat": [21, 72, 203, 204], "achiev": [22, 189, 205, 207, 208, 209, 210], "prepend_tag": 22, "append_tag": 22, "thu": [22, 36, 137, 209], "consid": [22, 32, 36, 50, 51, 52, 133], "come": [22, 30, 140, 208], "alia": [24, 164], "similar": [25, 41, 46, 47, 48, 136, 145, 204, 205, 207, 208, 210], "stackexchangedpair": 25, "question": [25, 203, 204, 205, 207], "answer": [25, 203, 205, 207], "nanswer": 25, "summar": [26, 44, 203, 204], "dialogu": [26, 44, 203], "nsummari": [26, 203], "summari": [26, 32, 44, 133, 177, 204], "adher": [27, 28], "could": [27, 208], "remain": [27, 28, 135, 208], "unmask": [27, 28], "sharegpt": [28, 40], "gpt": [28, 125, 205], "eos_id": [29, 88, 155, 157], "length": [29, 30, 32, 33, 34, 35, 37, 39, 41, 43, 45, 47, 48, 62, 63, 67, 72, 73, 79, 88, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 122, 125, 128, 130, 131, 147, 148, 149, 150, 155, 157, 159, 166, 188], "last": [29, 34, 47, 135, 148, 204], "replac": [29, 33, 39, 42, 44, 134, 208], "forth": [30, 204], "empti": [30, 202], "shorter": 30, "min": [30, 161, 208], "invalid": 30, "convert_to_messag": [31, 203], "chat_format": [31, 40, 45, 203, 204], "chatformat": [31, 40, 204], "load_dataset_kwarg": [31, 33, 35, 36, 37, 40, 41, 43, 47, 48], "multiturn": [31, 203], "prepar": [31, 203, 209], "truncat": [31, 33, 34, 35, 37, 41, 43, 45, 47, 48, 62, 72, 88, 106, 118, 151, 155, 157, 204], "local": [31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 92, 123, 187, 192, 199, 202, 203, 205, 206], "g": [31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 125, 133, 140, 159, 161, 165, 177, 193, 207, 208, 209, 210], "csv": [31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 203, 204], "filepath": [31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48], "data_fil": [31, 33, 35, 36, 37, 40, 42, 43, 44, 47, 48, 203, 204], "load_dataset": [31, 33, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 203, 204], "huggingfac": [31, 33, 35, 37, 40, 41, 43, 47, 48, 114, 122, 123, 135, 136, 137, 139, 165, 166, 202, 205], "co": [31, 33, 35, 37, 40, 41, 43, 47, 48, 114, 122, 123, 165, 166, 205], "doc": [31, 33, 35, 36, 37, 40, 41, 43, 47, 48, 72, 88, 164, 170, 176, 181, 186, 187, 192, 202, 204, 205], "en": [31, 33, 35, 37, 40, 41, 43, 47, 48, 209], "package_refer": [31, 33, 35, 37, 40, 41, 43, 47, 48], "loading_method": [31, 33, 35, 37, 40, 41, 43, 47, 48], "extra": [31, 36, 199, 203, 208, 209, 210], "still": [31, 36, 170, 208, 209, 210], "unless": 31, "concaten": [32, 62, 72, 106, 118, 150, 153, 157], "sub": [32, 186], "unifi": [32, 114], "were": [32, 133, 138, 142, 203, 206, 209], "simplifi": [32, 136, 202, 208], "simultan": 32, "intern": [32, 170], "aggreg": 32, "transpar": 32, "index": [32, 34, 125, 130, 131, 132, 135, 148, 149, 150, 188, 199, 203, 205], "howev": [32, 123, 199], "constitu": 32, "might": [32, 202, 205], "larg": [32, 141, 193, 202, 210], "comput": [32, 36, 73, 79, 89, 93, 98, 101, 125, 126, 130, 131, 136, 137, 139, 159, 160, 177, 192, 205, 209, 210], "cumul": 32, "maintain": [32, 210], "indic": [32, 33, 34, 52, 125, 130, 131, 132, 133, 138, 147, 151, 159, 164, 181, 203], "deleg": 32, "retriev": [32, 36, 175], "lead": [32, 106, 154], "high": [32, 36, 200, 208], "scale": [32, 56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 107, 108, 109, 110, 119, 120, 137, 141, 148, 161, 172, 208, 209, 210], "strategi": 32, "stream": [32, 176], "demand": 32, "deriv": [32, 126, 131, 132], "dataset1": 32, "mycustomdataset": 32, "params1": 32, "dataset2": 32, "params2": 32, "concat_dataset": 32, "total": [32, 135, 138, 148, 179, 197, 201, 205, 207, 208], "data_point": 32, "1500": 32, "element": [32, 205], "accomplish": [32, 40, 43, 47], "instruct_dataset": [32, 204], "vicgal": [32, 204], "gpt4": [32, 204], "alpacainstructtempl": [32, 43, 204], "samsum": [32, 44, 204], "summarizetempl": [32, 203, 204], "focus": [32, 206], "enhanc": [32, 133, 210], "divers": 32, "machin": [32, 139, 173, 202, 205], "instructtempl": [33, 35, 204], "contribut": [33, 39, 42, 44, 138, 148], "variabl": [33, 35, 43, 168, 181, 204, 210], "disabl": [33, 35, 37, 41, 43, 47, 48, 142, 192, 209], "recommend": [33, 35, 37, 39, 41, 43, 47, 48, 186, 203, 205, 210], "highest": [33, 35, 37, 39, 41, 43, 47, 48], "sequenc": [33, 34, 35, 37, 39, 41, 43, 45, 47, 48, 62, 63, 67, 72, 73, 79, 88, 89, 93, 98, 101, 106, 107, 109, 111, 113, 118, 119, 121, 125, 128, 130, 131, 133, 148, 149, 150, 151, 155, 157, 159, 188, 203], "ds": [34, 45], "padding_idx": [34, 149, 150, 188], "max_pack": 34, "split_across_pack": [34, 47], "greedi": 34, "pack": [34, 38, 39, 40, 42, 43, 44, 45, 47, 48, 125, 130, 131, 132, 209], "done": [34, 145, 174, 183, 208, 209, 210], "outsid": [34, 192, 193, 208], "sampler": [34, 206], "part": [34, 139, 203, 210], "buffer": 34, "enough": [34, 203], "attent": [34, 52, 56, 57, 58, 63, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 113, 119, 120, 121, 122, 125, 128, 130, 131, 132, 145, 146, 159, 207, 208, 210], "lower": [34, 208], "triangular": 34, "cross": [34, 159], "attend": [34, 125, 131, 132, 159], "rel": [34, 125, 130, 131, 132, 136, 177, 208], "pad": [34, 133, 138, 148, 149, 150, 151, 161, 162, 188, 204], "max": [34, 45, 62, 72, 106, 118, 131, 133, 135, 155, 157, 202, 208], "wise": 34, "collat": [34, 188, 204], "made": [34, 40, 43, 47, 130, 205], "smaller": [34, 137, 205, 207, 208, 209, 210], "jam": 34, "vari": 34, "s1": [34, 72, 106, 154], "s2": [34, 72, 106, 154], "s3": 34, "s4": 34, "contamin": 34, "input_po": [34, 125, 128, 130, 131, 132], "matrix": 34, "causal": [34, 125, 131, 132], "continu": [34, 133, 204], "increment": 34, "move": [34, 47, 131], "entir": [34, 47, 183, 203, 210], "avoid": [34, 47, 129, 133, 134, 137, 192, 202, 209, 210], "sentenc": [34, 47, 106], "message_transform": 36, "model_transform": [36, 42, 44], "prompt_templ": [36, 42, 44], "filter_fn": 36, "supervis": 36, "remot": 36, "At": [36, 131], "uniqu": [36, 72, 168], "extract": [36, 41, 156], "becaus": [36, 67, 128, 131, 133, 167, 202, 203, 209], "against": [36, 194, 209, 210], "round": [36, 209], "involv": [36, 209], "incorpor": [36, 136, 204], "media": 36, "unit": [36, 183, 200], "row": [36, 125, 131, 132, 161, 203], "happen": 36, "ti": [36, 67], "agnost": [36, 204], "treat": [36, 133, 142, 170, 203], "final": [36, 56, 57, 58, 63, 67, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 119, 120, 126, 131, 142, 145, 146, 205, 207, 208, 210], "modal": 36, "minimum": 36, "gear": 36, "whenev": [36, 208], "commun": [36, 203, 204, 205], "chatmlformat": [36, 40], "filter": [36, 209], "prior": [36, 39, 40, 42, 43, 44, 45, 47, 48], "ref": [36, 122, 123, 187], "add_eo": [37, 47, 62, 72, 88, 106, 118, 154, 155, 203], "freeform": [37, 47], "unstructur": [37, 47, 48], "corpu": [37, 41, 47, 48], "tabular": [37, 47], "txt": [37, 47, 184, 204, 206], "eo": [37, 47, 106, 118, 123, 154, 157, 203, 204], "yahma": [38, 43], "variant": [38, 42, 44], "version": [38, 67, 79, 93, 101, 107, 109, 119, 125, 172, 194, 199, 203, 207, 209, 210], "page": [38, 48, 199, 200, 202, 206, 207], "tatsu": 39, "lab": 39, "codebas": [39, 42, 44, 205], "anyth": [39, 41, 45, 46], "subset": [39, 41, 42, 44, 45, 46, 48, 67, 79, 93, 101, 107, 109, 119, 143], "10": [39, 41, 42, 44, 45, 46, 48, 133, 150, 188, 205, 207, 209, 210], "alpaca_d": 39, "batch_siz": [39, 42, 44, 125, 128, 131, 132, 136, 137, 139, 149, 151, 205, 209], "conversation_styl": [40, 204], "chatdataset": [40, 45, 203, 204], "friendli": [40, 43, 47, 172, 203], "check": [40, 49, 50, 51, 52, 131, 133, 145, 174, 181, 194, 198, 203, 205, 206, 208], "huggingfaceh4": 40, "no_robot": 40, "2096": [40, 43, 47], "packeddataset": [40, 42, 43, 44, 47, 48, 204], "ccdv": 41, "cnn_dailymail": 41, "textcompletiondataset": [41, 47, 48, 204], "cnn": 41, "dailymail": 41, "articl": [41, 48], "highlight": [41, 210], "_transform": [42, 44, 133], "liweili": 42, "c4_200m": 42, "sftdataset": [42, 44], "mirror": [42, 44], "llama_recip": [42, 44], "grammarerrorcorrectiontempl": [42, 44], "grammar_d": 42, "alpaca_clean": 43, "samsung": 44, "samsum_d": 44, "open": [45, 64, 65, 204, 205], "orca": 45, "slimorca": 45, "dedup": 45, "1024": [45, 46, 204, 209], "prescrib": 45, "least": [45, 207, 208, 209], "though": [45, 203], "351": 45, "82": 45, "391": 45, "221": 45, "220": 45, "193": 45, "12": [45, 133, 150, 199, 209], "471": 45, "lvwerra": [46, 204], "stack": [46, 133, 193, 204], "exchang": [46, 204], "preferencedataset": [46, 204], "stackexchangepair": 46, "allenai": [47, 204, 209], "c4": [47, 204, 209], "data_dir": [47, 204], "realnewslik": [47, 204], "wikitext_document_level": 48, "wikitext": [48, 209], "103": [48, 205], "wikipedia": 48, "clip": [49, 50, 51, 52, 133, 138], "max_num_til": [49, 50, 52, 133, 160], "tile": [49, 50, 51, 52, 133, 159, 160, 163], "patch": [49, 50, 51, 52, 133, 159], "document": [49, 50, 51, 52, 125, 137, 164, 175, 183, 202, 204], "vision_transform": [49, 50, 51, 52], "visiontransform": [49, 50, 51, 52], "divid": [49, 50, 51, 52, 133, 159, 160, 163], "dimension": [49, 50, 51, 52, 133], "aspect_ratio": [49, 50, 133], "bsz": [49, 50, 133, 172], "n_img": [49, 50, 133], "n_tile": [49, 50, 133], "n_token": [49, 50, 51, 133], "aspect": [49, 50, 200], "ratio": [49, 50, 136, 137, 138], "crop": [49, 50, 51, 52, 133, 163], "tile_s": [50, 51, 52, 133, 159, 160, 163], "patch_siz": [50, 51, 52, 133, 159], "local_token_positional_embed": 50, "equival": [50, 137, 139], "_position_embed": [50, 133], "tokenpositionalembed": [50, 133], "gate": [50, 168, 202, 206], "global_token_positional_embed": 50, "advanc": [50, 51, 52, 133, 204], "40": [50, 51, 52, 133, 159, 210], "400": [50, 51, 52, 133, 159, 163], "10x10": [50, 51, 52, 133, 159], "grid": [50, 51, 52, 133, 159], "k": [50, 125, 208], "th": 50, "cls_output_dim": [52, 133], "out_indic": [52, 133], "output_cls_project": 52, "in_channel": [52, 133], "transformerencoderlay": 52, "cl": [52, 133, 204], "head": [52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 130, 131, 168, 207], "intermedi": [52, 63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 133, 167, 190, 207, 210], "fourth": [52, 133], "determin": [52, 146, 161], "channel": [52, 133, 209], "code_llama2": [53, 54, 55, 56, 57, 58, 59, 60, 61, 202], "transformerdecod": [53, 54, 55, 56, 57, 58, 59, 60, 61, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 119, 120, 121, 122, 124, 172, 208], "w": [53, 54, 55, 64, 65, 74, 75, 76, 77, 90, 91, 99, 100, 112, 114, 133, 162, 186, 187, 203, 205, 208, 210], "arxiv": [53, 54, 55, 59, 60, 61, 70, 71, 74, 75, 76, 77, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 125, 129, 130, 133, 136, 137, 138, 139, 147, 159], "org": [53, 54, 55, 59, 60, 61, 70, 71, 72, 74, 75, 76, 77, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 125, 129, 130, 133, 136, 137, 138, 139, 147, 159, 164, 170, 176, 181, 186, 190, 192, 199], "pdf": [53, 54, 55, 147, 159], "2308": [53, 54, 55], "12950": [53, 54, 55], "lora_attn_modul": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 145, 146, 208, 210], "q_proj": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 125, 145, 146, 208, 209, 210], "k_proj": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 125, 145, 146, 208, 209, 210], "v_proj": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 125, 145, 146, 208, 209, 210], "output_proj": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 125, 145, 146, 208, 209, 210], "apply_lora_to_mlp": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 145, 146, 208], "apply_lora_to_output": [56, 57, 58, 59, 60, 61, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 145, 146, 208], "lora_rank": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 208], "lora_alpha": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 208], "float": [56, 57, 58, 59, 60, 61, 63, 67, 68, 69, 70, 71, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 94, 95, 96, 97, 98, 101, 102, 103, 104, 105, 107, 108, 109, 110, 111, 113, 116, 117, 119, 120, 121, 124, 125, 129, 135, 136, 137, 138, 139, 141, 147, 148, 172, 177, 182, 184, 185, 186, 187, 208, 209, 210], "16": [56, 57, 58, 59, 60, 61, 68, 69, 70, 71, 80, 81, 82, 83, 84, 85, 86, 87, 94, 95, 96, 97, 102, 103, 104, 105, 108, 110, 116, 117, 120, 124, 133, 150, 208, 210], "lora_dropout": [56, 57, 58, 59, 60, 61, 67, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 101, 107, 109, 119], "05": [56, 57, 58, 59, 60, 61, 73, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "quantize_bas": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 141, 210], "lora": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 141, 142, 145, 146, 165, 183, 198, 200, 203, 206, 207], "code_llama2_13b": 56, "tloen": [56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120], "8bb8579e403dc78e37fe81ffbb253c413007323f": [56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120], "l41": [56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120], "l43": [56, 57, 58, 68, 69, 80, 81, 82, 83, 94, 95, 102, 103, 120], "linear": [56, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 79, 80, 81, 82, 83, 84, 85, 86, 87, 93, 94, 95, 96, 97, 101, 102, 103, 104, 105, 107, 108, 109, 110, 116, 117, 119, 120, 124, 131, 140, 141, 145, 146, 208, 209, 210], "mlp": [56, 57, 58, 63, 67, 68, 69, 73, 79, 80, 81, 82, 83, 89, 93, 94, 95, 98, 101, 102, 103, 107, 108, 109, 110, 111, 113, 119, 120, 121, 131, 132, 145, 146, 207, 208], "low": [56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 107, 108, 109, 110, 119, 120, 141, 205, 208, 210], "approxim": [56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 107, 108, 109, 110, 119, 120, 141, 208], "factor": [56, 57, 58, 67, 68, 69, 79, 80, 81, 82, 83, 93, 94, 95, 101, 102, 103, 107, 108, 109, 110, 119, 120, 141, 147, 161, 205], "dropout": [56, 57, 58, 63, 67, 73, 79, 80, 81, 82, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 141, 208, 210], "probabl": [56, 57, 58, 67, 79, 80, 81, 82, 93, 101, 107, 109, 119, 136, 137, 138, 139, 141, 172, 205], "code_llama2_70b": 57, "code_llama2_7b": 58, "qlora": [59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 134, 198, 200, 207, 208], "per": [59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 128, 133, 134, 136, 148, 159, 160, 202, 209, 210], "paper": [59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 136, 137, 139, 159, 208, 210], "ab": [59, 60, 61, 70, 71, 74, 75, 76, 77, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 125, 129, 130, 133, 136, 137, 138, 139], "2305": [59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124, 125, 136, 139], "14314": [59, 60, 61, 70, 71, 84, 85, 86, 87, 96, 97, 104, 105, 116, 117, 124], "lora_code_llama2_13b": 59, "lora_code_llama2_70b": 60, "lora_code_llama2_7b": 61, "gemma": [62, 64, 65, 66, 67, 68, 69, 70, 71, 168], "sentencepiec": [62, 72, 106, 118, 154, 207], "pretrain": [62, 72, 88, 106, 118, 154, 155, 202, 203, 206, 208, 210], "spm_model": [62, 72, 106, 118, 154, 203], "tokenized_text": [62, 72, 88, 106, 118, 154, 155], "add_bo": [62, 72, 88, 106, 118, 154, 155, 203], "31587": [62, 72, 88, 106, 118, 154, 155], "29644": [62, 72, 88, 106, 118, 154, 155], "102": [62, 72, 88, 106, 118, 154, 155], "tokenizer_path": [62, 72, 106, 118], "separ": [62, 72, 106, 118, 157, 165, 203, 206, 207, 208, 210], "concat": [62, 72, 106, 118, 157], "1788": [62, 72, 106, 118, 157], "2643": [62, 72, 106, 118, 157], "13": [62, 72, 106, 118, 133, 150, 151, 157, 210], "1792": [62, 72, 106, 118, 157], "9508": [62, 72, 106, 118, 157], "465": [62, 72, 106, 118, 157], "22137": [62, 72, 106, 118, 157], "2933": [62, 72, 106, 118, 157], "join": [62, 72, 106, 118, 157], "attribut": [62, 72, 106, 118, 142, 157, 171], "head_dim": [63, 67, 125, 128, 131], "intermediate_dim": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "attn_dropout": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 131], "norm_ep": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "1e": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 129], "06": [63, 67, 129, 208], "rope_bas": [63, 67, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "10000": [63, 67, 107, 109, 111, 113, 119, 121, 130], "norm_embed": [63, 67], "gemmatransformerdecod": [63, 64, 65, 67, 68, 69, 70, 71], "transformerdecoderlay": [63, 73, 89, 98, 111, 131], "rm": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121], "norm": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 131, 132], "space": [63, 73, 89, 98, 111, 131], "slide": [63, 111, 122], "window": [63, 111, 122, 204], "vocabulari": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 208], "queri": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 131, 132, 207], "mha": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 131], "dimens": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125, 128, 130, 131, 133, 141, 207, 208, 210], "onto": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125], "scaled_dot_product_attent": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125], "epsilon": [63, 67, 73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 138], "rotari": [63, 67, 107, 109, 111, 113, 119, 121, 130, 207], "10_000": [63, 67, 107, 109, 111, 113, 121], "blog": [64, 65], "technolog": [64, 65], "develop": [64, 65, 210], "gemmatoken": 66, "gemma_2b": 68, "gemma_7b": 69, "lora_gemma_2b": 70, "lora_gemma_7b": 71, "card": [72, 88], "regist": [72, 88, 92, 118, 123, 126, 134, 189, 210], "strongli": 72, "beforehand": 72, "html": [72, 164, 170, 176, 181, 186, 190, 192, 198], "problem": [72, 106], "due": [72, 106, 154, 208, 210], "whitespac": [72, 106, 154], "slice": [72, 106], "gqa": [73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125], "mqa": [73, 79, 89, 93, 98, 101, 107, 109, 111, 113, 119, 121, 125], "kvcach": [73, 79, 89, 93, 98, 101, 119, 125, 131], "scale_hidden_dim_for_mlp": [73, 79, 89, 93, 98, 101], "2307": [74, 75, 76, 77], "09288": [74, 75, 76, 77], "classif": [77, 109, 113, 114, 168], "reward": [77, 83, 87, 110, 114, 117, 136, 137, 139, 147, 148, 168], "llama2_70b": 81, "llama2_7b": [82, 208], "classifi": [83, 109, 113, 114, 204], "llama2_reward_7b": [83, 168], "lora_llama2_13b": 84, "lora_llama2_70b": 85, "lora_llama2_7b": [86, 208], "lora_llama2_reward_7b": 87, "special_token": [88, 118, 155, 203], "tiktoken": [88, 155, 207], "left": [88, 118, 149, 208], "canon": [88, 92, 118, 123], "tt_model": [88, 155], "token_id": [88, 106, 152, 155], "truncate_at_eo": [88, 155], "skip_special_token": [88, 155], "show": [88, 155, 159, 199, 202, 203, 208], "skip": [88, 155], "tokenize_head": 88, "tokenize_end": 88, "header": [88, 203], "eom": 88, "wether": 88, "500000": [89, 93, 98, 101], "special_tokens_path": [92, 123], "llama3token": [92, 203], "similarli": [92, 123, 204, 209], "llama3_70b": 94, "llama3_8b": [95, 172, 207, 209], "lora_llama3_70b": 96, "lora_llama3_8b": 97, "llama3_1": [99, 100, 101, 102, 103, 104, 105], "llama3_1_70b": 102, "llama3_1_8b": 103, "lora_llama3_1_70b": 104, "lora_llama3_1_8b": 105, "trim_leading_whitespac": [106, 154], "unbatch": [106, 154], "bo": [106, 123, 154, 157, 203, 204], "trim": [106, 154], "num_class": [109, 113], "announc": 112, "ray2333": 114, "feedback": [114, 136], "mistraltoken": [115, 203], "lora_mistral_7b": 116, "lora_mistral_reward_7b": 117, "ignore_system_prompt": 118, "phi3_mini": [120, 168], "phi": [122, 123, 168], "128k": 122, "nor": 122, "phi3minitoken": 123, "tokenizer_config": 123, "spm": 123, "lm": [123, 138], "unk": 123, "augment": [123, 210], "endoftext": 123, "phi3minisentencepiecebasetoken": 123, "lora_phi3_mini": 124, "pos_embed": [125, 208, 209], "kv_cach": 125, "introduc": [125, 129, 141, 203, 204, 208, 209, 210], "13245v1": 125, "multihead": 125, "extrem": 125, "share": [125, 204, 205], "credit": 125, "lightn": 125, "lit": 125, "lit_gpt": 125, "v": [125, 131, 208], "q": [125, 208], "n_kv_head": 125, "rotarypositionalembed": [125, 208, 209], "cach": [125, 128, 130, 131, 199, 202], "rope": [125, 130], "seq_length": [125, 132, 172], "boolean": [125, 131, 132, 164], "softmax": [125, 131, 132], "j": [125, 131, 132], "seq_len": 125, "bigger": 125, "n_h": [125, 130], "num": [125, 130], "n_kv": 125, "kv": [125, 128, 131, 209], "emb": [125, 131], "h_d": [125, 130], "gate_proj": 126, "down_proj": 126, "up_proj": 126, "silu": 126, "feed": [126, 132], "network": [126, 142, 208, 210], "fed": [126, 203], "multipli": 126, "subclass": [126, 170], "although": [126, 208, 209], "afterward": 126, "former": 126, "hook": [126, 134, 189, 210], "latter": 126, "layernorm": 127, "standalon": 128, "past": 128, "expand": 128, "dpython": [128, 131, 134, 191, 195], "reset": [128, 131, 177], "k_val": 128, "v_val": 128, "assert": 128, "longer": [128, 204], "h": [128, 133, 162, 199, 202], "ep": 129, "root": [129, 186, 187], "squar": 129, "1910": 129, "07467": 129, "verfic": [129, 130], "small": [129, 205], "divis": [129, 163], "propos": 130, "2104": 130, "09864": 130, "l80": 130, "upto": 130, "init": [130, 177, 187, 210], "exceed": 130, "freq": 130, "recomput": 130, "geometr": 130, "progress": [130, 206], "rotat": 130, "angl": 130, "todo": 130, "effici": [130, 145, 175, 198, 200, 205, 206, 208, 209], "belong": [131, 171], "reduc": [131, 136, 200, 204, 208, 209, 210], "statement": 131, "improv": [131, 139, 155, 175, 207, 208], "readabl": [131, 205], "caches_are_en": 131, "arang": 131, "prompt_length": 131, "causal_mask": 131, "m_": 131, "seq": 131, "reset_cach": 131, "setup_cach": 131, "attn": [132, 208, 209, 210], "causalselfattent": [132, 208, 209], "sa_norm": 132, "mlp_norm": 132, "ff": 132, "token_pos_embed": 133, "pre_tile_pos_emb": 133, "post_tile_pos_emb": 133, "cls_project": 133, "vit": 133, "11929": 133, "convolut": 133, "flatten": 133, "downscal": [133, 161, 162], "800x400": 133, "400x400": 133, "clipimagetransform": 133, "broken": 133, "down": [133, 167, 204, 208, 210], "whole": 133, "num_til": [133, 163], "101": 133, "pool": 133, "tiledtokenpositionalembed": 133, "tilepositionalembed": 133, "tile_pos_emb": 133, "even": [133, 199, 202, 203, 204, 207, 208, 210], "8x8": 133, "14": [133, 150, 209, 210], "15": [133, 150, 175, 203, 205, 208, 210], "17": [133, 150, 208], "18": [133, 150, 207], "19": [133, 150, 210], "20": [133, 150, 151, 209], "21": 133, "22": 133, "23": [133, 135], "24": [133, 163, 206, 207], "25": [133, 205], "26": 133, "27": [133, 205], "28": [133, 205], "29": [133, 210], "30": [133, 151, 209], "31": [133, 207], "33": 133, "34": 133, "35": [133, 210], "36": 133, "37": 133, "38": [133, 205], "39": 133, "41": 133, "42": 133, "43": 133, "44": 133, "45": 133, "46": 133, "47": 133, "48": [133, 205, 210], "49": 133, "50": [133, 151, 163, 205], "51": 133, "52": [133, 206], "53": 133, "54": 133, "55": [133, 206], "56": 133, "57": [133, 208, 210], "58": 133, "59": [133, 210], "60": 133, "61": [133, 205], "62": 133, "63": 133, "64": [133, 208], "num_patches_per_til": 133, "emb_dim": 133, "greater": [133, 194], "constain": 133, "anim": [133, 204], "max_n_img": 133, "n_channel": 133, "hidden_st": 133, "vision_util": 133, "tile_crop": 133, "num_channel": 133, "image_s": [133, 162], "800": [133, 162], "patch_grid_s": 133, "random": [133, 192, 206], "rand": [133, 161, 162, 163], "nch": 133, "tile_cropped_imag": 133, "batch_imag": 133, "unsqueez": 133, "batch_aspect_ratio": 133, "clip_vision_encod": 133, "common_util": 134, "bfloat16": [134, 191, 205, 206, 207, 208, 209], "offload_to_cpu": 134, "nf4": [134, 210], "restor": 134, "higher": [134, 137, 207, 209, 210], "offload": [134, 210], "increas": [134, 135, 136, 207, 208, 209], "peak": [134, 177, 182, 205, 207, 208, 210], "gpu": [134, 202, 205, 206, 207, 208, 209, 210], "_register_state_dict_hook": 134, "m": [134, 172, 203, 209], "mymodul": 134, "_after_": 134, "nf4tensor": [134, 210], "unquant": [134, 209, 210], "unus": 134, "num_warmup_step": 135, "num_training_step": 135, "num_cycl": [135, 193], "last_epoch": 135, "lambdalr": 135, "rate": [135, 200, 206], "schedul": [135, 193, 206], "linearli": 135, "decreas": [135, 204, 208, 209, 210], "cosin": 135, "v4": 135, "src": 135, "l104": 135, "warmup": [135, 193], "phase": 135, "wave": 135, "half": 135, "lr_schedul": 135, "beta": 136, "label_smooth": 136, "dpo": [136, 137, 139, 142, 150], "18290": 136, "intuit": [136, 137, 139], "dispref": 136, "dynam": [136, 209], "degener": 136, "occur": 136, "naiv": 136, "trl": [136, 137, 139], "librari": [136, 137, 139, 170, 174, 176, 192, 198, 200, 202, 204, 210], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 136, "dpo_train": [136, 137, 139], "l844": 136, "retain": [136, 210], "2009": 136, "01325": 136, "polici": [136, 137, 138, 139, 142, 148, 164, 175, 183, 190], "align": [136, 203], "regular": [136, 137, 209, 210], "baselin": [136, 138, 205, 208], "rather": 136, "overhead": [136, 209], "temperatur": [136, 137, 139, 172, 205], "uncertainti": 136, "policy_chosen_logp": [136, 137, 139], "policy_rejected_logp": [136, 137, 139], "reference_chosen_logp": [136, 137, 139], "reference_rejected_logp": [136, 137, 139], "chosen": [136, 137, 139, 193, 204], "reject": [136, 137, 139, 204], "chosen_reward": [136, 137, 139], "rejected_reward": [136, 137, 139], "tau": 137, "optimis": 137, "ipo": 137, "2310": 137, "12036": 137, "pi": 137, "pi_ref": 137, "regress": [137, 139], "gap": 137, "likelihood": 137, "he": 137, "weaker": 137, "regularis": 137, "logprob": [137, 148], "word": [137, 209], "unlik": [137, 145], "toward": 137, "4dce042a3863db1d375358e8c8092b874b02934b": [137, 139], "l1143": 137, "reciproc": 137, "larger": [137, 167, 205, 207], "value_clip_rang": 138, "value_coeff": 138, "proxim": 138, "1707": 138, "06347": 138, "eqn": 138, "vwxyzjn": 138, "ccc19538e817e98a60d3253242ac15e2a562cb49": 138, "lm_human_preference_detail": 138, "train_policy_acceler": 138, "l719": 138, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 138, "ppo2": 138, "l68": 138, "l75": 138, "coeffici": [138, 148], "pi_old_logprob": 138, "pi_logprob": 138, "phi_old_valu": 138, "phi_valu": 138, "padding_mask": [138, 151], "value_padding_mask": 138, "old": 138, "predict": [138, 147, 148, 172], "participag": 138, "five": 138, "policy_loss": 138, "value_loss": 138, "clipfrac": 138, "fraction": 138, "gamma": [139, 147], "statist": 139, "rso": 139, "hing": 139, "2309": 139, "06657": 139, "logist": 139, "slic": 139, "10425": 139, "almost": [139, 208], "vector": [139, 203], "svm": 139, "counter": 139, "l1141": 139, "peft": [140, 141, 142, 143, 144, 145, 146, 165, 208, 210], "protocol": 140, "adapter_param": [140, 141, 142, 143, 144], "proj": 140, "in_dim": [140, 141, 208, 210], "out_dim": [140, 141, 208, 210], "bia": [140, 141, 208, 209, 210], "loralinear": [140, 208, 210], "alpha": [141, 208, 210], "use_bia": 141, "perturb": 141, "decomposit": [141, 208], "matric": [141, 183, 208, 210], "trainabl": [141, 144, 183, 208, 210], "mapsto": 141, "w_0x": 141, "r": [141, 208], "bax": 141, "lora_a": [141, 208, 210], "lora_b": [141, 208, 210], "temporarili": 142, "neural": [142, 208, 210], "caller": 142, "whose": [142, 189], "yield": 142, "get_adapter_param": [144, 208], "base_miss": 145, "base_unexpect": 145, "lora_miss": 145, "lora_unexpect": 145, "validate_state_dict_for_lora": [145, 208], "reli": [145, 157, 205, 207], "unexpect": 145, "strict": [145, 208], "pull": [145, 202], "120600": 145, "assertionerror": [145, 146, 150], "nonempti": 145, "full_model_state_dict_kei": 146, "lora_state_dict_kei": 146, "base_model_state_dict_kei": 146, "confirm": [146, 199], "lora_modul": 146, "complement": 146, "disjoint": 146, "overlap": 146, "rlhf": [147, 148, 149, 150, 151, 204], "lmbda": 147, "estim": [147, 148], "1506": 147, "02438": 147, "reponse_len": [147, 148], "receiv": [147, 203], "discount": 147, "gae": 147, "lambda": 147, "particip": [147, 159], "score": 148, "ref_logprob": 148, "kl_coeff": 148, "valid_score_idx": 148, "kl": 148, "response_len": 148, "total_reward": 148, "combin": [148, 160], "diverg": 148, "kl_reward": 148, "ignore_idx": [150, 188], "input_id": 150, "chosen_input_id": [150, 204], "rejected_input_id": [150, 204], "chosen_label": [150, 204], "rejected_label": [150, 204], "stop_token": [151, 172], "fill_valu": 151, "stop": [151, 172], "sequence_length": 151, "pad_id": 151, "been": [151, 175, 203, 209], "stop_token_id": 151, "869": 151, "eos_mask": 151, "truncated_sequ": 151, "light": 154, "sentencepieceprocessor": 154, "prefix": 154, "bos_id": [155, 157], "lightweight": [155, 203], "break": 155, "substr": 155, "repetit": 155, "speed": [155, 193, 207, 209, 210], "identif": 155, "regex": 155, "chunk": 155, "present": [155, 167], "absent": 155, "tokenizer_json_path": 156, "heavili": 157, "beggin": 157, "runtimeerror": [157, 169, 174, 180], "satisfi": [157, 161, 205], "loos": 158, "image_token_id": 159, "laid": 159, "fig": 159, "flamingo": 159, "2204": 159, "14198": 159, "immedi": 159, "until": 159, "img1": 159, "img2": 159, "img3": 159, "dog": 159, "cat": 159, "text_seq_len": 159, "image_seq_len": 159, "equal": [159, 163, 194], "resolut": [160, 161, 162], "1x1": 160, "1x2": 160, "2x1": 160, "side": [160, 161, 162], "height": [160, 161, 162], "width": [160, 161, 162, 209], "224": [160, 161, 162], "896": 160, "448": [160, 161, 162], "672": [160, 161], "possible_resolut": 161, "resize_to_max_canva": 161, "canva": 161, "resiz": [161, 162], "distort": [161, 162], "select": 161, "smallest": 161, "upscal": [161, 162], "2x": 161, "5x": 161, "canvas": 161, "condit": [161, 172, 181, 202, 204], "pick": 161, "lowest": [161, 208], "area": [161, 205], "minim": [161, 204, 206, 208, 209, 210], "200": [161, 163, 210], "300": [161, 162, 163, 205], "scale_height": 161, "1200": 161, "3600": 161, "2400": 161, "scale_width": 161, "7467": 161, "4933": 161, "scaling_factor": 161, "upscaling_opt": 161, "selected_scal": 161, "150528": 161, "100352": 161, "optimal_canva": 161, "target_s": 162, "resampl": 162, "interpolationmod": 162, "max_upscaling_s": 162, "exce": 162, "torchvis": 162, "nearest": 162, "nearest_exact": 162, "bilinear": 162, "bicub": 162, "1194": 162, "1344": 162, "stai": 162, "600": [162, 163], "500": [162, 208], "1000": [162, 164, 209], "488": 162, "channel_s": 163, "4x6": 163, "2x3": 163, "datatyp": [164, 210], "denot": 164, "integ": [164, 188, 192], "auto_wrap_polici": [164, 175, 190], "submodul": [164, 183], "obei": 164, "contract": 164, "get_fsdp_polici": 164, "modules_to_wrap": [164, 175, 183], "min_num_param": 164, "my_fsdp_polici": 164, "recurs": [164, 183, 186], "isinst": [164, 204], "sum": [164, 208], "p": [164, 169, 208, 209, 210], "numel": [164, 208], "stabl": [164, 181, 186, 192, 199], "safe_seri": 165, "from_pretrain": 165, "0001_of_0003": 165, "0002_of_0003": 165, "preserv": [165, 210], "weight_map": [165, 205], "convert_weight": 165, "_model_typ": [165, 168], "intermediate_checkpoint": [165, 166, 167], "adapter_onli": [165, 166, 167], "_weight_map": 165, "shard": [166, 207], "wip": 166, "qualnam": 168, "boundari": 168, "distinguish": 168, "mistral_reward_7b": 168, "qwen2": 168, "my_new_model": 168, "my_custom_state_dict_map": 168, "optim_map": 169, "bare": 169, "bone": 169, "distribut": [169, 173, 180, 181, 190, 192, 200, 202, 206, 207], "optim_dict": [169, 171, 189], "cfg_optim": 169, "ckpt": 169, "optim_ckpt": 169, "placeholder_optim_dict": 169, "optiminbackwardwrapp": 169, "get_optim_kei": 169, "arbitrari": [169, 208], "hyperparamet": [169, 200, 206, 208, 210], "optim_ckpt_map": 169, "loadabl": 169, "argpars": 170, "argumentpars": 170, "builtin": 170, "said": 170, "noth": 170, "consult": 170, "info": [170, 206], "parse_known_arg": 170, "namespac": 170, "act": 170, "precid": 170, "parse_arg": 170, "too": [170, 207], "optimizerinbackwardwrapp": 171, "top": [171, 210], "named_paramet": 171, "max_generated_token": 172, "top_k": [172, 205], "custom_generate_next_token": 172, "prune": [172, 210], "compil": [172, 205, 207, 210], "generate_next_token": 172, "hi": [172, 203], "my": [172, 202, 203, 204, 205, 207], "jeremi": 172, "float32": 174, "bf16": [174, 210], "inde": [174, 205], "kernel": 174, "isn": [174, 202], "hardwar": [174, 200, 204, 205, 208], "memory_efficient_fsdp_wrap": [175, 209], "maxim": [175, 183, 198, 200], "workload": [175, 209], "alongsid": 175, "ac": 175, "fullyshardeddataparallel": [175, 183], "fsdppolicytyp": [175, 183], "handler": 176, "reset_stat": 177, "track": 177, "alloc": [177, 182, 183, 207, 210], "reserv": [177, 182, 203, 210], "stat": [177, 182, 210], "int4": [178, 209], "4w": 178, "recogn": 178, "int8dynactint4weightquant": [178, 209], "8da4w": [178, 209], "int8dynactint4weightqatquant": [178, 209], "qat": [178, 198], "mode": [178, 205], "aka": 179, "master": 181, "port": [181, 202], "address": 181, "hold": [181, 206], "peak_memory_act": 182, "peak_memory_alloc": 182, "peak_memory_reserv": 182, "get_memory_stat": 182, "hierarch": 183, "requires_grad": [183, 208, 210], "filenam": 184, "log_": 184, "unixtimestamp": 184, "thread": 184, "safe": 184, "flush": [184, 185, 186, 187], "ndarrai": [184, 185, 186, 187], "scalar": [184, 185, 186, 187], "record": [184, 185, 186, 187, 193], "payload": [184, 185, 186, 187], "organize_log": 186, "tensorboard": 186, "subdirectori": 186, "compar": [186, 194, 205, 207, 208, 209, 210], "logdir": 186, "startup": 186, "tree": [186, 204, 205, 207], "tfevent": 186, "encount": 186, "frontend": 186, "organ": [186, 202], "accordingli": [186, 209], "my_log_dir": 186, "view": [186, 206], "my_metr": [186, 187], "termin": [186, 187], "entiti": 187, "bias": [187, 208, 210], "sent": 187, "usernam": 187, "my_project": 187, "my_ent": 187, "my_group": 187, "importerror": 187, "account": [187, 208, 210], "log_config": 187, "link": [187, 205, 207], "capecap": 187, "6053ofw0": 187, "torchtune_config_j67sb73v": 187, "longest": 188, "token_pair": 188, "soon": 189, "readi": [189, 198, 203, 209], "grad": 189, "acwrappolicytyp": 190, "author": [190, 200, 206, 210], "fsdp_adavnced_tutori": 190, "insid": 191, "contextmanag": 191, "debug_mod": 192, "pseudo": 192, "commonli": [192, 208, 210], "numpi": 192, "determinist": 192, "global": [192, 204], "warn": 192, "nondeterminist": 192, "cudnn": 192, "set_deterministic_debug_mod": 192, "profile_memori": 193, "with_stack": 193, "record_shap": 193, "with_flop": 193, "wait_step": 193, "warmup_step": 193, "active_step": 193, "profil": 193, "layout": 193, "trace": 193, "profileract": 193, "gradient_accumul": 193, "sensibl": 193, "default_schedul": 193, "reduct": [193, 208], "iter": [193, 195, 210], "scope": 193, "flop": 193, "wait": 193, "cycl": 193, "repeat": 193, "__version__": 194, "named_param": 195, "generated_examples_python": 196, "zip": 196, "galleri": [196, 201], "sphinx": 196, "000": [197, 201, 207], "execut": [197, 201], "generated_exampl": 197, "mem": [197, 201], "mb": [197, 201], "topic": 198, "gentl": 198, "introduct": 198, "first_finetune_tutori": 198, "workflow": [198, 204, 206, 208], "requisit": 199, "proper": [199, 206], "host": [199, 202, 206], "latest": [199, 206, 210], "And": [199, 205], "ls": [199, 202, 205, 206, 207], "welcom": [199, 202], "greatest": [199, 206], "contributor": 199, "cd": [199, 205], "commit": 199, "branch": 199, "url": 199, "whl": 199, "therebi": [199, 209, 210], "forc": 199, "reinstal": 199, "opt": [199, 206], "suffix": 199, "cu121": 199, "On": [200, 208], "pointer": 200, "emphas": 200, "simplic": 200, "component": 200, "prove": 200, "democrat": 200, "box": [200, 210], "zoo": 200, "varieti": [200, 208], "techniqu": [200, 205, 206, 207, 208, 209], "integr": [200, 205, 206, 207, 208, 209, 210], "excit": 200, "checkout": 200, "quickstart": 200, "attain": 200, "better": [200, 203, 204, 205, 209], "chekckpoint": 200, "embodi": 200, "philosophi": 200, "usabl": 200, "composit": 200, "hard": [200, 204], "outlin": 200, "unecessari": 200, "never": 200, "thoroughli": 200, "short": 202, "subcommand": 202, "anytim": 202, "symlink": 202, "auto": 202, "wrote": 202, "readm": [202, 205, 207], "md": 202, "lot": [202, 205], "recent": 202, "releas": [202, 207], "agre": 202, "term": 202, "perman": 202, "eat": 202, "bandwith": 202, "storag": [202, 210], "00030": 202, "ootb": 202, "full_finetune_single_devic": [202, 204, 205, 206], "7b_full_low_memori": [202, 205, 206], "8b_full_single_devic": [202, 204], "mini_full_low_memori": 202, "7b_full": [202, 205, 206], "13b_full": [202, 205, 206], "70b_full": 202, "edit": 202, "clobber": 202, "destin": 202, "lora_finetune_distribut": [202, 207, 208], "torchrun": 202, "8b_lora_single_devic": [202, 203, 207], "launch": [202, 203, 206], "nproc": 202, "node": 202, "worker": 202, "nnode": [202, 208, 209], "minimum_nod": 202, "maximum_nod": 202, "fail": 202, "rdzv": 202, "rendezv": 202, "endpoint": 202, "8b_lora": [202, 207], "bypass": 202, "vice": 202, "versa": 202, "fancy_lora": 202, "8b_fancy_lora": 202, "sai": [202, 203, 206], "know": [203, 204, 205, 208], "intend": 203, "nice": 203, "meet": 203, "overhaul": 203, "begin_of_text": 203, "start_header_id": 203, "end_header_id": 203, "eot_id": 203, "yet": [203, 205], "untrain": 203, "accompani": 203, "who": 203, "influenti": 203, "hip": 203, "hop": 203, "artist": 203, "2pac": 203, "rakim": 203, "c": 203, "na": 203, "flavor": [203, 204], "msg": 203, "formatted_messag": [203, 204], "nyou": [203, 204], "nwho": 203, "why": [203, 206, 208], "518": 203, "25580": 203, "29962": 203, "3532": 203, "14816": 203, "29903": 203, "6778": 203, "_spm_model": 203, "piece_to_id": 203, "place": [203, 204], "manual": [203, 210], "529": 203, "29879": 203, "29958": 203, "nhere": 203, "128000": [203, 209], "128009": 203, "pure": 203, "That": 203, "won": 203, "mess": 203, "govern": 203, "prime": 203, "strictli": 203, "ask": 203, "untouch": 203, "robust": 203, "onlin": 203, "forum": 203, "panda": 203, "pd": 203, "df": 203, "read_csv": 203, "your_fil": 203, "nrow": 203, "tolist": 203, "iloc": 203, "gp": 203, "satellit": 203, "thing": [203, 210], "dataclass": 203, "message_convert": 203, "input_msg": 203, "output_msg": 203, "But": [203, 205, 208], "mistralchatformat": 203, "custom_dataset": 203, "2048": 203, "honor": 203, "copi": [203, 205, 206, 207, 209, 210], "custom_8b_lora_single_devic": 203, "steer": 204, "wheel": 204, "publicli": 204, "great": [204, 205], "hood": [204, 210], "text_completion_dataset": [204, 209], "padded_col": 204, "upper": 204, "constraint": [204, 208], "slow": [204, 210], "signific": [204, 209], "speedup": [204, 205, 207], "my_data": 204, "fix": [204, 209], "goal": [204, 209], "respond": 204, "plant": 204, "miner": 204, "oak": 204, "copper": 204, "ore": 204, "eleph": 204, "customtempl": 204, "importlib": 204, "import_modul": 204, "mechan": 204, "search": 204, "often": [204, 208], "interpret": 204, "site": [204, 205], "runtim": 204, "pythonpath": 204, "chat_dataset": 204, "quit": [204, 210], "customchatformat": 204, "concatdataset": 204, "drive": 204, "rajpurkar": 204, "io": 204, "squad": 204, "explor": 204, "few": [204, 207, 208, 210], "adjust": [204, 209], "chosen_messag": 204, "transformed_sampl": 204, "key_chosen": 204, "rejected_messag": 204, "key_reject": 204, "c_mask": 204, "np": 204, "cross_entropy_ignore_idx": 204, "r_mask": 204, "stack_exchanged_paired_dataset": 204, "had": 204, "stackexchangedpairedtempl": 204, "response_j": 204, "response_k": 204, "rl": 204, "favorit": [205, 208], "seemlessli": 205, "beyond": [205, 210], "connect": [205, 209], "amount": 205, "natur": 205, "export": 205, "mobil": 205, "phone": 205, "leverag": [205, 207, 210], "plai": 205, "freez": [205, 208], "percentag": 205, "learnabl": 205, "keep": [205, 208], "16gb": [205, 208], "rtx": 205, "3090": 205, "4090": 205, "hour": 205, "7b_qlora_single_devic": [205, 206, 210], "473": 205, "98": [205, 210], "gb": [205, 207, 208, 209, 210], "484": 205, "01": [205, 206], "fact": [205, 207, 208], "third": 205, "realli": 205, "eleuther_ev": [205, 207, 209], "eleuther_evalu": [205, 207, 209], "lm_eval": [205, 207], "plan": 205, "custom_eval_config": [205, 207], "truthfulqa_mc2": [205, 207, 208], "measur": [205, 207], "propens": [205, 207], "shot": [205, 207, 209], "accuraci": [205, 207, 208, 209, 210], "324": 205, "loglikelihood": 205, "195": 205, "121": 205, "second": [205, 208, 210], "197": 205, "acc": [205, 209], "388": 205, "shown": [205, 209], "489": 205, "seem": 205, "custom_generation_config": [205, 207], "kick": 205, "interest": 205, "visit": 205, "bai": 205, "92": 205, "exploratorium": 205, "san": 205, "francisco": 205, "magazin": 205, "awesom": 205, "bridg": 205, "pretti": 205, "cool": 205, "96": [205, 210], "sec": [205, 207], "83": 205, "99": [205, 208], "72": 205, "littl": 205, "torchao": [205, 207, 209, 210], "int8_weight_onli": [205, 207], "int8_dynamic_activation_int8_weight": [205, 207], "ao": [205, 207], "quant_api": [205, 207], "_": [205, 207], "int4_weight_onli": [205, 207], "previous": [205, 207, 208], "benefit": 205, "doesn": 205, "fast": 205, "clone": [205, 208, 209, 210], "assumpt": 205, "new_dir": 205, "output_dict": 205, "sd_1": 205, "sd_2": 205, "dump": 205, "convert_hf_checkpoint": 205, "checkpoint_path": 205, "justin": 205, "school": 205, "math": 205, "teacher": 205, "ws": 205, "94": [205, 207], "bandwidth": [205, 207], "1391": 205, "84": 205, "thats": 205, "seamlessli": 205, "authent": [205, 206], "hopefulli": 205, "gave": 205, "grant": 206, "minut": 206, "agreement": 206, "altern": 206, "hackabl": 206, "singularli": 206, "technic": 206, "purpos": [206, 207], "depth": 206, "principl": 206, "boilerpl": 206, "substanti": [206, 208], "custom_config": 206, "replic": 206, "lorafinetunerecipesingledevic": 206, "lora_finetune_output": 206, "log_1713194212": 206, "3697006702423096": 206, "25880": [206, 210], "83it": 206, "monitor": 206, "tqdm": 206, "interv": 206, "e2": 206, "focu": 207, "128": [207, 208], "256": [207, 209], "theta": 207, "gain": 207, "basic": 207, "observ": [207, 209], "consum": [207, 210], "vram": [207, 208, 209], "overal": 207, "8b_qlora_single_devic": 207, "coupl": [207, 208, 210], "meta_model_0": [207, 209], "did": [207, 210], "122": 207, "sarah": 207, "busi": 207, "mum": 207, "young": 207, "children": 207, "live": 207, "north": 207, "east": 207, "england": 207, "135": 207, "88": 207, "138": 207, "346": 207, "09": 207, "139": 207, "broader": 207, "teach": 208, "straight": 208, "jump": 208, "unfamiliar": 208, "oppos": [208, 210], "momentum": 208, "relat": 208, "aghajanyan": 208, "et": 208, "al": 208, "hypothes": 208, "intrins": 208, "four": 208, "eight": 208, "practic": 208, "blue": 208, "rememb": 208, "approx": 208, "15m": 208, "8192": [208, 209], "65k": 208, "frozen_out": [208, 210], "lora_out": [208, 210], "omit": 208, "base_model": 208, "lora_model": 208, "lora_llama_2_7b": [208, 210], "alon": 208, "bit": [208, 209, 210], "in_featur": [208, 209], "out_featur": [208, 209], "inplac": 208, "feel": 208, "free": 208, "validate_missing_and_unexpected_for_lora": 208, "peft_util": 208, "set_trainable_param": 208, "fetch": 208, "lora_param": 208, "total_param": 208, "trainable_param": 208, "2f": 208, "6742609920": 208, "4194304": 208, "7b_lora": 208, "my_model_checkpoint_path": [208, 209, 210], "tokenizer_checkpoint": [208, 209, 210], "my_tokenizer_checkpoint_path": [208, 209, 210], "factori": 208, "benefici": 208, "impact": 208, "minor": 208, "good": 208, "lora_experiment_1": 208, "smooth": [208, 210], "curv": [208, 210], "ran": 208, "footprint": [208, 209], "commod": 208, "cogniz": 208, "ax": 208, "parallel": 208, "truthfulqa": 208, "475": 208, "87": 208, "508": 208, "86": 208, "504": 208, "04": 208, "514": 208, "absolut": 208, "4gb": 208, "tradeoff": 208, "potenti": 208, "awar": 209, "incur": [209, 210], "degrad": [209, 210], "perplex": 209, "simul": 209, "ultim": 209, "ptq": 209, "fake": 209, "kept": 209, "cast": 209, "nois": 209, "henc": 209, "x_q": 209, "int8": 209, "zp": 209, "x_float": 209, "qmin": 209, "qmax": 209, "clamp": 209, "x_fq": 209, "dequant": 209, "insert": 209, "proce": 209, "prepared_model": 209, "swap": 209, "int8dynactint4weightqatlinear": 209, "int8dynactint4weightlinear": 209, "train_loop": 209, "converted_model": 209, "demonstr": 209, "recov": 209, "modif": 209, "8b_qat_ful": 209, "custom_8b_qat_ful": 209, "2000": 209, "fake_quant_after_n_step": 209, "issu": 209, "futur": 209, "empir": 209, "led": 209, "presum": 209, "80gb": 209, "qat_distribut": 209, "op": 209, "mutat": 209, "5gb": 209, "custom_quant": 209, "groupsiz": 209, "poorli": 209, "custom_eleuther_evalu": 209, "fullmodeltorchtunecheckpoint": 209, "hellaswag": 209, "max_seq_length": 209, "my_eleuther_evalu": 209, "stderr": 209, "word_perplex": 209, "9148": 209, "byte_perplex": 209, "5357": 209, "bits_per_byt": 209, "6189": 209, "5687": 209, "0049": 209, "acc_norm": 209, "7536": 209, "0043": 209, "portion": [209, 210], "drop": 209, "74": 209, "048": 209, "190": 209, "7735": 209, "5598": 209, "6413": 209, "5481": 209, "0050": 209, "7390": 209, "0044": 209, "7251": 209, "4994": 209, "5844": 209, "5740": 209, "7610": 209, "outperform": 209, "importantli": 209, "characterist": 209, "187": 209, "958": 209, "halv": 209, "int4weightonlyquant": 209, "motiv": 209, "constrain": 209, "edg": 209, "smartphon": 209, "executorch": 209, "xnnpack": 209, "export_llama": 209, "use_sdpa_with_kv_cach": 209, "qmode": 209, "group_siz": 209, "get_bos_id": 209, "get_eos_id": 209, "128001": 209, "output_nam": 209, "llama3_8da4w": 209, "pte": 209, "881": 209, "oneplu": 209, "709": 209, "tok": 209, "815": 209, "316": 209, "364": 209, "highli": 210, "vanilla": 210, "held": 210, "therefor": 210, "bespok": 210, "normalfloat": 210, "8x": 210, "vast": 210, "major": 210, "normatfloat": 210, "doubl": 210, "themselv": 210, "deepdiv": 210, "idea": 210, "distinct": 210, "de": 210, "counterpart": 210, "set_default_devic": 210, "qlora_linear": 210, "memory_alloc": 210, "177": 210, "152": 210, "del": 210, "empty_cach": 210, "lora_linear": 210, "081": 210, "344": 210, "qlora_llama2_7b": 210, "qlora_model": 210, "essenti": 210, "reparametrize_as_dtype_state_dict_post_hook": 210, "slower": 210, "149": 210, "9157477021217346": 210, "02": 210, "08": 210, "15it": 210, "nightli": 210, "hundr": 210, "228": 210, "8158286809921265": 210, "95it": 210, "exercis": 210, "linear_nf4": 210, "to_nf4": 210, "linear_weight": 210, "autograd": 210, "incom": 210}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 3, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "PromptTemplate"], [23, 1, 1, "", "PromptTemplateInterface"], [24, 3, 1, "", "Role"], [25, 1, 1, "", "StackExchangedPairedTemplate"], [26, 3, 1, "", "SummarizeTemplate"], [27, 0, 1, "", "get_openai_messages"], [28, 0, 1, "", "get_sharegpt_messages"], [29, 0, 1, "", "truncate"], [30, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 4, 1, "", "contains_media"], [20, 2, 1, "", "from_dict"], [20, 4, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[25, 2, 1, "", "format"]], "torchtune.datasets": [[31, 1, 1, "", "ChatDataset"], [32, 1, 1, "", "ConcatDataset"], [33, 1, 1, "", "InstructDataset"], [34, 1, 1, "", "PackedDataset"], [35, 1, 1, "", "PreferenceDataset"], [36, 1, 1, "", "SFTDataset"], [37, 1, 1, "", "TextCompletionDataset"], [38, 0, 1, "", "alpaca_cleaned_dataset"], [39, 0, 1, "", "alpaca_dataset"], [40, 0, 1, "", "chat_dataset"], [41, 0, 1, "", "cnn_dailymail_articles_dataset"], [42, 0, 1, "", "grammar_dataset"], [43, 0, 1, "", "instruct_dataset"], [44, 0, 1, "", "samsum_dataset"], [45, 0, 1, "", "slimorca_dataset"], [46, 0, 1, "", "stack_exchanged_paired_dataset"], [47, 0, 1, "", "text_completion_dataset"], [48, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[49, 1, 1, "", "TilePositionalEmbedding"], [50, 1, 1, "", "TiledTokenPositionalEmbedding"], [51, 1, 1, "", "TokenPositionalEmbedding"], [52, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[49, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[50, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[51, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[53, 0, 1, "", "code_llama2_13b"], [54, 0, 1, "", "code_llama2_70b"], [55, 0, 1, "", "code_llama2_7b"], [56, 0, 1, "", "lora_code_llama2_13b"], [57, 0, 1, "", "lora_code_llama2_70b"], [58, 0, 1, "", "lora_code_llama2_7b"], [59, 0, 1, "", "qlora_code_llama2_13b"], [60, 0, 1, "", "qlora_code_llama2_70b"], [61, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[62, 1, 1, "", "GemmaTokenizer"], [63, 0, 1, "", "gemma"], [64, 0, 1, "", "gemma_2b"], [65, 0, 1, "", "gemma_7b"], [66, 0, 1, "", "gemma_tokenizer"], [67, 0, 1, "", "lora_gemma"], [68, 0, 1, "", "lora_gemma_2b"], [69, 0, 1, "", "lora_gemma_7b"], [70, 0, 1, "", "qlora_gemma_2b"], [71, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[62, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[72, 1, 1, "", "Llama2Tokenizer"], [73, 0, 1, "", "llama2"], [74, 0, 1, "", "llama2_13b"], [75, 0, 1, "", "llama2_70b"], [76, 0, 1, "", "llama2_7b"], [77, 0, 1, "", "llama2_reward_7b"], [78, 0, 1, "", "llama2_tokenizer"], [79, 0, 1, "", "lora_llama2"], [80, 0, 1, "", "lora_llama2_13b"], [81, 0, 1, "", "lora_llama2_70b"], [82, 0, 1, "", "lora_llama2_7b"], [83, 0, 1, "", "lora_llama2_reward_7b"], [84, 0, 1, "", "qlora_llama2_13b"], [85, 0, 1, "", "qlora_llama2_70b"], [86, 0, 1, "", "qlora_llama2_7b"], [87, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[72, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[88, 1, 1, "", "Llama3Tokenizer"], [89, 0, 1, "", "llama3"], [90, 0, 1, "", "llama3_70b"], [91, 0, 1, "", "llama3_8b"], [92, 0, 1, "", "llama3_tokenizer"], [93, 0, 1, "", "lora_llama3"], [94, 0, 1, "", "lora_llama3_70b"], [95, 0, 1, "", "lora_llama3_8b"], [96, 0, 1, "", "qlora_llama3_70b"], [97, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[88, 2, 1, "", "decode"], [88, 2, 1, "", "tokenize_message"], [88, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[98, 0, 1, "", "llama3_1"], [99, 0, 1, "", "llama3_1_70b"], [100, 0, 1, "", "llama3_1_8b"], [101, 0, 1, "", "lora_llama3_1"], [102, 0, 1, "", "lora_llama3_1_70b"], [103, 0, 1, "", "lora_llama3_1_8b"], [104, 0, 1, "", "qlora_llama3_1_70b"], [105, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[106, 1, 1, "", "MistralTokenizer"], [107, 0, 1, "", "lora_mistral"], [108, 0, 1, "", "lora_mistral_7b"], [109, 0, 1, "", "lora_mistral_classifier"], [110, 0, 1, "", "lora_mistral_reward_7b"], [111, 0, 1, "", "mistral"], [112, 0, 1, "", "mistral_7b"], [113, 0, 1, "", "mistral_classifier"], [114, 0, 1, "", "mistral_reward_7b"], [115, 0, 1, "", "mistral_tokenizer"], [116, 0, 1, "", "qlora_mistral_7b"], [117, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[106, 2, 1, "", "decode"], [106, 2, 1, "", "encode"], [106, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[118, 1, 1, "", "Phi3MiniTokenizer"], [119, 0, 1, "", "lora_phi3"], [120, 0, 1, "", "lora_phi3_mini"], [121, 0, 1, "", "phi3"], [122, 0, 1, "", "phi3_mini"], [123, 0, 1, "", "phi3_mini_tokenizer"], [124, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[118, 2, 1, "", "decode"], [118, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[125, 1, 1, "", "CausalSelfAttention"], [126, 1, 1, "", "FeedForward"], [127, 1, 1, "", "Fp32LayerNorm"], [128, 1, 1, "", "KVCache"], [129, 1, 1, "", "RMSNorm"], [130, 1, 1, "", "RotaryPositionalEmbeddings"], [131, 1, 1, "", "TransformerDecoder"], [132, 1, 1, "", "TransformerDecoderLayer"], [133, 1, 1, "", "VisionTransformer"], [135, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[125, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[126, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[127, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[128, 2, 1, "", "reset"], [128, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[129, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[130, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[131, 2, 1, "", "caches_are_enabled"], [131, 2, 1, "", "forward"], [131, 2, 1, "", "reset_caches"], [131, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[132, 2, 1, "", "forward"]], "torchtune.modules.VisionTransformer": [[133, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[134, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[136, 1, 1, "", "DPOLoss"], [137, 1, 1, "", "IPOLoss"], [138, 1, 1, "", "PPOLoss"], [139, 1, 1, "", "RSOLoss"]], "torchtune.modules.loss.DPOLoss": [[136, 2, 1, "", "forward"]], "torchtune.modules.loss.IPOLoss": [[137, 2, 1, "", "forward"]], "torchtune.modules.loss.PPOLoss": [[138, 2, 1, "", "forward"]], "torchtune.modules.loss.RSOLoss": [[139, 2, 1, "", "forward"]], "torchtune.modules.peft": [[140, 1, 1, "", "AdapterModule"], [141, 1, 1, "", "LoRALinear"], [142, 0, 1, "", "disable_adapter"], [143, 0, 1, "", "get_adapter_params"], [144, 0, 1, "", "set_trainable_params"], [145, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [146, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[140, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[141, 2, 1, "", "adapter_params"], [141, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[147, 0, 1, "", "estimate_advantages"], [148, 0, 1, "", "get_rewards_ppo"], [149, 0, 1, "", "left_padded_collate"], [150, 0, 1, "", "padded_collate_dpo"], [151, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.tokenizers": [[152, 1, 1, "", "BaseTokenizer"], [153, 1, 1, "", "ModelTokenizer"], [154, 1, 1, "", "SentencePieceBaseTokenizer"], [155, 1, 1, "", "TikTokenBaseTokenizer"], [156, 0, 1, "", "parse_hf_tokenizer_json"], [157, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[152, 2, 1, "", "decode"], [152, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[153, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[154, 2, 1, "", "decode"], [154, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[155, 2, 1, "", "decode"], [155, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[158, 1, 1, "", "Transform"], [159, 1, 1, "", "VisionCrossAttentionMask"], [160, 0, 1, "", "find_supported_resolutions"], [161, 0, 1, "", "get_canvas_best_fit"], [162, 0, 1, "", "resize_with_pad"], [163, 0, 1, "", "tile_crop"]], "torchtune.utils": [[164, 3, 1, "", "FSDPPolicyType"], [165, 1, 1, "", "FullModelHFCheckpointer"], [166, 1, 1, "", "FullModelMetaCheckpointer"], [167, 1, 1, "", "FullModelTorchTuneCheckpointer"], [168, 1, 1, "", "ModelType"], [169, 1, 1, "", "OptimizerInBackwardWrapper"], [170, 1, 1, "", "TuneRecipeArgumentParser"], [171, 0, 1, "", "create_optim_in_bwd_wrapper"], [172, 0, 1, "", "generate"], [173, 0, 1, "", "get_device"], [174, 0, 1, "", "get_dtype"], [175, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [176, 0, 1, "", "get_logger"], [177, 0, 1, "", "get_memory_stats"], [178, 0, 1, "", "get_quantizer_mode"], [179, 0, 1, "", "get_world_size_and_rank"], [180, 0, 1, "", "init_distributed"], [181, 0, 1, "", "is_distributed"], [182, 0, 1, "", "log_memory_stats"], [183, 0, 1, "", "lora_fsdp_wrap_policy"], [188, 0, 1, "", "padded_collate"], [189, 0, 1, "", "register_optim_in_bwd_hooks"], [190, 0, 1, "", "set_activation_checkpointing"], [191, 0, 1, "", "set_default_dtype"], [192, 0, 1, "", "set_seed"], [193, 0, 1, "", "setup_torch_profiler"], [194, 0, 1, "", "torch_version_ge"], [195, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[165, 2, 1, "", "load_checkpoint"], [165, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[166, 2, 1, "", "load_checkpoint"], [166, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[167, 2, 1, "", "load_checkpoint"], [167, 2, 1, "", "save_checkpoint"]], "torchtune.utils.OptimizerInBackwardWrapper": [[169, 2, 1, "", "get_optim_key"], [169, 2, 1, "", "load_state_dict"], [169, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[170, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[184, 1, 1, "", "DiskLogger"], [185, 1, 1, "", "StdoutLogger"], [186, 1, 1, "", "TensorBoardLogger"], [187, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[184, 2, 1, "", "close"], [184, 2, 1, "", "log"], [184, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[185, 2, 1, "", "close"], [185, 2, 1, "", "log"], [185, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[186, 2, 1, "", "close"], [186, 2, 1, "", "log"], [186, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[187, 2, 1, "", "close"], [187, 2, 1, "", "log"], [187, 2, 1, "", "log_config"], [187, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 17, 24, 26, 164, 198, 200, 202, 205, 207, 208, 209, 210], "config": [0, 7, 8, 202, 206], "data": [1, 5, 17, 24, 26, 203], "text": [1, 204, 207], "templat": [1, 203, 204], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 203, 204], "exampl": 2, "gener": [2, 172, 205, 207], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 202, 205, 206, 207, 208, 209], "llama3": [3, 89, 203, 207, 209], "1": 3, "llama2": [3, 73, 203, 205, 208, 210], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 111], "gemma": [3, 63], "clip": 3, "modul": 4, "compon": [4, 7], "build": [4, 199, 210], "block": 4, "base": 4, "token": [4, 203], "util": [4, 5, 164], "peft": 4, "loss": 4, "vision": 4, "transform": [4, 158], "reinforc": 4, "learn": 4, "from": [4, 203, 210], "human": 4, "feedback": 4, "rlhf": 4, "checkpoint": [5, 6, 9, 205], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 204, 208, 210], "manag": 5, "perform": [5, 208], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 200, 205], "format": [6, 204], "handl": 6, "differ": 6, "hfcheckpoint": 6, "metacheckpoint": 6, "torchtunecheckpoint": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 205, 208, 210], "put": [6, 210], "thi": 6, "all": [6, 7, 210], "togeth": [6, 210], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 204], "us": [7, 8, 203, 205, 210], "instanti": [7, 10], "referenc": 7, "other": [7, 205], "field": 7, "interpol": 7, "valid": [7, 13, 202], "your": [7, 8, 205, 206], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 200, 208, 209, 210], "ar": 8, "recip": [8, 202, 206, 208, 209], "script": 8, "run": [8, 202, 205], "cli": [8, 202], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "prompttempl": 22, "prompttemplateinterfac": 23, "role": 24, "stackexchangedpairedtempl": 25, "summarizetempl": 26, "get_openai_messag": 27, "get_sharegpt_messag": 28, "truncat": 29, "validate_messag": 30, "chatdataset": 31, "concatdataset": 32, "instructdataset": 33, "packeddataset": 34, "preferencedataset": 35, "sftdataset": 36, "textcompletiondataset": 37, "alpaca_cleaned_dataset": 38, "alpaca_dataset": 39, "chat_dataset": 40, "cnn_dailymail_articles_dataset": 41, "grammar_dataset": 42, "instruct_dataset": 43, "samsum_dataset": 44, "slimorca_dataset": 45, "stack_exchanged_paired_dataset": 46, "text_completion_dataset": 47, "wikitext_dataset": 48, "tilepositionalembed": 49, "tiledtokenpositionalembed": 50, "tokenpositionalembed": 51, "clip_vision_encod": 52, "code_llama2_13b": 53, "code_llama2_70b": 54, "code_llama2_7b": 55, "lora_code_llama2_13b": 56, "lora_code_llama2_70b": 57, "lora_code_llama2_7b": 58, "qlora_code_llama2_13b": 59, "qlora_code_llama2_70b": 60, "qlora_code_llama2_7b": 61, "gemmatoken": 62, "gemma_2b": 64, "gemma_7b": 65, "gemma_token": 66, "lora_gemma": 67, "lora_gemma_2b": 68, "lora_gemma_7b": 69, "qlora_gemma_2b": 70, "qlora_gemma_7b": 71, "llama2token": 72, "llama2_13b": 74, "llama2_70b": 75, "llama2_7b": 76, "llama2_reward_7b": 77, "llama2_token": 78, "lora_llama2": 79, "lora_llama2_13b": 80, "lora_llama2_70b": 81, "lora_llama2_7b": 82, "lora_llama2_reward_7b": 83, "qlora_llama2_13b": 84, "qlora_llama2_70b": 85, "qlora_llama2_7b": 86, "qlora_llama2_reward_7b": 87, "llama3token": 88, "llama3_70b": 90, "llama3_8b": 91, "llama3_token": 92, "lora_llama3": 93, "lora_llama3_70b": 94, "lora_llama3_8b": 95, "qlora_llama3_70b": 96, "qlora_llama3_8b": 97, "llama3_1": 98, "llama3_1_70b": 99, "llama3_1_8b": 100, "lora_llama3_1": 101, "lora_llama3_1_70b": 102, "lora_llama3_1_8b": 103, "qlora_llama3_1_70b": 104, "qlora_llama3_1_8b": 105, "mistraltoken": 106, "lora_mistr": 107, "lora_mistral_7b": 108, "lora_mistral_classifi": 109, "lora_mistral_reward_7b": 110, "mistral_7b": 112, "mistral_classifi": 113, "mistral_reward_7b": 114, "mistral_token": 115, "qlora_mistral_7b": 116, "qlora_mistral_reward_7b": 117, "phi3minitoken": 118, "lora_phi3": 119, "lora_phi3_mini": 120, "phi3": 121, "phi3_mini": 122, "phi3_mini_token": 123, "qlora_phi3_mini": 124, "causalselfattent": 125, "todo": [125, 132], "feedforward": 126, "fp32layernorm": 127, "kvcach": 128, "rmsnorm": 129, "rotarypositionalembed": 130, "transformerdecod": 131, "transformerdecoderlay": 132, "visiontransform": 133, "reparametrize_as_dtype_state_dict_post_hook": 134, "get_cosine_schedule_with_warmup": 135, "dpoloss": 136, "ipoloss": 137, "ppoloss": 138, "rsoloss": 139, "adaptermodul": 140, "loralinear": 141, "disable_adapt": 142, "get_adapter_param": 143, "set_trainable_param": 144, "validate_missing_and_unexpected_for_lora": 145, "validate_state_dict_for_lora": 146, "estimate_advantag": 147, "get_rewards_ppo": 148, "left_padded_col": 149, "padded_collate_dpo": 150, "truncate_sequence_at_first_stop_token": 151, "basetoken": 152, "modeltoken": 153, "sentencepiecebasetoken": 154, "tiktokenbasetoken": 155, "parse_hf_tokenizer_json": 156, "tokenize_messages_no_special_token": 157, "visioncrossattentionmask": 159, "find_supported_resolut": 160, "get_canvas_best_fit": 161, "resize_with_pad": 162, "tile_crop": 163, "fsdppolicytyp": 164, "fullmodelhfcheckpoint": 165, "fullmodelmetacheckpoint": 166, "fullmodeltorchtunecheckpoint": 167, "modeltyp": 168, "optimizerinbackwardwrapp": 169, "tunerecipeargumentpars": 170, "create_optim_in_bwd_wrapp": 171, "get_devic": 173, "get_dtyp": 174, "get_full_finetune_fsdp_wrap_polici": 175, "get_logg": 176, "get_memory_stat": 177, "get_quantizer_mod": 178, "get_world_size_and_rank": 179, "init_distribut": 180, "is_distribut": 181, "log_memory_stat": 182, "lora_fsdp_wrap_polici": 183, "disklogg": 184, "stdoutlogg": 185, "tensorboardlogg": 186, "wandblogg": 187, "padded_col": 188, "register_optim_in_bwd_hook": 189, "set_activation_checkpoint": 190, "set_default_dtyp": 191, "set_se": 192, "setup_torch_profil": 193, "torch_version_g": 194, "validate_expected_param_dtyp": 195, "comput": [197, 201], "time": [197, 201], "welcom": 198, "document": 198, "get": [198, 202, 207], "start": [198, 202], "tutori": 198, "instal": 199, "instruct": [199, 204, 207], "via": [199, 207], "pypi": 199, "git": 199, "clone": 199, "nightli": 199, "kei": 200, "concept": 200, "design": 200, "principl": 200, "download": [202, 205, 206], "list": 202, "built": [202, 204], "copi": 202, "fine": [203, 204, 206, 207], "tune": [203, 204, 206, 207], "chat": [203, 204], "chang": 203, "prompt": 203, "special": 203, "when": 203, "should": 203, "i": 203, "custom": [203, 204], "hug": [204, 205], "face": [204, 205], "set": 204, "max": 204, "sequenc": 204, "length": 204, "sampl": 204, "pack": 204, "unstructur": 204, "corpu": 204, "multipl": 204, "local": 204, "remot": 204, "fulli": 204, "end": 205, "workflow": 205, "7b": 205, "finetun": [205, 208, 209, 210], "evalu": [205, 207, 209], "eleutherai": [205, 207], "s": [205, 207], "eval": [205, 207], "har": [205, 207], "speed": 205, "up": 205, "quantiz": [205, 207, 209], "librari": 205, "upload": 205, "hub": 205, "first": 206, "llm": 206, "select": 206, "modifi": 206, "train": 206, "next": 206, "step": 206, "meta": 207, "8b": 207, "access": 207, "our": 207, "faster": 207, "how": 208, "doe": 208, "work": 208, "appli": [208, 209], "trade": 208, "off": 208, "qat": 209, "lower": 209, "devic": 209, "option": 209, "qlora": 210, "save": 210, "deep": 210, "dive": 210}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})