Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.modules.tokenizers.Tokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.profiler", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.modules.tokenizers.Tokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "gemma_2b", "gemma_7b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "lora_phi3_mini", "phi3_mini", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceTokenizer", "TikTokenTokenizer", "Tokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "profiler", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"For": [2, 5, 6, 7, 8, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 41, 42, 64, 69, 85, 90, 91, 98, 107, 112, 114, 120, 123, 124, 125, 126, 127, 128, 129, 130], "detail": [2, 6, 34, 39, 66, 84, 95, 103, 110, 114, 123, 126, 127, 128, 129, 130], "usag": [2, 71, 88, 89, 120, 123, 125, 126, 127, 128, 130], "guid": [2, 7, 9, 121, 124, 125, 127, 129], "pleas": [2, 5, 51, 52, 57, 60, 63, 84, 95, 103, 112, 120, 130], "see": [2, 5, 6, 9, 19, 21, 34, 39, 42, 51, 52, 57, 60, 63, 66, 74, 84, 88, 90, 95, 96, 103, 107, 110, 112, 114, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "our": [2, 6, 8, 121, 124, 125, 126, 127, 129, 130], "tutori": [2, 6, 112, 121, 124, 125, 126, 127, 128, 129, 130], "support": [2, 6, 8, 9, 10, 21, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 62, 64, 75, 86, 87, 89, 94, 97, 121, 123, 124, 125, 126, 127, 128, 129, 130], "sever": 2, "wide": 2, "us": [2, 4, 6, 9, 10, 12, 16, 19, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 64, 65, 67, 68, 69, 70, 71, 73, 76, 79, 81, 82, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 97, 103, 104, 105, 106, 107, 114, 119, 120, 121, 123, 125, 127, 128, 129], "help": [2, 6, 19, 69, 85, 90, 119, 120, 121, 123, 124, 125, 126, 127, 128, 130], "quickli": [2, 7, 31, 124, 125], "bootstrap": 2, "your": [2, 5, 9, 10, 26, 31, 106, 107, 119, 120, 121, 123, 124, 125, 128, 129, 130], "fine": [2, 6, 8, 9, 29, 119, 121, 126, 129], "tune": [2, 3, 6, 7, 8, 9, 12, 29, 119, 120, 121, 123, 126, 129, 130], "also": [2, 6, 7, 8, 9, 10, 34, 37, 41, 64, 69, 75, 93, 95, 97, 103, 107, 120, 123, 124, 125, 126, 127, 128, 129, 130], "common": [2, 4, 7, 123, 124, 125, 128, 129], "format": [2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 26, 28, 30, 32, 33, 34, 37, 39, 83, 85, 86, 87, 88, 123, 124, 126, 127, 128, 129], "like": [2, 6, 7, 8, 9, 26, 87, 120, 123, 124, 125, 126, 127, 129], "chat": [2, 15, 16, 19, 20, 23, 26, 34, 39], "model": [2, 6, 7, 8, 10, 16, 21, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 91, 92, 95, 97, 103, 110, 111, 112, 119, 121, 124, 125, 130], "instruct": [2, 3, 14, 16, 18, 20, 21, 28, 29, 30, 32, 33, 37, 41, 62, 119, 123, 124, 127, 129, 130], "These": [2, 4, 6, 7, 8, 10, 29, 90, 124, 125, 126, 127, 128, 129, 130], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 25, 28, 29, 30, 32, 33, 34, 36, 37, 38, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 69, 75, 76, 79, 80, 84, 85, 86, 88, 89, 91, 92, 94, 97, 101, 103, 109, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "especi": [2, 121, 123, 126], "specifi": [2, 6, 7, 8, 10, 34, 64, 69, 70, 73, 84, 92, 95, 98, 103, 107, 112, 123, 124, 125, 126, 127, 128, 130], "from": [2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 59, 65, 69, 70, 72, 74, 77, 80, 81, 85, 86, 87, 89, 90, 91, 92, 106, 107, 111, 118, 120, 122, 123, 125, 126, 127, 128, 129], "yaml": [2, 7, 8, 10, 11, 12, 34, 37, 41, 90, 107, 121, 123, 124, 125, 126, 127, 128, 129, 130], "config": [2, 6, 9, 10, 11, 12, 13, 34, 37, 41, 64, 79, 85, 89, 90, 107, 121, 124, 125, 126, 128, 129, 130], "represent": [2, 129, 130], "abov": [2, 6, 71, 101, 120, 126, 128, 129, 130], "all": [3, 4, 8, 13, 26, 27, 29, 34, 64, 65, 69, 71, 76, 82, 85, 89, 90, 91, 101, 111, 116, 117, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129], "famili": [3, 8, 32, 33, 35, 39, 40, 42, 88, 121, 123, 128], "download": [3, 6, 117, 120, 124, 125, 128, 129, 130], "meta": [3, 6, 19, 85, 86, 123, 124, 126, 127], "llama": [3, 6, 19, 26, 34, 67, 68, 85, 86, 123, 124, 126, 127, 128, 129], "8b": [3, 54, 55, 56, 61, 123, 124], "hf": [3, 6, 73, 85, 123, 124, 126, 127, 128], "token": [3, 6, 7, 8, 20, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 64, 68, 69, 70, 81, 82, 92, 95, 108, 123, 125, 126, 127, 128, 129, 130], "access_token": 3, "pre": [3, 19, 29, 120, 124, 125], "train": [3, 5, 6, 8, 9, 19, 26, 27, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 64, 68, 69, 70, 71, 72, 85, 86, 87, 94, 97, 103, 110, 119, 121, 123, 124, 125, 126, 128, 129, 130], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 41, 42, 67, 68, 76, 81, 84, 85, 88, 90, 95, 103, 106, 107, 112, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "hug": [3, 6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 72, 121, 123, 127, 128], "face": [3, 6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 72, 121, 123, 127, 128], "hub": [3, 6, 123, 125, 127], "follow": [3, 6, 8, 23, 26, 29, 64, 72, 87, 88, 89, 101, 107, 119, 120, 123, 125, 126, 127, 128, 129, 130], "command": [3, 8, 9, 90, 120, 123, 124, 125, 126, 127, 128, 129, 130], "2": [3, 6, 9, 25, 29, 39, 64, 81, 85, 86, 108, 109, 113, 114, 115, 124, 126, 127, 128, 129], "7b": [3, 6, 28, 30, 31, 32, 33, 35, 37, 41, 42, 44, 47, 50, 52, 58, 59, 85, 86, 124, 127, 128, 129, 130], "mini": [3, 61, 62, 63], "microsoft": [3, 62], "4k": [3, 62], "hf_token": 3, "ignor": [3, 6, 64, 65, 123], "pattern": [3, 82, 123], "ai": [3, 59, 64, 107, 124, 128], "mistralai": [3, 123], "v0": 3, "1": [3, 6, 8, 29, 39, 64, 69, 72, 73, 81, 82, 86, 88, 92, 101, 106, 107, 108, 109, 113, 114, 123, 124, 126, 127, 128, 129, 130], "size": [3, 6, 8, 10, 32, 33, 36, 38, 64, 66, 67, 68, 69, 99, 101, 121, 123, 125, 126, 127, 128, 129], "2b": [3, 43], "googl": [3, 43, 44], "offer": 5, "allow": [5, 27, 79, 106, 123, 130], "seamless": 5, "transit": 5, "between": [5, 6, 85, 88, 125, 126, 128, 129, 130], "interoper": [5, 6, 8, 121, 126, 130], "rest": [5, 124, 130], "ecosystem": [5, 6, 8, 121, 126, 128, 130], "comprehens": 5, "overview": [5, 7, 9, 119, 127, 129, 130], "deep": [5, 6, 7, 8, 9, 121, 127, 128], "dive": [5, 6, 7, 8, 9, 121, 127, 128], "enabl": [5, 7, 8, 9, 27, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 75, 110, 114, 128, 129, 130], "work": [5, 6, 8, 90, 121, 123, 126, 128, 130], "set": [5, 6, 7, 8, 9, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 41, 42, 68, 69, 76, 78, 84, 93, 95, 101, 103, 112, 113, 114, 121, 123, 124, 126, 127, 128, 129], "consumpt": [5, 27], "dure": [5, 6, 27, 28, 29, 32, 33, 36, 38, 64, 66, 68, 69, 70, 71, 97, 124, 126, 128, 129, 130], "provid": [5, 6, 7, 8, 10, 16, 21, 24, 26, 27, 28, 29, 30, 39, 69, 76, 87, 90, 93, 95, 107, 121, 123, 124, 125, 126, 127, 128], "debug": [5, 6, 7, 8, 123], "finetun": [5, 6, 7, 8, 48, 49, 50, 55, 56, 61, 119, 121, 127, 128], "job": [5, 9, 114, 127], "variou": [5, 18], "dataset": [5, 7, 14, 17, 18, 20, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 121, 127, 128], "walk": [6, 8, 106, 121, 124, 125, 126, 127, 130], "you": [6, 7, 8, 9, 10, 18, 19, 26, 28, 30, 31, 32, 33, 35, 37, 41, 42, 88, 90, 92, 106, 107, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "through": [6, 7, 8, 9, 65, 76, 121, 123, 124, 125, 126, 127, 130], "design": [6, 8], "behavior": [6, 103, 124, 125], "associ": [6, 7, 8, 92, 126, 129], "util": [6, 7, 8, 9, 10, 27, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 121, 126, 127, 128, 130], "what": [6, 7, 9, 19, 21, 31, 36, 38, 119, 124, 125, 126, 127, 128], "cover": [6, 7, 8, 9, 124, 126, 130], "how": [6, 7, 8, 9, 84, 112, 119, 123, 124, 125, 126, 127, 128, 130], "we": [6, 7, 8, 9, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 64, 66, 68, 69, 75, 81, 85, 86, 87, 92, 94, 98, 103, 111, 121, 123, 124, 125, 126, 127, 128, 129, 130], "them": [6, 7, 26, 27, 28, 30, 37, 65, 71, 81, 123, 124, 125, 126, 129, 130], "scenario": [6, 27], "full": [6, 7, 8, 34, 37, 51, 52, 57, 60, 63, 79, 80, 81, 121, 123, 125, 128, 129], "compos": 6, "compon": [6, 8, 13, 109, 110, 121, 125, 127, 129, 130], "which": [6, 7, 8, 27, 28, 29, 31, 32, 33, 36, 38, 48, 49, 50, 55, 56, 58, 61, 64, 68, 69, 70, 72, 79, 80, 81, 85, 86, 87, 89, 94, 104, 107, 112, 121, 123, 124, 125, 126, 127, 128, 129, 130], "plug": 6, "ani": [6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 26, 28, 30, 31, 34, 35, 37, 41, 42, 71, 77, 78, 79, 80, 81, 85, 86, 87, 89, 92, 100, 103, 114, 116, 123, 124, 125, 126, 127, 128, 129], "recip": [6, 7, 9, 10, 11, 12, 65, 79, 85, 86, 87, 121, 124, 125, 126, 128, 130], "evalu": [6, 8, 119, 121, 127, 129, 130], "gener": [6, 8, 14, 17, 22, 26, 28, 29, 30, 35, 39, 76, 81, 113, 114, 117, 119, 124, 125, 129, 130], "each": [6, 8, 15, 18, 27, 29, 48, 49, 50, 55, 56, 58, 61, 64, 68, 69, 70, 73, 79, 80, 81, 82, 109, 114, 121, 123, 125, 126, 127, 128, 129], "make": [6, 7, 8, 9, 64, 70, 121, 123, 126, 127, 128, 129, 130], "easi": [6, 8, 121, 125, 129], "understand": [6, 7, 8, 119, 121, 124, 125, 129, 130], "extend": [6, 8, 121], "befor": [6, 25, 28, 29, 30, 64, 69, 70, 75, 85, 123, 126], "let": [6, 7, 9, 123, 124, 125, 126, 127, 128, 129, 130], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 48, 49, 50, 55, 56, 58, 61, 64, 66, 68, 69, 70, 71, 73, 74, 77, 79, 80, 82, 84, 85, 86, 89, 93, 95, 97, 103, 106, 110, 112, 113, 121, 123, 124, 125, 127, 129, 130], "defin": [6, 7, 8, 65, 74, 75, 77, 125, 127, 129], "some": [6, 7, 16, 77, 78, 119, 121, 123, 124, 125, 126, 127, 129, 130], "concept": [6, 126, 127], "In": [6, 7, 8, 26, 68, 75, 84, 103, 106, 107, 124, 126, 128, 129, 130], "ll": [6, 7, 8, 82, 92, 98, 121, 124, 125, 126, 127, 128, 130], "talk": 6, "about": [6, 8, 73, 107, 121, 123, 124, 126, 127, 128, 129, 130], "take": [6, 7, 8, 10, 65, 66, 71, 85, 87, 90, 93, 109, 124, 125, 126, 127, 128, 129, 130], "close": [6, 8, 104, 105, 106, 107, 129], "look": [6, 7, 8, 91, 106, 120, 124, 125, 126, 127, 128, 129], "veri": [6, 27, 69, 123, 126], "simpli": [6, 7, 29, 123, 124, 125, 126, 128, 130], "dictat": 6, "state_dict": [6, 71, 79, 85, 86, 87, 88, 89, 129, 130], "store": [6, 27, 104, 107, 129, 130], "file": [6, 7, 8, 9, 10, 11, 12, 81, 82, 85, 86, 87, 90, 104, 107, 110, 118, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130], "disk": [6, 31, 104], "weight": [6, 8, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 71, 74, 75, 79, 85, 86, 87, 88, 98, 103, 107, 119, 123, 124, 126, 127, 128, 129, 130], "string": [6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 74, 81, 82, 83, 93, 94, 98, 123, 125], "kei": [6, 7, 9, 26, 28, 30, 37, 64, 66, 69, 70, 78, 79, 80, 85, 87, 89, 109, 123, 126, 127, 129, 130], "identifi": 6, "state": [6, 8, 71, 77, 78, 79, 80, 85, 86, 87, 89, 91, 126, 128, 129, 130], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 26, 28, 30, 31, 34, 35, 37, 41, 42, 71, 77, 78, 79, 80, 85, 86, 87, 89, 91, 97, 100, 102, 108, 109, 111, 125], "If": [6, 7, 13, 14, 17, 18, 21, 22, 24, 25, 26, 28, 30, 32, 33, 36, 37, 38, 39, 64, 68, 69, 70, 71, 73, 75, 80, 85, 86, 87, 88, 89, 92, 93, 94, 95, 97, 98, 100, 106, 107, 114, 116, 120, 123, 124, 125, 126, 127, 128, 129], "don": [6, 7, 8, 107, 114, 123, 124, 125, 126, 127, 128, 130], "t": [6, 7, 8, 82, 94, 107, 114, 123, 124, 125, 126, 127, 128, 130], "match": [6, 26, 28, 30, 37, 80, 120, 123, 125, 126, 128, 129], "up": [6, 8, 9, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 91, 123, 124, 125, 127, 128, 129, 130], "exactli": [6, 80], "those": [6, 88, 129], "definit": [6, 129], "either": [6, 80, 85, 92, 112, 123, 129, 130], "run": [6, 7, 9, 12, 65, 66, 69, 71, 85, 86, 87, 89, 91, 101, 106, 107, 111, 120, 121, 124, 125, 127, 128, 129, 130], "explicit": 6, "error": [6, 7, 25, 85, 114, 123], "load": [6, 8, 26, 27, 28, 29, 30, 31, 79, 85, 86, 87, 89, 90, 106, 124, 125, 126, 128, 129], "rais": [6, 10, 13, 21, 25, 34, 39, 64, 66, 69, 73, 79, 80, 85, 86, 87, 89, 94, 97, 100, 107, 109, 114, 116], "an": [6, 7, 8, 9, 10, 14, 20, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 64, 69, 73, 74, 76, 77, 78, 84, 85, 86, 87, 89, 93, 95, 107, 121, 123, 124, 125, 126, 127, 128, 129, 130], "except": [6, 20, 21, 125], "wors": 6, "silent": [6, 65], "succe": 6, "infer": [6, 19, 26, 64, 66, 68, 69, 70, 93, 119, 124, 126, 127, 128, 130], "expect": [6, 7, 10, 14, 17, 18, 22, 26, 28, 30, 34, 37, 68, 80, 89, 107, 116, 124, 125, 129], "addit": [6, 7, 8, 10, 26, 28, 30, 31, 34, 35, 37, 41, 42, 79, 84, 85, 86, 87, 94, 95, 100, 103, 104, 106, 107, 112, 121, 124, 127, 129], "line": [6, 8, 90, 123, 125, 127, 128], "need": [6, 7, 8, 9, 18, 26, 29, 39, 64, 65, 69, 103, 106, 107, 111, 120, 123, 124, 125, 126, 127, 128, 129, 130], "shape": [6, 64, 66, 68, 69, 70, 73, 75, 92], "valu": [6, 7, 23, 39, 43, 44, 45, 46, 47, 53, 54, 59, 64, 66, 67, 69, 70, 72, 79, 85, 88, 89, 90, 92, 104, 105, 106, 107, 109, 114, 123, 125, 127, 128, 129], "two": [6, 7, 25, 121, 126, 127, 128, 129, 130], "popular": [6, 121, 125, 126], "llama2": [6, 7, 8, 10, 19, 23, 26, 28, 30, 31, 32, 33, 35, 37, 39, 41, 42, 45, 46, 47, 48, 49, 50, 51, 52, 65, 69, 70, 81, 88, 119, 121, 123, 127, 128], "offici": [6, 19, 124, 127, 128], "implement": [6, 8, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 65, 67, 68, 72, 73, 74, 75, 85, 98, 106, 121, 125, 129, 130], "when": [6, 7, 8, 12, 20, 27, 29, 31, 64, 68, 69, 70, 71, 72, 79, 92, 95, 106, 111, 123, 126, 128, 129, 130], "websit": 6, "get": [6, 7, 8, 9, 26, 81, 94, 96, 97, 99, 120, 121, 124, 125, 126, 127, 129], "access": [6, 7, 8, 27, 85, 91, 123, 126, 127], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 29, 31, 64, 79, 85, 86, 87, 89, 91, 123, 124, 125, 126, 127, 128, 129, 130], "pth": [6, 126], "inspect": [6, 126, 129, 130], "content": [6, 20, 23, 26, 81, 124, 125], "easili": [6, 7, 121, 125, 129, 130], "torch": [6, 7, 27, 66, 69, 71, 72, 73, 87, 89, 91, 92, 93, 94, 97, 100, 101, 109, 110, 111, 112, 113, 114, 115, 116, 126, 127, 128, 129, 130], "import": [6, 7, 10, 34, 37, 41, 106, 107, 124, 125, 126, 127, 129, 130], "consolid": [6, 128], "00": [6, 118, 122, 127], "mmap": [6, 126], "true": [6, 7, 20, 28, 29, 32, 33, 34, 36, 37, 38, 41, 51, 52, 57, 60, 63, 64, 69, 70, 71, 76, 81, 82, 84, 85, 86, 87, 95, 97, 100, 101, 103, 106, 115, 123, 124, 125, 126, 128, 129, 130], "weights_onli": [6, 87], "map_loc": [6, 126], "cpu": [6, 8, 71, 94, 120, 123, 126, 130], "tensor": [6, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 85, 92, 104, 105, 106, 107, 108, 109, 113, 129, 130], "item": 6, "print": [6, 9, 27, 32, 33, 36, 38, 39, 81, 92, 115, 124, 125, 127, 129, 130], "f": [6, 9, 32, 33, 36, 38, 124, 126, 129, 130], "tok_embed": [6, 69], "32000": [6, 10, 129], "4096": [6, 10, 28, 30, 31, 32, 33, 35, 37, 41, 42, 64, 68, 125, 129], "len": [6, 27, 32, 33, 36, 38, 69], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 25, 26, 27, 28, 29, 30, 36, 38, 39, 40, 48, 49, 50, 55, 56, 61, 67, 68, 71, 72, 73, 76, 81, 82, 84, 85, 87, 90, 93, 94, 96, 98, 107, 110, 113, 115, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "contain": [6, 20, 27, 29, 31, 41, 64, 66, 68, 69, 70, 74, 77, 78, 79, 81, 82, 85, 86, 87, 89, 90, 91, 97, 102, 106, 108, 109, 124, 126, 128, 129], "includ": [6, 7, 8, 15, 18, 75, 85, 86, 90, 121, 123, 124, 125, 126, 127, 128, 129, 130], "input": [6, 14, 15, 18, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 64, 65, 67, 68, 69, 70, 75, 81, 85, 87, 108, 109, 114, 116, 124, 125, 129, 130], "embed": [6, 64, 66, 67, 68, 69, 95, 124, 128], "tabl": [6, 124, 130], "call": [6, 10, 20, 65, 71, 79, 90, 104, 105, 106, 107, 111, 124, 125, 129, 130], "layer": [6, 8, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 69, 70, 75, 79, 80, 84, 95, 121, 128, 129, 130], "have": [6, 7, 10, 64, 66, 74, 80, 87, 89, 90, 95, 103, 106, 110, 116, 120, 124, 125, 126, 127, 128, 129, 130], "dim": [6, 64, 65, 67, 68, 69], "most": [6, 7, 82, 124, 127, 129, 130], "within": [6, 7, 10, 26, 29, 39, 65, 92, 106, 114, 123, 125, 126, 128, 129, 130], "default": [6, 7, 16, 20, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 64, 65, 67, 68, 69, 70, 71, 72, 73, 75, 79, 81, 82, 85, 86, 87, 90, 92, 94, 99, 103, 104, 107, 108, 109, 110, 113, 114, 120, 123, 124, 125, 126, 128, 129, 130], "everi": [6, 8, 65, 106, 120, 123, 130], "repo": [6, 85, 86, 88, 123, 126], "first": [6, 7, 10, 25, 29, 66, 69, 82, 85, 90, 119, 121, 124, 125, 126, 128, 129, 130], "big": [6, 126], "split": [6, 29, 124, 125, 126], "across": [6, 8, 27, 85, 106, 114, 126, 128], "bin": [6, 123, 126], "To": [6, 7, 8, 9, 29, 85, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "correctli": [6, 8, 13, 79, 85, 120, 124, 127, 130], "piec": 6, "one": [6, 8, 25, 65, 73, 81, 87, 124, 125, 126, 127, 128, 130], "pytorch_model": [6, 126], "00001": [6, 123], "00002": [6, 123], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 29, 35, 75, 77, 79, 81, 86, 87, 89, 90, 92, 94, 95, 97, 98, 103, 123, 125, 126, 127, 128, 129, 130], "doe": [6, 21, 26, 29, 62, 64, 69, 70, 74, 85, 87, 89, 90, 123, 124, 126], "fewer": [6, 64], "sinc": [6, 7, 10, 65, 85, 87, 124, 126, 128], "instead": [6, 8, 29, 34, 37, 41, 65, 66, 75, 123, 126, 128, 129], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 26, 28, 30, 31, 37, 39, 41, 42, 74, 78, 80, 82, 85, 86, 87, 88, 89, 90, 91, 92, 93, 104, 105, 106, 107, 116, 123, 124, 126, 128], "caus": [6, 81], "try": [6, 7, 124, 126, 127, 128, 130], "same": [6, 7, 48, 49, 50, 55, 56, 61, 64, 66, 70, 81, 89, 90, 95, 107, 123, 124, 126, 128, 129, 130], "As": [6, 7, 8, 9, 75, 121, 126, 128, 130], "re": [6, 7, 82, 87, 121, 124, 126, 127, 128, 129], "care": [6, 65, 85, 87, 126, 128, 129], "end": [6, 8, 20, 27, 81, 82, 119, 121, 124, 128, 129], "number": [6, 8, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 64, 66, 69, 72, 85, 86, 87, 92, 99, 114, 123, 127, 129], "just": [6, 14, 121, 123, 124, 125, 127, 128, 129], "save": [6, 8, 9, 71, 85, 86, 87, 89, 95, 103, 107, 119, 123, 124, 125, 126, 128, 129], "less": [6, 39, 126, 127, 128, 130], "prone": 6, "manag": [6, 27, 76, 110, 113, 124], "invari": 6, "accept": [6, 7, 39, 81, 84, 125, 127, 130], "multipl": [6, 7, 8, 20, 26, 27, 64, 69, 70, 75, 104, 105, 106, 107, 109, 127, 128], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 124, 125, 126], "worri": [6, 124, 127], "explicitli": [6, 74, 121, 129], "convert": [6, 23, 26, 85, 108, 124, 126, 130], "time": [6, 81, 104, 106, 123, 124, 125, 126, 128, 130], "produc": [6, 89, 130], "back": [6, 25, 76, 85, 125, 129, 130], "origin": [6, 32, 33, 71, 75, 124, 126, 128, 129, 130], "form": [6, 7, 8, 25, 123], "One": [6, 126], "advantag": [6, 129], "being": [6, 85, 86, 87, 91, 93, 130], "should": [6, 7, 8, 15, 18, 19, 20, 21, 23, 29, 34, 37, 41, 48, 49, 50, 55, 56, 58, 61, 64, 65, 73, 74, 79, 80, 84, 90, 102, 104, 105, 106, 107, 120, 121, 125, 126, 127, 128, 129, 130], "abl": [6, 8, 126, 127, 128], "post": [6, 111, 130], "tool": [6, 125, 126, 127], "quantiz": [6, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 75, 87, 98, 119, 127, 130], "eval": [6, 119, 121], "without": [6, 7, 9, 79, 120, 121, 124, 126, 129], "code": [6, 8, 69, 117, 121, 125, 127], "chang": [6, 7, 9, 14, 87, 120, 126, 127, 128, 129, 130], "OR": 6, "convers": [6, 15, 16, 19, 21, 23, 25, 26, 34, 39, 85, 87, 88, 121, 124, 125, 126, 128, 129, 130], "script": [6, 9, 123, 126, 127, 128], "wai": [6, 7, 26, 79, 123, 124, 125, 126, 127, 128], "surround": [6, 8, 121], "load_checkpoint": [6, 8, 85, 86, 87, 88], "save_checkpoint": [6, 8, 9, 85, 86, 87], "method": [6, 7, 8, 9, 12, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 71, 74, 77, 79, 89, 90, 98, 120, 121, 125, 126, 128, 129, 130], "convertor": 6, "avail": [6, 8, 42, 90, 93, 94, 101, 121, 123, 126, 128, 129], "here": [6, 7, 9, 16, 36, 67, 68, 123, 124, 125, 126, 127, 128, 129, 130], "three": [6, 8, 73, 127], "hfcheckpoint": 6, "read": [6, 85, 86, 87, 121], "write": [6, 8, 85, 86, 87, 104, 124, 125, 127], "compat": [6, 85, 87], "transform": [6, 8, 26, 28, 30, 48, 49, 50, 55, 56, 58, 61, 69, 70, 72, 112, 129], "framework": [6, 8, 121], "mention": [6, 126, 130], "assum": [6, 14, 17, 18, 22, 28, 30, 37, 64, 68, 69, 70, 72, 77, 82, 89, 91, 94, 103, 126, 129], "checkpoint_dir": [6, 7, 85, 86, 87, 126, 128], "necessari": [6, 39, 104, 105, 106, 107, 124, 129], "json": [6, 85, 110, 123, 125, 126], "easiest": [6, 126, 127], "sure": [6, 7, 126, 127, 128, 129, 130], "everyth": [6, 8, 90, 121, 127], "flow": [6, 26, 28, 29, 30, 130], "By": [6, 123, 128, 129, 130], "safetensor": [6, 123], "output": [6, 18, 32, 33, 36, 39, 48, 49, 50, 55, 56, 58, 61, 64, 65, 67, 68, 69, 70, 75, 78, 79, 80, 87, 92, 95, 105, 110, 120, 123, 124, 125, 126, 127, 128, 129, 130], "dir": [6, 107, 120, 123, 126, 127, 128], "output_dir": [6, 7, 85, 86, 87, 110, 126, 128, 129, 130], "argument": [6, 7, 10, 18, 26, 28, 30, 31, 34, 35, 37, 39, 41, 42, 51, 52, 57, 60, 63, 64, 84, 90, 95, 100, 104, 106, 107, 112, 123, 124, 125, 128, 129], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 69, 123, 125, 126, 129, 130], "_component_": [6, 7, 9, 10, 34, 37, 41, 124, 125, 126, 128, 129], "fullmodelhfcheckpoint": [6, 126], "directori": [6, 7, 85, 86, 87, 104, 106, 107, 123, 126, 127, 128], "sort": [6, 85, 87], "id": [6, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 64, 68, 69, 70, 81, 82, 83, 85, 87, 92, 108, 109, 124, 125, 126], "so": [6, 7, 29, 85, 90, 120, 121, 124, 126, 127, 128, 129, 130], "order": [6, 8, 85, 87, 106, 107, 127], "matter": [6, 85, 87, 123, 129], "checkpoint_fil": [6, 7, 9, 85, 86, 87, 126, 128, 129, 130], "restart": [6, 123], "previou": [6, 29, 85, 86, 87], "more": [6, 7, 8, 34, 39, 66, 68, 79, 84, 87, 90, 107, 110, 112, 114, 121, 123, 125, 126, 127, 128, 129, 130], "next": [6, 29, 92, 128, 130], "section": [6, 8, 97, 119, 126, 128, 130], "recipe_checkpoint": [6, 85, 86, 87], "null": [6, 7], "usual": [6, 68, 85, 107, 123, 126, 129], "model_typ": [6, 85, 86, 87, 126, 128], "resume_from_checkpoint": [6, 85, 86, 87], "fals": [6, 7, 20, 23, 26, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 69, 70, 75, 76, 79, 81, 82, 85, 86, 87, 101, 110, 123, 124, 125, 126, 128, 129, 130], "requir": [6, 7, 27, 31, 39, 41, 85, 87, 89, 100, 101, 103, 106, 107, 109, 114, 120, 123, 124, 125, 127, 130], "param": [6, 8, 48, 49, 50, 55, 56, 61, 75, 77, 78, 80, 85, 103, 129, 130], "directli": [6, 7, 8, 10, 34, 37, 41, 84, 85, 123, 126, 127, 128, 129, 130], "ensur": [6, 7, 13, 25, 39, 64, 85, 87, 94, 121, 125, 127], "out": [6, 7, 8, 26, 28, 32, 33, 34, 36, 38, 85, 86, 119, 121, 123, 124, 126, 127, 128, 129, 130], "case": [6, 8, 9, 20, 64, 85, 89, 94, 98, 103, 104, 112, 121, 123, 124, 125, 126, 128, 129, 130], "discrep": [6, 85], "along": [6, 128, 129], "found": [6, 7, 9, 67, 68, 123, 129, 130], "metacheckpoint": 6, "github": [6, 10, 48, 49, 50, 55, 56, 61, 64, 67, 68, 72, 73, 79, 120, 125, 127], "repositori": [6, 19, 126, 127], "fullmodelmetacheckpoint": [6, 128], "torchtunecheckpoint": 6, "perform": [6, 29, 65, 76, 92, 121, 124, 126, 128, 130], "current": [6, 29, 62, 64, 66, 68, 69, 70, 86, 87, 95, 99, 104, 106, 111, 114, 126, 127, 128], "test": [6, 7, 8, 121, 124], "complet": [6, 8, 29, 35, 124, 125, 126, 127, 128], "written": [6, 7, 8, 85, 86, 104, 105, 106, 107, 121], "begin": [6, 29, 81, 82, 124, 128, 130], "partit": [6, 85, 130], "ha": [6, 74, 76, 77, 80, 81, 87, 89, 116, 125, 126, 127, 128, 129, 130], "standard": [6, 105, 121, 124, 126, 128], "key_1": [6, 87], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 126], "inform": [6, 20, 107, 112, 121, 123, 126, 127, 128], "subsequ": [6, 8], "recipe_st": [6, 85, 86, 87], "pt": [6, 9, 85, 86, 87, 126, 128], "epoch": [6, 8, 9, 72, 85, 86, 87, 123, 124, 126, 127, 128], "optim": [6, 7, 8, 27, 62, 72, 73, 87, 89, 91, 97, 109, 111, 124, 126, 127, 128, 129, 130], "etc": [6, 8, 85, 97, 127], "prevent": [6, 29, 123], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 69, 74, 81, 89, 110, 111, 114, 124, 125, 126, 129, 130], "updat": [6, 7, 8, 66, 89, 120, 124, 126, 127, 128, 129, 130], "hf_model_0001_0": [6, 126], "hf_model_0002_0": [6, 126], "both": [6, 27, 80, 123, 126, 129, 130], "adapt": [6, 74, 75, 76, 77, 78, 85, 86, 87, 124, 126, 129, 130], "merg": [6, 10, 11, 85, 126, 128, 130], "would": [6, 7, 9, 29, 69, 120, 124, 125, 126, 129, 130], "primari": [6, 7, 8, 127], "want": [6, 7, 8, 9, 10, 26, 92, 120, 123, 124, 125, 126, 127, 128, 129], "resum": [6, 8, 72, 85, 86, 87, 130], "initi": [6, 8, 12, 27, 29, 43, 44, 45, 46, 47, 53, 54, 59, 89, 100, 101, 127, 129, 130], "frozen": [6, 129, 130], "base": [6, 10, 28, 30, 39, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 68, 72, 73, 75, 76, 78, 79, 80, 85, 90, 93, 95, 103, 104, 119, 124, 126, 127, 128, 129, 130], "well": [6, 7, 8, 121, 123, 125, 126, 128, 130], "learnt": [6, 124, 126], "someth": [6, 8, 9, 124, 126], "NOT": 6, "refer": [6, 7, 8, 67, 68, 73, 76, 121, 129], "adapter_checkpoint": [6, 85, 86, 87], "adapter_0": [6, 126], "now": [6, 81, 89, 91, 124, 125, 126, 127, 128, 129, 130], "knowledg": 6, "creat": [6, 7, 10, 29, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 72, 84, 85, 86, 87, 91, 104, 106, 123, 124, 125, 126, 128, 130], "simpl": [6, 8, 119, 125, 127, 129, 130], "forward": [6, 8, 64, 65, 67, 68, 69, 70, 73, 75, 97, 128, 129, 130], "13b": [6, 45, 48, 51], "modeltyp": [6, 85, 86, 87], "llama2_13b": [6, 48], "right": [6, 85, 126, 128, 129], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 79, 89, 129], "successfulli": [6, 123, 127], "vocab": [6, 10, 69, 128], "70": [6, 53], "x": [6, 64, 65, 67, 68, 69, 70, 75, 92, 113, 129, 130], "randint": 6, "0": [6, 8, 29, 48, 49, 50, 51, 52, 64, 69, 72, 73, 75, 81, 92, 106, 107, 108, 109, 114, 115, 118, 122, 124, 125, 126, 127, 128, 129, 130], "no_grad": 6, "6": [6, 29, 67, 108, 109, 126, 130], "3989": 6, "9": [6, 109, 126, 130], "0531": 6, "3": [6, 29, 61, 62, 82, 88, 90, 96, 108, 109, 113, 123, 124, 126, 127, 128, 130], "2375": 6, "5": [6, 7, 72, 73, 108, 109, 110, 126, 127, 128], "2822": 6, "4": [6, 7, 39, 64, 108, 109, 115, 121, 123, 125, 126, 128, 129, 130], "4872": 6, "7469": 6, "8": [6, 32, 33, 36, 38, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 109, 126, 129, 130], "6737": 6, "11": [6, 109, 126, 128, 130], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 108, 109], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 26, 28, 30, 37, 39, 79, 107, 123, 124, 125, 126, 127, 128, 129], "find": [6, 8, 9, 123, 126, 127, 129], "list": [6, 7, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 74, 75, 79, 80, 81, 82, 83, 85, 86, 87, 90, 92, 96, 108, 109, 124, 125, 127, 128], "builder": [6, 35, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 124, 125, 130], "hope": 6, "deeper": [6, 127], "insight": [6, 126], "happi": [6, 126], "thi": [7, 8, 9, 10, 20, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 41, 42, 62, 64, 65, 68, 69, 70, 71, 72, 74, 76, 79, 80, 81, 84, 85, 86, 87, 89, 90, 92, 93, 94, 97, 101, 103, 104, 106, 107, 109, 110, 111, 112, 114, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130], "pars": [7, 10, 11, 82, 90, 124, 127], "effect": 7, "cli": [7, 9, 11, 12, 120, 126, 127], "prerequisit": [7, 124, 125, 126, 127, 128, 129, 130], "Be": [7, 124, 126, 127, 128, 129, 130], "familiar": [7, 124, 126, 127, 128, 129, 130], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 120, 124, 125, 127], "instal": [7, 9, 101, 106, 107, 119, 123, 126, 127, 128, 129, 130], "fundament": 7, "There": [7, 15, 25, 124, 126, 127, 128, 129], "entri": [7, 8, 127], "point": [7, 8, 23, 125, 126, 127, 128, 129, 130], "locat": [7, 123, 128, 129, 130], "thei": [7, 8, 20, 27, 69, 80, 90, 95, 123, 124, 125, 129], "truth": [7, 126, 128], "reproduc": 7, "overridden": [7, 65, 90], "quick": [7, 27], "experiment": 7, "modifi": [7, 8, 9, 71, 121, 126, 128, 129, 130], "serv": [7, 84, 125, 129], "particular": [7, 26, 27, 39, 84, 125, 129, 130], "seed": [7, 8, 9, 114, 127], "shuffl": [7, 29], "devic": [7, 8, 79, 89, 93, 94, 97, 123, 124, 126, 127, 128, 129], "cuda": [7, 93, 94, 97, 120, 126, 130], "dtype": [7, 8, 66, 69, 71, 94, 113, 116, 126, 130], "fp32": [7, 130], "enable_fsdp": 7, "mani": [7, 29, 125, 126], "object": [7, 10, 11, 15, 16, 19, 21, 64, 84, 98, 124], "keyword": [7, 10, 26, 28, 30, 31, 34, 35, 37, 39, 41, 42, 71, 124, 125], "loss": [7, 8, 28, 32, 33, 36, 38, 73, 127, 129, 130], "function": [7, 8, 10, 12, 26, 64, 65, 71, 73, 76, 79, 80, 84, 85, 92, 93, 99, 103, 109, 114, 121, 124, 125, 130], "exampl": [7, 8, 9, 10, 12, 16, 19, 21, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 64, 73, 74, 76, 81, 84, 85, 86, 88, 89, 92, 98, 106, 107, 108, 109, 113, 115, 117, 118, 120, 122, 123, 124, 125, 126, 128, 129, 130], "subfield": 7, "dotpath": 7, "wish": [7, 125], "exact": [7, 10, 126], "path": [7, 8, 9, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 81, 82, 85, 86, 87, 90, 110, 123, 124, 125, 126, 128, 129], "normal": [7, 26, 29, 67, 69, 70, 81, 124, 125, 129, 130], "python": [7, 82, 90, 96, 107, 114, 117, 123, 126], "alpaca_dataset": [7, 32, 125], "custom": [7, 8, 26, 28, 30, 34, 37, 41, 112, 121, 123, 126, 127, 128, 129], "train_on_input": [7, 23, 26, 28, 32, 33, 34, 36, 37, 38, 39, 124, 125], "onc": [7, 76, 126, 127, 128, 129, 130], "ve": [7, 66, 82, 124, 125, 126, 128, 129], "instanc": [7, 10, 27, 65, 71, 77, 78, 129], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 34, 123, 126, 130], "under": [7, 125, 126, 128, 130], "preced": [7, 10, 123, 128, 129], "actual": [7, 9, 26, 124], "throw": 7, "notic": [7, 124, 125, 129], "miss": [7, 79, 80, 129], "posit": [7, 10, 29, 64, 66, 68, 69, 70, 128], "anoth": [7, 126], "handl": [7, 12, 20, 27, 81, 124, 126, 129, 130], "def": [7, 8, 9, 12, 84, 88, 124, 125, 129, 130], "dictconfig": [7, 8, 10, 11, 12, 13, 107], "arg": [7, 10, 69, 71, 74, 83, 90, 105], "tupl": [7, 10, 27, 39, 66, 71, 73, 81, 82, 84, 90, 99, 108, 109, 116], "kwarg": [7, 10, 71, 74, 83, 90, 100, 104, 105, 106, 107, 112, 125], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 71, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 85, 86, 87, 89, 90, 93, 94, 96, 97, 98, 100, 102, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 124, 125], "mean": [7, 64, 67, 69, 70, 103, 123, 124, 125, 127, 129], "pass": [7, 10, 26, 27, 28, 30, 31, 34, 35, 37, 41, 42, 64, 65, 71, 76, 80, 84, 87, 94, 95, 97, 100, 103, 106, 107, 112, 123, 124, 125, 129, 130], "add": [7, 9, 26, 29, 82, 87, 88, 90, 125, 126, 128, 129, 130], "d": [7, 20, 64, 66, 69, 82, 123, 124, 129], "llama2_token": [7, 126], "tmp": [7, 89, 124, 127, 128], "option": [7, 8, 14, 17, 18, 22, 24, 26, 28, 29, 30, 31, 34, 35, 37, 39, 41, 42, 48, 49, 50, 55, 56, 58, 61, 64, 68, 69, 70, 71, 79, 80, 81, 82, 85, 86, 87, 92, 93, 94, 96, 98, 104, 107, 110, 114, 120, 121, 123, 125, 126], "bool": [7, 20, 23, 26, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 71, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 95, 97, 100, 101, 103, 106, 110, 112, 115, 124, 130], "max_seq_len": [7, 10, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 64, 66, 68, 69, 81, 82, 124, 125], "int": [7, 9, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 66, 67, 68, 69, 72, 75, 81, 82, 83, 84, 85, 86, 87, 92, 95, 99, 103, 104, 105, 106, 107, 108, 109, 112, 114, 123, 124, 125, 129, 130], "512": [7, 32, 33, 125, 130], "instructdataset": [7, 32, 33, 36, 37, 38, 125], "alreadi": [7, 88, 100, 103, 120, 123, 125, 126, 129], "overwrit": [7, 87, 120], "duplic": [7, 8, 121, 123], "sometim": 7, "than": [7, 25, 39, 64, 66, 84, 87, 88, 115, 116, 124, 125, 126, 127, 128, 129, 130], "resolv": [7, 11, 127], "alpaca": [7, 14, 32, 33, 48, 49, 50, 55, 56, 61, 125], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 104, 105, 106, 107], "disklogg": 7, "log_dir": [7, 104, 106, 107], "conveni": [7, 8, 123], "verifi": [7, 93, 94, 95, 124, 127, 129], "properli": [7, 79, 101, 123], "experi": [7, 107, 119, 121, 124, 128, 129], "wa": [7, 79, 124, 126, 128, 129, 130], "cp": [7, 120, 123, 124, 126, 127, 128], "7b_lora_single_devic": [7, 126, 127, 129, 130], "my_config": 7, "discuss": [7, 127, 129], "guidelin": 7, "while": [7, 8, 48, 49, 50, 55, 56, 61, 65, 121, 126, 130], "mai": [7, 9, 95, 110, 124, 125, 127, 129], "tempt": 7, "put": [7, 8, 127, 129], "much": [7, 126, 128, 129, 130], "give": [7, 125, 129], "maximum": [7, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 64, 66, 68, 69, 82, 123], "flexibl": [7, 27, 125], "switch": 7, "encourag": [7, 129], "clariti": 7, "significantli": 7, "easier": [7, 126, 127], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 29, 31, 41, 73, 125, 130], "expos": [7, 8, 87, 124, 127], "parent": [7, 123], "modul": [7, 10, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 88, 91, 95, 103, 111, 112, 114, 124, 127, 129, 130], "__init__": [7, 8, 129, 130], "py": [7, 10, 48, 49, 50, 55, 56, 61, 64, 66, 67, 68, 72, 73, 123, 126, 128], "guarante": 7, "stabil": [7, 121, 130], "underscor": 7, "_alpaca": 7, "collect": [7, 92, 127], "differ": [7, 9, 26, 27, 28, 30, 81, 88, 109, 116, 121, 123, 124, 126, 128, 129, 130], "itself": 7, "via": [7, 9, 34, 37, 41, 75, 85, 129, 130], "pair": [7, 40, 108, 109, 125], "k1": [7, 8], "v1": [7, 8, 42], "k2": [7, 8], "v2": [7, 8, 125], "lora_finetune_single_devic": [7, 123, 124, 126, 127, 128, 129, 130], "checkpoint": [7, 8, 71, 82, 85, 86, 87, 88, 89, 107, 112, 121, 123, 128, 129, 130], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 31, 34, 37, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 77, 78, 81, 82, 83, 85, 86, 87, 88, 89, 90, 104, 105, 106, 107, 124, 125, 127, 129, 130], "assign": [7, 31], "nest": 7, "dot": 7, "notat": [7, 64, 68, 69], "certain": [7, 124], "flag": [7, 8, 28, 32, 33, 36, 38, 84, 87, 95, 123, 130], "built": [7, 9, 40, 120, 124, 127, 130], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 26], "specif": [7, 8, 10, 95, 124, 125, 126, 130], "pytorch": [7, 8, 69, 71, 79, 84, 101, 106, 110, 112, 114, 119, 120, 121, 128, 129, 130], "llama3": [7, 26, 39, 53, 54, 55, 56, 57, 88, 92, 95, 119, 123, 125], "8b_full": [7, 123, 125], "adamw": [7, 129], "lr": [7, 72], "2e": 7, "fuse": [7, 111], "nproc_per_nod": [7, 125, 128, 129], "full_finetune_distribut": [7, 123, 125, 126, 127], "core": [8, 121, 125, 127, 130], "i": [8, 19, 21, 64, 69, 70, 71, 78, 82, 89, 92, 125, 126, 128, 130], "structur": [8, 15, 16, 19, 21, 26, 124, 125, 126], "new": [8, 35, 59, 66, 88, 104, 106, 124, 126, 127, 128, 129, 130], "user": [8, 15, 16, 19, 20, 21, 23, 25, 26, 64, 81, 124, 125, 127], "thought": [8, 121, 127, 130], "target": [8, 121], "pipelin": [8, 121], "llm": [8, 119, 121, 125, 126, 129], "eg": [8, 69, 85, 121], "meaning": [8, 121, 126], "featur": [8, 9, 120, 121, 126, 127], "fsdp": [8, 84, 89, 95, 103, 121, 127, 128], "activ": [8, 65, 97, 102, 112, 121, 130], "gradient": [8, 103, 111, 121, 126, 128, 129, 130], "accumul": [8, 111, 121], "mix": [8, 123, 125, 126], "precis": [8, 71, 94, 121, 127, 130], "appli": [8, 26, 28, 30, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 67, 68, 69, 70, 79, 80, 112, 121, 130], "given": [8, 10, 18, 25, 75, 76, 83, 92, 93, 94, 98, 103, 111, 115, 121, 129], "complex": 8, "becom": [8, 120, 125], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 69, 88, 123, 125], "methodolog": 8, "reason": [8, 92, 126], "possibl": [8, 26, 29, 34, 123, 125], "trade": 8, "off": [8, 81, 126], "memori": [8, 27, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 71, 79, 95, 97, 102, 103, 119, 121, 126, 127, 128], "vs": [8, 127], "qualiti": [8, 126, 129], "believ": 8, "best": [8, 124], "suit": [8, 127], "b": [8, 64, 66, 68, 69, 70, 75, 103, 107, 129, 130], "fit": [8, 26, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 125], "solut": 8, "result": [8, 81, 126, 128, 129, 130], "meant": [8, 71, 89], "depend": [8, 9, 14, 85, 123, 125, 126, 129, 130], "level": [8, 91, 96, 103, 121, 130], "expertis": 8, "routin": 8, "yourself": [8, 123, 128, 129], "exist": [8, 120, 123, 126, 127, 128, 130], "ad": [8, 81, 87, 88, 124, 129, 130], "ones": 8, "modular": [8, 121], "build": [8, 34, 37, 41, 121, 128, 129], "block": [8, 29, 48, 49, 50, 55, 56, 58, 61, 79, 80, 121], "wandb": [8, 9, 107, 127], "log": [8, 11, 73, 96, 97, 102, 104, 105, 106, 107, 126, 127, 128, 130], "fulli": [8, 27], "nativ": [8, 119, 121, 129, 130], "correct": [8, 17, 36, 67, 68, 69, 93, 121, 124, 125], "numer": [8, 121], "pariti": [8, 121], "verif": 8, "extens": [8, 87, 121], "comparison": [8, 129, 130], "benchmark": [8, 114, 121, 126, 128, 129], "limit": [8, 89, 125], "hidden": [8, 65], "behind": 8, "100": [8, 28, 32, 33, 36, 38, 39, 92, 108, 109, 110, 129, 130], "prefer": [8, 40, 73, 109, 121, 123, 125], "over": [8, 72, 90, 121, 126, 128, 129, 130], "unnecessari": 8, "abstract": [8, 15, 18, 83, 121, 127, 130], "No": [8, 87, 121], "inherit": [8, 90, 121, 125], "go": [8, 19, 21, 81, 121, 125, 126, 127, 130], "upon": [8, 27, 128], "figur": [8, 129, 130], "spectrum": 8, "decid": 8, "interact": [8, 119, 127], "start": [8, 9, 27, 82, 88, 120, 121, 124, 125, 126, 127], "paradigm": 8, "consist": [8, 42, 127], "configur": [8, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 70, 121, 124, 127, 128, 129, 130], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 97, 98, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 121, 123, 124, 125, 126, 127, 128, 129, 130], "overrid": [8, 11, 12, 123, 126, 127, 128, 130], "togeth": [8, 29, 107, 127, 129], "valid": [8, 25, 79, 80, 116, 120, 126, 127], "environ": [8, 93, 101, 120, 123, 126, 127], "logic": [8, 88, 121, 127, 129], "api": [8, 9, 51, 52, 57, 60, 63, 79, 123, 124, 126, 127, 128, 130], "closer": [8, 129], "monolith": [8, 121], "trainer": [8, 73], "A": [8, 9, 23, 27, 29, 64, 69, 70, 71, 73, 75, 79, 81, 82, 84, 89, 90, 97, 98, 102, 103, 108, 109, 118, 119, 122, 123, 124, 126, 129, 130], "wrapper": [8, 81, 82, 89, 91, 123, 129], "around": [8, 26, 81, 82, 97, 110, 123, 124, 126, 129, 130], "extern": [8, 125], "primarili": [8, 27, 129], "eleutherai": [8, 121, 129], "har": [8, 121, 129], "control": [8, 28, 32, 33, 36, 38, 76, 114, 126], "multi": [8, 26, 64, 79, 128], "stage": 8, "distil": 8, "oper": [8, 27, 76, 110, 114], "turn": [8, 20, 25, 26, 82, 124], "dataload": [8, 29, 32, 33, 36, 38], "applic": [8, 64, 85, 86, 107], "clean": [8, 9, 32], "after": [8, 64, 66, 67, 69, 70, 79, 103, 104, 105, 106, 107, 124, 130], "process": [8, 9, 71, 99, 100, 114, 125, 127, 130], "group": [8, 64, 99, 100, 104, 105, 106, 107, 123, 128], "init_process_group": [8, 100], "backend": [8, 123], "gloo": 8, "els": [8, 90, 107, 121, 130], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 27, 87, 90, 95, 125, 127, 128, 129], "stuff": 8, "carri": 8, "relev": [8, 20, 123, 126, 129], "interfac": [8, 15, 18, 27, 74], "metric": [8, 127], "logger": [8, 96, 102, 104, 105, 106, 107, 127], "self": [8, 9, 29, 48, 49, 50, 55, 56, 58, 61, 64, 69, 70, 74, 79, 80, 85, 88, 89, 125, 129, 130], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 84, 95, 103, 110, 112, 124], "_model": [8, 89], "_setup_model": 8, "_token": [8, 125], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 89, 91, 111, 130], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 73, 114, 123, 128], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 29], "batch": [8, 29, 32, 33, 36, 38, 64, 66, 68, 69, 73, 81, 108, 109, 121, 125, 127, 128, 129], "enumer": 8, "_autocast": 8, "logit": [8, 92], "label": [8, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 73, 108, 109], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 104, 105, 106, 107], "step": [8, 29, 69, 72, 82, 91, 104, 105, 106, 107, 110, 111, 119, 126, 129, 130], "learn": [8, 27, 72, 121, 124, 125, 127, 128, 129, 130], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 24, 25, 26, 28, 29, 30, 31, 34, 35, 37, 39, 41, 42, 64, 66, 68, 69, 70, 76, 78, 79, 80, 81, 82, 85, 86, 87, 88, 92, 93, 94, 96, 98, 102, 104, 105, 106, 107, 111, 112, 113, 114, 116, 124, 125, 126], "fullfinetunerecip": 8, "direct": [8, 73, 109, 120], "wandblogg": [9, 129, 130], "workspac": 9, "seen": [9, 129, 130], "screenshot": 9, "below": [9, 68, 84, 125, 128, 129, 130], "packag": [9, 106, 107, 120], "pip": [9, 106, 107, 120, 126, 128], "Then": [9, 76, 127], "login": [9, 107, 123, 126], "project": [9, 48, 49, 50, 55, 56, 58, 61, 64, 65, 79, 80, 95, 107, 119, 129, 130], "grab": [9, 128], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 101, 124], "exit": [9, 120, 123], "resourc": [9, 104, 105, 106, 107], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 28, 29, 30, 31, 37, 39, 41, 64, 68, 69, 70, 92, 124, 126], "desir": [9, 26, 113, 124], "suggest": 9, "approach": [9, 27, 125], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 126], "_output_dir": [9, 85, 86, 87], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 12, 20, 23, 24, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 62, 64, 66, 67, 68, 69, 70, 71, 73, 75, 77, 81, 82, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 103, 109, 110, 112, 113, 114, 115, 125, 126, 129, 130], "descript": [9, 34, 39, 123], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 23, 26, 29, 32, 33, 36, 38, 102, 125], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 26, 28, 30, 31, 34, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 59, 60, 61, 62, 63, 64, 67, 68, 72, 73, 79, 84, 85, 86, 90, 96, 101, 106, 107, 110, 112, 114, 120, 125, 126], "com": [10, 48, 49, 50, 55, 56, 61, 64, 67, 68, 72, 73, 79, 120], "facebookresearch": [10, 67, 68], "blob": [10, 48, 49, 50, 55, 56, 61, 64, 67, 68, 72, 73], "main": [10, 12, 64, 67, 68, 120, 126, 128], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 69], "32": [10, 128, 129, 130], "num_head": [10, 64, 66, 68, 69], "num_kv_head": [10, 64, 66], "vocab_s": 10, "must": [10, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 74, 82, 90, 130], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 62, 64, 66, 67, 68, 69, 70, 72, 73, 74, 75, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 113, 114, 115, 124, 125, 129, 130], "nn": [10, 64, 65, 66, 69, 70, 71, 74, 76, 77, 78, 84, 91, 103, 111, 112, 116, 129, 130], "parsed_yaml": 10, "embed_dim": [10, 64, 68, 70, 129], "valueerror": [10, 21, 25, 34, 39, 64, 66, 69, 73, 85, 86, 87, 94, 97, 114, 116], "recipe_nam": 11, "rank": [11, 48, 49, 50, 55, 56, 58, 61, 75, 99, 101, 114, 127, 129, 130], "zero": [11, 66, 67, 126, 128], "displai": 11, "callabl": [12, 26, 28, 30, 69, 76, 84, 92, 95, 98, 103, 112], "With": [12, 126, 129, 130], "my_recip": 12, "foo": 12, "bar": [12, 121, 127], "instanti": [13, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 56, 58, 59, 61, 62, 89], "configerror": 13, "cannot": [13, 87, 128], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 97, 104, 105, 106, 107, 125, 126, 130], "prompt": [14, 15, 17, 18, 19, 21, 22, 23, 26, 28, 30, 32, 33, 34, 36, 37, 38, 39, 41, 69, 81, 92, 125, 126, 128], "templat": [14, 15, 17, 18, 22, 26, 28, 30, 32, 33, 36, 37, 38, 39], "style": [14, 29, 32, 33, 34, 39, 130], "slightli": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 125], "map": [14, 17, 18, 22, 23, 26, 27, 28, 29, 30, 37, 78, 85, 89, 91, 104, 105, 106, 107, 111, 124, 125, 126, 129], "column_map": [14, 17, 18, 22, 26, 28, 30, 37, 125], "placehold": [14, 15, 17, 18, 22, 26, 28, 30, 37, 125], "column": [14, 17, 18, 22, 26, 28, 30, 31, 37, 41, 64, 69, 70, 124, 125], "ident": [14, 17, 18, 21, 22, 28, 29, 30, 37, 126], "role": [15, 20, 23, 26, 81, 124, 125], "system": [15, 16, 19, 20, 21, 23, 25, 26, 81, 124, 125], "assist": [15, 16, 19, 20, 23, 25, 26, 81, 92, 124, 125], "messag": [15, 16, 19, 21, 23, 25, 26, 34, 81, 82, 83, 120, 123, 124, 125], "accord": [15, 21, 124], "openai": [16, 34, 125], "markup": 16, "languag": [16, 75, 92, 129], "It": [16, 21, 123, 124, 125, 130], "huggingfac": [16, 26, 28, 30, 31, 34, 35, 37, 41, 42, 62, 72, 73, 85, 86, 123, 126], "im_start": 16, "context": [16, 62, 76, 110, 113, 125], "im_end": 16, "goe": [16, 76], "respons": [16, 73, 81, 125, 126, 127, 128], "appropri": [16, 19, 21, 27, 72, 85, 125, 130], "tag": [16, 19, 21, 26, 82, 104, 105, 106, 107, 124], "grammar": [17, 36, 125], "sentenc": [17, 29, 81], "alwai": [18, 90], "human": [19, 23, 124], "taken": [19, 129, 130], "inst": [19, 21, 26, 124, 125], "sy": [19, 124, 125], "respect": [19, 27, 78, 124, 125], "honest": [19, 124, 125], "am": [19, 21, 124, 125, 126, 128], "pari": [19, 21, 125], "capit": [19, 21, 125], "franc": [19, 21, 125], "known": [19, 21, 81, 98, 125], "its": [19, 21, 29, 64, 68, 69, 70, 111, 114, 123, 125, 126, 128, 129], "stun": [19, 21, 125], "liter": [20, 48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 79, 80], "mask": [20, 28, 29, 32, 33, 36, 38, 64, 69, 70, 81, 82, 124, 125], "ipython": 20, "eot": 20, "dataclass": [20, 124], "repres": [20, 109, 124], "individu": [20, 29, 97, 107, 112, 124, 125], "tiktoken": [20, 82, 128], "special": [20, 26, 81, 82, 89, 125], "variabl": [20, 26, 27, 28, 30, 37, 101, 130], "writer": 20, "whether": [20, 23, 26, 28, 32, 33, 34, 36, 37, 38, 39, 41, 48, 49, 50, 55, 56, 58, 61, 71, 75, 79, 80, 81, 82, 84, 94, 97, 124, 125], "correspond": [20, 74, 77, 94, 109, 127, 128], "consecut": [20, 25], "from_dict": [20, 124], "construct": [20, 129], "dictionari": [20, 29, 97, 102, 104, 105, 106, 107, 109, 126], "mistral": [21, 26, 39, 58, 59, 60, 88, 123, 124, 126, 127], "llama2chatformat": [21, 124, 125], "summar": [22, 38, 124, 125], "task": [22, 27, 35, 124, 125, 126, 128, 129, 130], "dialogu": [22, 38, 124], "dialog": 22, "adher": 23, "sharegpt": [23, 34], "gpt": [23, 64, 126], "remain": [23, 72, 129], "unmask": 23, "eos_id": 24, "length": [24, 25, 27, 28, 29, 30, 31, 32, 33, 35, 37, 39, 41, 42, 62, 64, 66, 68, 69, 81, 82, 86, 108, 109], "last": [24, 29, 72, 125], "replac": [24, 28, 32, 33, 36, 38, 71, 129], "forth": [25, 125], "come": [25, 74, 129], "empti": [25, 123], "shorter": 25, "min": [25, 129], "invalid": 25, "convert_to_messag": [26, 124], "chat_format": [26, 34, 39, 124, 125], "chatformat": [26, 34, 125], "load_dataset_kwarg": [26, 28, 30, 31, 34, 35, 37, 41, 42], "multiturn": [26, 124], "prepar": [26, 124], "truncat": [26, 28, 29, 30, 31, 35, 37, 39, 41, 42, 81, 82, 125], "encod": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 73, 81, 82, 83, 124], "decod": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 69, 81, 82, 83, 92, 124], "anyth": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42], "load_dataset": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 124, 125], "co": [26, 28, 30, 31, 34, 35, 37, 41, 42, 62, 85, 86, 126], "doc": [26, 28, 30, 31, 34, 35, 37, 41, 42, 84, 90, 96, 101, 106, 107, 110, 114, 123, 126], "en": [26, 28, 30, 31, 34, 35, 37, 41, 42], "package_refer": [26, 28, 30, 31, 34, 35, 37, 41, 42], "loading_method": [26, 28, 30, 31, 34, 35, 37, 41, 42], "text": [26, 29, 31, 35, 41, 42, 81, 82, 83, 124, 126], "extra": [26, 120, 129, 130], "still": [26, 90, 129, 130], "where": [26, 27, 32, 33, 36, 38, 64, 69, 75, 81, 95, 103, 109, 125], "unless": 26, "check": [26, 34, 69, 79, 94, 101, 115, 119, 124, 126, 127, 129], "concaten": [27, 81, 83, 109], "sub": [27, 106], "unifi": 27, "were": [27, 76, 124, 127], "simplifi": [27, 123, 129], "simultan": 27, "intern": [27, 90], "aggreg": 27, "transpar": 27, "index": [27, 64, 68, 69, 70, 72, 108, 109, 120, 124, 126], "howev": [27, 120], "constitu": 27, "might": [27, 123, 126], "larg": [27, 75, 123, 130], "comput": [27, 64, 65, 68, 69, 73, 97, 114, 126, 130], "cumul": 27, "maintain": [27, 130], "indic": [27, 29, 64, 68, 69, 70, 84, 101, 124], "deleg": 27, "retriev": [27, 95], "lead": [27, 81], "high": [27, 121, 129], "scale": [27, 48, 49, 50, 55, 56, 58, 61, 75, 92, 129, 130], "consid": 27, "strategi": 27, "stream": [27, 96], "demand": 27, "deriv": [27, 65, 69, 70], "_dataset": 27, "_len": 27, "total": [27, 72, 99, 118, 122, 126, 128, 129], "combin": 27, "_index": 27, "lookup": 27, "dataset1": 27, "mycustomdataset": 27, "params1": 27, "dataset2": 27, "params2": 27, "concat_dataset": 27, "data_point": 27, "1500": 27, "element": [27, 82, 126], "focus": [27, 127], "enhanc": [27, 130], "divers": 27, "machin": [27, 93, 123, 126], "instructtempl": [28, 30, 125], "contribut": [28, 32, 33, 36, 38], "disabl": [28, 30, 31, 35, 37, 41, 42, 76, 114], "recommend": [28, 30, 31, 32, 33, 35, 37, 41, 42, 106, 124, 126, 130], "highest": [28, 30, 31, 32, 33, 35, 37, 41, 42], "sequenc": [28, 29, 30, 31, 32, 33, 35, 37, 39, 41, 42, 64, 66, 68, 69, 81, 82, 108, 109, 124], "ds": [29, 39], "max_pack": 29, "split_across_pack": 29, "greedi": 29, "pack": [29, 32, 33, 34, 36, 37, 38, 39, 41, 64, 68, 69, 70], "done": [29, 79, 94, 103, 129, 130], "preprocess": 29, "outsid": [29, 114, 126, 128, 129], "sampler": [29, 127], "part": [29, 124, 130], "buffer": 29, "long": [29, 124, 129], "enough": [29, 124], "attent": [29, 48, 49, 50, 55, 56, 58, 61, 62, 64, 66, 68, 69, 70, 79, 80, 128, 129, 130], "lower": [29, 129], "triangular": 29, "cross": 29, "attend": [29, 64, 69, 70], "rel": [29, 64, 68, 69, 70, 97, 129], "pad": [29, 92, 108, 109, 125], "max": [29, 39, 69, 72, 81, 123, 129], "wise": 29, "collat": [29, 108, 125], "made": [29, 34, 37, 41, 68, 126], "smaller": [29, 126, 128, 129, 130], "jam": 29, "vari": 29, "s1": [29, 81], "s2": [29, 81], "s3": 29, "s4": 29, "contamin": 29, "input_po": [29, 64, 66, 68, 69, 70], "matrix": 29, "causal": [29, 64, 69, 70], "continu": [29, 125], "increment": 29, "move": [29, 69], "entir": [29, 103, 124, 130], "avoid": [29, 67, 71, 114, 123, 130], "freeform": [31, 41], "unstructur": [31, 42], "corpu": [31, 35, 42], "local": [31, 41, 107, 114, 120, 123, 124, 126, 127], "tabular": [31, 41], "yahma": [32, 37], "codebas": [32, 33, 36, 38, 126], "prior": [32, 33, 34, 36, 37, 38, 39, 41], "alpaca_d": [32, 33], "batch_siz": [32, 33, 36, 38, 64, 66, 69, 70, 73, 126], "tatsu": 33, "lab": 33, "conversation_styl": [34, 125], "chatdataset": [34, 39, 124, 125], "friendli": [34, 37, 41, 92, 124], "huggingfaceh4": 34, "no_robot": 34, "chatmlformat": 34, "2096": [34, 37, 41], "accomplish": [34, 37, 41], "packeddataset": [34, 37, 41, 125], "ccdv": 35, "cnn_dailymail": 35, "textcompletiondataset": [35, 41, 42, 125], "similar": [35, 40, 42, 79, 125, 126, 128, 129, 130], "cnn": 35, "dailymail": 35, "articl": [35, 42], "extract": 35, "highlight": [35, 130], "liweili": 36, "c4_200m": 36, "variant": [36, 38], "mirror": [36, 38], "llama_recip": [36, 38], "grammar_d": 36, "alpaca_clean": 37, "alpacainstructtempl": [37, 125], "samsum": [38, 125], "summari": [38, 97, 125], "samsum_d": 38, "open": [39, 43, 44, 125, 126], "orca": 39, "slimorca": 39, "dedup": 39, "1024": [39, 40, 125], "prescrib": 39, "least": [39, 128, 129], "though": [39, 124], "10": [39, 108, 109, 126, 128, 130], "351": 39, "82": [39, 126], "391": 39, "221": 39, "220": 39, "193": 39, "12": [39, 109, 120], "471": 39, "lvwerra": [40, 125], "stack": [40, 125], "exchang": [40, 125], "preferencedataset": [40, 125], "stackexchangepair": 40, "textdataset": 41, "omit": [41, 129], "allenai": [41, 125], "c4": [41, 125], "data_dir": [41, 125], "realnewslik": [41, 125], "wikitext": 42, "subset": [42, 77], "103": [42, 126], "raw": 42, "wikipedia": 42, "page": [42, 120, 121, 123, 127, 128], "gemma": [43, 44, 88], "gemmatransformerdecod": [43, 44], "w": [43, 44, 45, 46, 47, 53, 54, 59, 106, 107, 124, 126, 129, 130], "blog": [43, 44], "technolog": [43, 44], "develop": [43, 44, 130], "transformerdecod": [45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 92, 129], "arxiv": [45, 46, 47, 51, 52, 57, 60, 63, 64, 67, 68, 73], "org": [45, 46, 47, 51, 52, 57, 60, 63, 64, 67, 68, 73, 84, 90, 96, 101, 106, 110, 112, 114, 120], "ab": [45, 46, 47, 51, 52, 57, 60, 63, 68, 73], "2307": [45, 46, 47], "09288": [45, 46, 47], "70b": [46, 49, 53, 55, 128], "lora_attn_modul": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 79, 80, 129, 130], "q_proj": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 79, 80, 129, 130], "k_proj": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 79, 80, 129, 130], "v_proj": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 79, 80, 129, 130], "output_proj": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 79, 80, 129, 130], "apply_lora_to_mlp": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 79, 80, 129], "apply_lora_to_output": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 79, 80, 129], "lora_rank": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 129], "lora_alpha": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 129], "float": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 64, 67, 72, 73, 75, 92, 97, 102, 104, 105, 106, 107, 129, 130], "16": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 109, 129, 130], "lora_dropout": [48, 49, 50, 51, 52], "05": [48, 49, 50, 51, 52], "quantize_bas": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 75, 130], "lora": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 75, 76, 79, 80, 85, 103, 119, 121, 124, 127, 128], "tloen": [48, 49, 50, 55, 56, 61], "8bb8579e403dc78e37fe81ffbb253c413007323f": [48, 49, 50, 55, 56, 61], "l41": [48, 49, 50, 55, 56, 61], "l43": [48, 49, 50, 55, 56, 61], "linear": [48, 49, 50, 51, 52, 55, 56, 57, 58, 60, 61, 63, 69, 74, 75, 79, 80, 129, 130], "mlp": [48, 49, 50, 55, 56, 58, 61, 69, 70, 79, 80, 128, 129], "final": [48, 49, 50, 55, 56, 58, 61, 65, 69, 76, 79, 80, 82, 126, 128, 129, 130], "low": [48, 49, 50, 55, 56, 58, 61, 75, 126, 129, 130], "approxim": [48, 49, 50, 55, 56, 58, 61, 75, 129], "factor": [48, 49, 50, 55, 56, 58, 61, 75, 126], "llama2_70b": 49, "llama2_7b": [50, 129], "qlora": [51, 52, 57, 60, 63, 71, 119, 121, 128, 129], "per": [51, 52, 57, 60, 63, 66, 71, 123, 128, 130], "paper": [51, 52, 57, 60, 63, 129, 130], "2305": [51, 52, 57, 60, 63, 64, 73], "14314": [51, 52, 57, 60, 63], "lora_llama2_13b": 51, "lora_llama2_7b": [52, 129], "llama3_70b": 55, "llama3_8b": [56, 92, 128], "lora_llama3_8b": 57, "announc": 59, "lora_mistral_7b": 60, "phi3": [61, 62, 63, 88, 123], "phi3_mini": [61, 88], "ref": [62, 107], "phi": [62, 88], "128k": 62, "nor": 62, "slide": 62, "window": [62, 125], "lora_phi3_mini": 63, "head_dim": [64, 66, 69], "pos_embed": [64, 129], "kv_cach": 64, "kvcach": [64, 69], "attn_dropout": [64, 69], "head": [64, 66, 68, 69, 88, 128], "queri": [64, 66, 69, 70, 128], "gqa": 64, "introduc": [64, 67, 75, 124, 125, 129, 130], "pdf": [64, 67], "13245v1": 64, "version": [64, 92, 115, 120, 124, 128, 130], "multihead": 64, "mha": [64, 69], "n": [64, 81, 82, 118, 122, 124, 125], "extrem": 64, "share": [64, 125, 126], "mqa": 64, "credit": 64, "document": [64, 84, 95, 103, 123, 125], "lightn": 64, "lit": 64, "lit_gpt": 64, "v": [64, 69, 129], "k": [64, 129], "q": [64, 129], "n_kv_head": 64, "dimens": [64, 66, 68, 69, 75, 128, 129, 130], "calcul": [64, 69, 128], "e": [64, 71, 74, 78, 85, 89, 97, 120, 126, 128, 129, 130], "g": [64, 74, 85, 97, 128, 129, 130], "rotarypositionalembed": [64, 129], "cach": [64, 66, 68, 69, 120, 123], "rope": [64, 68], "dropout": [64, 75, 129, 130], "onto": 64, "scaled_dot_product_attent": 64, "seq_length": [64, 70, 92], "boolean": [64, 69, 70, 84], "softmax": [64, 69, 70], "row": [64, 69, 70, 124], "j": [64, 69, 70], "seq_len": 64, "bigger": 64, "n_h": [64, 68], "num": [64, 68], "n_kv": 64, "kv": [64, 66, 69], "emb": [64, 69], "h_d": [64, 68], "gate_proj": 65, "down_proj": 65, "up_proj": 65, "silu": 65, "feed": [65, 70], "network": [65, 76, 129, 130], "fed": [65, 124], "multipli": 65, "subclass": [65, 90], "although": [65, 129], "afterward": 65, "former": 65, "regist": [65, 71, 111, 130], "hook": [65, 71, 111, 130], "latter": 65, "standalon": 66, "past": 66, "becaus": [66, 69, 87, 123, 124, 126, 128], "expand": 66, "dpython": [66, 69, 71], "reset": [66, 69, 97], "k_val": 66, "v_val": 66, "h": [66, 120, 123], "longer": [66, 125], "ep": 67, "1e": 67, "06": [67, 129], "root": [67, 106, 107], "squar": 67, "1910": 67, "07467": 67, "verfic": [67, 68], "small": [67, 126], "divis": 67, "10000": 68, "rotari": [68, 128], "propos": 68, "2104": 68, "09864": 68, "l450": 68, "upto": 68, "init": [68, 97, 107, 130], "exceed": 68, "freq": 68, "recomput": 68, "geometr": 68, "progress": [68, 127], "rotat": 68, "angl": 68, "todo": 68, "effici": [68, 79, 95, 119, 121, 126, 127, 129], "transformerdecoderlay": 69, "norm": [69, 70], "space": 69, "belong": [69, 91], "reduc": [69, 121, 125, 129, 130], "statement": 69, "improv": [69, 95, 126, 128, 129], "readabl": [69, 126], "At": 69, "arang": 69, "prompt_length": 69, "causal_mask": 69, "m_": 69, "seq": 69, "reset_cach": 69, "setup_cach": 69, "attn": [70, 129, 130], "causalselfattent": [70, 129], "sa_norm": 70, "mlp_norm": 70, "ff": 70, "common_util": 71, "bfloat16": [71, 113, 126, 127, 128, 129], "offload_to_cpu": 71, "nf4": [71, 130], "restor": 71, "higher": [71, 128, 130], "offload": [71, 130], "increas": [71, 72, 128, 129], "peak": [71, 97, 102, 126, 128, 129, 130], "gpu": [71, 123, 126, 127, 128, 129, 130], "_register_state_dict_hook": 71, "m": [71, 82, 92, 124], "mymodul": 71, "_after_": 71, "nf4tensor": [71, 130], "unquant": [71, 126, 130], "unus": 71, "num_warmup_step": 72, "num_training_step": 72, "num_cycl": 72, "last_epoch": 72, "lambdalr": 72, "rate": [72, 121, 127], "schedul": [72, 110, 127], "linearli": 72, "decreas": [72, 125, 129, 130], "cosin": 72, "v4": 72, "23": [72, 128], "src": 72, "l104": 72, "warmup": [72, 110], "phase": 72, "wave": 72, "half": 72, "lr_schedul": 72, "beta": 73, "label_smooth": 73, "loss_typ": 73, "sigmoid": 73, "dpo": [73, 76, 109], "18290": 73, "trl": 73, "librari": [73, 90, 94, 96, 114, 119, 121, 123, 125, 130], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 73, "dpo_train": 73, "l844": 73, "temperatur": [73, 92, 126], "uncertainti": 73, "hing": 73, "ipo": 73, "kto_pair": 73, "policy_chosen_logp": 73, "policy_rejected_logp": 73, "reference_chosen_logp": 73, "reference_rejected_logp": 73, "polici": [73, 76, 84, 95, 103, 112], "probabl": [73, 75, 92, 126], "chosen": [73, 125], "reject": [73, 125], "chosen_reward": 73, "reward": 73, "rejected_reward": 73, "unknown": 73, "peft": [74, 75, 76, 77, 78, 79, 80, 85, 129, 130], "protocol": 74, "adapter_param": [74, 75, 76, 77, 78], "proj": 74, "in_dim": [74, 75, 129, 130], "out_dim": [74, 75, 129, 130], "bia": [74, 75, 129, 130], "loralinear": [74, 129, 130], "alpha": [75, 129, 130], "use_bia": 75, "perturb": 75, "decomposit": [75, 129], "matric": [75, 103, 129, 130], "trainabl": [75, 78, 103, 129, 130], "mapsto": 75, "w_0x": 75, "r": [75, 82, 129], "bax": 75, "lora_a": [75, 129, 130], "lora_b": [75, 129, 130], "temporarili": 76, "neural": [76, 129, 130], "treat": [76, 90, 124], "attribut": [76, 81, 91], "caller": 76, "whose": [76, 111], "yield": 76, "get_adapter_param": [78, 129], "base_miss": 79, "base_unexpect": 79, "lora_miss": 79, "lora_unexpect": 79, "validate_state_dict_for_lora": [79, 129], "unlik": [79, 126, 128], "reli": 79, "unexpect": 79, "strict": [79, 129], "pull": [79, 123], "120600": 79, "assertionerror": [79, 80, 109], "nonempti": 79, "full_model_state_dict_kei": 80, "lora_state_dict_kei": 80, "base_model_state_dict_kei": 80, "confirm": [80, 120], "determin": 80, "lora_modul": 80, "complement": 80, "disjoint": 80, "union": [80, 104, 105, 106, 107, 112, 114], "non": [80, 81], "overlap": 80, "sentencepieceprocessor": 81, "pretrain": [81, 82, 123, 124, 127, 129, 130], "spm_model": [81, 124], "tokenized_text": 81, "hello": [81, 124, 126, 128], "world": [81, 99, 101, 126], "add_bo": [81, 82, 83, 124], "add_eo": [81, 82, 83, 124], "31587": 81, "29644": 81, "102": 81, "trim_leading_whitespac": 81, "prefix": 81, "unbatch": 81, "prepend": [81, 82], "bo": [81, 82, 124, 125], "append": [81, 120], "eo": [81, 82, 124, 125], "trim": 81, "whitespac": 81, "underli": [81, 130], "sentencepiec": [81, 128], "due": [81, 129, 130], "tokenize_messag": [81, 82, 83, 124, 125], "problem": 81, "slice": 81, "tokenizer_path": 81, "separ": [81, 85, 124, 127, 128, 129, 130], "concat": 81, "1788": 81, "2643": 81, "13": [81, 109, 126, 128, 130], "1792": 81, "9508": 81, "465": 81, "22137": 81, "2933": 81, "join": 81, "llama3_tiktoken": 82, "p": [82, 84, 89, 129, 130], "l": 82, "all_special_token": 82, "bos_token": 82, "begin_of_text": [82, 124], "eos_token": 82, "end_of_text": 82, "start_header_id": [82, 124], "end_header_id": [82, 124], "step_id": 82, "eom_id": 82, "eot_id": [82, 124], "python_tag": 82, "identif": 82, "regex": 82, "second": [82, 126, 128, 129, 130], "uniqu": [82, 88], "256": [82, 126, 128], "header": [82, 124], "token_id": [82, 83], "truncate_at_eo": 82, "tokenize_head": 82, "datatyp": [84, 130], "denot": 84, "integ": [84, 108, 114], "auto_wrap_polici": [84, 95, 112], "submodul": [84, 103], "obei": 84, "contract": 84, "get_fsdp_polici": 84, "modules_to_wrap": [84, 95, 103], "min_num_param": 84, "my_fsdp_polici": 84, "recurs": [84, 103, 106], "isinst": [84, 125], "sum": [84, 129], "numel": [84, 129], "1000": 84, "functool": 84, "partial": 84, "stabl": [84, 101, 106, 110, 114, 120], "html": [84, 90, 96, 101, 106, 110, 112, 114, 119], "alia": 84, "from_pretrain": 85, "0001_of_0003": 85, "0002_of_0003": 85, "preserv": [85, 130], "weight_map": [85, 126], "convert_weight": 85, "_model_typ": [85, 88], "intermediate_checkpoint": [85, 86, 87], "_weight_map": 85, "shard": [86, 128], "wip": 86, "larger": [87, 126, 128], "present": 87, "down": [87, 125, 129, 130], "intermedi": [87, 112, 128, 130], "qualnam": 88, "boundari": 88, "distinguish": 88, "gate": [88, 123, 127], "my_new_model": 88, "my_custom_state_dict_map": 88, "mistral_reward": 88, "classif": 88, "mistral_classifi": 88, "optim_map": 89, "bare": 89, "bone": 89, "distribut": [89, 93, 100, 101, 112, 114, 121, 123, 127, 128], "optim_dict": [89, 91, 111], "cfg_optim": 89, "ckpt": 89, "optim_ckpt": 89, "placeholder_optim_dict": 89, "optiminbackwardwrapp": 89, "get_optim_kei": 89, "arbitrari": [89, 129], "hyperparamet": [89, 121, 127, 129, 130], "optim_ckpt_map": 89, "runtimeerror": [89, 94, 100], "loadabl": 89, "argpars": 90, "argumentpars": 90, "builtin": 90, "said": 90, "noth": 90, "consult": 90, "info": [90, 127], "parse_known_arg": 90, "namespac": 90, "act": 90, "precid": 90, "parse_arg": 90, "properti": [90, 129], "too": [90, 128], "optimizerinbackwardwrapp": 91, "top": [91, 126, 130], "named_paramet": 91, "max_generated_token": 92, "pad_id": 92, "top_k": [92, 126], "stop_token": 92, "custom_generate_next_token": 92, "condit": [92, 101, 123, 125], "bsz": 92, "predict": 92, "prune": [92, 130], "stop": 92, "compil": [92, 126, 128, 130], "generate_next_token": 92, "llama3_token": [92, 128], "hi": [92, 124], "my": [92, 123, 124, 125, 126, 128], "jeremi": 92, "float32": 94, "bf16": [94, 130], "request": [94, 125, 126], "inde": [94, 126], "kernel": 94, "isn": [94, 123], "hardwar": [94, 121, 125, 126, 129], "memory_efficient_fsdp_wrap": 95, "maxim": [95, 103, 119, 121], "been": [95, 128], "workload": 95, "15": [95, 109, 124, 126, 129, 130], "alongsid": 95, "ac": 95, "fullyshardeddataparallel": [95, 103], "fsdppolicytyp": [95, 103], "handler": 96, "reset_stat": 97, "track": 97, "alloc": [97, 102, 103, 128, 130], "reserv": [97, 102, 124, 130], "stat": [97, 102, 130], "int4": 98, "4w": [98, 126, 128], "recogn": 98, "mode": [98, 126], "aka": 99, "master": 101, "port": [101, 123], "address": 101, "hold": [101, 127], "peak_memory_act": 102, "peak_memory_alloc": 102, "peak_memory_reserv": 102, "get_memory_stat": 102, "own": [103, 114, 123, 124, 125, 126, 129], "unit": [103, 121], "hierarch": 103, "requires_grad": [103, 129, 130], "filenam": 104, "log_": 104, "unixtimestamp": 104, "txt": [104, 125, 127], "thread": 104, "safe": 104, "flush": [104, 105, 106, 107], "ndarrai": [104, 105, 106, 107], "scalar": [104, 105, 106, 107], "record": [104, 105, 106, 107], "payload": [104, 105, 106, 107], "organize_log": 106, "tensorboard": 106, "subdirectori": 106, "compar": [106, 115, 126, 129, 130], "logdir": 106, "startup": 106, "tree": [106, 125, 126], "tfevent": 106, "encount": 106, "frontend": 106, "organ": [106, 123], "accordingli": 106, "my_log_dir": 106, "view": [106, 126, 127], "my_metr": [106, 107], "termin": [106, 107], "entiti": 107, "bias": 107, "sent": 107, "usernam": 107, "my_project": 107, "my_ent": 107, "my_group": 107, "importerror": 107, "account": [107, 129, 130], "log_config": 107, "link": [107, 126], "capecap": 107, "6053ofw0": 107, "torchtune_config_j67sb73v": 107, "padding_idx": [108, 109], "ignore_idx": [108, 109], "longest": 108, "token_pair": 108, "input_id": 109, "chosen_input_id": [109, 125], "chosen_label": [109, 125], "rejected_input_id": [109, 125], "rejected_label": [109, 125], "14": [109, 130], "17": [109, 126, 129], "18": [109, 128], "19": [109, 126, 128, 130], "20": 109, "torchtune_perf_trac": 110, "contextmanag": [110, 113], "wait": 110, "trace": 110, "speed": [110, 128, 130], "reduct": [110, 129], "soon": 111, "readi": [111, 119, 124], "grad": 111, "achiev": [111, 126, 128, 129, 130], "acwrappolicytyp": 112, "describ": [112, 125], "author": [112, 121, 127, 130], "fsdp_adavnced_tutori": 112, "insid": 113, "debug_mod": 114, "pseudo": 114, "random": [114, 127], "commonli": [114, 126, 129, 130], "numpi": 114, "determinist": 114, "global": [114, 125], "warn": 114, "nondeterminist": 114, "addition": [114, 125, 129], "cudnn": 114, "set_deterministic_debug_mod": 114, "algorithm": 114, "greater": 115, "equal": 115, "against": [115, 130], "__version__": 115, "named_param": 116, "iter": [116, 130], "generated_examples_python": 117, "zip": 117, "galleri": [117, 122], "sphinx": 117, "000": [118, 122, 128], "execut": [118, 122], "generated_exampl": 118, "mem": [118, 122], "mb": [118, 122], "topic": 119, "gentl": 119, "introduct": 119, "first_finetune_tutori": 119, "workflow": [119, 125, 127, 129], "requisit": 120, "proper": [120, 127], "host": [120, 123, 127], "latest": [120, 127, 130], "And": [120, 126, 128], "ls": [120, 123, 126, 127, 128], "welcom": [120, 123], "show": [120, 123, 124, 129], "greatest": [120, 127], "contributor": 120, "cd": [120, 126], "even": [120, 123, 124, 125, 128, 129, 130], "commit": 120, "branch": 120, "url": 120, "whl": 120, "therebi": [120, 130], "forc": 120, "reinstal": 120, "opt": [120, 127], "suffix": 120, "cu121": 120, "On": [121, 129], "pointer": 121, "emphas": 121, "aspect": 121, "simplic": 121, "component": 121, "reus": 121, "prove": 121, "democrat": 121, "box": [121, 130], "zoo": 121, "varieti": [121, 129], "techniqu": [121, 126, 127, 129], "integr": [121, 126, 127, 128, 129, 130], "excit": 121, "checkout": 121, "quickstart": 121, "attain": 121, "better": [121, 124, 125, 126], "chekckpoint": 121, "embodi": 121, "philosophi": 121, "usabl": 121, "composit": 121, "hard": [121, 125], "outlin": 121, "unecessari": 121, "never": 121, "thoroughli": 121, "short": 123, "subcommand": 123, "anytim": 123, "symlink": 123, "auto": 123, "wrote": 123, "readm": 123, "md": 123, "lot": [123, 126], "recent": 123, "releas": [123, 128], "agre": 123, "term": 123, "perman": 123, "eat": 123, "bandwith": 123, "storag": [123, 130], "00030": 123, "ootb": 123, "full_finetune_single_devic": [123, 125, 126, 127], "7b_full_low_memori": [123, 126, 127], "code_llama2": 123, "8b_full_single_devic": [123, 125], "mini_full_low_memori": 123, "7b_full": [123, 126, 127], "13b_full": [123, 126, 127], "70b_full": 123, "edit": 123, "destin": 123, "lora_finetune_distribut": [123, 128, 129], "torchrun": 123, "8b_lora_single_devic": [123, 124, 128], "launch": [123, 124, 127], "nproc": 123, "node": 123, "worker": 123, "nnode": [123, 129], "minimum_nod": 123, "maximum_nod": 123, "fail": 123, "rdzv": 123, "rendezv": 123, "endpoint": 123, "8b_lora": [123, 128], "bypass": 123, "vice": 123, "versa": 123, "fancy_lora": 123, "8b_fancy_lora": 123, "sai": [123, 124, 127], "further": [123, 125, 129, 130], "know": [124, 125, 126, 128, 129], "align": 124, "intend": 124, "nice": 124, "meet": 124, "overhaul": 124, "accompani": 124, "who": 124, "influenti": 124, "hip": 124, "hop": 124, "artist": [124, 128], "2pac": 124, "rakim": 124, "c": 124, "na": 124, "flavor": [124, 125], "msg": 124, "formatted_messag": [124, 125], "nyou": [124, 125], "nwho": 124, "sentencepiecetoken": 124, "why": [124, 127, 129], "user_messag": 124, "518": 124, "25580": 124, "29962": 124, "3532": 124, "14816": 124, "29903": 124, "6778": 124, "piece_to_id": 124, "vector": 124, "place": 124, "manual": [124, 130], "529": 124, "29879": 124, "29958": 124, "tiktokentoken": 124, "nhere": 124, "_encode_special_token": 124, "128000": 124, "128009": 124, "pure": 124, "That": 124, "won": [124, 126, 128], "mess": 124, "govern": 124, "prime": 124, "strictli": 124, "summarizetempl": [124, 125], "lightweight": 124, "ask": 124, "untouch": 124, "nsummari": 124, "robust": 124, "csv": [124, 125], "question": [124, 125, 126, 128], "answer": [124, 126, 128], "onlin": 124, "forum": 124, "panda": 124, "pd": 124, "df": 124, "read_csv": 124, "your_fil": 124, "nrow": 124, "tolist": 124, "iloc": 124, "gp": 124, "receiv": 124, "commun": [124, 125, 126], "satellit": 124, "thing": [124, 130], "message_convert": 124, "input_msg": 124, "output_msg": 124, "assistant_messag": 124, "But": [124, 126, 128, 129], "mistralchatformat": 124, "custom_dataset": 124, "2048": 124, "data_fil": [124, 125], "honor": 124, "copi": [124, 126, 127, 128, 130], "custom_8b_lora_single_devic": 124, "steer": 125, "wheel": 125, "publicli": 125, "great": [125, 126], "hood": [125, 126, 130], "text_completion_dataset": 125, "padded_col": 125, "upper": 125, "constraint": [125, 129], "slow": [125, 130], "signific": 125, "speedup": [125, 128], "minim": [125, 127, 129, 130], "my_data": 125, "instruct_dataset": 125, "fix": 125, "goal": 125, "agnost": 125, "respond": 125, "classifi": 125, "anim": 125, "plant": 125, "miner": 125, "oak": 125, "copper": 125, "ore": 125, "eleph": 125, "customtempl": 125, "cl": 125, "chat_dataset": 125, "quit": [125, 130], "similarli": 125, "incorpor": 125, "advanc": 125, "customchatformat": 125, "vicgal": 125, "gpt4": 125, "drive": 125, "rajpurkar": 125, "io": 125, "squad": 125, "explor": 125, "rlhf": 125, "few": [125, 128, 129, 130], "adjust": 125, "chosen_messag": 125, "transformed_sampl": 125, "key_chosen": 125, "rejected_messag": 125, "key_reject": 125, "c_mask": 125, "np": 125, "cross_entropy_ignore_idx": 125, "r_mask": 125, "stack_exchanged_paired_dataset": 125, "had": 125, "stackexchangedpairedtempl": 125, "response_j": 125, "response_k": 125, "rl": 125, "favorit": [126, 128, 129], "seemlessli": 126, "beyond": [126, 130], "connect": 126, "amount": 126, "natur": 126, "export": 126, "mobil": 126, "phone": 126, "leverag": [126, 128, 130], "plai": 126, "freez": [126, 129], "percentag": 126, "learnabl": 126, "keep": [126, 129], "16gb": [126, 129], "rtx": 126, "3090": 126, "4090": 126, "hour": 126, "7b_qlora_single_devic": [126, 127, 130], "473": 126, "98": [126, 130], "gb": [126, 128, 129, 130], "50": 126, "484": 126, "01": [126, 127], "fact": [126, 128, 129], "third": 126, "realli": 126, "eleuther_ev": [126, 128], "eleuther_evalu": [126, 128], "lm_eval": [126, 128], "plan": 126, "custom_eval_config": [126, 128], "truthfulqa_mc2": [126, 128, 129], "measur": [126, 128], "propens": [126, 128], "shot": [126, 128], "accuraci": [126, 128, 129, 130], "baselin": [126, 129], "324": 126, "loglikelihood": 126, "195": 126, "121": 126, "27": 126, "197": 126, "acc": 126, "388": 126, "38": 126, "shown": 126, "489": 126, "48": [126, 130], "seem": 126, "custom_generation_config": [126, 128], "kick": 126, "300": 126, "interest": 126, "site": 126, "visit": 126, "bai": 126, "area": 126, "92": [126, 128], "exploratorium": 126, "san": 126, "francisco": 126, "magazin": 126, "awesom": 126, "bridg": 126, "pretti": 126, "cool": 126, "96": [126, 130], "61": 126, "sec": [126, 128], "25": 126, "83": 126, "99": [126, 129], "72": 126, "littl": 126, "saw": 126, "took": [126, 128], "torchao": [126, 128, 130], "bit": [126, 128, 129, 130], "custom_quantization_config": [126, 128], "68": 126, "76": 126, "69": 126, "95": [126, 128], "67": 126, "engin": [126, 128], "fullmodeltorchtunecheckpoint": [126, 128], "int4weightonlyquant": [126, 128], "groupsiz": [126, 128], "did": [126, 128, 130], "park": 126, "sit": 126, "hill": 126, "beauti": 126, "62": [126, 128], "85": 126, "sped": 126, "almost": [126, 128, 129], "3x": [126, 128], "benefit": 126, "doesn": 126, "yet": 126, "fast": 126, "clone": [126, 129, 130], "assumpt": 126, "satisfi": 126, "new_dir": 126, "output_dict": 126, "sd_1": 126, "sd_2": 126, "dump": 126, "convert_hf_checkpoint": 126, "checkpoint_path": 126, "justin": 126, "school": 126, "math": 126, "teacher": 126, "ws": 126, "94": [126, 128], "28": 126, "bandwidth": [126, 128], "1391": 126, "84": 126, "thats": 126, "seamlessli": 126, "authent": [126, 127], "hopefulli": 126, "gave": 126, "grant": 127, "minut": 127, "agreement": 127, "altern": 127, "hackabl": 127, "singularli": 127, "technic": 127, "purpos": [127, 128], "depth": 127, "principl": 127, "boilerpl": 127, "substanti": [127, 129], "custom_config": 127, "replic": 127, "lorafinetunerecipesingledevic": 127, "lora_finetune_output": 127, "log_1713194212": 127, "52": 127, "3697006702423096": 127, "25880": [127, 130], "24": [127, 128], "55": 127, "83it": 127, "monitor": 127, "tqdm": 127, "interv": 127, "e2": 127, "focu": 128, "128": [128, 129], "theta": 128, "gain": 128, "illustr": 128, "basic": 128, "observ": 128, "consum": [128, 130], "vram": [128, 129], "overal": 128, "8b_qlora_single_devic": 128, "coupl": [128, 129, 130], "meta_model_0": 128, "122": 128, "sarah": 128, "busi": 128, "mum": 128, "young": 128, "children": 128, "live": 128, "north": 128, "east": 128, "england": 128, "135": 128, "88": 128, "138": 128, "346": 128, "09": 128, "139": 128, "31": 128, "far": 128, "drill": 128, "90": 128, "93": 128, "91": 128, "104": 128, "four": [128, 129], "again": 128, "jake": 128, "disciplin": 128, "passion": 128, "draw": 128, "paint": 128, "57": [128, 129, 130], "broader": 128, "teach": 129, "straight": 129, "jump": 129, "unfamiliar": 129, "oppos": [129, 130], "momentum": 129, "could": 129, "relat": 129, "aghajanyan": 129, "et": 129, "al": 129, "hypothes": 129, "intrins": 129, "often": 129, "eight": 129, "practic": 129, "imag": 129, "left": 129, "blue": 129, "rememb": 129, "approx": 129, "15m": 129, "8192": 129, "65k": 129, "frozen_out": [129, 130], "lora_out": [129, 130], "base_model": 129, "choos": 129, "lora_model": 129, "lora_llama_2_7b": [129, 130], "alon": 129, "in_featur": 129, "out_featur": 129, "inplac": 129, "feel": 129, "free": 129, "whenev": 129, "peft_util": 129, "set_trainable_param": 129, "fetch": 129, "lora_param": 129, "total_param": 129, "trainable_param": 129, "2f": 129, "6742609920": 129, "4194304": 129, "7b_lora": 129, "my_model_checkpoint_path": [129, 130], "tokenizer_checkpoint": [129, 130], "my_tokenizer_checkpoint_path": [129, 130], "factori": 129, "benefici": 129, "impact": 129, "minor": 129, "good": 129, "64": 129, "lora_experiment_1": 129, "smooth": [129, 130], "curv": [129, 130], "500": 129, "ran": 129, "footprint": 129, "commod": 129, "cogniz": 129, "ax": 129, "parallel": 129, "truthfulqa": 129, "previous": 129, "475": 129, "87": 129, "508": 129, "86": 129, "504": 129, "04": 129, "514": 129, "lowest": 129, "absolut": 129, "4gb": 129, "tradeoff": 129, "potenti": 129, "highli": 130, "vanilla": 130, "held": 130, "therefor": 130, "bespok": 130, "normalfloat": 130, "8x": 130, "retain": 130, "vast": 130, "major": 130, "degrad": 130, "normatfloat": 130, "doubl": 130, "themselv": 130, "deepdiv": 130, "idea": 130, "distinct": 130, "de": 130, "incur": 130, "counterpart": 130, "set_default_devic": 130, "qlora_linear": 130, "memory_alloc": 130, "177": 130, "152": 130, "byte": 130, "del": 130, "empty_cach": 130, "lora_linear": 130, "081": 130, "344": 130, "qlora_llama2_7b": 130, "qlora_model": 130, "essenti": 130, "reparametrize_as_dtype_state_dict_post_hook": 130, "35": 130, "40": 130, "29": 130, "slower": 130, "149": 130, "9157477021217346": 130, "02": 130, "08": 130, "15it": 130, "nightli": 130, "200": 130, "hundr": 130, "228": 130, "8158286809921265": 130, "59": 130, "95it": 130, "exercis": 130, "portion": 130, "augment": 130, "linear_nf4": 130, "to_nf4": 130, "linear_weight": 130, "autograd": 130, "regular": 130, "incom": 130}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "SummarizeTemplate"], [23, 0, 1, "", "sharegpt_to_llama2_messages"], [24, 0, 1, "", "truncate"], [25, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[22, 2, 1, "", "format"]], "torchtune.datasets": [[26, 1, 1, "", "ChatDataset"], [27, 1, 1, "", "ConcatDataset"], [28, 1, 1, "", "InstructDataset"], [29, 1, 1, "", "PackedDataset"], [30, 1, 1, "", "PreferenceDataset"], [31, 1, 1, "", "TextCompletionDataset"], [32, 0, 1, "", "alpaca_cleaned_dataset"], [33, 0, 1, "", "alpaca_dataset"], [34, 0, 1, "", "chat_dataset"], [35, 0, 1, "", "cnn_dailymail_articles_dataset"], [36, 0, 1, "", "grammar_dataset"], [37, 0, 1, "", "instruct_dataset"], [38, 0, 1, "", "samsum_dataset"], [39, 0, 1, "", "slimorca_dataset"], [40, 0, 1, "", "stack_exchanged_paired_dataset"], [41, 0, 1, "", "text_completion_dataset"], [42, 0, 1, "", "wikitext_dataset"]], "torchtune.models.gemma": [[43, 0, 1, "", "gemma_2b"], [44, 0, 1, "", "gemma_7b"]], "torchtune.models.llama2": [[45, 0, 1, "", "llama2_13b"], [46, 0, 1, "", "llama2_70b"], [47, 0, 1, "", "llama2_7b"], [48, 0, 1, "", "lora_llama2_13b"], [49, 0, 1, "", "lora_llama2_70b"], [50, 0, 1, "", "lora_llama2_7b"], [51, 0, 1, "", "qlora_llama2_13b"], [52, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[53, 0, 1, "", "llama3_70b"], [54, 0, 1, "", "llama3_8b"], [55, 0, 1, "", "lora_llama3_70b"], [56, 0, 1, "", "lora_llama3_8b"], [57, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[58, 0, 1, "", "lora_mistral_7b"], [59, 0, 1, "", "mistral_7b"], [60, 0, 1, "", "qlora_mistral_7b"]], "torchtune.models.phi3": [[61, 0, 1, "", "lora_phi3_mini"], [62, 0, 1, "", "phi3_mini"], [63, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[64, 1, 1, "", "CausalSelfAttention"], [65, 1, 1, "", "FeedForward"], [66, 1, 1, "", "KVCache"], [67, 1, 1, "", "RMSNorm"], [68, 1, 1, "", "RotaryPositionalEmbeddings"], [69, 1, 1, "", "TransformerDecoder"], [70, 1, 1, "", "TransformerDecoderLayer"], [72, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[64, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[65, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[66, 2, 1, "", "reset"], [66, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[67, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[68, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[69, 2, 1, "", "forward"], [69, 2, 1, "", "reset_caches"], [69, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[70, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[71, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[73, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[73, 2, 1, "", "forward"]], "torchtune.modules.peft": [[74, 1, 1, "", "AdapterModule"], [75, 1, 1, "", "LoRALinear"], [76, 0, 1, "", "disable_adapter"], [77, 0, 1, "", "get_adapter_params"], [78, 0, 1, "", "set_trainable_params"], [79, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [80, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[74, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[75, 2, 1, "", "adapter_params"], [75, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[81, 1, 1, "", "SentencePieceTokenizer"], [82, 1, 1, "", "TikTokenTokenizer"], [83, 1, 1, "", "Tokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[81, 2, 1, "", "decode"], [81, 2, 1, "", "encode"], [81, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[82, 2, 1, "", "decode"], [82, 2, 1, "", "encode"], [82, 2, 1, "", "tokenize_message"], [82, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.Tokenizer": [[83, 2, 1, "", "decode"], [83, 2, 1, "", "encode"], [83, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[84, 4, 1, "", "FSDPPolicyType"], [85, 1, 1, "", "FullModelHFCheckpointer"], [86, 1, 1, "", "FullModelMetaCheckpointer"], [87, 1, 1, "", "FullModelTorchTuneCheckpointer"], [88, 1, 1, "", "ModelType"], [89, 1, 1, "", "OptimizerInBackwardWrapper"], [90, 1, 1, "", "TuneRecipeArgumentParser"], [91, 0, 1, "", "create_optim_in_bwd_wrapper"], [92, 0, 1, "", "generate"], [93, 0, 1, "", "get_device"], [94, 0, 1, "", "get_dtype"], [95, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [96, 0, 1, "", "get_logger"], [97, 0, 1, "", "get_memory_stats"], [98, 0, 1, "", "get_quantizer_mode"], [99, 0, 1, "", "get_world_size_and_rank"], [100, 0, 1, "", "init_distributed"], [101, 0, 1, "", "is_distributed"], [102, 0, 1, "", "log_memory_stats"], [103, 0, 1, "", "lora_fsdp_wrap_policy"], [108, 0, 1, "", "padded_collate"], [109, 0, 1, "", "padded_collate_dpo"], [110, 0, 1, "", "profiler"], [111, 0, 1, "", "register_optim_in_bwd_hooks"], [112, 0, 1, "", "set_activation_checkpointing"], [113, 0, 1, "", "set_default_dtype"], [114, 0, 1, "", "set_seed"], [115, 0, 1, "", "torch_version_ge"], [116, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[85, 2, 1, "", "load_checkpoint"], [85, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[86, 2, 1, "", "load_checkpoint"], [86, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[87, 2, 1, "", "load_checkpoint"], [87, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[88, 3, 1, "", "GEMMA"], [88, 3, 1, "", "LLAMA2"], [88, 3, 1, "", "LLAMA3"], [88, 3, 1, "", "MISTRAL"], [88, 3, 1, "", "MISTRAL_REWARD"], [88, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[89, 2, 1, "", "get_optim_key"], [89, 2, 1, "", "load_state_dict"], [89, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[90, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[104, 1, 1, "", "DiskLogger"], [105, 1, 1, "", "StdoutLogger"], [106, 1, 1, "", "TensorBoardLogger"], [107, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[104, 2, 1, "", "close"], [104, 2, 1, "", "log"], [104, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[105, 2, 1, "", "close"], [105, 2, 1, "", "log"], [105, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[106, 2, 1, "", "close"], [106, 2, 1, "", "log"], [106, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[107, 2, 1, "", "close"], [107, 2, 1, "", "log"], [107, 2, 1, "", "log_config"], [107, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 84, 119, 121, 123, 126, 128, 129, 130], "config": [0, 7, 8, 123, 127], "data": [1, 5, 124], "instruct": [1, 120, 125, 128], "templat": [1, 124, 125], "chat": [1, 124, 125], "format": [1, 6, 125], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 124, 125], "exampl": 2, "gener": [2, 92, 126, 128], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 123, 126, 127, 128, 129], "llama3": [3, 124, 128], "llama2": [3, 124, 126, 129, 130], "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 120, 130], "block": 4, "token": [4, 83, 124], "peft": 4, "util": [4, 5, 84], "loss": 4, "checkpoint": [5, 6, 9, 126], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 125, 129, 130], "manag": 5, "perform": [5, 129], "profil": [5, 110], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 121, 126], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 126, 129, 130], "put": [6, 130], "thi": 6, "all": [6, 7, 130], "togeth": [6, 130], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 125], "us": [7, 8, 124, 126, 130], "instanti": [7, 10], "referenc": 7, "other": [7, 126], "field": 7, "interpol": 7, "valid": [7, 13, 123], "your": [7, 8, 126, 127], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 121, 129, 130], "ar": 8, "recip": [8, 123, 127, 129], "script": 8, "run": [8, 123, 126], "cli": [8, 123], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "summarizetempl": 22, "sharegpt_to_llama2_messag": 23, "truncat": 24, "validate_messag": 25, "chatdataset": 26, "concatdataset": 27, "instructdataset": 28, "packeddataset": 29, "preferencedataset": 30, "textcompletiondataset": 31, "alpaca_cleaned_dataset": 32, "alpaca_dataset": 33, "chat_dataset": 34, "cnn_dailymail_articles_dataset": 35, "grammar_dataset": 36, "instruct_dataset": 37, "samsum_dataset": 38, "slimorca_dataset": 39, "stack_exchanged_paired_dataset": 40, "text_completion_dataset": 41, "wikitext_dataset": 42, "gemma_2b": 43, "gemma_7b": 44, "llama2_13b": 45, "llama2_70b": 46, "llama2_7b": 47, "lora_llama2_13b": 48, "lora_llama2_70b": 49, "lora_llama2_7b": 50, "qlora_llama2_13b": 51, "qlora_llama2_7b": 52, "llama3_70b": 53, "llama3_8b": 54, "lora_llama3_70b": 55, "lora_llama3_8b": 56, "qlora_llama3_8b": 57, "lora_mistral_7b": 58, "mistral_7b": 59, "qlora_mistral_7b": 60, "lora_phi3_mini": 61, "phi3_mini": 62, "qlora_phi3_mini": 63, "causalselfattent": 64, "todo": [64, 70], "feedforward": 65, "kvcach": 66, "rmsnorm": 67, "rotarypositionalembed": 68, "transformerdecod": 69, "transformerdecoderlay": 70, "reparametrize_as_dtype_state_dict_post_hook": 71, "get_cosine_schedule_with_warmup": 72, "dpoloss": 73, "adaptermodul": 74, "loralinear": 75, "disable_adapt": 76, "get_adapter_param": 77, "set_trainable_param": 78, "validate_missing_and_unexpected_for_lora": 79, "validate_state_dict_for_lora": 80, "sentencepiecetoken": 81, "tiktokentoken": 82, "fsdppolicytyp": 84, "fullmodelhfcheckpoint": 85, "fullmodelmetacheckpoint": 86, "fullmodeltorchtunecheckpoint": 87, "modeltyp": 88, "optimizerinbackwardwrapp": 89, "tunerecipeargumentpars": 90, "create_optim_in_bwd_wrapp": 91, "get_devic": 93, "get_dtyp": 94, "get_full_finetune_fsdp_wrap_polici": 95, "get_logg": 96, "get_memory_stat": 97, "get_quantizer_mod": 98, "get_world_size_and_rank": 99, "init_distribut": 100, "is_distribut": 101, "log_memory_stat": 102, "lora_fsdp_wrap_polici": 103, "disklogg": 104, "stdoutlogg": 105, "tensorboardlogg": 106, "wandblogg": 107, "padded_col": 108, "padded_collate_dpo": 109, "register_optim_in_bwd_hook": 111, "set_activation_checkpoint": 112, "set_default_dtyp": 113, "set_se": 114, "torch_version_g": 115, "validate_expected_param_dtyp": 116, "comput": [118, 122], "time": [118, 122], "welcom": 119, "document": 119, "get": [119, 123, 128], "start": [119, 123], "tutori": 119, "instal": 120, "via": [120, 128], "pypi": 120, "git": 120, "clone": 120, "nightli": 120, "kei": 121, "concept": 121, "design": 121, "principl": 121, "download": [123, 126, 127], "list": 123, "built": [123, 125], "copi": 123, "fine": [124, 125, 127, 128], "tune": [124, 125, 127, 128], "chang": 124, "from": [124, 130], "prompt": 124, "special": 124, "when": 124, "should": 124, "i": 124, "custom": [124, 125], "hug": [125, 126], "face": [125, 126], "set": 125, "max": 125, "sequenc": 125, "length": 125, "sampl": 125, "pack": 125, "unstructur": 125, "text": [125, 128], "corpu": 125, "multipl": 125, "local": 125, "remot": 125, "fulli": 125, "end": 126, "workflow": 126, "7b": 126, "finetun": [126, 129, 130], "evalu": [126, 128], "eleutherai": [126, 128], "s": [126, 128], "eval": [126, 128], "har": [126, 128], "speed": 126, "up": 126, "quantiz": [126, 128], "librari": 126, "upload": 126, "hub": 126, "first": 127, "llm": 127, "select": 127, "modifi": 127, "train": 127, "next": 127, "step": 127, "meta": 128, "8b": 128, "access": 128, "our": 128, "faster": 128, "how": 129, "doe": 129, "work": 129, "appli": 129, "trade": 129, "off": 129, "qlora": 130, "save": 130, "deep": 130, "dive": 130}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})