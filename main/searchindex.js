Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_generation", "api_ref_models", "api_ref_modules", "api_ref_rlhf", "api_ref_training", "api_ref_utilities", "basics/chat_datasets", "basics/datasets_overview", "basics/instruct_datasets", "basics/message_transforms", "basics/messages", "basics/model_transforms", "basics/multimodal_datasets", "basics/packing", "basics/preference_datasets", "basics/prompt_templates", "basics/text_completion_datasets", "basics/tokenizers", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaToMessages", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.Message", "generated/torchtune.data.OpenAIToMessages", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.format_content_with_images", "generated/torchtune.data.left_pad_sequence", "generated/torchtune.data.load_image", "generated/torchtune.data.padded_collate", "generated/torchtune.data.padded_collate_dpo", "generated/torchtune.data.padded_collate_sft", "generated/torchtune.data.padded_collate_tiled_images_and_mask", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.hh_rlhf_helpful_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.multimodal.llava_instruct_dataset", "generated/torchtune.datasets.multimodal.the_cauldron_dataset", "generated/torchtune.datasets.preference_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.generation.generate", "generated/torchtune.generation.generate_next_token", "generated/torchtune.generation.get_causal_mask_from_padding_mask", "generated/torchtune.generation.get_position_ids_from_padding_mask", "generated/torchtune.generation.sample", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_405b", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.llama3_2.llama3_2_1b", "generated/torchtune.models.llama3_2.llama3_2_3b", "generated/torchtune.models.llama3_2.lora_llama3_2_1b", "generated/torchtune.models.llama3_2.lora_llama3_2_3b", "generated/torchtune.models.llama3_2.qlora_llama3_2_1b", "generated/torchtune.models.llama3_2.qlora_llama3_2_3b", "generated/torchtune.models.llama3_2_vision.Llama3VisionEncoder", "generated/torchtune.models.llama3_2_vision.Llama3VisionProjectionHead", "generated/torchtune.models.llama3_2_vision.Llama3VisionTransform", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_11b", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_decoder", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_encoder", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_transform", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_decoder", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_encoder", "generated/torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.MultiHeadAttention", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TanhGate", "generated/torchtune.modules.TiedLinear", "generated/torchtune.modules.TransformerCrossAttentionLayer", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerSelfAttentionLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.delete_kv_caches", "generated/torchtune.modules.common_utils.disable_kv_cache", "generated/torchtune.modules.common_utils.local_kv_cache", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss", "generated/torchtune.modules.loss.ForwardKLLoss", "generated/torchtune.modules.loss.ForwardKLWithChunkedOutputLoss", "generated/torchtune.modules.model_fusion.DeepFusionModel", "generated/torchtune.modules.model_fusion.FusionEmbedding", "generated/torchtune.modules.model_fusion.FusionLayer", "generated/torchtune.modules.model_fusion.get_fusion_params", "generated/torchtune.modules.model_fusion.register_fusion_module", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.rlhf.estimate_advantages", "generated/torchtune.rlhf.get_rewards_ppo", "generated/torchtune.rlhf.loss.DPOLoss", "generated/torchtune.rlhf.loss.PPOLoss", "generated/torchtune.rlhf.loss.RSOLoss", "generated/torchtune.rlhf.loss.SimPOLoss", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.training.FSDPPolicyType", "generated/torchtune.training.FormattedCheckpointFiles", "generated/torchtune.training.FullModelHFCheckpointer", "generated/torchtune.training.FullModelMetaCheckpointer", "generated/torchtune.training.FullModelTorchTuneCheckpointer", "generated/torchtune.training.ModelType", "generated/torchtune.training.OptimizerInBackwardWrapper", "generated/torchtune.training.apply_selective_activation_checkpointing", "generated/torchtune.training.create_optim_in_bwd_wrapper", "generated/torchtune.training.get_cosine_schedule_with_warmup", "generated/torchtune.training.get_dtype", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.training.get_lr", "generated/torchtune.training.get_memory_stats", "generated/torchtune.training.get_quantizer_mode", "generated/torchtune.training.get_unmasked_sequence_lengths", "generated/torchtune.training.get_world_size_and_rank", "generated/torchtune.training.init_distributed", "generated/torchtune.training.is_distributed", "generated/torchtune.training.log_memory_stats", "generated/torchtune.training.lora_fsdp_wrap_policy", "generated/torchtune.training.metric_logging.CometLogger", "generated/torchtune.training.metric_logging.DiskLogger", "generated/torchtune.training.metric_logging.StdoutLogger", "generated/torchtune.training.metric_logging.TensorBoardLogger", "generated/torchtune.training.metric_logging.WandBLogger", "generated/torchtune.training.register_optim_in_bwd_hooks", "generated/torchtune.training.set_activation_checkpointing", "generated/torchtune.training.set_default_dtype", "generated/torchtune.training.set_seed", "generated/torchtune.training.setup_torch_profiler", "generated/torchtune.training.update_state_dict_for_classifier", "generated/torchtune.training.validate_expected_param_dtype", "generated/torchtune.utils.batch_to_device", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.torch_version_ge", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "recipes/lora_finetune_single_device", "recipes/qat_distributed", "recipes/recipes_overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/llama_kd_tutorial", "tutorials/lora_finetune", "tutorials/memory_optimizations", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_generation.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_rlhf.rst", "api_ref_training.rst", "api_ref_utilities.rst", "basics/chat_datasets.rst", "basics/datasets_overview.rst", "basics/instruct_datasets.rst", "basics/message_transforms.rst", "basics/messages.rst", "basics/model_transforms.rst", "basics/multimodal_datasets.rst", "basics/packing.rst", "basics/preference_datasets.rst", "basics/prompt_templates.rst", "basics/text_completion_datasets.rst", "basics/tokenizers.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaToMessages.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.OpenAIToMessages.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.format_content_with_images.rst", "generated/torchtune.data.left_pad_sequence.rst", "generated/torchtune.data.load_image.rst", "generated/torchtune.data.padded_collate.rst", "generated/torchtune.data.padded_collate_dpo.rst", "generated/torchtune.data.padded_collate_sft.rst", "generated/torchtune.data.padded_collate_tiled_images_and_mask.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.hh_rlhf_helpful_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.multimodal.llava_instruct_dataset.rst", "generated/torchtune.datasets.multimodal.the_cauldron_dataset.rst", "generated/torchtune.datasets.preference_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.generation.generate.rst", "generated/torchtune.generation.generate_next_token.rst", "generated/torchtune.generation.get_causal_mask_from_padding_mask.rst", "generated/torchtune.generation.get_position_ids_from_padding_mask.rst", "generated/torchtune.generation.sample.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.llama3_2.llama3_2_1b.rst", "generated/torchtune.models.llama3_2.llama3_2_3b.rst", "generated/torchtune.models.llama3_2.lora_llama3_2_1b.rst", "generated/torchtune.models.llama3_2.lora_llama3_2_3b.rst", "generated/torchtune.models.llama3_2.qlora_llama3_2_1b.rst", "generated/torchtune.models.llama3_2.qlora_llama3_2_3b.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionEncoder.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionProjectionHead.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionTransform.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_11b.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_decoder.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_encoder.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_transform.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_decoder.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_encoder.rst", "generated/torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.MultiHeadAttention.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TanhGate.rst", "generated/torchtune.modules.TiedLinear.rst", "generated/torchtune.modules.TransformerCrossAttentionLayer.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerSelfAttentionLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.delete_kv_caches.rst", "generated/torchtune.modules.common_utils.disable_kv_cache.rst", "generated/torchtune.modules.common_utils.local_kv_cache.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss.rst", "generated/torchtune.modules.loss.ForwardKLLoss.rst", "generated/torchtune.modules.loss.ForwardKLWithChunkedOutputLoss.rst", "generated/torchtune.modules.model_fusion.DeepFusionModel.rst", "generated/torchtune.modules.model_fusion.FusionEmbedding.rst", "generated/torchtune.modules.model_fusion.FusionLayer.rst", "generated/torchtune.modules.model_fusion.get_fusion_params.rst", "generated/torchtune.modules.model_fusion.register_fusion_module.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.rlhf.estimate_advantages.rst", "generated/torchtune.rlhf.get_rewards_ppo.rst", "generated/torchtune.rlhf.loss.DPOLoss.rst", "generated/torchtune.rlhf.loss.PPOLoss.rst", "generated/torchtune.rlhf.loss.RSOLoss.rst", "generated/torchtune.rlhf.loss.SimPOLoss.rst", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.training.FSDPPolicyType.rst", "generated/torchtune.training.FormattedCheckpointFiles.rst", "generated/torchtune.training.FullModelHFCheckpointer.rst", "generated/torchtune.training.FullModelMetaCheckpointer.rst", "generated/torchtune.training.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.training.ModelType.rst", "generated/torchtune.training.OptimizerInBackwardWrapper.rst", "generated/torchtune.training.apply_selective_activation_checkpointing.rst", "generated/torchtune.training.create_optim_in_bwd_wrapper.rst", "generated/torchtune.training.get_cosine_schedule_with_warmup.rst", "generated/torchtune.training.get_dtype.rst", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.training.get_lr.rst", "generated/torchtune.training.get_memory_stats.rst", "generated/torchtune.training.get_quantizer_mode.rst", "generated/torchtune.training.get_unmasked_sequence_lengths.rst", "generated/torchtune.training.get_world_size_and_rank.rst", "generated/torchtune.training.init_distributed.rst", "generated/torchtune.training.is_distributed.rst", "generated/torchtune.training.log_memory_stats.rst", "generated/torchtune.training.lora_fsdp_wrap_policy.rst", "generated/torchtune.training.metric_logging.CometLogger.rst", "generated/torchtune.training.metric_logging.DiskLogger.rst", "generated/torchtune.training.metric_logging.StdoutLogger.rst", "generated/torchtune.training.metric_logging.TensorBoardLogger.rst", "generated/torchtune.training.metric_logging.WandBLogger.rst", "generated/torchtune.training.register_optim_in_bwd_hooks.rst", "generated/torchtune.training.set_activation_checkpointing.rst", "generated/torchtune.training.set_default_dtype.rst", "generated/torchtune.training.set_seed.rst", "generated/torchtune.training.setup_torch_profiler.rst", "generated/torchtune.training.update_state_dict_for_classifier.rst", "generated/torchtune.training.validate_expected_param_dtype.rst", "generated/torchtune.utils.batch_to_device.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "recipes/lora_finetune_single_device.rst", "recipes/qat_distributed.rst", "recipes/recipes_overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/llama_kd_tutorial.rst", "tutorials/lora_finetune.rst", "tutorials/memory_optimizations.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.generation", "torchtune.models", "torchtune.modules", "torchtune.rlhf", "torchtune.training", "torchtune.utils", "Chat Datasets", "Datasets Overview", "Instruct Datasets", "Message Transforms", "Messages", "Multimodal Transforms", "Multimodal Datasets", "Sample packing", "Preference Datasets", "Prompt Templates", "Text-completion Datasets", "Tokenizers", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaToMessages", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "Message", "OpenAIToMessages", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "format_content_with_images", "left_pad_sequence", "load_image", "padded_collate", "padded_collate_dpo", "padded_collate_sft", "padded_collate_tiled_images_and_mask", "truncate", "validate_messages", "ConcatDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "hh_rlhf_helpful_dataset", "instruct_dataset", "llava_instruct_dataset", "the_cauldron_dataset", "preference_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "wikitext_dataset", "generate", "generate_next_token", "get_causal_mask_from_padding_mask", "get_position_ids_from_padding_mask", "sample", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_405b", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_405b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "llama3_2_1b", "llama3_2_3b", "lora_llama3_2_1b", "lora_llama3_2_3b", "qlora_llama3_2_1b", "qlora_llama3_2_3b", "Llama3VisionEncoder", "Llama3VisionProjectionHead", "Llama3VisionTransform", "llama3_2_vision_11b", "llama3_2_vision_decoder", "llama3_2_vision_encoder", "llama3_2_vision_transform", "lora_llama3_2_vision_11b", "lora_llama3_2_vision_decoder", "lora_llama3_2_vision_encoder", "qlora_llama3_2_vision_11b", "MistralChatTemplate", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "FeedForward", "Fp32LayerNorm", "KVCache", "MultiHeadAttention", "RMSNorm", "RotaryPositionalEmbeddings", "TanhGate", "TiedLinear", "TransformerCrossAttentionLayer", "TransformerDecoder", "TransformerSelfAttentionLayer", "VisionTransformer", "delete_kv_caches", "disable_kv_cache", "local_kv_cache", "reparametrize_as_dtype_state_dict_post_hook", "CEWithChunkedOutputLoss", "ForwardKLLoss", "ForwardKLWithChunkedOutputLoss", "DeepFusionModel", "FusionEmbedding", "FusionLayer", "get_fusion_params", "register_fusion_module", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "estimate_advantages", "get_rewards_ppo", "DPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "truncate_sequence_at_first_stop_token", "torchtune.training.FSDPPolicyType", "FormattedCheckpointFiles", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "apply_selective_activation_checkpointing", "create_optim_in_bwd_wrapper", "get_cosine_schedule_with_warmup", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_lr", "get_memory_stats", "get_quantizer_mode", "get_unmasked_sequence_lengths", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "update_state_dict_for_classifier", "validate_expected_param_dtype", "batch_to_device", "get_device", "get_logger", "torch_version_ge", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "LoRA Single Device Finetuning", "Distributed Quantization-Aware Training (QAT)", "Recipes Overview", "Computation times", "torchtune CLI", "Fine-Tuning Llama3 with Chat Data", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Distilling Llama3.1 8B into Llama3.2 1B using Knowledge Distillation", "Fine-Tuning Llama2 with LoRA", "Memory Optimization Overview", "Fine-Tuning Llama3 with QAT", "Fine-Tuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 4, 9, 10, 12, 14, 15, 16, 17, 18, 20, 30, 31, 32, 34, 36, 41, 53, 55, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 144, 147, 148, 152, 160, 166, 167, 168, 175, 176, 177, 264, 267, 268, 271, 272, 274, 276, 277, 279, 280], "prompt": [1, 9, 10, 11, 12, 17, 30, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 54, 55, 58, 59, 61, 62, 63, 66, 67, 68, 69, 72, 73, 93, 99, 105, 118, 143, 147, 152, 161, 168, 178, 188, 198, 215, 273, 275], "chat": [1, 2, 10, 12, 15, 17, 31, 36, 41, 55, 59, 99, 168, 267], "includ": [1, 9, 10, 11, 15, 17, 18, 20, 21, 23, 24, 37, 38, 55, 76, 80, 90, 100, 115, 124, 145, 146, 147, 149, 150, 157, 168, 174, 188, 204, 210, 227, 228, 266, 269, 271, 272, 273, 274, 275, 276, 277, 280], "some": [1, 16, 17, 19, 20, 21, 23, 31, 155, 199, 201, 206, 207, 264, 266, 267, 268, 271, 272, 273, 274, 276, 277, 278, 279, 280], "specif": [1, 5, 10, 11, 14, 18, 20, 23, 24, 26, 54, 55, 64, 65, 143, 211, 236, 268, 272, 273, 278, 279, 280], "format": [1, 2, 7, 10, 18, 20, 35, 44, 45, 54, 55, 58, 59, 62, 63, 66, 99, 143, 152, 211, 226, 227, 228, 229, 230, 271, 272, 273, 274, 275, 277, 278], "differ": [1, 9, 16, 17, 18, 20, 23, 25, 47, 52, 59, 63, 77, 78, 79, 143, 190, 212, 220, 230, 257, 266, 267, 268, 271, 272, 273, 275, 276, 277, 278, 279, 280], "dataset": [1, 12, 13, 14, 16, 18, 23, 30, 32, 34, 35, 36, 41, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 220, 266, 274, 275, 276, 279], "model": [1, 2, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26, 30, 31, 32, 34, 35, 36, 41, 52, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 214, 215, 216, 218, 219, 220, 221, 222, 223, 227, 228, 229, 230, 232, 233, 236, 238, 245, 246, 251, 252, 256, 264, 266, 267, 268, 272, 280], "convert": [1, 9, 10, 13, 20, 21, 32, 34, 36, 41, 48, 54, 55, 59, 64, 65, 66, 74, 141, 227, 273, 279, 280], "from": [1, 2, 4, 10, 12, 13, 14, 16, 18, 21, 22, 23, 24, 25, 26, 30, 32, 35, 36, 41, 44, 45, 46, 49, 52, 53, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 81, 82, 83, 91, 92, 99, 101, 102, 103, 104, 118, 142, 143, 147, 158, 160, 168, 175, 176, 177, 178, 179, 182, 187, 188, 189, 190, 191, 192, 193, 195, 196, 197, 200, 201, 202, 203, 206, 209, 212, 214, 217, 220, 222, 223, 226, 227, 228, 229, 231, 233, 234, 246, 249, 250, 251, 256, 263, 265, 268, 270, 271, 273, 274, 275, 276, 277, 278, 279], "common": [1, 2, 5, 9, 13, 14, 23, 215, 271, 272, 275, 277, 278, 279], "schema": [1, 9, 10, 11, 15], "convers": [1, 12, 15, 17, 18, 20, 21, 32, 41, 51, 54, 55, 59, 64, 66, 68, 227, 229, 230, 266, 272, 273, 277, 278, 280], "json": [1, 9, 11, 12, 15, 17, 20, 21, 36, 41, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 147, 168, 178, 214, 227, 271, 272, 273, 279], "list": [1, 9, 10, 13, 14, 17, 18, 20, 21, 23, 32, 35, 37, 43, 44, 46, 47, 48, 49, 50, 51, 52, 54, 55, 59, 60, 64, 65, 66, 71, 72, 80, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 142, 143, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 188, 190, 195, 197, 198, 199, 200, 203, 204, 208, 209, 210, 211, 212, 213, 215, 217, 226, 227, 228, 229, 246, 260, 269, 272, 273, 274, 275, 278, 279], "us": [1, 2, 4, 5, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 25, 26, 28, 31, 34, 35, 37, 43, 46, 49, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 78, 79, 80, 99, 100, 106, 115, 118, 119, 124, 128, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 164, 168, 170, 174, 178, 179, 181, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 202, 205, 208, 212, 213, 217, 218, 219, 220, 221, 223, 225, 227, 228, 230, 231, 235, 236, 238, 245, 246, 247, 248, 249, 250, 254, 256, 258, 259, 264, 265, 266, 267, 268, 269, 271, 274, 275, 277, 278, 279], "collect": [1, 23, 274], "sampl": [1, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 22, 25, 32, 34, 35, 36, 41, 43, 49, 53, 54, 55, 56, 61, 62, 64, 65, 66, 67, 68, 70, 72, 73, 182, 184, 188, 189, 190, 198, 216, 217, 222, 272, 273, 278], "batch": [1, 10, 16, 24, 46, 47, 48, 49, 53, 58, 61, 64, 65, 67, 78, 141, 142, 181, 182, 184, 187, 188, 189, 190, 193, 198, 200, 218, 219, 220, 222, 223, 240, 255, 258, 266, 274, 275, 277, 278], "handl": [1, 12, 15, 16, 23, 28, 30, 52, 55, 143, 212, 213, 272, 273, 277, 278, 280], "ani": [1, 5, 10, 12, 13, 14, 15, 16, 20, 21, 23, 24, 26, 28, 29, 32, 35, 36, 37, 41, 43, 46, 49, 50, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 79, 180, 188, 194, 198, 200, 201, 206, 207, 208, 209, 210, 211, 212, 215, 227, 228, 229, 231, 242, 245, 246, 254, 257, 271, 272, 274, 277, 278, 279], "pad": [1, 44, 46, 47, 48, 49, 53, 72, 74, 75, 188, 190, 217, 219, 221, 224, 240], "miscellan": 1, "modifi": [1, 20, 23, 24, 25, 192, 194, 231, 266, 273, 275, 276, 277, 278, 279, 280], "For": [2, 7, 9, 11, 13, 15, 17, 18, 19, 20, 21, 23, 24, 32, 34, 35, 36, 37, 41, 49, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 90, 94, 100, 106, 115, 119, 124, 128, 142, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 182, 188, 190, 195, 198, 199, 202, 205, 216, 227, 233, 239, 246, 250, 252, 254, 265, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "detail": [2, 9, 11, 12, 15, 20, 21, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 159, 181, 190, 195, 221, 225, 236, 245, 254, 267, 268, 271, 273, 274, 275, 276, 277, 278, 279, 280], "usag": [2, 20, 194, 195, 197, 226, 230, 231, 255, 265, 271, 273, 274, 275, 278, 279, 280], "guid": [2, 22, 23, 25, 32, 34, 36, 41, 59, 61, 62, 63, 64, 65, 66, 67, 68, 223, 246, 266, 272, 274, 276, 277], "pleas": [2, 7, 33, 39, 42, 77, 78, 79, 80, 87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 190, 195, 225, 236, 245, 252, 265, 268, 269, 273, 275, 280], "see": [2, 7, 9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 25, 33, 39, 42, 46, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 87, 88, 89, 97, 98, 99, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 152, 159, 162, 163, 169, 181, 187, 189, 190, 200, 203, 210, 211, 216, 225, 230, 236, 245, 246, 250, 252, 254, 260, 265, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "overview": [2, 7, 23, 25, 198, 264, 267, 268, 274, 276, 277, 280], "support": [2, 4, 10, 14, 15, 16, 17, 20, 21, 22, 24, 25, 26, 35, 36, 53, 54, 55, 58, 59, 60, 61, 64, 65, 66, 67, 68, 71, 76, 94, 106, 119, 128, 141, 148, 149, 150, 152, 153, 155, 164, 167, 168, 170, 180, 182, 190, 199, 200, 204, 222, 228, 229, 231, 235, 238, 239, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "sever": [2, 278], "wide": [2, 9, 182, 276], "onli": [2, 4, 15, 17, 21, 22, 25, 34, 35, 41, 53, 54, 55, 60, 66, 72, 76, 80, 94, 106, 119, 128, 143, 148, 149, 150, 152, 153, 155, 164, 170, 182, 186, 188, 190, 195, 197, 201, 204, 206, 208, 212, 227, 228, 229, 231, 235, 236, 238, 239, 245, 271, 273, 274, 276, 277, 278, 279, 280], "help": [2, 10, 17, 18, 21, 62, 99, 188, 190, 198, 227, 246, 264, 265, 266, 271, 272, 273, 274, 276, 278, 279, 280], "quickli": [2, 10, 23, 37, 56, 267, 272, 278], "bootstrap": [2, 10], "your": [2, 7, 9, 10, 11, 12, 13, 15, 16, 17, 20, 22, 25, 26, 37, 56, 59, 63, 66, 78, 79, 80, 146, 150, 190, 199, 246, 249, 250, 256, 264, 265, 266, 267, 268, 271, 272, 275, 276, 277, 278, 279, 280], "fine": [2, 9, 10, 11, 15, 17, 18, 19, 21, 22, 24, 25, 35, 53, 54, 55, 70, 256, 264, 266, 267, 268, 269, 273], "tune": [2, 4, 9, 10, 11, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 35, 53, 54, 55, 70, 256, 264, 265, 266, 267, 268, 269, 271, 273], "also": [2, 9, 11, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 52, 59, 63, 66, 70, 72, 73, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 168, 170, 174, 182, 188, 191, 204, 223, 236, 238, 245, 246, 250, 256, 259, 265, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "like": [2, 6, 11, 21, 22, 23, 24, 25, 168, 190, 195, 197, 199, 229, 265, 271, 272, 273, 274, 276, 277, 278, 279], "These": [2, 5, 12, 14, 17, 18, 20, 21, 23, 24, 26, 53, 54, 66, 190, 217, 267, 269, 272, 273, 274, 275, 277, 278, 279, 280], "ar": [2, 5, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 25, 26, 30, 34, 37, 38, 41, 44, 46, 47, 51, 53, 54, 55, 58, 59, 63, 64, 65, 66, 72, 74, 75, 78, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 143, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 181, 187, 188, 189, 190, 192, 198, 199, 200, 204, 205, 208, 209, 217, 219, 225, 227, 228, 230, 231, 233, 235, 237, 238, 243, 245, 255, 256, 265, 266, 267, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "especi": [2, 266, 271, 273, 278], "specifi": [2, 11, 15, 17, 19, 21, 23, 24, 26, 30, 32, 34, 36, 41, 43, 59, 61, 62, 63, 64, 65, 66, 67, 68, 72, 74, 76, 93, 100, 105, 106, 115, 118, 119, 124, 128, 145, 147, 149, 161, 168, 170, 174, 178, 182, 188, 189, 196, 197, 198, 225, 236, 239, 245, 250, 252, 255, 268, 269, 271, 272, 273, 274, 275, 278, 279, 280], "yaml": [2, 16, 17, 19, 23, 24, 26, 27, 28, 52, 59, 63, 66, 70, 250, 266, 269, 271, 272, 273, 274, 275, 277, 279, 280], "config": [2, 9, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28, 29, 52, 59, 63, 66, 70, 182, 208, 227, 231, 246, 250, 255, 266, 267, 268, 269, 272, 273, 275, 276, 277, 278, 279, 280], "represent": [2, 226, 276, 277, 279, 280], "abov": [2, 4, 9, 15, 16, 17, 19, 21, 54, 194, 243, 265, 268, 273, 275, 277, 278, 279, 280], "text": [4, 5, 9, 10, 11, 14, 17, 18, 20, 34, 35, 36, 37, 38, 41, 43, 49, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 143, 199, 200, 210, 212, 213, 215, 217, 272, 273, 279], "version": [4, 57, 72, 94, 106, 119, 128, 148, 153, 155, 164, 170, 182, 261, 265, 275, 278, 279, 280], "famili": [4, 21, 24, 58, 60, 64, 65, 68, 69, 71, 230, 266, 271, 275, 276], "import": [4, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 59, 63, 64, 65, 66, 70, 76, 190, 191, 192, 193, 220, 246, 249, 250, 272, 273, 274, 275, 276, 277, 279, 280], "you": [4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 35, 37, 54, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 99, 181, 188, 190, 193, 195, 197, 200, 202, 230, 246, 249, 250, 256, 264, 265, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "need": [4, 9, 11, 13, 15, 17, 18, 19, 21, 22, 23, 24, 25, 37, 53, 55, 182, 188, 190, 198, 199, 223, 245, 246, 249, 250, 251, 265, 267, 268, 269, 271, 272, 273, 274, 275, 277, 278, 280], "request": [4, 235, 273], "access": [4, 21, 23, 24, 52, 227, 233, 267, 268, 271, 273, 274], "hug": [4, 10, 21, 31, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 147, 168, 178, 214, 234, 266, 271, 274, 275], "face": [4, 10, 21, 31, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 147, 168, 178, 214, 234, 266, 271, 274, 275], "befor": [4, 18, 21, 37, 51, 53, 64, 77, 78, 80, 90, 94, 146, 150, 182, 187, 188, 189, 190, 195, 197, 198, 200, 204, 213, 227, 246, 268, 271, 273, 278, 279], "download": [4, 10, 15, 21, 64, 262, 265, 267, 268, 272, 275, 276, 277, 279, 280], "To": [4, 9, 11, 12, 13, 15, 16, 17, 18, 19, 21, 23, 24, 25, 53, 64, 188, 190, 200, 227, 256, 265, 266, 268, 269, 271, 273, 274, 275, 276, 277, 278, 279, 280], "1b": [4, 16, 135, 137, 139, 264], "meta": [4, 14, 15, 19, 20, 21, 99, 184, 227, 228, 267, 268, 271, 272, 273, 274, 276], "output": [4, 11, 12, 13, 19, 20, 21, 30, 34, 44, 52, 54, 55, 58, 61, 63, 67, 68, 72, 80, 84, 85, 86, 90, 94, 100, 104, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 141, 142, 145, 146, 148, 149, 150, 153, 154, 155, 156, 157, 160, 164, 165, 170, 173, 174, 179, 180, 182, 184, 185, 187, 188, 189, 190, 195, 197, 198, 199, 200, 204, 207, 208, 209, 217, 229, 236, 248, 255, 256, 265, 267, 268, 271, 273, 274, 275, 276, 277, 278, 280], "dir": [4, 20, 21, 250, 265, 267, 268, 271, 273, 274, 275, 276, 279], "tmp": [4, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 23, 231, 267, 268, 272, 274, 276], "ignor": [4, 9, 11, 21, 41, 70, 186, 187, 189, 196, 197, 232, 256, 267, 268, 271, 276], "pattern": [4, 18, 213, 267, 268, 271, 276], "origin": [4, 14, 15, 16, 19, 20, 21, 57, 58, 62, 194, 199, 200, 204, 267, 268, 272, 273, 275, 276, 277, 278, 279, 280], "consolid": [4, 21, 267, 268, 276], "00": [4, 15, 21, 59, 63, 263, 267, 268, 270, 274, 276], "pth": [4, 21, 226, 267, 268, 273, 276], "hf": [4, 9, 17, 19, 20, 21, 220, 222, 227, 271, 272, 273, 274, 275], "token": [4, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 23, 24, 35, 41, 46, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 90, 93, 94, 100, 105, 106, 115, 118, 119, 124, 128, 143, 145, 146, 147, 149, 150, 153, 155, 157, 159, 161, 164, 166, 168, 170, 174, 178, 182, 184, 187, 188, 189, 190, 195, 197, 198, 199, 200, 210, 211, 212, 213, 214, 215, 217, 219, 221, 224, 236, 240, 267, 271, 273, 274, 275, 276, 277, 278, 279, 280], "hf_token": [4, 20, 268, 276], "3b": [4, 136, 138, 140], "The": [4, 9, 10, 11, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 31, 35, 45, 46, 51, 52, 53, 54, 55, 59, 62, 63, 64, 65, 66, 69, 77, 78, 79, 80, 84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 141, 143, 146, 148, 149, 150, 153, 155, 164, 165, 170, 171, 172, 173, 180, 183, 184, 185, 186, 190, 194, 195, 196, 197, 198, 199, 200, 205, 210, 211, 212, 213, 214, 215, 217, 218, 220, 221, 222, 223, 225, 227, 229, 231, 234, 235, 237, 239, 246, 250, 253, 255, 259, 260, 261, 265, 266, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "reus": [4, 266], "llama3_token": [4, 14, 16, 19, 20, 64, 65, 72, 272, 275], "class": [4, 12, 13, 14, 20, 23, 25, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 52, 53, 54, 55, 56, 64, 65, 77, 78, 79, 80, 93, 99, 104, 105, 118, 141, 142, 143, 147, 152, 155, 159, 160, 161, 168, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 195, 196, 197, 198, 199, 200, 201, 203, 204, 206, 207, 210, 211, 212, 213, 216, 217, 220, 221, 222, 223, 226, 227, 228, 229, 230, 231, 246, 247, 248, 249, 250, 269, 272, 274, 276, 277, 278, 280], "languag": [4, 15, 31, 72, 149, 199, 200, 204, 220, 256, 277, 278], "11b": [4, 144, 151], "8b": [4, 14, 15, 19, 20, 117, 121, 123, 127, 129, 131, 134, 165, 264, 267, 268, 271, 272, 279], "70b": [4, 82, 85, 88, 102, 108, 112, 116, 120, 122, 126, 130, 133, 275], "405b": [4, 125, 129, 132], "weight": [4, 20, 21, 24, 49, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 144, 147, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 186, 194, 203, 204, 208, 212, 220, 227, 228, 229, 230, 239, 250, 256, 264, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "can": [4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 29, 32, 34, 35, 36, 37, 38, 41, 49, 52, 54, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 80, 143, 146, 150, 183, 184, 186, 187, 188, 190, 195, 197, 198, 200, 202, 205, 212, 213, 225, 227, 230, 232, 236, 245, 246, 249, 250, 252, 255, 264, 265, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "instead": [4, 9, 11, 13, 15, 21, 24, 30, 44, 53, 54, 70, 80, 128, 149, 150, 181, 186, 190, 204, 223, 271, 275, 277, 278, 279], "builder": [4, 9, 10, 11, 12, 14, 15, 16, 21, 57, 59, 60, 63, 66, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96, 97, 98, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 151, 154, 156, 158, 160, 162, 163, 165, 167, 169, 171, 172, 173, 175, 176, 177, 272, 278, 280], "all": [4, 5, 10, 13, 14, 18, 20, 24, 29, 34, 35, 37, 41, 44, 46, 49, 52, 53, 54, 55, 80, 118, 141, 147, 168, 178, 182, 186, 188, 190, 191, 192, 193, 194, 198, 199, 200, 202, 205, 216, 227, 231, 233, 237, 243, 251, 257, 258, 262, 264, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279], "7b": [4, 9, 11, 13, 17, 18, 19, 20, 21, 60, 71, 83, 86, 89, 92, 96, 103, 104, 109, 110, 113, 114, 154, 156, 158, 160, 163, 173, 177, 227, 228, 272, 274, 275, 277, 280], "13b": [4, 21, 81, 84, 87, 101, 107, 111], "codellama": 4, "size": [4, 13, 14, 15, 21, 24, 26, 44, 49, 58, 61, 64, 65, 67, 78, 79, 80, 141, 142, 143, 144, 146, 147, 148, 150, 181, 182, 183, 184, 187, 188, 189, 190, 193, 195, 197, 198, 199, 200, 217, 218, 219, 240, 241, 243, 266, 268, 271, 273, 274, 275, 277, 278, 279], "0": [4, 9, 11, 13, 14, 15, 17, 19, 21, 24, 44, 46, 47, 48, 49, 53, 59, 63, 66, 72, 73, 75, 76, 80, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 100, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 142, 143, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 175, 176, 181, 182, 188, 190, 191, 192, 193, 199, 204, 215, 220, 221, 222, 223, 224, 234, 239, 240, 246, 249, 250, 254, 259, 261, 263, 268, 270, 272, 273, 274, 275, 277, 278, 279, 280], "5b": [4, 171, 172, 175, 176, 278], "qwen2": [4, 170, 171, 172, 173, 175, 176, 177, 178, 230, 278], "exampl": [4, 18, 20, 21, 22, 23, 24, 25, 26, 28, 32, 34, 36, 37, 41, 43, 44, 45, 46, 47, 48, 49, 52, 53, 55, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 74, 75, 76, 80, 142, 143, 146, 150, 181, 182, 190, 191, 192, 193, 195, 197, 198, 199, 200, 202, 203, 205, 210, 211, 212, 213, 215, 216, 220, 222, 223, 224, 225, 226, 227, 228, 230, 231, 239, 240, 246, 249, 250, 253, 256, 259, 260, 261, 262, 263, 265, 267, 268, 270, 271, 272, 273, 275, 276, 277, 278, 279, 280], "none": [4, 9, 15, 24, 25, 27, 29, 30, 32, 34, 36, 41, 49, 50, 51, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 80, 93, 100, 105, 106, 115, 118, 119, 124, 128, 141, 142, 143, 145, 147, 149, 161, 168, 178, 179, 181, 182, 184, 187, 188, 189, 190, 191, 192, 193, 198, 200, 205, 207, 208, 209, 212, 215, 217, 218, 219, 221, 227, 228, 229, 230, 231, 232, 235, 239, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 271, 273, 279], "mini": [4, 20, 165, 166, 167, 168, 169], "4k": [4, 20, 166, 167, 168], "microsoft": [4, 167, 168], "ai": [4, 11, 13, 18, 54, 55, 158, 250, 272, 275], "thi": [4, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 30, 32, 33, 34, 35, 36, 41, 42, 43, 44, 46, 47, 49, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 78, 79, 80, 90, 94, 100, 106, 115, 119, 124, 128, 141, 143, 145, 146, 149, 150, 152, 153, 155, 157, 159, 164, 166, 167, 168, 170, 174, 179, 181, 182, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 198, 199, 200, 202, 203, 205, 208, 209, 210, 211, 212, 213, 215, 216, 217, 219, 220, 221, 223, 225, 226, 227, 228, 229, 231, 234, 235, 238, 240, 243, 245, 246, 247, 249, 250, 251, 252, 254, 256, 258, 259, 264, 265, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "v0": [4, 9, 13, 17, 18, 20, 152], "mistralai": [4, 20, 271], "2b": [4, 91, 95], "gemma2": 4, "googl": [4, 91, 92], "gguf": 4, "compon": [4, 6, 13, 20, 21, 24, 29, 47, 54, 55, 64, 65, 266, 269, 274, 276, 277, 280], "multimod": [4, 10, 13, 35, 41, 55, 64, 65, 198, 265], "encod": [4, 5, 14, 20, 49, 55, 72, 73, 80, 141, 142, 144, 145, 146, 148, 149, 150, 182, 187, 188, 189, 193, 198, 199, 200, 202, 210, 212, 213, 215, 217, 220, 223, 272], "perform": [5, 11, 12, 16, 18, 19, 20, 21, 53, 72, 190, 195, 205, 216, 223, 266, 267, 268, 272, 273, 275, 276, 279, 280], "direct": [5, 17, 24, 47, 84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165, 220, 265, 269], "id": [5, 13, 16, 20, 21, 46, 47, 48, 49, 53, 60, 64, 65, 71, 72, 73, 75, 76, 143, 182, 184, 188, 189, 198, 210, 211, 212, 213, 214, 215, 217, 227, 229, 246, 272, 273], "decod": [5, 9, 11, 13, 14, 15, 17, 19, 20, 59, 63, 66, 72, 90, 94, 100, 106, 115, 119, 124, 128, 142, 143, 144, 145, 146, 148, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 182, 187, 188, 189, 193, 198, 200, 202, 210, 212, 213, 272], "typic": [5, 9, 11, 19, 23, 32, 36, 41, 49, 53, 54, 55, 56, 70, 168, 202, 220, 223, 278, 279, 280], "byte": [5, 20, 213, 278, 280], "pair": [5, 17, 20, 23, 47, 48, 62, 66, 69, 213], "underli": [5, 12, 17, 20, 212, 278, 280], "helper": 5, "method": [5, 12, 13, 14, 18, 20, 21, 23, 24, 25, 28, 45, 54, 56, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 143, 188, 194, 195, 198, 201, 202, 203, 206, 208, 210, 211, 231, 239, 265, 266, 277, 280], "two": [5, 14, 17, 18, 21, 23, 34, 49, 51, 64, 65, 72, 73, 78, 190, 199, 202, 217, 224, 226, 266, 268, 273, 274, 275, 277, 278, 279, 280], "pre": [5, 9, 10, 11, 16, 17, 18, 19, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 99, 144, 147, 148, 190, 198, 200, 202, 268, 272], "train": [5, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 30, 32, 34, 49, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 99, 143, 144, 147, 148, 180, 182, 184, 188, 189, 194, 195, 197, 198, 199, 200, 202, 220, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 264, 266, 267, 269, 271, 272, 273, 275, 276, 277, 278, 279, 280], "function": [5, 21, 23, 24, 26, 28, 44, 45, 46, 47, 59, 63, 66, 72, 78, 79, 80, 146, 150, 179, 182, 190, 191, 194, 205, 208, 209, 220, 221, 225, 227, 241, 254, 256, 258, 259, 266, 276, 280], "preprocess": [5, 53, 190], "imag": [5, 10, 14, 34, 35, 36, 41, 43, 45, 49, 55, 64, 65, 77, 78, 79, 80, 141, 142, 143, 144, 146, 147, 148, 150, 190, 199, 217, 277], "loss": [6, 9, 11, 13, 23, 24, 35, 37, 54, 55, 58, 59, 61, 63, 66, 67, 68, 195, 196, 197, 220, 221, 222, 223, 274, 276, 277, 280], "algorithm": [6, 20, 218, 223, 254], "ppo": [6, 218, 219, 220, 221, 269], "dpo": [6, 17, 47, 54, 205, 220, 222, 223, 269], "offer": 7, "allow": [7, 52, 200, 208, 249, 268, 271, 278, 279, 280], "seamless": 7, "transit": 7, "between": [7, 9, 17, 18, 20, 21, 54, 59, 66, 145, 149, 187, 188, 192, 198, 219, 221, 223, 227, 230, 246, 273, 275, 276, 277, 279, 280], "interoper": [7, 21, 24, 266, 273, 280], "rest": [7, 272, 278, 280], "ecosystem": [7, 21, 24, 266, 273, 275, 280], "comprehens": [7, 278], "deep": [7, 21, 22, 23, 24, 25, 200, 202, 266, 269, 274, 275, 278], "dive": [7, 21, 22, 23, 24, 25, 266, 268, 269, 274, 275, 278], "util": [7, 13, 15, 21, 23, 24, 26, 44, 46, 49, 141, 232, 249, 251, 252, 258, 259, 260, 261, 266, 273, 274, 278, 280], "work": [7, 21, 24, 34, 41, 186, 199, 200, 266, 268, 271, 273, 275, 278, 280], "set": [7, 9, 11, 16, 17, 19, 21, 22, 23, 24, 25, 32, 35, 36, 41, 49, 53, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 100, 106, 115, 119, 124, 128, 143, 145, 148, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182, 184, 187, 188, 191, 192, 193, 198, 205, 207, 225, 231, 236, 243, 245, 246, 252, 253, 254, 255, 258, 259, 266, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279], "enabl": [7, 10, 16, 20, 22, 23, 24, 25, 52, 84, 85, 86, 87, 88, 89, 95, 96, 97, 98, 107, 108, 109, 110, 111, 112, 113, 114, 120, 121, 122, 123, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 151, 154, 156, 162, 163, 165, 169, 171, 172, 173, 175, 176, 182, 187, 188, 189, 191, 192, 193, 198, 200, 204, 254, 255, 268, 275, 277, 278, 280], "consumpt": [7, 52, 74, 267, 278], "dure": [7, 10, 21, 53, 58, 59, 61, 63, 66, 67, 68, 181, 182, 184, 188, 189, 190, 194, 198, 199, 217, 223, 238, 267, 268, 272, 273, 275, 277, 278, 279, 280], "control": [7, 12, 17, 20, 24, 35, 58, 59, 61, 63, 66, 67, 68, 192, 193, 200, 205, 246, 254, 268, 273, 278], "lr": [7, 23, 231, 234, 237, 276, 278], "process": [7, 10, 13, 14, 16, 24, 25, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 80, 146, 150, 190, 194, 241, 242, 254, 274, 279, 280], "variou": 7, "provid": [7, 10, 11, 13, 21, 23, 24, 26, 31, 32, 34, 36, 41, 45, 46, 50, 52, 53, 72, 74, 80, 182, 186, 188, 190, 198, 205, 215, 220, 229, 236, 246, 250, 255, 259, 266, 267, 268, 271, 272, 273, 274, 275, 278], "debug": [7, 21, 23, 24, 246, 271], "finetun": [7, 21, 23, 24, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 198, 264, 266, 268, 274, 275, 278], "job": [7, 25, 254, 274], "involv": [9, 11, 16, 19, 55, 279], "multi": [9, 17, 24, 182, 275], "turn": [9, 17, 24, 32, 35, 36, 41, 51, 54, 66, 272, 278], "multipl": [9, 15, 16, 17, 21, 23, 24, 32, 35, 36, 41, 47, 52, 55, 66, 141, 142, 182, 188, 189, 190, 198, 204, 246, 247, 248, 249, 250, 255, 274, 275, 276, 278], "back": [9, 20, 21, 51, 205, 227, 277, 278, 280], "forth": [9, 51], "user": [9, 11, 12, 13, 14, 15, 17, 18, 20, 24, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 51, 54, 55, 59, 63, 66, 93, 100, 105, 106, 115, 118, 119, 124, 128, 145, 147, 149, 153, 155, 157, 159, 161, 164, 166, 168, 170, 174, 178, 182, 215, 269, 272, 274, 279], "assist": [9, 11, 12, 13, 14, 15, 17, 18, 20, 30, 31, 32, 34, 35, 36, 37, 38, 40, 41, 43, 51, 54, 55, 59, 66, 72, 93, 99, 105, 118, 147, 161, 168, 178, 215, 272], "role": [9, 12, 13, 14, 15, 17, 18, 20, 32, 35, 36, 37, 38, 41, 43, 54, 55, 59, 66, 93, 105, 118, 143, 147, 161, 168, 178, 215, 272], "content": [9, 12, 14, 15, 17, 18, 20, 21, 32, 35, 36, 37, 38, 41, 43, 54, 55, 59, 66, 215, 272], "what": [9, 13, 14, 15, 17, 21, 22, 23, 25, 35, 36, 54, 55, 59, 63, 66, 99, 152, 190, 264, 269, 272, 273, 274, 275, 278], "answer": [9, 14, 15, 18, 39, 63, 273, 275], "ultim": [9, 279], "question": [9, 14, 15, 18, 39, 63, 273, 275], "life": 9, "42": [9, 72, 190], "That": [9, 272], "s": [9, 11, 12, 13, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 28, 31, 36, 41, 51, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 84, 85, 86, 99, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 141, 142, 143, 148, 149, 150, 152, 153, 154, 155, 156, 164, 165, 168, 170, 173, 174, 181, 182, 184, 188, 189, 190, 194, 198, 201, 202, 203, 206, 208, 209, 213, 220, 222, 223, 224, 225, 227, 228, 231, 236, 238, 240, 245, 246, 249, 252, 253, 256, 258, 259, 266, 271, 272, 274, 276, 277, 278, 279, 280], "ridicul": 9, "oh": 9, "i": [9, 11, 13, 17, 18, 19, 24, 35, 66, 72, 99, 141, 142, 152, 182, 187, 188, 189, 190, 194, 198, 207, 226, 231, 273, 275, 278, 279, 280], "know": [9, 272, 273, 276, 277], "more": [9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 23, 24, 37, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 181, 190, 195, 202, 208, 225, 226, 229, 246, 250, 252, 254, 258, 266, 267, 268, 269, 271, 273, 274, 275, 276, 277, 278, 279, 280], "structur": [9, 11, 12, 13, 18, 24, 36, 38, 41, 59, 118, 143, 147, 168, 178, 217, 272, 273, 279], "than": [9, 11, 15, 17, 23, 49, 51, 72, 74, 181, 182, 190, 220, 225, 229, 230, 257, 258, 261, 272, 273, 274, 275, 276, 277, 278, 280], "freeform": [9, 11, 56, 70], "associ": [9, 10, 11, 21, 23, 24, 72, 73, 80, 90, 100, 115, 124, 145, 149, 157, 174, 246, 273, 277], "where": [9, 11, 13, 15, 17, 18, 19, 35, 37, 44, 47, 58, 72, 74, 75, 78, 104, 141, 142, 160, 179, 182, 188, 190, 192, 195, 197, 198, 204, 212, 217, 218, 220, 221, 224, 236, 240, 245, 276, 278], "thei": [9, 10, 11, 18, 20, 23, 24, 52, 64, 65, 80, 141, 146, 150, 188, 190, 200, 209, 236, 271, 272, 277, 278, 279], "learn": [9, 11, 24, 52, 199, 200, 202, 231, 234, 237, 266, 267, 268, 269, 272, 274, 275, 277, 278, 279, 280], "simpli": [9, 11, 12, 13, 15, 19, 21, 23, 53, 55, 220, 271, 272, 273, 275, 276, 278, 280], "predict": [9, 11, 72, 73, 76, 218, 219, 221, 267], "next": [9, 11, 21, 53, 70, 72, 73, 80, 190, 217, 267, 275, 280], "respond": 9, "accur": 9, "primari": [9, 11, 15, 17, 19, 21, 23, 24, 54, 55, 269, 274], "entri": [9, 11, 15, 17, 19, 23, 24, 46, 49, 269, 274, 278], "point": [9, 11, 15, 17, 19, 20, 23, 24, 45, 59, 215, 269, 273, 274, 275, 277, 279, 280], "torchtun": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 265, 267, 268, 269, 272, 274, 278], "chat_dataset": [9, 11, 12, 17, 272], "let": [9, 10, 11, 15, 17, 21, 23, 25, 271, 272, 273, 274, 275, 276, 277, 278, 280], "follow": [9, 10, 11, 14, 15, 18, 21, 24, 35, 36, 37, 41, 49, 53, 54, 55, 63, 66, 143, 182, 187, 217, 221, 229, 230, 231, 234, 243, 250, 255, 264, 265, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "data": [9, 11, 12, 13, 14, 15, 18, 20, 22, 30, 31, 32, 34, 35, 36, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 99, 147, 178, 190, 216, 220, 222, 238, 246, 247, 248, 249, 250, 258, 267, 268, 273, 278, 279, 280], "directli": [9, 11, 12, 13, 15, 21, 23, 24, 26, 30, 54, 55, 59, 63, 64, 66, 70, 220, 225, 227, 271, 273, 274, 275, 277, 278, 279, 280], "llm": [9, 10, 11, 20, 24, 198, 200, 264, 265, 266, 267, 269, 273, 275, 276, 277], "my_data": [9, 11, 12, 15, 272], "human": [9, 15, 17, 35, 41, 59, 99, 220, 221, 222, 272], "valu": [9, 15, 21, 23, 32, 34, 36, 41, 44, 46, 47, 49, 58, 59, 61, 62, 63, 66, 67, 68, 69, 72, 73, 75, 76, 81, 82, 83, 90, 91, 92, 94, 100, 101, 102, 103, 104, 106, 115, 116, 117, 119, 124, 125, 126, 127, 128, 135, 136, 143, 145, 149, 153, 155, 157, 158, 159, 160, 164, 166, 170, 174, 175, 176, 177, 181, 182, 183, 187, 188, 189, 196, 197, 198, 200, 208, 218, 219, 221, 224, 227, 230, 231, 234, 240, 246, 247, 248, 249, 250, 254, 268, 271, 272, 274, 275, 277, 278, 279], "gpt": [9, 15, 41, 59, 73, 272, 273], "mistral": [9, 13, 17, 18, 20, 143, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 230, 271, 272, 273, 274], "mistral_token": [9, 13, 17, 18, 20], "m_token": [9, 13, 17, 18, 19, 20], "path": [9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 34, 41, 45, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 93, 105, 118, 143, 147, 161, 168, 178, 212, 213, 214, 227, 228, 229, 255, 271, 272, 273, 275, 277], "1": [9, 13, 15, 17, 18, 19, 20, 21, 24, 34, 41, 44, 46, 47, 48, 49, 53, 68, 72, 73, 75, 76, 77, 78, 100, 106, 115, 119, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 142, 143, 145, 149, 152, 153, 155, 157, 159, 164, 166, 170, 171, 172, 174, 175, 176, 181, 182, 188, 190, 191, 192, 193, 195, 196, 197, 212, 213, 215, 220, 221, 222, 223, 228, 230, 234, 240, 243, 246, 249, 250, 253, 254, 266, 267, 271, 272, 273, 274, 277, 278, 279, 280], "prompt_templ": [9, 11, 13, 15, 17, 18, 93, 105, 118, 143, 147, 161, 168, 178], "mistralchattempl": [9, 13, 17, 18, 161, 272], "max_seq_len": [9, 11, 13, 15, 16, 17, 19, 20, 23, 26, 46, 49, 50, 53, 58, 59, 60, 61, 63, 64, 65, 67, 68, 70, 71, 90, 93, 94, 100, 105, 106, 115, 118, 119, 124, 128, 143, 145, 147, 149, 153, 155, 157, 159, 161, 164, 166, 168, 170, 174, 178, 181, 182, 184, 188, 193, 279], "8192": [9, 11, 13, 15, 16, 17, 19, 20, 147, 277, 279], "ds": [9, 11, 14, 15, 17, 19, 53, 68, 272], "sourc": [9, 11, 12, 15, 17, 19, 21, 23, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 272, 273, 279], "data_fil": [9, 11, 12, 15, 17, 19, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 272], "split": [9, 11, 12, 13, 15, 17, 19, 21, 43, 52, 53, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 213, 272, 273, 279], "conversation_column": [9, 59, 272], "conversation_styl": [9, 59, 272], "By": [9, 11, 21, 268, 271, 276, 277, 278, 279, 280], "default": [9, 11, 15, 21, 23, 30, 31, 32, 34, 35, 36, 41, 44, 47, 48, 49, 50, 53, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 143, 144, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 182, 183, 184, 187, 188, 189, 194, 196, 197, 198, 204, 208, 212, 213, 215, 217, 218, 219, 220, 223, 227, 228, 229, 231, 234, 235, 241, 245, 246, 247, 250, 253, 254, 255, 265, 268, 271, 272, 273, 275, 276, 277, 278, 279, 280], "true": [9, 11, 12, 13, 14, 15, 16, 21, 23, 30, 35, 44, 52, 53, 56, 57, 58, 59, 61, 63, 64, 65, 66, 67, 68, 70, 71, 74, 75, 80, 87, 88, 89, 90, 94, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 143, 144, 151, 162, 163, 169, 182, 187, 188, 189, 191, 192, 193, 194, 195, 196, 198, 200, 205, 212, 213, 215, 217, 218, 221, 224, 225, 227, 228, 229, 236, 237, 238, 240, 242, 243, 246, 249, 255, 261, 267, 271, 272, 273, 275, 277, 278, 279, 280], "train_on_input": [9, 11, 12, 17, 23, 30, 32, 34, 36, 41, 52, 57, 58, 59, 61, 62, 63, 66, 67, 68, 69], "new_system_prompt": [9, 11, 12, 32, 34, 36, 41, 59, 61, 62, 63, 64, 65, 66, 67, 68], "tokenized_dict": [9, 11, 14, 15, 17, 19], "label": [9, 11, 19, 24, 46, 47, 48, 49, 53, 60, 68, 71, 195, 196, 197, 220, 223, 276], "print": [9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 25, 43, 49, 52, 58, 61, 64, 65, 67, 68, 72, 143, 190, 191, 192, 193, 212, 213, 215, 261, 272, 274, 277, 279, 280], "inst": [9, 13, 18, 20, 99, 143, 152, 272], "733": [9, 13, 20], "16289": [9, 13, 20], "28793": [9, 13, 20], "1824": 9, "349": 9, "272": 9, "4372": 9, "In": [9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 54, 78, 79, 80, 146, 150, 184, 188, 190, 204, 225, 245, 249, 250, 268, 272, 273, 275, 276, 277, 278, 279, 280], "_component_": [9, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 52, 59, 63, 66, 70, 255, 268, 272, 273, 275, 276, 277, 278, 279], "null": [9, 21, 23, 279], "have": [9, 12, 13, 17, 20, 21, 23, 26, 34, 35, 54, 59, 66, 74, 78, 79, 80, 141, 146, 150, 180, 181, 182, 183, 186, 188, 190, 191, 192, 193, 195, 197, 198, 203, 209, 217, 223, 226, 229, 231, 236, 237, 249, 257, 265, 272, 273, 274, 275, 276, 277, 278, 279, 280], "singl": [9, 15, 16, 17, 18, 21, 23, 26, 32, 34, 36, 41, 46, 52, 53, 54, 55, 56, 59, 66, 70, 78, 79, 80, 93, 104, 105, 118, 141, 142, 143, 146, 147, 150, 160, 161, 168, 182, 188, 190, 198, 227, 228, 229, 230, 231, 233, 269, 271, 272, 273, 274, 275, 276, 277, 278, 280], "name": [9, 11, 12, 13, 15, 17, 19, 21, 22, 23, 25, 27, 30, 32, 34, 36, 41, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 203, 207, 209, 213, 227, 228, 229, 230, 231, 233, 246, 247, 248, 249, 250, 256, 257, 259, 271, 272, 273, 275, 278, 279], "messag": [9, 10, 11, 14, 15, 17, 18, 20, 30, 31, 32, 34, 36, 37, 38, 41, 43, 51, 54, 55, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 93, 105, 118, 143, 147, 161, 168, 211, 215, 265, 271, 272], "contain": [9, 10, 12, 13, 14, 15, 17, 19, 21, 32, 34, 35, 41, 46, 47, 48, 49, 53, 54, 55, 56, 59, 64, 70, 118, 143, 147, 168, 178, 181, 182, 184, 188, 189, 198, 201, 203, 206, 207, 208, 213, 215, 218, 224, 227, 228, 229, 231, 233, 238, 244, 249, 255, 256, 258, 272, 273, 275, 277], "topic": [9, 264], "per": [9, 15, 46, 87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 142, 143, 151, 162, 163, 169, 181, 190, 194, 217, 219, 220, 271, 278, 279, 280], "could": [9, 17, 18, 237, 276, 277], "system": [9, 11, 12, 17, 18, 31, 32, 34, 35, 36, 37, 38, 40, 41, 43, 51, 54, 55, 59, 61, 62, 63, 64, 65, 66, 67, 68, 93, 99, 105, 118, 147, 152, 161, 168, 178, 215, 272], "tool": [9, 17, 18, 21, 35, 37, 55, 152, 246, 273, 274], "call": [9, 13, 17, 20, 21, 26, 35, 37, 55, 64, 65, 152, 182, 188, 190, 191, 192, 194, 198, 208, 246, 247, 248, 249, 250, 251, 255, 256, 272, 277, 280], "return": [9, 12, 14, 17, 18, 20, 26, 28, 35, 37, 43, 44, 45, 46, 47, 48, 49, 50, 53, 54, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 192, 193, 195, 196, 197, 198, 199, 200, 201, 203, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 253, 254, 255, 259, 260, 261, 276, 277, 280], "dai": [9, 19], "todai": 9, "It": [9, 13, 15, 31, 35, 37, 54, 55, 59, 61, 63, 64, 65, 67, 69, 143, 146, 150, 152, 186, 188, 190, 198, 220, 223, 246, 271, 272, 276, 280], "tuesdai": 9, "about": [9, 12, 13, 17, 21, 24, 64, 65, 190, 220, 223, 246, 250, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "tomorrow": 9, "wednesdai": 9, "As": [9, 11, 15, 21, 23, 24, 25, 204, 266, 273, 278, 280], "an": [9, 11, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 45, 49, 51, 52, 56, 59, 61, 63, 64, 65, 66, 67, 70, 71, 77, 78, 79, 106, 119, 128, 143, 146, 148, 150, 153, 155, 159, 164, 170, 171, 172, 175, 176, 182, 186, 188, 190, 198, 199, 200, 202, 203, 205, 206, 207, 211, 216, 217, 220, 225, 226, 227, 228, 229, 231, 232, 236, 237, 246, 250, 255, 259, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "slimorca": [9, 68], "pass": [9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 23, 26, 35, 37, 52, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 90, 94, 100, 106, 115, 119, 124, 128, 148, 153, 155, 157, 159, 164, 166, 170, 174, 182, 188, 192, 193, 194, 198, 205, 209, 213, 221, 225, 229, 235, 236, 238, 242, 245, 246, 249, 250, 252, 255, 271, 272, 277, 279, 280], "repo": [9, 11, 15, 17, 19, 21, 64, 227, 228, 230, 271, 273], "select": [9, 232], "one": [9, 10, 11, 12, 15, 17, 21, 24, 32, 34, 36, 41, 46, 49, 51, 59, 65, 66, 190, 195, 197, 215, 229, 246, 273, 274, 275, 278, 280], "most": [9, 11, 12, 15, 17, 19, 21, 23, 35, 37, 272, 274, 277, 278, 280], "gemma": [9, 11, 17, 19, 91, 92, 93, 94, 95, 96, 97, 98, 186, 230, 278], "gemma_token": [9, 11, 17, 19], "g_token": [9, 11, 17, 19], "open": [9, 19, 45, 68, 91, 92, 273], "orca": [9, 68], "dedup": [9, 68], "recip": [9, 10, 11, 15, 17, 19, 21, 22, 23, 25, 26, 27, 28, 143, 188, 198, 227, 228, 229, 266, 267, 268, 272, 273, 275, 278, 280], "via": [9, 11, 13, 15, 16, 17, 19, 22, 23, 25, 54, 59, 63, 66, 70, 182, 188, 189, 204, 227, 277, 280], "http": [9, 11, 15, 26, 45, 56, 60, 62, 64, 70, 71, 73, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 94, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 151, 153, 154, 155, 156, 158, 160, 162, 163, 164, 165, 167, 168, 169, 171, 172, 173, 175, 176, 177, 182, 183, 184, 190, 195, 196, 217, 218, 220, 221, 222, 223, 225, 227, 228, 234, 243, 246, 249, 250, 252, 254, 260, 265, 273, 275, 276], "ha": [9, 17, 21, 63, 72, 142, 185, 187, 188, 190, 193, 195, 197, 198, 201, 203, 205, 206, 209, 224, 229, 231, 256, 257, 272, 273, 274, 275, 276, 277, 278, 280], "addition": [9, 21, 212, 213, 223, 254, 272, 277], "argument": [9, 11, 15, 21, 23, 26, 33, 39, 42, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 225, 236, 242, 246, 247, 249, 250, 252, 271, 272, 277, 279], "load_dataset": [9, 11, 15, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 272], "document": [9, 11, 15, 16, 77, 78, 79, 80, 182, 188, 189, 225, 236, 245, 267, 269, 271, 278], "file": [9, 10, 11, 15, 21, 22, 23, 24, 25, 26, 27, 28, 45, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 143, 147, 168, 178, 212, 213, 214, 227, 228, 229, 247, 250, 255, 263, 266, 268, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280], "raw": [9, 10, 12, 13, 15, 20, 43], "vari": [9, 49, 53, 188], "field": [9, 13, 14, 26, 30, 34, 35, 41, 43, 53, 54, 55, 58, 64, 65, 244], "indic": [9, 13, 15, 17, 18, 49, 52, 53, 74, 75, 80, 146, 150, 182, 184, 188, 189, 190, 198, 199, 217, 218, 221, 224, 225, 240, 243, 272], "There": [9, 23, 51, 78, 272, 274, 275, 276, 277, 278], "few": [9, 200, 275, 277, 280], "standard": [9, 11, 13, 14, 16, 18, 21, 33, 54, 55, 59, 62, 100, 106, 115, 119, 124, 128, 143, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182, 248, 266, 272, 273, 275, 276], "across": [9, 21, 24, 49, 52, 227, 249, 254, 273, 275, 276, 279], "mani": [9, 13, 15, 18, 23, 53, 267, 268, 273, 276], "we": [9, 10, 11, 17, 18, 19, 20, 21, 22, 23, 24, 25, 46, 49, 53, 54, 55, 59, 60, 66, 71, 72, 76, 181, 182, 184, 188, 189, 190, 192, 195, 197, 198, 204, 220, 223, 227, 228, 229, 235, 239, 245, 251, 256, 266, 267, 268, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "ipython": [9, 13, 18, 35, 37, 40, 54, 55, 93, 105, 118, 147, 161, 168, 178], "transform": [9, 10, 15, 21, 24, 30, 32, 34, 54, 55, 58, 59, 61, 62, 64, 65, 66, 67, 68, 80, 84, 85, 86, 90, 94, 95, 96, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 142, 143, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 159, 164, 165, 166, 170, 171, 172, 173, 174, 187, 188, 189, 190, 200, 217, 234, 252, 277, 278, 279], "sharegpttomessag": [9, 12, 59, 68], "expect": [9, 11, 12, 14, 15, 17, 18, 19, 21, 23, 26, 30, 32, 34, 35, 36, 41, 45, 49, 54, 55, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 142, 143, 184, 198, 209, 231, 246, 250, 257, 272, 277, 279], "code": [9, 11, 12, 15, 18, 20, 21, 24, 81, 82, 83, 84, 85, 86, 87, 88, 89, 188, 246, 262, 266, 274], "openaitomessag": [9, 12, 59, 66], "If": [9, 12, 13, 15, 16, 18, 20, 21, 23, 29, 32, 34, 35, 36, 41, 43, 45, 46, 49, 50, 51, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 74, 76, 80, 93, 100, 105, 106, 115, 118, 119, 124, 128, 141, 143, 145, 147, 149, 161, 168, 170, 174, 178, 181, 182, 184, 186, 188, 189, 190, 192, 193, 194, 195, 197, 198, 204, 209, 215, 227, 228, 229, 230, 231, 232, 235, 236, 237, 238, 239, 242, 246, 249, 250, 254, 255, 257, 259, 265, 271, 272, 273, 274, 275, 276, 277, 278, 279], "doe": [9, 16, 21, 43, 49, 53, 66, 70, 90, 152, 157, 167, 182, 186, 188, 189, 191, 192, 193, 196, 197, 198, 203, 215, 227, 229, 231, 256, 271, 273, 279], "fit": [9, 24, 53, 60, 70, 71, 190, 220, 272], "creat": [9, 12, 15, 18, 21, 23, 26, 37, 53, 55, 59, 66, 74, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 95, 96, 97, 98, 101, 102, 103, 104, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 151, 154, 156, 158, 160, 162, 163, 165, 167, 169, 171, 172, 173, 175, 176, 177, 181, 182, 188, 189, 190, 198, 225, 227, 228, 229, 233, 234, 246, 247, 249, 271, 273, 280], "custom": [9, 14, 15, 20, 23, 24, 30, 37, 54, 55, 59, 63, 64, 65, 66, 70, 93, 105, 118, 147, 161, 168, 178, 252, 266, 267, 268, 271, 274, 275, 277, 278], "dialogu": [9, 15, 42, 67, 272], "defin": [9, 16, 21, 23, 24, 37, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 182, 187, 188, 198, 201, 203, 204, 206, 219, 274, 277], "same": [9, 10, 14, 17, 21, 23, 37, 44, 77, 78, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 142, 165, 171, 172, 173, 180, 181, 183, 185, 186, 187, 189, 190, 193, 198, 200, 215, 221, 223, 224, 231, 236, 237, 250, 256, 258, 268, 271, 272, 273, 275, 276, 277, 278, 279, 280], "wai": [9, 13, 18, 21, 23, 54, 55, 208, 226, 271, 273, 274, 275, 276], "instruct_dataset": [9, 11, 12, 52], "info": [9, 260, 274], "slimorca_dataset": [9, 23], "vlm": [10, 15], "found": [10, 21, 22, 23, 25, 183, 184, 227, 228, 229, 268, 271, 276, 277, 280], "hub": [10, 21, 54, 55, 271, 274], "local": [10, 13, 45, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 118, 147, 168, 178, 246, 250, 254, 265, 271, 272, 273, 274], "remot": [10, 13, 34, 41, 45, 54, 55], "url": [10, 15, 34, 36, 41, 45, 265], "built": [10, 12, 22, 23, 25, 62, 66, 69, 265, 272, 274, 280], "project": [10, 22, 25, 80, 84, 85, 86, 90, 94, 100, 104, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 141, 142, 145, 146, 148, 149, 150, 153, 154, 155, 156, 157, 160, 164, 165, 170, 173, 174, 179, 182, 188, 190, 198, 202, 208, 209, 230, 236, 246, 250, 264, 277, 278, 280], "workflow": [10, 264, 274, 277], "prefer": [10, 12, 24, 47, 54, 62, 66, 69, 220, 221, 222, 223, 266, 269, 271, 278], "align": [10, 64, 65, 220, 272, 276], "continu": [10, 19, 53, 190, 246], "pretrain": [10, 141, 142, 143, 198, 200, 202, 212, 213, 271, 272, 274, 277, 280], "beyond": [10, 273, 278, 280], "those": [10, 21, 230, 273, 275, 277], "full": [10, 12, 15, 21, 23, 24, 33, 39, 42, 54, 71, 87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 148, 151, 162, 163, 169, 198, 208, 209, 215, 232, 265, 266, 269, 271, 273, 275, 277, 278, 279], "customiz": 10, "task": [10, 11, 15, 17, 18, 33, 39, 42, 52, 60, 143, 267, 272, 273, 275, 276, 277, 278, 279, 280], "supervis": [10, 19, 55], "rlhf": [10, 54, 62, 218, 219, 220, 221, 222, 223, 224], "complet": [10, 11, 17, 21, 24, 36, 53, 60, 70, 168, 272, 273, 274, 275, 278], "input": [10, 11, 12, 13, 14, 19, 20, 21, 30, 34, 46, 47, 48, 49, 53, 54, 55, 58, 60, 61, 63, 64, 65, 67, 68, 71, 77, 78, 79, 80, 93, 105, 118, 141, 142, 143, 146, 147, 150, 161, 168, 170, 174, 179, 180, 182, 183, 184, 185, 186, 187, 188, 189, 190, 196, 197, 198, 199, 200, 204, 212, 213, 217, 227, 229, 237, 254, 257, 272, 277, 280], "queri": [10, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 188, 189, 198, 275, 278], "time": [10, 15, 16, 21, 59, 63, 90, 157, 192, 195, 197, 215, 218, 247, 249, 255, 268, 271, 272, 273, 275, 280], "which": [10, 11, 13, 15, 16, 17, 18, 19, 20, 21, 23, 24, 45, 46, 52, 53, 56, 58, 59, 61, 63, 66, 67, 68, 70, 75, 76, 84, 85, 86, 93, 94, 95, 96, 105, 106, 107, 108, 109, 110, 118, 119, 120, 121, 128, 129, 130, 131, 137, 138, 143, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 161, 164, 165, 168, 170, 171, 172, 173, 181, 182, 184, 188, 189, 190, 191, 192, 193, 198, 200, 208, 209, 212, 227, 228, 229, 231, 234, 235, 247, 250, 252, 256, 266, 267, 268, 269, 271, 272, 273, 274, 276, 277, 278, 279, 280], "take": [10, 11, 12, 15, 17, 21, 23, 24, 26, 47, 54, 55, 64, 65, 66, 141, 181, 190, 194, 200, 227, 229, 258, 259, 268, 272, 273, 274, 275, 276, 277, 278, 280], "object": [10, 12, 13, 14, 18, 20, 23, 26, 27, 80, 182, 220, 223, 225, 239], "appli": [10, 11, 14, 18, 21, 24, 46, 54, 55, 58, 64, 65, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 100, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 145, 148, 149, 150, 151, 153, 154, 155, 156, 157, 162, 163, 164, 165, 169, 170, 171, 172, 173, 174, 182, 186, 187, 188, 189, 198, 208, 209, 252, 266, 267, 276, 278, 280], "templat": [10, 30, 31, 33, 37, 38, 39, 42, 54, 55, 58, 61, 67, 93, 99, 105, 118, 143, 147, 152, 161, 168, 178], "anyth": [10, 60, 258], "els": [10, 11, 18, 24, 250, 266, 280], "requir": [10, 14, 16, 18, 20, 21, 23, 46, 47, 52, 54, 55, 56, 64, 65, 66, 70, 143, 186, 188, 199, 227, 229, 231, 239, 242, 243, 245, 246, 249, 250, 254, 255, 265, 268, 271, 272, 274, 278, 279, 280], "particular": [10, 12, 18, 20, 23, 52, 143, 225, 277, 280], "collat": [10, 46, 48, 49, 53], "packag": [10, 22, 25, 246, 249, 250, 265], "togeth": [10, 24, 53, 195, 250, 269, 274, 277, 278, 279], "form": [11, 17, 21, 23, 24, 30, 43, 51, 54, 55, 271], "command": [11, 16, 20, 22, 24, 25, 265, 268, 269, 271, 272, 273, 274, 275, 276, 277, 279, 280], "respons": [11, 12, 17, 18, 20, 31, 32, 34, 35, 36, 41, 54, 55, 59, 61, 62, 63, 64, 65, 66, 67, 68, 215, 218, 219, 220, 222, 223, 273, 274, 275], "along": [11, 21, 277], "option": [11, 17, 20, 21, 23, 24, 30, 32, 34, 36, 41, 49, 50, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 79, 80, 84, 85, 86, 93, 94, 95, 96, 100, 105, 106, 107, 108, 109, 110, 115, 118, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 141, 142, 143, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 161, 164, 165, 168, 170, 171, 172, 173, 174, 178, 179, 182, 184, 187, 188, 189, 190, 193, 194, 198, 208, 209, 210, 212, 215, 217, 218, 219, 221, 227, 228, 229, 231, 232, 235, 239, 246, 247, 250, 254, 255, 259, 260, 265, 266, 271, 272, 273], "describ": [11, 252], "hand": [11, 35], "here": [11, 13, 14, 15, 17, 19, 20, 21, 22, 23, 25, 31, 61, 64, 65, 183, 184, 237, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280], "grammar": [11, 18, 33, 61], "correct": [11, 13, 18, 24, 33, 61, 183, 184, 188, 259, 266, 272], "head": [11, 80, 90, 94, 100, 106, 115, 119, 124, 128, 141, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 184, 188, 198, 202, 230, 275], "csv": [11, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], "incorrect": [11, 18], "cat": [11, 15, 18, 217], "grammarerrorcorrectiontempl": [11, 18, 61], "prepend": [11, 13, 15, 18, 32, 34, 36, 37, 38, 41, 59, 61, 62, 63, 64, 65, 66, 67, 68, 93, 105, 118, 143, 147, 161, 168, 178, 212], "everi": [11, 21, 24, 61, 62, 66, 67, 68, 77, 78, 79, 145, 149, 190, 192, 249, 255, 265, 271, 278, 280], "our": [11, 12, 19, 21, 24, 266, 267, 268, 269, 272, 273, 274, 276, 277, 278, 279, 280], "column_map": [11, 12, 15, 17, 30, 32, 34, 36, 41, 52, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69], "english": [11, 18, 33], "ncorrect": [11, 33], "mask": [11, 12, 13, 14, 16, 18, 20, 35, 37, 49, 53, 55, 58, 59, 61, 63, 64, 65, 66, 67, 68, 73, 74, 75, 143, 182, 187, 188, 189, 198, 211, 215, 217, 218, 221, 240, 272, 276], "out": [11, 14, 17, 19, 21, 23, 24, 58, 59, 61, 63, 66, 67, 68, 74, 75, 217, 227, 228, 240, 264, 266, 267, 268, 269, 271, 272, 273, 274, 275, 277, 278, 280], "100": [11, 17, 24, 47, 48, 49, 58, 59, 61, 63, 66, 67, 68, 72, 195, 196, 197, 199, 276, 277, 280], "27957": 11, "736": 11, "577": 11, "anoth": [11, 12, 15, 23, 55, 186, 246, 273, 278], "c4": [11, 70, 279], "200m": 11, "liweili": [11, 61], "c4_200m": [11, 61], "chang": [11, 12, 15, 20, 21, 22, 23, 25, 30, 32, 34, 63, 65, 69, 229, 265, 271, 273, 274, 275, 276, 277, 278, 279, 280], "remap": 11, "each": [11, 14, 17, 18, 19, 21, 24, 37, 38, 41, 46, 47, 49, 52, 53, 54, 55, 77, 78, 79, 80, 84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 142, 143, 146, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 182, 184, 188, 189, 190, 195, 197, 198, 200, 208, 209, 215, 217, 218, 219, 220, 222, 223, 240, 254, 255, 266, 268, 269, 271, 273, 274, 277, 278, 279], "them": [11, 14, 17, 18, 21, 23, 52, 66, 190, 194, 200, 215, 258, 268, 271, 272, 273, 277, 278, 279, 280], "someth": [11, 21, 24, 25, 272, 273, 279], "hello": [11, 12, 13, 18, 20, 43, 212, 213, 260, 272, 273, 275], "world": [11, 12, 13, 18, 20, 43, 212, 213, 241, 243, 260, 273], "bye": [11, 12], "robot": [11, 14], "am": [11, 13, 15, 59, 63, 99, 152, 272, 273, 275], "want": [11, 18, 21, 23, 24, 25, 26, 49, 54, 55, 72, 202, 265, 271, 272, 273, 274, 275, 276, 277, 278], "add": [11, 12, 13, 15, 18, 20, 22, 23, 25, 49, 53, 56, 70, 143, 152, 190, 202, 213, 215, 229, 230, 272, 273, 275, 277, 280], "prompttempl": [11, 30, 33, 39, 42, 143], "relev": [11, 13, 24, 187, 188, 189, 198, 271, 273, 277, 278], "inform": [11, 13, 21, 246, 250, 252, 266, 271, 273, 274], "mai": [11, 15, 16, 23, 25, 59, 72, 190, 193, 199, 236, 256, 267, 268, 272, 274, 276, 277, 278], "automat": [11, 15, 16, 18, 20, 22, 23, 25, 26, 58, 59, 271, 273, 280], "alpaca_dataset": [11, 16, 23, 57], "grammar_dataset": 11, "samsum_dataset": 11, "dictionari": [12, 13, 14, 35, 37, 43, 46, 47, 48, 53, 54, 55, 93, 105, 118, 147, 161, 168, 178, 238, 244, 246, 247, 248, 249, 250, 258, 273], "onc": [12, 20, 23, 37, 188, 198, 273, 274, 275, 277, 280], "repres": [12, 35, 47, 77, 78, 190, 226, 232, 272, 278, 279], "prepar": [12, 14, 272, 279], "paramet": [12, 13, 14, 15, 24, 26, 27, 28, 29, 30, 32, 34, 35, 36, 37, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 264, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 279, 280], "ad": [12, 15, 18, 20, 24, 37, 49, 77, 78, 79, 145, 149, 159, 190, 198, 199, 202, 212, 215, 229, 230, 272, 277, 278, 279, 280], "column": [12, 15, 17, 19, 30, 32, 34, 36, 41, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 182, 188, 189, 198, 279], "expos": [12, 23, 24, 229, 269, 274], "so": [12, 17, 18, 19, 21, 23, 53, 64, 186, 190, 227, 265, 266, 272, 273, 274, 275, 277, 278, 279, 280], "don": [12, 13, 17, 18, 19, 21, 23, 24, 250, 254, 271, 272, 273, 274, 276, 278, 280], "t": [12, 13, 17, 18, 19, 21, 23, 24, 44, 141, 142, 195, 200, 235, 250, 254, 271, 272, 273, 274, 276, 278, 280], "worri": [12, 21, 272, 274], "itself": [12, 23], "do": [12, 14, 17, 20, 21, 22, 24, 35, 46, 64, 66, 192, 208, 215, 246, 250, 256, 271, 273, 274, 275, 277, 278, 279], "well": [12, 17, 21, 23, 24, 266, 271, 273, 275, 276, 278, 280], "own": [12, 17, 20, 21, 37, 245, 254, 271, 272, 273, 275, 276, 277], "flexibl": [12, 23, 52, 278], "inherit": [12, 13, 18, 24, 266], "__call__": [12, 14, 18, 64, 65, 143], "A": [12, 14, 18, 24, 25, 32, 33, 36, 39, 41, 42, 46, 47, 48, 49, 52, 53, 66, 80, 178, 182, 186, 187, 188, 189, 190, 194, 198, 204, 208, 212, 213, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 230, 231, 237, 238, 239, 244, 245, 263, 264, 270, 271, 272, 277, 278, 279, 280], "simpl": [12, 21, 24, 190, 223, 264, 274, 277, 279, 280], "contriv": [12, 18], "would": [12, 14, 18, 21, 23, 25, 37, 53, 188, 190, 198, 265, 272, 273, 277, 278, 280], "inde": [12, 235, 273], "quit": [12, 278, 280], "similar": [12, 15, 59, 60, 62, 64, 65, 66, 69, 70, 71, 208, 220, 273, 275, 276, 277, 278, 280], "inputoutputtomessag": [12, 13, 61, 67], "modul": [12, 14, 20, 23, 26, 64, 65, 77, 78, 79, 80, 141, 142, 143, 146, 150, 155, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 220, 221, 222, 225, 230, 232, 233, 236, 245, 251, 252, 254, 274, 276, 277, 280], "type": [12, 13, 14, 15, 20, 25, 26, 28, 35, 36, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 193, 194, 195, 196, 197, 198, 199, 200, 201, 204, 206, 210, 211, 212, 213, 214, 215, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 233, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 252, 253, 254, 255, 257, 259, 260, 261, 268, 273, 277, 278, 279, 280], "map": [12, 14, 18, 20, 21, 30, 32, 34, 36, 37, 41, 46, 52, 53, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 93, 105, 118, 143, 147, 161, 168, 178, 207, 213, 214, 227, 231, 233, 246, 247, 248, 249, 250, 251, 255, 273, 277], "messagetransform": 12, "def": [12, 14, 18, 20, 23, 24, 25, 28, 64, 65, 225, 230, 276, 277, 280], "self": [12, 14, 17, 18, 19, 20, 24, 25, 53, 64, 65, 84, 85, 86, 90, 94, 95, 96, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 145, 148, 149, 150, 153, 154, 155, 156, 157, 159, 164, 165, 166, 170, 171, 172, 173, 174, 182, 187, 188, 189, 195, 197, 198, 200, 203, 208, 209, 227, 230, 231, 276, 277, 280], "str": [12, 14, 20, 23, 26, 27, 30, 32, 34, 35, 36, 37, 41, 43, 45, 46, 47, 48, 49, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 93, 105, 118, 143, 147, 148, 151, 161, 168, 178, 194, 199, 200, 201, 203, 204, 206, 207, 208, 209, 210, 211, 212, 213, 214, 226, 227, 228, 229, 230, 231, 232, 235, 238, 239, 242, 244, 246, 247, 248, 249, 250, 254, 255, 256, 257, 259, 260, 261, 278], "eot": [12, 13, 18, 35, 143], "fals": [12, 13, 14, 15, 17, 18, 21, 23, 32, 34, 35, 36, 41, 44, 52, 53, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 80, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 144, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 174, 182, 188, 189, 191, 192, 193, 198, 199, 200, 204, 205, 208, 212, 224, 227, 228, 229, 240, 243, 255, 256, 271, 272, 273, 275, 277, 279, 280], "_messag": 12, "0x7fb0a10094e0": 12, "0x7fb0a100a290": 12, "msg": [12, 13, 15, 18, 20, 272], "text_cont": [12, 13, 15, 18, 35, 272], "how": [12, 13, 17, 21, 22, 23, 24, 25, 190, 225, 246, 252, 264, 267, 268, 271, 272, 273, 274, 275, 278, 279, 280], "manipul": 12, "must": [12, 16, 26, 37, 52, 64, 65, 182, 192, 203, 226, 246, 280], "sftdataset": [12, 23, 54, 57, 58, 59, 61, 63, 64, 65, 67, 68], "py": [12, 23, 26, 73, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 181, 183, 184, 196, 220, 221, 222, 223, 234, 271, 273, 275], "custom_dataset": 12, "load_dataset_kwarg": [12, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], "message_transform": [12, 54, 55], "mymessagetransform": 12, "model_transform": [12, 14, 15, 54, 55, 61, 64, 65, 67, 68, 143], "chosenrejectedtomessag": [12, 62, 66], "core": [13, 24, 54, 55, 266, 269, 274, 280], "govern": [13, 272], "serv": [13, 18, 23, 32, 34, 36, 41, 59, 61, 62, 63, 64, 65, 66, 67, 68, 215, 225, 277], "interfac": [13, 24, 37, 38, 52, 203, 216], "api": [13, 24, 25, 33, 39, 42, 54, 55, 58, 64, 65, 87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 208, 246, 265, 269, 271, 272, 274, 275, 280], "oper": [13, 24, 190, 205, 216, 254, 279], "send": 13, "other": [13, 14, 17, 19, 21, 24, 26, 34, 37, 52, 229, 236, 255, 258, 267, 268, 272, 274, 275, 276, 277, 278, 279], "special": [13, 15, 18, 35, 41, 118, 143, 145, 147, 149, 168, 178, 190, 199, 210, 211, 213, 214, 215, 217, 231], "individu": [13, 35, 53, 198, 238, 250, 252, 272], "ref": [13, 54, 55, 58, 64, 65, 167, 168, 250], "constructor": [13, 20], "ident": [13, 17, 19, 44, 46, 53, 64, 66, 152, 188, 273, 279], "from_dict": [13, 35, 272], "becaus": [13, 20, 54, 55, 94, 181, 188, 190, 198, 229, 271, 272, 279], "correspond": [13, 17, 20, 35, 47, 73, 74, 75, 201, 203, 206, 218, 221, 235, 268, 274, 275, 278, 279], "begin": [13, 21, 53, 70, 190, 213, 215, 272, 275, 280], "pil": [13, 14, 15, 35, 36, 43, 45], "img_msg": 13, "place": [13, 15, 19, 256, 272, 278], "new": [13, 14, 15, 18, 20, 24, 36, 41, 58, 60, 61, 62, 64, 66, 67, 68, 158, 181, 199, 200, 230, 246, 247, 249, 272, 273, 274, 275, 276, 277, 280], "mode": [13, 14, 15, 193, 232, 239, 246, 273], "rgb": [13, 14, 15, 141], "4": [13, 14, 15, 21, 23, 44, 46, 47, 48, 49, 75, 80, 143, 146, 150, 181, 182, 190, 239, 240, 261, 266, 268, 271, 273, 275, 276, 277, 278, 279, 280], "appropri": [13, 35, 52, 75, 99, 199, 227, 234, 280], "case": [13, 15, 21, 24, 25, 35, 37, 54, 78, 79, 80, 146, 150, 190, 192, 227, 231, 235, 239, 245, 247, 252, 266, 271, 272, 273, 275, 277, 278, 280], "load_imag": [13, 15], "both": [13, 14, 20, 21, 36, 49, 52, 62, 66, 179, 198, 200, 202, 209, 271, 273, 276, 277, 279, 280], "image_path": [13, 15], "jpg": [13, 15, 34, 41, 45], "tag": [13, 15, 18, 20, 37, 41, 43, 93, 99, 105, 118, 143, 147, 152, 161, 168, 178, 246, 247, 248, 249, 250, 272], "placehold": [13, 15, 41, 226], "should": [13, 14, 15, 17, 19, 21, 23, 24, 32, 34, 35, 36, 37, 41, 46, 53, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 84, 85, 86, 94, 95, 96, 99, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 141, 145, 148, 149, 150, 152, 153, 154, 155, 156, 157, 159, 164, 165, 166, 170, 171, 172, 173, 174, 181, 182, 188, 190, 193, 198, 203, 208, 209, 218, 221, 225, 226, 244, 246, 247, 248, 249, 250, 265, 266, 273, 274, 275, 276, 277, 278, 279, 280], "insert": [13, 200, 279], "format_content_with_imag": [13, 15], "image_tag": [13, 15, 41, 43], "conveni": [13, 23, 24, 45, 271], "discuss": [13, 18, 20, 23, 273, 274, 275, 277], "prompttemplateinterfac": [13, 18, 93, 105, 118, 147, 161, 168, 178], "templated_msg": [13, 18], "contains_media": [13, 15, 35], "get_media": [13, 14, 15, 35], "4x4": 13, "0x7f8d27e72740": 13, "tokenize_messsag": 13, "hi": [13, 19, 72, 272], "tokenize_messag": [13, 14, 20, 35, 54, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 143, 211, 215, 272], "22557": 13, "1526": [13, 20], "28808": 13, "28705": [13, 20], "28748": [13, 20], "15359": 13, "28725": 13, "315": [13, 19], "837": 13, "396": 13, "16107": 13, "13892": 13, "28723": 13, "2": [13, 14, 16, 20, 21, 25, 44, 46, 47, 48, 49, 51, 53, 68, 75, 76, 77, 78, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 181, 182, 190, 212, 213, 215, 221, 223, 224, 227, 228, 230, 239, 240, 253, 254, 255, 261, 268, 272, 273, 274, 275, 277, 278, 279], "modal": [14, 15, 55, 143, 200], "current": [14, 15, 17, 21, 34, 41, 53, 66, 74, 90, 94, 106, 119, 128, 148, 149, 150, 153, 155, 157, 164, 167, 170, 181, 182, 184, 188, 189, 198, 221, 228, 229, 231, 236, 239, 241, 247, 249, 251, 254, 268, 269, 274, 275, 276, 278, 279], "intend": [14, 258, 272], "drop": [14, 143, 199, 276, 279], "replac": [14, 15, 41, 50, 58, 59, 61, 63, 66, 67, 68, 143, 194, 199, 256, 277], "llama3_2_vis": [14, 15, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151], "llama3visiontransform": [14, 15, 147], "modeltoken": [14, 20, 23, 35, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 215], "__init__": [14, 23, 24, 64, 65, 276, 277, 280], "transform_imag": 14, "clipimagetransform": [14, 64, 65, 143, 190], "xattn_mask": 14, "visioncrossattentionmask": [14, 143, 216], "224": [14, 15, 143], "llama": [14, 15, 16, 19, 20, 21, 99, 141, 143, 144, 145, 146, 147, 149, 150, 183, 184, 227, 228, 267, 268, 271, 272, 273, 274, 275, 276, 277], "3": [14, 15, 16, 19, 20, 21, 44, 46, 47, 48, 49, 53, 75, 76, 80, 141, 143, 144, 145, 146, 147, 149, 150, 152, 165, 167, 168, 190, 230, 239, 240, 253, 260, 267, 268, 271, 272, 273, 274, 275, 276, 279, 280], "tile_s": [14, 78, 79, 80, 143, 146, 150, 190, 217], "patch_siz": [14, 78, 79, 80, 143, 146, 150, 190, 217], "14": [14, 47, 143, 190, 279, 280], "skip_special_token": [14, 15, 66, 143], "begin_of_text": [14, 15, 20, 272], "start_header_id": [14, 15, 272], "end_header_id": [14, 15, 272], "n": [14, 15, 17, 18, 20, 33, 37, 39, 42, 182, 190, 215, 263, 270, 271, 272, 279], "eot_id": [14, 15, 20, 272], "na": [14, 272], "encoder_input": [14, 15, 49, 187, 188, 198], "shape": [14, 15, 21, 46, 49, 72, 73, 74, 75, 77, 78, 79, 80, 141, 142, 143, 146, 150, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 195, 196, 197, 198, 199, 200, 204, 217, 218, 219, 220, 221, 222, 223, 224, 240, 255, 256, 276], "num_til": [14, 15, 141, 142, 190], "num_channel": [14, 15, 190], "tile_height": [14, 15], "tile_width": [14, 15], "torch": [14, 15, 21, 23, 44, 46, 47, 48, 49, 72, 73, 74, 75, 76, 77, 78, 79, 80, 141, 142, 143, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 204, 218, 219, 220, 221, 222, 223, 224, 229, 231, 233, 234, 235, 237, 238, 239, 240, 242, 243, 249, 251, 252, 253, 254, 255, 256, 257, 258, 259, 261, 265, 268, 273, 274, 275, 276, 277, 278, 280], "just": [14, 18, 21, 266, 268, 271, 272, 274, 275, 277, 278, 279], "the_cauldron_dataset": [14, 15], "subset": [14, 15, 46, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 94, 106, 119, 128, 148, 153, 155, 164, 170, 201, 206], "ai2d": [14, 65], "respir": 14, "combust": 14, "give": [14, 20, 23, 226, 276, 277, 278], "choic": [14, 17], "oxygen": 14, "b": [14, 24, 44, 46, 141, 142, 181, 182, 184, 188, 189, 198, 204, 218, 219, 223, 240, 250, 277, 280], "carbon": 14, "dioxid": 14, "c": [14, 44, 46, 49, 64, 141, 272], "nitrogen": 14, "d": [14, 23, 35, 64, 141, 142, 181, 182, 188, 198, 271, 272, 276, 277, 279], "heat": 14, "letter": 14, "mymultimodaltransform": 14, "my_tokenizer_build": 14, "myimagetransform": 14, "add_eo": [14, 56, 70, 143, 212, 213, 272], "bool": [14, 18, 20, 23, 30, 32, 34, 35, 36, 41, 44, 53, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 80, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 143, 144, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 174, 182, 187, 188, 189, 194, 195, 196, 198, 200, 204, 208, 209, 211, 212, 213, 215, 218, 224, 225, 227, 228, 229, 236, 238, 242, 243, 245, 246, 249, 252, 255, 256, 261, 278, 280], "tupl": [14, 18, 20, 23, 26, 37, 47, 72, 73, 79, 93, 105, 118, 143, 147, 161, 168, 178, 181, 190, 194, 211, 215, 218, 219, 220, 221, 222, 223, 224, 225, 241, 255, 256, 257], "int": [14, 20, 23, 25, 46, 47, 48, 49, 50, 53, 60, 64, 65, 71, 72, 73, 74, 76, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 93, 94, 95, 96, 97, 98, 100, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 118, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 178, 181, 182, 183, 184, 187, 188, 189, 190, 193, 195, 196, 197, 198, 199, 200, 204, 210, 211, 212, 213, 214, 215, 217, 224, 225, 227, 228, 229, 231, 232, 234, 236, 241, 245, 246, 247, 248, 249, 250, 252, 254, 255, 271, 276, 277, 278, 280], "logic": [14, 24, 30, 55, 211, 230, 266, 269, 274, 277], "infer": [14, 18, 21, 49, 55, 90, 99, 157, 181, 182, 184, 188, 189, 198, 217, 259, 264, 268, 269, 272, 273, 274, 275, 279, 280], "vision": [14, 15, 55, 80, 141, 143, 144, 145, 146, 147, 148, 149, 150, 151, 199, 230], "aspect_ratio": [14, 49, 77, 78, 141, 190], "append": [14, 18, 37, 38, 93, 105, 118, 143, 147, 161, 168, 178, 188, 198, 212, 246, 265], "addit": [14, 20, 21, 23, 24, 26, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 143, 145, 146, 149, 150, 152, 192, 199, 200, 208, 220, 225, 227, 228, 229, 235, 236, 242, 245, 246, 247, 249, 250, 252, 266, 272, 274, 277, 278], "kei": [14, 20, 21, 23, 25, 32, 34, 36, 41, 46, 47, 54, 55, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 187, 188, 189, 198, 200, 207, 208, 209, 223, 227, 229, 231, 246, 255, 271, 273, 274, 277, 278, 280], "e": [15, 17, 18, 35, 45, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 141, 142, 143, 146, 150, 182, 190, 194, 198, 203, 207, 217, 226, 227, 231, 238, 255, 259, 265, 268, 273, 275, 277, 278, 279, 280], "g": [15, 17, 45, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 77, 78, 79, 80, 141, 142, 143, 146, 150, 182, 190, 198, 203, 217, 226, 227, 238, 255, 259, 268, 275, 277, 278, 279, 280], "base": [15, 17, 19, 21, 26, 35, 37, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 100, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 143, 144, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 184, 204, 205, 207, 208, 209, 219, 220, 222, 223, 227, 234, 236, 237, 245, 247, 256, 259, 264, 272, 273, 274, 275, 276, 277, 278, 280], "multimodal_chat_dataset": 15, "visual": [15, 200], "note": [15, 20, 21, 23, 94, 198, 203, 231, 251, 254, 256, 268, 272, 273, 276, 277, 278, 279, 280], "get": [15, 21, 22, 23, 24, 25, 49, 143, 231, 235, 238, 241, 246, 260, 265, 266, 267, 268, 272, 273, 274, 276, 277, 278, 279], "below": [15, 22, 25, 46, 225, 275, 276, 277, 280], "clock": 15, "10": [15, 44, 46, 47, 48, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 190, 199, 273, 275, 278, 279, 280], "llama3_2_vision_transform": 15, "questionanswertempl": [15, 18, 69], "image_s": [15, 144, 147, 148, 151, 190], "560": [15, 144, 147, 148, 151], "image_dir": [15, 34, 41, 64], "home": [15, 23, 34, 41, 45], "nquestion": 15, "nit": 15, "00am": 15, "sharegpt": [15, 41, 59, 272], "q1": [15, 32, 54, 59, 66], "a1": [15, 32, 54, 59], "sharegpt4v": 15, "lin": 15, "chen": 15, "renam": 15, "themselv": [15, 280], "write": [15, 21, 24, 227, 228, 229, 247, 274], "pathlib": 15, "pil_imag": 15, "Then": [15, 19, 25, 205, 274, 276, 278], "relat": [15, 187, 188, 198, 277], "user_messag": [15, 33, 39, 42, 143, 272], "rel": [15, 16, 53, 182, 184, 188, 189, 198, 220, 238, 276, 277], "locat": [15, 20, 23, 34, 41, 271, 275, 277, 279, 280], "long": [15, 53, 213, 272, 277], "image_dog": 15, "image_cat": 15, "image_bird": 15, "dog": [15, 217], "bird": [15, 45], "best": [15, 17, 24, 268, 272, 276, 278], "pet": 15, "three": [15, 21, 24, 49, 143, 220, 222, 223, 269, 274], "referenc": 15, "huggingfac": [15, 56, 60, 62, 70, 71, 160, 167, 168, 175, 176, 177, 220, 222, 223, 227, 228, 234, 271, 273], "co": [15, 56, 60, 62, 70, 71, 160, 167, 168, 175, 176, 177, 227, 228, 273], "easili": [15, 21, 23, 266, 276, 277, 279, 280], "img": 15, "when": [15, 16, 17, 19, 20, 21, 23, 24, 28, 52, 53, 54, 55, 56, 66, 70, 72, 74, 181, 182, 184, 188, 189, 190, 192, 193, 194, 195, 197, 198, 199, 205, 208, 219, 234, 236, 249, 251, 256, 267, 271, 273, 275, 276, 277, 278, 279, 280], "llava_instruct_dataset": 15, "concaten": [16, 20, 47, 52, 146, 150, 211, 215], "sequenc": [16, 44, 46, 47, 48, 49, 53, 56, 60, 64, 65, 70, 71, 74, 75, 90, 93, 94, 100, 105, 106, 115, 118, 119, 124, 128, 141, 142, 143, 145, 147, 149, 153, 155, 157, 159, 161, 164, 166, 168, 170, 174, 178, 181, 182, 184, 187, 188, 189, 190, 193, 198, 200, 213, 215, 217, 219, 223, 224, 240, 272], "upto": [16, 184], "maximum": [16, 23, 46, 49, 50, 53, 60, 71, 74, 77, 78, 80, 90, 93, 94, 100, 105, 106, 115, 118, 119, 124, 128, 143, 145, 146, 147, 149, 150, 153, 155, 157, 159, 161, 164, 166, 168, 170, 174, 181, 182, 184, 187, 188, 189, 193, 198, 200, 217, 226, 271], "length": [16, 44, 46, 48, 49, 50, 51, 52, 53, 60, 71, 74, 90, 93, 94, 100, 105, 106, 115, 118, 119, 124, 128, 141, 142, 143, 145, 147, 149, 153, 155, 157, 159, 161, 164, 166, 167, 168, 170, 174, 178, 181, 182, 184, 187, 188, 189, 193, 195, 197, 198, 200, 213, 217, 218, 219, 228, 240, 246, 278], "slow": [16, 278, 280], "down": [16, 190, 229, 277, 278, 280], "first": [16, 21, 23, 26, 41, 51, 53, 64, 75, 80, 146, 150, 188, 190, 191, 192, 198, 224, 227, 264, 266, 267, 272, 273, 275, 276, 277, 279, 280], "introduc": [16, 84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165, 182, 183, 200, 204, 223, 268, 272, 276, 277, 278, 279, 280], "signific": [16, 278, 279], "speedup": [16, 273, 275], "depend": [16, 24, 25, 227, 255, 271, 273, 276, 277, 278, 280], "done": [16, 53, 188, 208, 235, 245, 256, 277, 279, 280], "iter": [16, 255, 256, 257, 280], "through": [16, 17, 21, 22, 23, 24, 25, 54, 80, 146, 150, 179, 181, 190, 200, 205, 266, 267, 268, 269, 271, 272, 273, 274, 276, 278, 279, 280], "greedi": [16, 53], "upon": [16, 24, 52, 188, 192, 198, 275], "initi": [16, 21, 24, 28, 52, 53, 81, 82, 83, 91, 92, 101, 102, 103, 104, 116, 117, 125, 126, 127, 135, 136, 158, 160, 175, 176, 177, 220, 231, 242, 243, 256, 268, 274, 277, 280], "max": [16, 49, 53, 178, 188, 190, 198, 213, 226, 234, 271, 277], "make": [16, 18, 21, 22, 23, 24, 25, 144, 190, 266, 271, 273, 274, 275, 276, 277, 278, 279, 280], "sure": [16, 21, 23, 273, 274, 275, 276, 277, 278, 279, 280], "packeddataset": [16, 52, 57, 58, 59, 61, 63, 67, 68, 70, 71], "llama3": [16, 19, 20, 23, 64, 65, 72, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 143, 145, 147, 148, 149, 151, 164, 195, 197, 230, 236, 264, 266, 267, 268, 271, 273, 278], "load": [16, 21, 24, 34, 41, 45, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 198, 208, 227, 228, 229, 231, 249, 256, 273, 275, 277], "isinst": [16, 225], "line": [16, 21, 22, 24, 269, 271, 274, 275], "run": [16, 21, 22, 23, 25, 28, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 188, 194, 195, 227, 228, 229, 231, 232, 233, 243, 246, 249, 250, 251, 265, 266, 267, 268, 269, 272, 274, 275, 276, 277, 278, 279, 280], "full_finetune_single_devic": [16, 237, 271, 273, 274], "llama3_2": [16, 135, 136, 137, 138, 139, 140, 191, 192, 193, 230, 276], "1b_full_single_devic": 16, "posit": [16, 23, 26, 53, 73, 75, 77, 78, 79, 80, 90, 94, 124, 128, 141, 146, 150, 153, 155, 157, 159, 164, 166, 181, 182, 184, 187, 188, 189, 190, 198, 199, 275], "prevent": [16, 21, 53, 220, 271, 278], "irrelev": 16, "cross": [16, 49, 53, 145, 149, 187, 195, 197, 198, 200, 217, 276], "attend": [16, 53, 182, 187, 188, 189, 198, 217], "pytorch": [16, 23, 24, 73, 188, 194, 195, 225, 243, 249, 252, 254, 255, 264, 265, 266, 268, 273, 275, 277, 278, 279, 280], "flex": 16, "attent": [16, 49, 53, 73, 74, 75, 80, 84, 85, 86, 90, 94, 95, 96, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 145, 146, 148, 149, 150, 153, 154, 155, 156, 157, 159, 164, 165, 166, 167, 170, 171, 172, 173, 174, 181, 182, 184, 187, 188, 189, 191, 198, 200, 208, 209, 217, 275, 277, 278, 280], "flash": 16, "non": [16, 196, 197, 209, 219, 276], "causal": [16, 53, 74, 182, 188, 189, 198], "hardwar": [16, 235, 266, 273, 277, 278], "cuda": [16, 23, 235, 238, 255, 259, 265, 273, 278, 280], "devic": [16, 23, 24, 193, 231, 235, 238, 258, 259, 269, 271, 272, 273, 274, 275, 277, 278, 280], "ture": 16, "sdpa": 16, "memori": [16, 20, 24, 52, 53, 56, 60, 70, 71, 186, 188, 194, 195, 197, 198, 208, 236, 238, 244, 245, 255, 264, 266, 267, 268, 273, 274, 275, 276, 279], "effici": [16, 208, 236, 264, 266, 267, 273, 274, 277, 279], "fallback": 16, "while": [16, 23, 24, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 199, 266, 268, 273, 278, 279, 280], "retain": [16, 220, 278, 280], "reward": [17, 104, 110, 114, 156, 160, 163, 218, 219, 220, 222, 223, 230], "downstream": 17, "captur": 17, "optim": [17, 18, 21, 23, 24, 47, 52, 54, 90, 157, 167, 220, 221, 222, 223, 229, 231, 233, 234, 237, 238, 251, 255, 267, 268, 269, 272, 273, 274, 275, 276, 277, 280], "ground": [17, 195, 196, 197, 278], "truth": [17, 23, 195, 196, 197, 273, 275], "usual": [17, 20, 21, 184, 188, 224, 227, 240, 250, 271, 273, 277, 278], "outcom": 17, "binari": 17, "comparison": [17, 24, 277, 280], "annot": 17, "accord": [17, 18, 64, 65, 75, 152, 272], "criterion": 17, "style": [17, 30, 53, 57, 58, 59, 68, 200, 280], "interact": [17, 24, 54, 66, 264, 269, 274], "free": [17, 223, 269, 277], "preference_dataset": 17, "my_preference_dataset": [17, 66], "chosen_convers": [17, 66], "hole": [17, 66], "my": [17, 18, 22, 66, 72, 271, 272, 273, 275], "trouser": [17, 66], "fix": [17, 19, 66, 279], "rejected_convers": [17, 66], "off": [17, 24, 37, 66, 267, 268, 273, 279], "chosen": [17, 32, 54, 62, 66, 69, 220, 222, 223, 255], "reject": [17, 32, 54, 62, 66, 69, 220, 222, 223], "rejected_input_id": [17, 47, 66], "nwhat": 17, "ntake": 17, "rejected_label": [17, 47], "128006": 17, "78191": 17, "128007": 17, "271": 17, "18293": 17, "1124": 17, "1022": 17, "13": [17, 19, 20, 47, 190, 215, 224, 280], "128009": [17, 272], "accomplish": [17, 19, 52, 59, 63, 66, 70], "ve": [17, 20, 23, 181, 268, 271, 272, 273, 275, 276, 277, 278], "shown": [17, 273, 279], "di": 17, "look": [17, 18, 21, 23, 24, 233, 249, 265, 272, 273, 274, 275, 276, 277, 279], "anthrop": [17, 62], "harmless": [17, 62], "granni": 17, "her": [17, 19], "mobil": [17, 273], "phone": [17, 273], "issu": [17, 269, 279], "grandmoth": 17, "manag": [17, 21, 52, 192, 193, 205, 246, 253, 272], "behavior": [17, 21, 245, 272], "thing": [17, 278, 280], "grandma": 17, "feel": [17, 269, 277], "box": [17, 266, 268, 280], "hh_rlhf_helpful_dataset": 17, "ll": [17, 19, 21, 23, 24, 72, 239, 266, 268, 272, 273, 274, 275, 276, 278, 279, 280], "hendrydong": 17, "preference_700k": 17, "stack_exchange_paired_dataset": 17, "purpos": [18, 64, 65, 274, 275], "whenev": [18, 143, 195, 277], "llama2": [18, 21, 23, 24, 26, 60, 71, 81, 82, 83, 84, 85, 86, 87, 88, 89, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 143, 179, 188, 189, 230, 264, 267, 271, 274, 275, 278, 279], "were": [18, 19, 20, 52, 141, 190, 205, 221, 274, 279], "ensur": [18, 20, 21, 23, 29, 51, 54, 55, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182, 191, 227, 229, 235, 266, 274], "gear": [18, 143], "after": [18, 19, 22, 24, 37, 55, 64, 65, 93, 105, 118, 143, 147, 161, 168, 181, 182, 185, 186, 188, 189, 198, 200, 224, 245, 246, 247, 248, 249, 250, 268, 272, 273, 275, 279, 280], "summar": [18, 42, 67, 272, 278], "summarizetempl": [18, 67, 272], "commun": [18, 143, 273, 278], "chatmltempl": [18, 143, 178], "gec_templ": 18, "extend": [18, 20, 21, 24, 266], "customprompttempl": 18, "achiev": [18, 37, 251, 268, 273, 275, 276, 277, 279, 280], "prepend_tag": [18, 37], "append_tag": [18, 37], "thu": [18, 30, 37, 54, 55, 188, 278, 279], "now": [18, 21, 181, 192, 231, 233, 268, 272, 273, 274, 275, 276, 277, 279, 280], "empti": [18, 46, 49, 51, 76, 271], "standalon": [18, 181], "my_custom_templ": 18, "Is": 18, "overhyp": 18, "advanc": [18, 78, 79, 80, 146, 150, 190], "configur": [18, 20, 24, 54, 55, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 94, 106, 119, 128, 148, 153, 164, 170, 246, 266, 268, 269, 272, 274, 275, 276, 277, 278, 279, 280], "doesn": [18, 273], "neatli": 18, "fall": 18, "implement": [18, 20, 21, 24, 54, 56, 58, 59, 60, 61, 62, 63, 66, 67, 68, 69, 70, 71, 179, 183, 184, 185, 190, 196, 203, 204, 210, 211, 216, 220, 221, 222, 223, 227, 234, 239, 249, 266, 268, 276, 277, 278, 279, 280], "protocol": [18, 20, 203, 210, 211, 216], "arg": [18, 20, 23, 26, 38, 79, 180, 188, 194, 200, 203, 210, 211, 216, 248, 255, 268, 279], "whether": [18, 30, 32, 34, 35, 36, 41, 46, 49, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 84, 85, 86, 90, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 143, 144, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 174, 194, 196, 198, 204, 208, 209, 212, 213, 225, 235, 237, 238, 246, 256, 272, 276], "being": [18, 21, 55, 192, 227, 228, 229, 233, 259, 278, 279, 280], "sai": [18, 271, 272, 274], "eureka": 18, "eurekatempl": 18, "formatted_dialogu": 18, "llama2chattempl": [18, 105, 152, 178, 272], "paradigm": [19, 24, 267, 278], "unstructur": [19, 56, 70, 71], "unlabel": 19, "manner": 19, "text_complet": 19, "odyssei": 19, "clear": 19, "river": 19, "oceanu": 19, "had": 19, "got": [19, 49], "sea": 19, "went": 19, "till": 19, "reach": 19, "aeaean": 19, "island": 19, "dawn": 19, "sunris": 19, "drew": 19, "ship": 19, "sand": 19, "shore": 19, "sleep": 19, "wait": [19, 255], "break": [19, 143, 213], "child": 19, "morn": 19, "rosi": 19, "finger": 19, "appear": [19, 278], "sent": [19, 250], "men": 19, "circ": 19, "hous": 19, "fetch": [19, 277], "bodi": 19, "elpenor": 19, "cut": 19, "firewood": 19, "wood": 19, "headland": 19, "jut": 19, "wept": 19, "over": [19, 20, 24, 35, 55, 196, 197, 220, 234, 266, 268, 271, 273, 276, 277, 278, 280], "him": 19, "lament": 19, "funer": 19, "rite": 19, "armour": 19, "been": [19, 72, 74, 181, 188, 198, 224, 231, 236, 272, 279], "burn": 19, "ash": 19, "rais": [19, 21, 26, 29, 32, 34, 36, 41, 43, 45, 46, 49, 51, 52, 58, 59, 61, 63, 64, 65, 67, 68, 70, 74, 80, 170, 181, 182, 186, 187, 188, 190, 191, 192, 193, 208, 209, 215, 227, 228, 229, 231, 235, 237, 238, 242, 246, 250, 254, 256, 257, 258], "cairn": 19, "stone": 19, "top": [19, 73, 76, 146, 150, 233, 278, 280], "oar": 19, "he": 19, "row": [19, 54, 55, 182, 188, 189, 198], "text_completion_dataset": [19, 279], "128000": [19, 272, 279], "6153": 19, "584": 19, "1051": 19, "2867": 19, "279": 19, "15140": 19, "22302": 19, "355": 19, "11": [19, 21, 44, 46, 47, 190, 273, 279, 280], "323": 19, "1047": 19, "2751": 19, "704": 19, "1139": 19, "1825": 19, "9581": 19, "4024": 19, "389": 19, "12222": 19, "8813": 19, "362": 19, "12791": 19, "5420": 19, "13218": 19, "1405": 19, "1070": 19, "374": 19, "39493": 19, "64919": 19, "439": 19, "304": 19, "1023": 19, "7634": 19, "1226": 19, "1243": 19, "24465": 19, "1057": 19, "8448": 19, "311": 19, "70163": 19, "1077": 19, "31284": 19, "6212": 19, "30315": 19, "1938": 19, "1288": 19, "1464": 19, "128001": [19, 279], "similarli": [19, 118, 147, 168, 178, 279], "wikimedia": 19, "wikipedia": [19, 45, 71], "cnn_dailymail_articles_dataset": 19, "index": [20, 47, 48, 49, 52, 53, 182, 184, 189, 196, 198, 219, 234, 259, 265, 272, 273], "embed": [20, 21, 77, 78, 79, 80, 90, 94, 100, 106, 115, 119, 124, 128, 141, 142, 143, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 183, 184, 187, 188, 190, 198, 199, 200, 202, 236, 272, 275, 278, 279], "vector": [20, 222, 272], "understood": 20, "plai": [20, 273, 278], "necessari": [20, 21, 54, 55, 246, 247, 248, 249, 250, 272, 277], "phi3": [20, 21, 164, 165, 167, 168, 169, 230, 271], "phi3_mini_token": 20, "p_token": 20, "phi": [20, 167, 168, 230], "32010": 20, "29871": 20, "1792": [20, 215], "9508": [20, 215], "32007": 20, "32001": 20, "4299": 20, "2933": [20, 215], "nuser": 20, "nmodel": 20, "sentencepiec": [20, 212, 275], "tiktoken": [20, 143, 213, 275], "host": [20, 265, 271, 274, 278], "distribut": [20, 76, 231, 242, 243, 252, 254, 259, 266, 269, 271, 274, 275, 276, 278], "alongsid": [20, 236], "cd": [20, 265, 273], "ls": [20, 265, 269, 271, 273, 274, 275], "alreadi": [20, 23, 32, 36, 41, 61, 62, 64, 65, 66, 67, 68, 181, 182, 192, 193, 198, 230, 242, 265, 271, 273, 276, 277], "_token": [20, 24], "mistraltoken": [20, 161, 272], "adher": [20, 36, 41], "arbitrarili": 20, "small": [20, 183, 273], "seq": [20, 188, 198], "len": [20, 21, 49, 52, 58, 61, 64, 65, 67, 188, 190, 198], "demonstr": [20, 278, 279], "7": [20, 21, 44, 46, 47, 48, 49, 181, 190, 217, 221], "6312": 20, "28709": 20, "assign": [20, 23, 54, 55], "uniqu": [20, 54, 55, 230], "abil": 20, "experiment": [20, 23], "NOT": [20, 21, 90, 143, 157], "correctli": [20, 21, 24, 29, 208, 227, 265, 269, 272, 274, 280], "presenc": [20, 30], "certain": [20, 21, 23, 255, 272], "proper": [20, 265, 274], "end_of_text": 20, "special_token": [20, 143, 213, 272], "added_token": 20, "128257": 20, "128258": 20, "remain": [20, 36, 41, 234, 276, 277, 278], "special_tokens_path": [20, 118, 147, 168, 178], "basetoken": 20, "actual": [20, 22, 23, 25, 30, 32, 34, 54, 55, 58, 61, 62, 63, 65, 66, 67, 69, 143, 268, 272, 279], "string": [20, 21, 34, 35, 37, 43, 59, 60, 93, 105, 118, 143, 147, 161, 168, 178, 203, 210, 212, 213, 215, 226, 232, 235, 239, 246, 259, 271, 278], "kwarg": [20, 23, 26, 38, 178, 180, 187, 189, 194, 200, 203, 210, 211, 216, 242, 246, 247, 248, 249, 250, 252, 255], "dict": [20, 21, 23, 24, 25, 26, 30, 32, 34, 35, 36, 37, 41, 43, 46, 47, 48, 49, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 93, 105, 118, 143, 147, 161, 168, 178, 187, 189, 194, 198, 200, 201, 206, 207, 208, 209, 210, 211, 213, 214, 216, 227, 228, 229, 231, 233, 238, 242, 244, 246, 251, 256, 258], "given": [20, 24, 26, 43, 46, 51, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 75, 143, 192, 193, 204, 205, 210, 211, 219, 235, 239, 245, 251, 259, 261, 266, 277], "token_id": [20, 143, 210, 213], "its": [20, 53, 99, 152, 155, 182, 184, 188, 189, 198, 200, 231, 251, 254, 271, 272, 273, 275, 277, 278], "sentencepiecebasetoken": [20, 210], "bpe": 20, "sp_token": 20, "reason": [20, 24, 72, 273, 278, 279], "walk": [21, 24, 249, 266, 272, 273, 274, 279, 280], "design": [21, 24, 223], "cover": [21, 22, 23, 24, 25, 272, 273, 280], "scenario": [21, 52, 143], "compos": [21, 190], "plug": [21, 278], "evalu": [21, 24, 264, 266, 268, 269, 274, 276, 277, 280], "gener": [21, 24, 46, 53, 60, 70, 73, 74, 75, 76, 143, 192, 193, 205, 218, 237, 246, 253, 254, 255, 262, 264, 268, 272, 276, 277, 278, 279, 280], "easi": [21, 24, 266, 277, 278], "understand": [21, 23, 24, 200, 264, 266, 267, 272, 277, 278, 280], "concept": [21, 269, 273, 274, 278], "talk": 21, "close": [21, 24, 246, 247, 248, 249, 250, 277], "veri": [21, 52, 188, 198, 271, 273, 278], "dictat": 21, "state_dict": [21, 194, 199, 200, 208, 227, 228, 229, 230, 231, 256, 277, 280], "store": [21, 54, 55, 246, 247, 250, 277, 278, 280], "disk": [21, 56, 247], "identifi": [21, 246], "state": [21, 24, 142, 188, 190, 192, 194, 198, 201, 206, 207, 208, 209, 218, 220, 227, 228, 229, 231, 233, 256, 273, 275, 277, 278, 280], "match": [21, 43, 209, 246, 256, 265, 271, 273, 275, 277], "up": [21, 22, 24, 25, 49, 53, 60, 71, 143, 188, 192, 193, 198, 213, 217, 233, 246, 255, 267, 268, 269, 271, 272, 274, 275, 277, 278, 280], "exactli": [21, 209, 226, 279], "definit": [21, 277], "either": [21, 46, 54, 55, 72, 182, 188, 189, 209, 227, 246, 252, 265, 271, 277, 278, 279, 280], "explicit": 21, "error": [21, 23, 33, 51, 227, 254, 271], "except": [21, 35, 152, 215, 276], "wors": [21, 278], "silent": 21, "succe": 21, "popular": [21, 198, 266, 273], "offici": [21, 99, 272, 274, 275], "websit": 21, "inspect": [21, 273, 277, 280], "mmap": [21, 273], "weights_onli": [21, 229], "map_loc": [21, 273], "cpu": [21, 24, 193, 194, 235, 255, 259, 265, 271, 273, 278, 280], "tensor": [21, 44, 46, 47, 48, 49, 72, 73, 74, 75, 76, 77, 78, 79, 80, 141, 142, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 190, 194, 195, 196, 197, 198, 199, 200, 204, 218, 219, 220, 221, 222, 223, 224, 227, 240, 246, 247, 248, 249, 250, 253, 256, 258, 276, 277, 278, 280], "item": 21, "f": [21, 25, 58, 61, 64, 65, 67, 226, 272, 273, 276, 277, 280], "tok_embed": [21, 188, 198, 199], "32000": [21, 26, 277], "4096": [21, 26, 60, 71, 182, 184, 277, 279], "292": 21, "tabl": [21, 199, 272, 273, 275, 276, 278, 280], "layer": [21, 24, 80, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 100, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 142, 145, 146, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 160, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 182, 185, 186, 187, 188, 189, 190, 191, 192, 193, 198, 200, 202, 204, 208, 209, 225, 232, 236, 266, 267, 275, 277, 278, 279, 280], "dim": [21, 49, 141, 142, 179, 182, 183, 184, 188, 195, 197, 198, 276], "within": [21, 23, 26, 53, 72, 76, 77, 94, 106, 119, 128, 148, 149, 150, 153, 155, 164, 170, 190, 192, 193, 249, 254, 255, 271, 277, 280], "big": 21, "bin": [21, 271, 273], "piec": 21, "pytorch_model": [21, 273], "00001": [21, 271, 276], "00002": [21, 271, 276], "embed_token": 21, "241": 21, "Not": 21, "fewer": [21, 182], "sinc": [21, 23, 26, 54, 55, 197, 227, 229, 272, 273, 275, 278, 279], "mismatch": 21, "caus": [21, 212], "try": [21, 23, 272, 273, 274, 275, 280], "re": [21, 23, 192, 200, 223, 229, 266, 267, 268, 272, 273, 274, 277, 278], "care": [21, 227, 229, 273, 275, 277], "end": [21, 24, 35, 56, 70, 143, 213, 215, 264, 266, 272, 275, 277, 279], "number": [21, 24, 43, 49, 53, 60, 71, 72, 77, 78, 80, 90, 94, 100, 106, 115, 119, 124, 128, 141, 142, 143, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 188, 190, 196, 197, 217, 227, 228, 229, 231, 232, 234, 241, 254, 255, 271, 274, 276, 277, 278], "save": [21, 24, 25, 188, 194, 195, 197, 198, 227, 228, 229, 231, 236, 245, 250, 264, 268, 271, 272, 273, 275, 277, 278, 279], "less": [21, 49, 72, 273, 274, 275, 278, 280], "prone": 21, "invari": 21, "accept": [21, 23, 225, 274, 278, 280], "explicitli": [21, 203, 266, 277], "produc": [21, 231, 268, 279, 280], "One": [21, 49, 279], "advantag": [21, 218, 221, 268, 277], "abl": [21, 24, 273, 274, 279], "post": [21, 190, 251, 255, 268, 273, 275, 279, 280], "quantiz": [21, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 204, 229, 239, 264, 265, 267, 269, 274, 280], "eval": [21, 264, 266, 279], "without": [21, 23, 25, 182, 186, 188, 192, 198, 208, 265, 266, 268, 272, 273, 277, 278, 279], "OR": 21, "script": [21, 25, 269, 271, 273, 274, 275], "surround": [21, 24, 266], "load_checkpoint": [21, 24, 227, 228, 229, 230], "save_checkpoint": [21, 24, 25, 227, 228, 229], "permut": 21, "behav": 21, "further": [21, 190, 223, 271, 276, 277, 278, 279, 280], "illustr": [21, 64, 65, 275], "whilst": [21, 267, 278], "folder": 21, "read": [21, 227, 228, 229, 266, 278], "compat": [21, 227, 229, 278, 279], "framework": [21, 24, 266], "mention": [21, 273, 280], "assum": [21, 34, 41, 44, 46, 64, 93, 105, 118, 147, 161, 168, 178, 181, 182, 184, 189, 198, 199, 201, 206, 213, 231, 233, 234, 235, 237, 272, 273, 277], "checkpoint_dir": [21, 23, 227, 228, 229, 273, 275, 276, 279], "easiest": [21, 273, 274], "everyth": [21, 24, 266, 269, 274], "flow": [21, 53, 279, 280], "safetensor": [21, 226, 227, 271, 276], "output_dir": [21, 23, 227, 228, 229, 255, 273, 275, 277, 279, 280], "snippet": 21, "explain": [21, 278], "setup": [21, 23, 24, 74, 181, 182, 187, 188, 189, 191, 192, 193, 198, 200, 232, 255, 271, 273, 277, 280], "fullmodelhfcheckpoint": [21, 273, 276], "directori": [21, 23, 34, 41, 64, 227, 228, 229, 247, 249, 250, 255, 271, 273, 274, 275], "sort": [21, 227, 229], "order": [21, 22, 24, 227, 229, 249, 250, 274, 278], "matter": [21, 227, 229, 271, 277], "checkpoint_fil": [21, 23, 25, 227, 228, 229, 273, 275, 276, 277, 279, 280], "restart": [21, 271], "previou": [21, 53, 227, 228, 229, 276], "section": [21, 24, 238, 264, 273, 275, 278, 280], "recipe_checkpoint": [21, 227, 228, 229, 279], "model_typ": [21, 227, 228, 229, 273, 275, 279], "resume_from_checkpoint": [21, 227, 228, 229], "param": [21, 24, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 144, 148, 165, 171, 172, 173, 201, 202, 204, 206, 207, 209, 227, 277, 279, 280], "discrep": [21, 227], "github": [21, 26, 73, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 183, 184, 195, 196, 220, 221, 222, 223, 234, 265, 273, 275, 276], "repositori": [21, 54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 99, 267, 268, 273, 274], "fullmodelmetacheckpoint": [21, 275, 279], "test": [21, 23, 24, 72, 266, 268, 272, 278], "written": [21, 23, 24, 227, 228, 246, 247, 248, 249, 250, 266], "partit": [21, 227, 280], "key_1": [21, 229], "weight_1": 21, "key_2": 21, "weight_2": 21, "mid": 21, "chekpoint": 21, "middl": [21, 200, 273, 278], "subsequ": [21, 24, 181, 188, 190, 217], "recipe_st": [21, 227, 228, 229], "pt": [21, 25, 227, 228, 229, 273, 275, 276, 279], "epoch": [21, 24, 25, 227, 228, 229, 231, 234, 271, 272, 273, 274, 275, 279], "etc": [21, 24, 142, 227, 238, 274], "flood": 21, "overwritten": 21, "updat": [21, 23, 24, 37, 181, 182, 188, 198, 216, 220, 221, 227, 231, 255, 258, 265, 272, 273, 274, 275, 277, 278, 279, 280], "hf_model_0001_0": [21, 273, 276], "hf_model_0002_0": [21, 273], "adapt": [21, 84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 142, 153, 154, 155, 156, 164, 165, 198, 200, 201, 203, 204, 205, 206, 207, 227, 228, 229, 245, 267, 272, 273, 277, 280], "merg": [21, 26, 27, 178, 227, 273, 275, 280], "tutori": [21, 252, 266, 267, 268, 272, 273, 274, 275, 276, 277, 278, 279, 280], "save_adapter_weights_onli": 21, "choos": [21, 59, 277], "resum": [21, 24, 227, 228, 229, 234, 280], "frozen": [21, 142, 148, 151, 199, 220, 277, 278, 280], "learnt": [21, 272, 273], "refer": [21, 23, 24, 183, 184, 186, 190, 195, 205, 219, 220, 221, 222, 223, 246, 266, 277, 278, 279], "adapter_checkpoint": [21, 227, 228, 229], "adapter_0": [21, 273], "knowledg": [21, 264], "forward": [21, 24, 77, 78, 79, 141, 142, 179, 180, 182, 183, 184, 185, 187, 188, 189, 190, 192, 193, 195, 196, 197, 198, 199, 200, 204, 220, 221, 222, 223, 238, 255, 275, 276, 277, 278, 280], "modeltyp": [21, 227, 228, 229], "llama2_13b": [21, 107], "right": [21, 46, 49, 75, 188, 227, 273, 275, 277], "pytorch_fil": 21, "00003": [21, 226, 276], "torchtune_sd": 21, "load_state_dict": [21, 198, 199, 200, 208, 231, 256, 277], "successfulli": [21, 271, 274], "vocab": [21, 26, 178, 188, 198, 199, 275], "70": [21, 116], "x": [21, 44, 72, 73, 74, 77, 78, 79, 141, 142, 179, 180, 182, 183, 184, 185, 187, 188, 189, 190, 198, 199, 200, 204, 240, 253, 276, 277, 279, 280], "randint": 21, "no_grad": 21, "6": [21, 44, 46, 47, 48, 49, 53, 90, 94, 183, 190, 240, 268, 279, 280], "3989": 21, "9": [21, 44, 46, 47, 49, 181, 190, 240, 273, 279, 280], "0531": 21, "2375": 21, "5": [21, 23, 44, 46, 47, 48, 49, 74, 190, 220, 223, 224, 234, 273, 274, 275, 276, 278], "2822": 21, "4872": 21, "7469": 21, "8": [21, 44, 46, 47, 49, 58, 61, 64, 65, 67, 84, 85, 86, 87, 88, 89, 95, 96, 97, 98, 107, 108, 109, 110, 111, 112, 113, 114, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 154, 156, 162, 163, 165, 169, 171, 172, 173, 181, 190, 195, 197, 273, 276, 277, 278, 279, 280], "6737": 21, "0023": 21, "8235": 21, "6819": 21, "2424": 21, "0109": 21, "6915": 21, "3618": 21, "1628": 21, "8594": 21, "5857": 21, "1151": 21, "7808": 21, "2322": 21, "8850": 21, "9604": 21, "7624": 21, "6040": 21, "3159": 21, "5849": 21, "8039": 21, "9322": 21, "2010": [21, 190], "6824": 21, "8929": 21, "8465": 21, "3794": 21, "3500": 21, "6145": 21, "5931": 21, "find": [21, 22, 24, 25, 220, 271, 273, 274, 276, 277, 278], "hope": 21, "deeper": [21, 267, 268, 274, 278], "insight": [21, 273], "happi": [21, 273], "start": [22, 24, 25, 45, 75, 215, 230, 246, 265, 266, 272, 273, 274, 276, 278, 279], "cometlogg": 22, "checkpoint": [22, 23, 24, 194, 198, 200, 213, 226, 227, 228, 229, 230, 231, 232, 250, 252, 256, 266, 268, 271, 275, 276, 277, 279, 280], "workspac": [22, 25, 246], "seen": [22, 25, 277, 280], "screenshot": [22, 25], "instal": [22, 23, 25, 243, 246, 249, 250, 264, 271, 273, 274, 275, 276, 277, 278, 279, 280], "comet_ml": [22, 246], "featur": [22, 24, 25, 265, 266, 267, 268, 273, 274, 278], "pip": [22, 25, 246, 249, 250, 265, 273, 275, 278], "login": [22, 25, 246, 250, 271, 273], "metric_logg": [22, 23, 24, 25], "metric_log": [22, 23, 25, 246, 247, 248, 249, 250], "experiment_nam": [22, 246], "experi": [22, 23, 246, 250, 264, 266, 275, 276, 277], "grab": [22, 25, 275], "hyperparamet": [22, 223, 231, 266, 274, 277, 280], "tab": [22, 25], "asset": 22, "artifact": [22, 25, 255], "click": [22, 25], "pars": [23, 26, 27, 214, 269, 274], "effect": [23, 223, 276, 278, 279], "cli": [23, 25, 27, 28, 265, 267, 273, 274, 278], "prerequisit": [23, 272, 273, 274, 275, 276, 277, 279, 280], "Be": [23, 272, 273, 274, 275, 276, 277, 278, 279, 280], "familiar": [23, 272, 273, 274, 275, 276, 277, 279, 280], "fundament": [23, 279], "reproduc": [23, 246], "overridden": [23, 255], "quick": 23, "seed": [23, 24, 25, 254, 274, 279], "shuffl": [23, 53, 279], "dtype": [23, 24, 76, 181, 182, 187, 188, 189, 191, 192, 193, 194, 198, 200, 235, 253, 257, 273, 276, 278, 279, 280], "fp32": [23, 188, 195, 197, 278, 279, 280], "enable_fsdp": 23, "keyword": [23, 26, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 194, 272], "subfield": 23, "dotpath": [23, 93, 105, 118, 147, 161, 168, 178], "wish": [23, 181, 192, 256], "exact": [23, 26, 273], "normal": [23, 53, 143, 180, 182, 183, 187, 188, 189, 195, 196, 197, 212, 272, 277, 279, 280], "python": [23, 246, 250, 254, 260, 262, 271, 273, 279], "instanc": [23, 26, 52, 106, 119, 128, 148, 153, 155, 164, 170, 171, 172, 175, 176, 194, 201, 206, 207, 277], "cfg": [23, 24, 27, 28, 29], "under": [23, 255, 278, 280], "preced": [23, 26, 271, 275, 277], "throw": 23, "notic": [23, 77, 78, 79, 190, 272, 277], "miss": [23, 208, 209, 255, 277], "dictconfig": [23, 24, 26, 27, 28, 29, 246, 250, 255], "mean": [23, 143, 182, 183, 187, 188, 189, 198, 218, 245, 271, 272, 274, 277, 279], "llama2_token": [23, 272, 273], "llama2token": [23, 105], "512": [23, 80, 280], "overwrit": [23, 229, 256, 265, 271], "duplic": [23, 24, 266, 271], "sometim": 23, "resolv": [23, 27, 274], "alpaca": [23, 30, 52, 57, 58, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 276], "disklogg": 23, "log_dir": [23, 247, 249, 250], "verifi": [23, 235, 236, 259, 272, 274, 277], "properli": [23, 208, 243, 271], "wa": [23, 34, 41, 49, 74, 78, 79, 80, 146, 150, 190, 208, 272, 277, 279, 280], "cp": [23, 265, 271, 272, 273, 274, 275, 279], "7b_lora_single_devic": [23, 273, 274, 277, 280], "my_config": [23, 271], "guidelin": 23, "tempt": 23, "put": [23, 24, 269, 274, 277, 279], "much": [23, 199, 223, 273, 275, 277, 279, 280], "switch": 23, "encourag": [23, 223, 277, 278], "clariti": 23, "significantli": [23, 220, 267, 268, 278], "easier": [23, 273, 274], "dont": 23, "privat": 23, "parent": [23, 271], "guarante": 23, "stabil": [23, 195, 197, 266, 268, 278, 279, 280], "underscor": 23, "_alpaca": 23, "k1": [23, 24], "v1": [23, 24, 71], "k2": [23, 24], "v2": [23, 24, 246], "lora": [23, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 204, 205, 208, 209, 227, 245, 264, 266, 269, 272, 274, 275, 276], "lora_finetune_single_devic": [23, 267, 271, 272, 273, 274, 275, 276, 277, 278, 280], "my_model_checkpoint": 23, "file_1": 23, "file_2": 23, "my_tokenizer_path": 23, "nest": [23, 258], "dot": 23, "notat": [23, 49, 141, 142, 182, 184, 188, 198, 218, 219, 240], "flag": [23, 24, 35, 58, 59, 61, 63, 66, 67, 68, 225, 229, 236, 271, 278, 280], "bitsandbyt": [23, 278], "pagedadamw8bit": [23, 278], "delet": [23, 188, 191, 192, 193, 198], "foreach": 23, "8b_full": [23, 271], "adamw": [23, 277, 278], "2e": [23, 278], "fuse": [23, 145, 149, 198, 199, 200, 201, 251, 279], "nproc_per_nod": [23, 268, 275, 277, 279], "full_finetune_distribut": [23, 237, 271, 273, 274], "thought": [24, 266, 269, 274, 280], "target": [24, 74, 196, 197, 223, 266, 276], "pipelin": [24, 266, 268], "eg": [24, 188, 198, 227, 266], "meaning": [24, 266, 273], "fsdp": [24, 186, 225, 231, 236, 245, 274, 275], "activ": [24, 80, 179, 232, 238, 244, 252, 255, 266, 268, 279, 280], "gradient": [24, 196, 197, 245, 251, 255, 266, 268, 273, 275, 277, 280], "accumul": [24, 251, 255, 266, 268], "mix": [24, 180, 271, 273, 278], "precis": [24, 180, 194, 235, 266, 268, 274, 280], "complex": 24, "becom": [24, 190, 265], "harder": 24, "anticip": 24, "architectur": [24, 99, 152, 188, 190, 198, 200, 230, 271], "methodolog": 24, "possibl": [24, 53, 226, 271, 278], "trade": [24, 278], "vs": [24, 274], "qualiti": [24, 273, 277, 279], "believ": 24, "suit": [24, 274, 278], "solut": 24, "result": [24, 64, 80, 146, 150, 190, 197, 215, 217, 255, 268, 273, 275, 276, 277, 278, 279, 280], "meant": [24, 194, 231], "level": [24, 54, 55, 195, 197, 216, 233, 245, 260, 266, 276, 280], "expertis": 24, "routin": 24, "yourself": [24, 271, 275, 277], "exist": [24, 193, 200, 231, 246, 265, 271, 273, 274, 275, 280], "ones": [24, 49, 181], "modular": [24, 266], "build": [24, 70, 80, 90, 100, 115, 124, 145, 146, 149, 150, 157, 159, 174, 226, 266, 275, 277], "block": [24, 53, 84, 85, 86, 90, 94, 95, 96, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 145, 148, 149, 150, 153, 154, 155, 156, 157, 164, 165, 170, 171, 172, 173, 174, 182, 188, 189, 208, 209, 266], "wandb": [24, 25, 250, 274], "log": [24, 27, 220, 221, 222, 223, 238, 244, 246, 247, 248, 249, 250, 260, 273, 274, 275, 276, 277, 278, 280], "fulli": [24, 52, 148], "nativ": [24, 264, 266, 277, 279, 280], "numer": [24, 65, 266, 268, 279], "pariti": [24, 266], "verif": [24, 183], "extens": [24, 229, 266], "benchmark": [24, 254, 266, 273, 275, 277, 279], "limit": [24, 231, 276, 278, 279], "hidden": [24, 80, 142, 146, 150, 179, 188, 190], "behind": 24, "unnecessari": 24, "abstract": [24, 210, 211, 266, 274, 280], "No": [24, 229, 266], "go": [24, 80, 99, 146, 150, 152, 190, 215, 266, 273, 274, 276, 278, 280], "figur": [24, 277, 280], "spectrum": 24, "decid": 24, "avail": [24, 34, 41, 71, 198, 200, 235, 243, 259, 266, 271, 273, 275, 277, 278], "consist": [24, 32, 36, 41, 64, 65, 71, 269, 274], "overrid": [24, 27, 28, 32, 36, 41, 61, 62, 64, 65, 66, 67, 68, 256, 269, 271, 273, 274, 275, 276, 280], "valid": [24, 51, 75, 196, 208, 209, 219, 237, 256, 257, 265, 269, 273, 274], "environ": [24, 243, 246, 259, 265, 269, 271, 273, 274, 279], "closer": [24, 276, 277], "monolith": [24, 266], "trainer": [24, 220, 222, 223], "wrapper": [24, 180, 212, 213, 231, 233, 271, 277], "around": [24, 143, 180, 212, 213, 238, 271, 272, 273, 277, 278, 279, 280], "extern": 24, "primarili": [24, 52, 277], "eleutherai": [24, 71, 266, 276, 277, 279], "har": [24, 266, 276, 277, 279], "stage": [24, 190], "distil": [24, 264], "dataload": [24, 53, 58, 61, 64, 65, 67], "applic": [24, 227, 228, 250], "clean": [24, 25, 57, 276], "group": [24, 182, 241, 242, 246, 247, 248, 249, 250, 271, 275, 279], "init_process_group": [24, 242], "backend": [24, 271, 279], "gloo": 24, "nccl": 24, "fullfinetunerecipedistribut": 24, "cleanup": 24, "stuff": 24, "carri": [24, 55], "metric": [24, 274, 276, 278, 279], "logger": [24, 244, 246, 247, 248, 249, 250, 260, 274], "_devic": 24, "get_devic": 24, "_dtype": 24, "get_dtyp": 24, "ckpt_dict": 24, "wrap": [24, 200, 225, 232, 236, 245, 252, 272], "_model": [24, 231], "_setup_model": 24, "_setup_token": 24, "_optim": 24, "_setup_optim": 24, "_loss_fn": 24, "_setup_loss": 24, "_sampler": 24, "_dataload": 24, "_setup_data": 24, "backward": [24, 231, 233, 251, 255, 280], "zero_grad": 24, "curr_epoch": 24, "rang": [24, 199, 220, 221, 223, 254, 271, 275, 279], "epochs_run": [24, 25], "total_epoch": [24, 25], "idx": [24, 53], "enumer": 24, "_autocast": 24, "logit": [24, 72, 73, 76, 195, 196, 197, 240, 276], "global_step": 24, "_log_every_n_step": 24, "_metric_logg": 24, "log_dict": [24, 246, 247, 248, 249, 250], "step": [24, 53, 54, 55, 64, 65, 188, 198, 218, 231, 233, 234, 246, 247, 248, 249, 250, 251, 255, 264, 268, 273, 277, 279, 280], "decor": [24, 28], "recipe_main": [24, 28], "fullfinetunerecip": 24, "wandblogg": [25, 277, 280], "tip": 25, "straggler": 25, "background": 25, "crash": 25, "otherwis": [25, 44, 46, 49, 78, 79, 80, 146, 150, 186, 188, 190, 243, 246, 272, 279], "exit": [25, 192, 193, 205, 265, 271], "resourc": [25, 246, 247, 248, 249, 250, 278, 279], "kill": 25, "ps": 25, "aux": 25, "grep": 25, "awk": 25, "xarg": 25, "desir": [25, 54, 55, 253, 272, 278], "suggest": [25, 276], "approach": [25, 52, 276], "full_finetun": 25, "joinpath": 25, "_checkpoint": [25, 273], "_output_dir": [25, 227, 228, 229], "torchtune_model_": 25, "with_suffix": 25, "wandb_at": 25, "descript": [25, 271], "whatev": 25, "metadata": [25, 279], "seed_kei": 25, "epochs_kei": 25, "total_epochs_kei": 25, "max_steps_kei": 25, "max_steps_per_epoch": [25, 279], "add_fil": 25, "log_artifact": 25, "hydra": 26, "facebook": 26, "research": 26, "com": [26, 73, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173, 183, 184, 195, 196, 220, 221, 222, 223, 234, 246, 265, 273, 275, 276], "facebookresearch": [26, 183], "blob": [26, 73, 84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 168, 171, 172, 173, 183, 184, 196, 220, 221, 222, 223, 234], "main": [26, 28, 168, 183, 184, 265, 268, 273, 275], "_intern": 26, "_instantiate2": 26, "l148": 26, "omegaconf": 26, "num_lay": [26, 80, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 188, 190, 198, 200], "32": [26, 181, 190, 198, 200, 246, 275, 277, 278, 279, 280], "num_head": [26, 80, 90, 94, 100, 106, 115, 119, 124, 128, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182, 184, 188], "num_kv_head": [26, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 181, 182], "vocab_s": [26, 72, 73, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 195, 196, 197, 199], "nn": [26, 44, 46, 49, 80, 141, 142, 179, 181, 182, 186, 187, 188, 189, 190, 191, 192, 193, 194, 198, 199, 200, 201, 202, 203, 205, 206, 207, 225, 232, 233, 245, 251, 252, 256, 257, 276, 277, 280], "parsed_yaml": 26, "embed_dim": [26, 77, 78, 79, 80, 90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182, 184, 187, 188, 189, 190, 199, 200, 256, 277], "valueerror": [26, 32, 34, 36, 41, 43, 45, 46, 49, 51, 52, 58, 59, 61, 63, 64, 65, 67, 68, 70, 170, 181, 182, 190, 191, 192, 193, 227, 228, 229, 235, 238, 254, 257], "recipe_nam": 27, "rank": [27, 84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 204, 241, 243, 254, 267, 274, 277, 280], "zero": [27, 49, 181, 183, 188, 198, 226, 273, 275, 279], "displai": 27, "callabl": [28, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 80, 188, 205, 225, 236, 239, 245, 252], "With": [28, 273, 276, 277, 279, 280], "my_recip": 28, "foo": 28, "bar": [28, 266, 274, 278], "instanti": [29, 37, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 96, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 124, 125, 126, 127, 128, 129, 130, 131, 135, 136, 137, 138, 144, 145, 146, 147, 148, 149, 150, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 165, 166, 167, 168, 170, 171, 172, 173, 174, 175, 176, 177, 178, 231], "configerror": 29, "cannot": [29, 45, 229, 275], "equival": [30, 34, 78, 222, 223], "condit": [30, 72, 243, 271], "dedic": 30, "due": [30, 212, 277, 278, 280], "keep": [30, 32, 34, 36, 41, 62, 63, 65, 66, 69, 199, 273, 277, 278], "openai": [31, 36, 59, 221], "markup": 31, "im_start": 31, "context": [31, 167, 192, 193, 205, 253, 255, 278], "im_end": 31, "goe": [31, 205], "a2": [32, 54], "present": [32, 36, 41, 61, 62, 64, 65, 66, 67, 68, 213, 229, 256], "functool": [33, 39, 42, 225], "partial": [33, 39, 42, 225], "_prompt_templ": [33, 39, 42], "assistant_messag": [33, 39, 42], "respect": [34, 52, 99, 181, 207, 255, 272], "final": [34, 41, 54, 55, 84, 85, 86, 90, 94, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 145, 146, 148, 149, 150, 153, 154, 155, 156, 157, 164, 165, 170, 173, 174, 179, 188, 198, 208, 209, 273, 275, 276, 277, 278, 280], "leav": [34, 41, 278], "liter": [35, 37, 40, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 147, 148, 149, 150, 151, 153, 154, 155, 156, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 178, 208, 209], "union": [35, 45, 46, 57, 58, 59, 61, 63, 67, 68, 70, 71, 93, 105, 118, 147, 161, 168, 178, 188, 198, 209, 227, 232, 237, 246, 247, 248, 249, 250, 252, 254], "interleav": [35, 217], "attach": 35, "writer": 35, "calcul": [35, 37, 75, 141, 143, 182, 187, 189, 190, 218, 219, 221, 275], "consecut": [35, 51, 181, 217], "last": [35, 50, 53, 70, 188, 219, 231, 234], "properti": [35, 277, 278], "media": [35, 55], "classmethod": 35, "construct": [35, 62, 217, 269, 277], "image_url": 36, "unmask": [36, 41, 196], "consid": [37, 52, 54, 55, 78, 79, 80, 146, 150, 190, 278], "come": [37, 51, 203, 277, 278], "nanswer": 39, "alia": [40, 225], "alwai": [41, 246, 256, 272, 278], "nsummari": [42, 272], "summari": [42, 52, 67, 190, 238], "batch_first": 44, "padding_valu": 44, "float": [44, 72, 73, 76, 84, 85, 86, 87, 88, 89, 90, 94, 95, 96, 97, 98, 100, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 143, 148, 149, 150, 151, 153, 154, 155, 156, 157, 159, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 182, 183, 204, 218, 219, 220, 221, 222, 223, 231, 234, 237, 238, 244, 246, 247, 248, 249, 250, 277, 278, 279, 280], "rnn": [44, 46, 49], "pad_sequ": [44, 46, 49], "variabl": [44, 230, 243, 246, 278, 280], "left": [44, 46, 49, 143, 188, 277], "longest": [44, 48, 49], "trail": 44, "dimens": [44, 49, 90, 94, 100, 106, 115, 119, 124, 128, 141, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 179, 181, 182, 184, 188, 190, 199, 204, 275, 277, 278, 280], "element": [44, 46, 49, 52, 196, 240, 273], "12": [44, 46, 47, 68, 190, 265, 279], "image_loc": 45, "www": [45, 246], "org": [45, 64, 81, 82, 83, 84, 85, 87, 88, 89, 94, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 114, 119, 120, 121, 122, 123, 130, 131, 132, 133, 134, 137, 138, 139, 140, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 182, 183, 184, 190, 217, 218, 220, 221, 222, 223, 225, 243, 249, 252, 254, 260, 265], "en": [45, 56, 60, 62, 70, 71, 279], "pad_direct": [46, 49], "keys_to_pad": 46, "padding_idx": [46, 47, 48, 49, 53], "left_pad_sequ": [46, 49], "integ": [46, 48, 199, 225, 226, 232, 254], "batch_siz": [46, 58, 61, 64, 65, 67, 181, 182, 187, 188, 189, 191, 192, 193, 195, 196, 197, 198, 199, 200, 220, 222, 224, 273, 278, 279], "ignore_idx": [47, 48, 49], "input_id": [47, 240], "chosen_input_id": [47, 66], "chosen_label": 47, "15": [47, 190, 236, 272, 273, 277, 280], "16": [47, 84, 85, 86, 87, 88, 89, 95, 96, 97, 98, 107, 108, 109, 110, 111, 112, 113, 114, 120, 121, 122, 123, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 154, 156, 162, 163, 165, 169, 171, 172, 173, 181, 190, 277, 280], "17": [47, 190, 277], "18": [47, 190, 275], "19": [47, 190, 280], "20": [47, 190, 224, 279], "token_pair": 48, "padded_col": 48, "pad_max_til": 49, "pad_max_imag": 49, "tile": [49, 77, 78, 79, 80, 141, 142, 143, 144, 146, 147, 148, 150, 190, 217], "aspect": [49, 77, 78, 266], "ratio": [49, 77, 78, 220, 221], "text_seq_len": [49, 217], "n_tile": [49, 77, 78, 190], "h": [49, 141, 181, 190, 195, 197, 265, 271], "w": [49, 81, 82, 83, 91, 92, 101, 102, 103, 104, 116, 117, 125, 126, 127, 135, 136, 141, 158, 160, 175, 176, 177, 190, 246, 249, 250, 272, 273, 277, 280], "h_ratio": 49, "w_ratio": 49, "encoder_mask": [49, 187, 188, 198], "image_seq_len": [49, 217], "channel": [49, 80, 141, 143, 146, 150, 190, 279], "height": [49, 141], "largest": 49, "bsz": [49, 72, 73, 74, 75, 77, 78, 190, 195, 197], "max_num_imag": 49, "max_num_til": [49, 77, 78, 80, 143, 146, 150, 190, 217], "tokens_per_til": 49, "image_id": 49, "four": [49, 277], "model_input": 49, "max_text_seq_len": 49, "40": [49, 78, 79, 80, 146, 150, 190, 217, 278, 280], "did": [49, 275, 280], "extra": [49, 143, 198, 265, 272, 277, 278, 279, 280], "second": [49, 182, 199, 273, 277, 278, 280], "eos_id": [50, 143, 213, 215], "shorter": [51, 188], "min": [51, 277], "invalid": 51, "sub": [52, 249], "unifi": [52, 160], "simplifi": [52, 220, 271, 276, 277], "simultan": 52, "intern": 52, "aggreg": 52, "transpar": 52, "howev": [52, 168, 265, 276, 278], "constitu": 52, "might": [52, 192, 199, 202, 271, 273, 278], "larg": [52, 195, 197, 204, 255, 271, 278, 280], "comput": [52, 54, 55, 100, 106, 115, 119, 124, 128, 141, 142, 145, 149, 170, 174, 182, 184, 188, 189, 195, 197, 198, 217, 220, 222, 223, 238, 254, 268, 273, 276, 278, 279, 280], "cumul": 52, "maintain": [52, 200, 267, 278, 280], "deleg": 52, "retriev": [52, 54, 55, 188, 236], "lead": [52, 212, 226, 268], "high": [52, 54, 55, 266, 276, 277, 278], "scale": [52, 72, 73, 76, 84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 183, 185, 187, 189, 204, 219, 223, 277, 278, 279, 280], "strategi": [52, 268], "stream": [52, 260, 278], "demand": 52, "deriv": [52, 179, 188, 189], "instans": 52, "dataset1": 52, "mycustomdataset": 52, "params1": 52, "dataset2": 52, "params2": 52, "concat_dataset": 52, "total": [52, 219, 221, 234, 241, 263, 270, 273, 275, 276, 277, 278], "data_point": 52, "1500": 52, "vicgal": 52, "gpt4": 52, "samsum": [52, 67], "focus": [52, 269, 274, 278], "enhanc": [52, 190, 223, 278, 280], "divers": 52, "machin": [52, 222, 259, 271, 273], "max_pack": 53, "split_across_pack": [53, 70], "pack": [53, 57, 58, 59, 61, 63, 64, 65, 67, 68, 70, 71, 182, 184, 188, 189, 198, 279], "outsid": [53, 254, 255, 277], "sampler": [53, 274], "part": [53, 199, 222, 272, 280], "buffer": [53, 188, 198, 278], "enough": [53, 272], "lower": [53, 268, 276, 277], "triangular": 53, "wise": 53, "made": [53, 59, 63, 66, 70, 143, 273], "smaller": [53, 199, 273, 275, 276, 277, 278, 279, 280], "jam": 53, "s1": [53, 212], "s2": [53, 212], "s3": 53, "s4": 53, "contamin": 53, "input_po": [53, 73, 182, 184, 188, 189, 198], "matrix": [53, 187, 188, 198], "increment": 53, "move": [53, 70, 188, 258, 278], "entir": [53, 70, 195, 202, 245, 272, 280], "avoid": [53, 70, 183, 190, 194, 254, 271, 279, 280], "truncat": [53, 60, 70, 71, 93, 105, 118, 143, 147, 161, 168, 178, 213, 224], "sentenc": [53, 70], "filter_fn": [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], "techniqu": [54, 266, 267, 268, 273, 274, 275, 276, 277, 278, 279], "separ": [54, 200, 215, 227, 272, 274, 275, 277, 280], "repons": 54, "At": [54, 55, 188, 198], "extract": [54, 55, 60, 214], "against": [54, 55, 223, 261, 279, 280], "unit": [54, 55, 245, 266], "filepath": [54, 55, 56, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71], "filter": [54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 279], "prior": [54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 256], "doc": [54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 225, 243, 246, 249, 250, 254, 260, 271, 273], "round": [55, 279], "incorpor": [55, 220], "happen": [55, 195, 197], "ti": [55, 94, 170, 174, 186, 278], "agnost": 55, "treat": [55, 190, 205, 272], "minimum": [55, 64, 65], "corpu": [56, 60, 70, 71], "package_refer": [56, 60, 62, 70, 71], "loading_method": [56, 60, 62, 70, 71], "tabular": [56, 70], "txt": [56, 70, 178, 247, 274], "eo": [56, 70, 168, 212, 215, 272], "yahma": 57, "variant": [57, 61, 67], "page": [57, 71, 265, 266, 271, 274, 275, 278], "tatsu": 58, "lab": [58, 73], "codebas": [58, 273], "independ": 58, "contribut": [58, 59, 61, 63, 66, 67, 68, 196, 197, 219, 221], "alpacatomessag": 58, "alpaca_d": 58, "altern": [59, 63, 66, 192, 274, 278], "friendli": [59, 63, 66, 70, 72, 272], "recommend": [59, 60, 61, 66, 67, 69, 71, 152, 188, 195, 246, 249, 272, 273, 278, 280], "toward": [59, 223], "my_dataset": [59, 63], "london": [59, 63], "ccdv": 60, "cnn_dailymail": 60, "textcompletiondataset": [60, 70, 71], "cnn": 60, "dailymail": 60, "articl": [60, 71], "highlight": [60, 280], "disabl": [60, 71, 188, 192, 198, 205, 254, 279], "highest": [60, 71], "conjunct": [61, 67, 69, 188, 278], "grammar_d": 61, "rlhflow": 62, "hh": 62, "preferencedataset": [62, 66, 69], "liuhaotian": 64, "llava": 64, "150k": 64, "coco": 64, "train2017": 64, "llava_instruct_150k": 64, "2017": 64, "visit": [64, 273], "cocodataset": 64, "wget": 64, "zip": [64, 262], "unzip": 64, "minim": [64, 65, 274, 276, 277, 279, 280], "clip": [64, 65, 77, 78, 79, 80, 141, 142, 143, 146, 150, 190, 221], "mymodeltransform": [64, 65], "tokenizer_path": [64, 65], "image_transform": [64, 65], "yet": [64, 65, 152, 272, 273], "llava_instruct_d": 64, "huggingfacem4": 65, "the_cauldron": 65, "cauldron": 65, "card": 65, "cauldron_d": 65, "compris": 66, "share": [66, 182, 186, 273], "c1": 66, "r1": 66, "chosen_messag": 66, "rejected_messag": 66, "samsung": 67, "samsum_d": 67, "351": 68, "82": 68, "391": 68, "221": 68, "220": 68, "193": 68, "471": 68, "lvwerra": 69, "stack": [69, 190, 255], "exchang": 69, "allenai": [70, 279], "data_dir": 70, "realnewslik": 70, "wikitext_document_level": 71, "wikitext": [71, 279], "103": [71, 273], "transformerdecod": [72, 73, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 145, 149, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 187, 189, 198, 199, 200, 277], "max_generated_token": 72, "pad_id": [72, 224], "temperatur": [72, 73, 76, 220, 222, 223, 273], "top_k": [72, 73, 76, 273], "stop_token": [72, 224], "rng": 72, "custom_generate_next_token": 72, "seq_length": [72, 73, 74, 187, 189, 199, 200], "prune": [72, 76, 280], "probabl": [72, 76, 84, 85, 86, 94, 95, 96, 106, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 204, 220, 221, 222, 223, 273, 276], "stop": [72, 224], "random": [72, 190, 254, 274], "compil": [72, 195, 273, 275, 278, 280], "generate_next_token": 72, "llama3_8b": [72, 121, 129, 198, 275, 278, 279], "manual_se": 72, "tolist": 72, "jeremi": 72, "m": [72, 194, 272, 279], "seq_len": [72, 74, 75, 188], "num_generated_token": 72, "q": [73, 76, 182, 277], "randomli": [73, 76, 256], "softmax": [73, 76, 182, 188, 189, 198, 276], "trick": [73, 76], "fast": [73, 273], "32971d3129541c5bfb4f715abc33d1c5f408d204": 73, "l40": 73, "k": [73, 76, 78, 182, 277], "padding_mask": [74, 75, 221, 224], "target_seq_len": 74, "suitabl": 74, "scaled_dot_product_attent": [74, 90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174, 182], "static": 74, "kv": [74, 181, 182, 188, 189, 191, 192, 193, 198, 279], "cach": [74, 181, 182, 184, 187, 188, 189, 191, 192, 193, 198, 200, 265, 271], "longer": [74, 181, 278], "boolean": [74, 75, 80, 182, 187, 188, 189, 198, 200, 225, 240], "assertionerror": [74, 80, 181, 187, 188, 208, 209, 256], "shift": [75, 188], "uniform_": 76, "int32": 76, "patch": [77, 78, 79, 80, 142, 143, 146, 150, 190, 217], "check": [77, 78, 79, 80, 187, 188, 189, 190, 198, 200, 208, 235, 243, 261, 264, 266, 267, 268, 269, 272, 273, 274, 277, 278], "vision_transform": [77, 78, 79, 80], "visiontransform": [77, 78, 79, 80], "divid": [77, 78, 79, 80, 143, 146, 150, 190, 196, 197, 217], "dimension": [77, 78, 79, 80, 146, 150, 190], "n_img": [77, 78, 190], "n_tokens_per_til": [77, 78, 79], "crop": [77, 78, 79, 80, 141, 146, 150, 190], "local_token_positional_embed": 78, "_position_embed": [78, 190], "tokenpositionalembed": [78, 190], "gate": [78, 185, 230, 267, 268, 271, 274], "global_token_positional_embed": 78, "400": [78, 79, 80, 146, 150, 190, 217], "10x10": [78, 79, 80, 146, 150, 190, 217], "grid": [78, 79, 80, 146, 150, 190, 217], "th": [78, 181], "silu": [80, 179], "cls_output_dim": [80, 190], "attn_bia": 80, "out_indic": [80, 190], "output_cls_project": 80, "in_channel": [80, 146, 150, 190], "intermediate_act": 80, "transformerencoderlay": 80, "cl": [80, 142, 190], "mlp": [80, 84, 85, 86, 90, 94, 95, 96, 100, 106, 107, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 145, 148, 149, 150, 153, 154, 155, 156, 157, 159, 164, 165, 166, 170, 171, 172, 173, 174, 187, 188, 189, 208, 209, 275, 277, 278], "bia": [80, 186, 203, 204, 256, 277, 279, 280], "intermedi": [80, 90, 94, 100, 106, 115, 119, 124, 128, 145, 146, 149, 150, 153, 155, 157, 159, 164, 166, 170, 174, 190, 229, 252, 275, 280], "fourth": [80, 146, 150, 190], "determin": [80, 146, 150, 209], "divis": [80, 183], "code_llama2": [81, 82, 83, 84, 85, 86, 87, 88, 89, 271], "arxiv": [81, 82, 83, 84, 85, 87, 88, 89, 94, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 114, 119, 120, 121, 122, 123, 130, 131, 132, 133, 134, 137, 138, 139, 140, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 182, 183, 184, 190, 217, 218, 220, 221, 222, 223], "pdf": [81, 82, 83, 217, 218], "2308": [81, 82, 83], "12950": [81, 82, 83], "lora_attn_modul": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 208, 209, 267, 277, 278, 280], "q_proj": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 182, 208, 209, 267, 277, 278, 279, 280], "k_proj": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 182, 208, 209, 267, 277, 278, 279, 280], "v_proj": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 182, 208, 209, 267, 277, 278, 279, 280], "output_proj": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 182, 208, 209, 277, 278, 279, 280], "apply_lora_to_mlp": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 208, 209, 267, 277, 278], "apply_lora_to_output": [84, 85, 86, 87, 88, 89, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 173, 208, 209, 277, 278], "lora_rank": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 267, 277, 278], "lora_alpha": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 267, 277, 278], "lora_dropout": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 278], "use_dora": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 130, 131, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173], "quantize_bas": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 204, 280], "code_llama2_13b": 84, "tloen": [84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173], "8bb8579e403dc78e37fe81ffbb253c413007323f": [84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173], "l41": [84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173], "l43": [84, 85, 86, 95, 96, 107, 108, 109, 110, 120, 121, 129, 130, 131, 137, 138, 165, 171, 172, 173], "linear": [84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 128, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 142, 148, 149, 150, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 170, 171, 172, 173, 186, 188, 203, 204, 208, 209, 277, 278, 279, 280], "low": [84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 204, 267, 273, 276, 277, 280], "approxim": [84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 204, 277], "factor": [84, 85, 86, 94, 95, 96, 106, 107, 108, 109, 110, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 164, 165, 170, 171, 172, 173, 204, 218, 273], "dropout": [84, 85, 86, 90, 94, 95, 96, 100, 106, 108, 109, 110, 115, 119, 120, 121, 124, 128, 129, 130, 131, 137, 138, 148, 149, 150, 153, 154, 155, 156, 157, 159, 164, 165, 166, 170, 171, 172, 173, 174, 182, 204, 277, 278, 280], "decompos": [84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165], "magnitud": [84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165, 278], "dora": [84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 128, 130, 131, 137, 138, 149, 150, 153, 154, 155, 156, 164, 165], "ab": [84, 85, 87, 88, 89, 94, 95, 96, 97, 98, 101, 102, 103, 104, 106, 107, 108, 109, 111, 112, 113, 114, 119, 120, 121, 122, 123, 130, 131, 132, 133, 134, 137, 138, 139, 140, 151, 153, 154, 155, 156, 162, 163, 164, 165, 169, 182, 183, 184, 190, 220, 221, 222, 223], "2402": [84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165], "09353": [84, 85, 94, 95, 96, 106, 107, 108, 109, 119, 120, 121, 130, 131, 137, 138, 153, 154, 155, 156, 164, 165], "code_llama2_70b": 85, "code_llama2_7b": 86, "qlora": [87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 194, 264, 266, 267, 275, 277], "paper": [87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 217, 220, 222, 223, 276, 277, 280], "2305": [87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169, 182, 220, 222], "14314": [87, 88, 89, 97, 98, 111, 112, 113, 114, 122, 123, 132, 133, 134, 139, 140, 151, 162, 163, 169], "lora_code_llama2_13b": 87, "lora_code_llama2_70b": 88, "lora_code_llama2_7b": 89, "head_dim": [90, 94, 181, 182, 188], "intermediate_dim": [90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174], "attn_dropout": [90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174, 182, 188], "norm_ep": [90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174], "1e": [90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174, 183, 276], "06": [90, 94, 183, 277], "rope_bas": [90, 94, 100, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174], "10000": [90, 94, 100, 153, 155, 157, 159, 164, 166, 184], "norm_embed": [90, 94], "transformerselfattentionlay": [90, 100, 115, 124, 157, 174, 187, 188, 198, 200], "rm": [90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174], "norm": [90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 188], "space": [90, 100, 115, 124, 145, 149, 157, 174, 188, 202, 278], "slide": [90, 157, 167], "window": [90, 157, 167], "vocabulari": [90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 195, 197, 277, 278], "mha": [90, 94, 100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182, 188], "onto": [90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174, 182, 202], "epsilon": [90, 94, 100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174, 221], "rotari": [90, 94, 100, 124, 128, 153, 155, 157, 159, 164, 166, 184, 275], "10_000": [90, 94, 153, 155, 157, 159, 166], "blog": [91, 92], "technolog": [91, 92], "develop": [91, 92, 265, 280], "gemmatoken": 93, "_templatetyp": [93, 105, 118, 147, 161, 168, 178], "gemma_2b": 95, "gemma_7b": 96, "lora_gemma_2b": 97, "lora_gemma_7b": 98, "taken": [99, 277, 280], "sy": [99, 272], "honest": [99, 272], "pari": [99, 152], "capit": [99, 152], "franc": [99, 152], "known": [99, 152, 239, 279], "stun": [99, 152], "05": [100, 106, 115, 119, 124, 128, 153, 155, 157, 159, 164, 166, 170, 174], "gqa": [100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182], "mqa": [100, 106, 115, 119, 124, 128, 145, 149, 153, 155, 157, 159, 164, 166, 170, 174, 182], "kvcach": [100, 106, 115, 119, 124, 128, 145, 149, 164, 170, 174, 182, 188, 191, 192, 193, 198], "scale_hidden_dim_for_mlp": [100, 106, 115, 119, 124, 128, 145, 149, 170, 174], "2307": [101, 102, 103, 104], "09288": [101, 102, 103, 104], "classif": [104, 155, 159, 160, 230], "llama2_70b": 108, "llama2_7b": [109, 277], "classifi": [110, 155, 159, 160, 256, 278], "llama2_reward_7b": [110, 230], "lora_llama2_13b": 111, "lora_llama2_70b": 112, "lora_llama2_7b": [113, 277], "lora_llama2_reward_7b": 114, "500000": [115, 119, 124, 128, 145, 149], "llama3token": [118, 143, 211], "regist": [118, 143, 147, 168, 178, 194, 251, 280], "canon": [118, 143, 147, 168, 178], "llama3_70b": 120, "lora_llama3_70b": 122, "lora_llama3_8b": 123, "scale_factor": [124, 128], "500_000": [124, 128], "rope": [124, 128, 170, 174, 182, 184], "llama3_1": [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 267, 276], "llama3_1_70b": 130, "llama3_1_8b": 131, "lora_llama3_1_405b": 132, "lora_llama3_1_70b": 133, "lora_llama3_1_8b": 134, "llama3_2_1b": [137, 191, 192, 193], "llama3_2_3b": 138, "lora_llama3_2_1b": 139, "lora_llama3_2_3b": 140, "projection_head": [141, 198, 202], "combin": [141, 143, 146, 150, 188, 198, 200, 202, 219, 276], "learnabl": [141, 185, 198, 200, 273], "fusion": [141, 144, 145, 146, 148, 149, 150, 198, 199, 200, 201, 202], "encoder_dim": [141, 142], "decoder_dim": [141, 142], "num_img": [141, 142], "num_emb": [141, 142], "broken": [141, 142, 190, 200], "width": [141, 279], "clip_embeds_per_til": 141, "emb": [141, 142, 182, 187, 188, 198], "num_hidden_input": 142, "sequenti": [142, 198, 202], "num_hidden": 142, "hidden_st": [142, 190], "image_mean": 143, "image_std": 143, "tranform": 143, "possible_resolut": 143, "448": [143, 144, 147, 148], "deviat": 143, "still": [143, 195, 197, 199, 200, 267, 277, 279, 280], "transformed_data": 143, "img1": [143, 217], "img2": [143, 217], "31587": [143, 212, 213], "29644": [143, 212, 213], "102": [143, 212, 213], "truncate_at_eo": [143, 213], "show": [143, 217, 265, 267, 268, 271, 272, 276, 277], "skip": [143, 182], "tokenize_head": 143, "tokenize_end": 143, "header": 143, "eom": 143, "wether": 143, "decoder_train": [144, 148, 151, 198], "encoder_train": [144, 148, 151, 198], "fusion_train": [144, 148, 151, 198], "deepfusionmodel": [144, 148, 151], "trainabl": [144, 148, 200, 204, 207, 245, 277, 278, 280], "resiz": [144, 147, 148], "fusion_interv": [145, 149], "num_special_token": [145, 149], "encoder_max_seq_len": [145, 149, 187, 188, 189, 193, 198, 200], "causalselfattent": [145, 149], "interv": [145, 149, 274], "clip_embed_dim": [146, 150], "clip_num_lay": [146, 150], "clip_hidden_st": [146, 150], "num_layers_project": [146, 150], "decoder_embed_dim": [146, 150], "llama3visionencod": [146, 150], "spatial": [146, 150], "backbon": [146, 150], "trainbl": 148, "decoder_lora": 149, "fusion_lora": [149, 150], "encoder_lora": 150, "lora_llama3_2_vision_11b": 151, "num_class": [155, 159, 256], "announc": 158, "ray2333": 160, "feedback": [160, 220], "lora_mistral_7b": 162, "lora_mistral_reward_7b": 163, "phi3_mini": [165, 230], "128k": 167, "nor": 167, "phi3minitoken": 168, "tokenizer_config": 168, "spm": 168, "lm": [168, 221, 276], "bo": [168, 212, 215, 272], "unk": 168, "augment": [168, 280], "endoftext": 168, "phi3minisentencepiecebasetoken": 168, "lora_phi3_mini": 169, "1000000": [170, 174], "tie_word_embed": [170, 171, 172, 174, 175, 176], "qwen2transformerdecod": 170, "period": [170, 174], "word": [170, 174, 279], "qwen2_0_5b": [171, 186], "qwen2_1_5b": [172, 186], "qwen2_7b": 173, "qwen": [175, 176, 177], "merges_fil": 178, "qwen2token": 178, "gate_proj": 179, "down_proj": 179, "up_proj": 179, "feed": [179, 187, 189], "network": [179, 205, 277, 280], "fed": [179, 272], "multipli": [179, 278], "in_dim": [179, 203, 204, 277, 278, 280], "out_dim": [179, 188, 203, 204, 277, 278, 280], "layernorm": 180, "past": 181, "expand": 181, "dpython": [181, 182, 187, 188, 189, 193, 194, 198, 200, 253, 257], "reset": [181, 182, 187, 188, 189, 198, 200, 238], "k_val": 181, "v_val": 181, "fill": 181, "bfloat16": [181, 194, 253, 273, 274, 275, 277, 278, 279], "greater": [181, 190, 261], "pos_embed": [182, 187, 277, 279], "q_norm": 182, "k_norm": 182, "kv_cach": [182, 191, 192, 193], "is_caus": 182, "13245v1": 182, "multihead": 182, "extrem": 182, "credit": 182, "litgpt": 182, "v": [182, 188, 198, 277], "n_kv_head": 182, "rotarypositionalembed": [182, 277, 279], "rmsnorm": 182, "vice": [182, 271], "versa": [182, 271], "y": 182, "s_x": 182, "s_y": 182, "_masktyp": [182, 188, 189], "score": [182, 188, 189, 219], "encoder_max_cache_seq_len": [182, 188, 189], "j": [182, 187, 188, 189, 198], "blockmask": [182, 188, 189], "create_block_mask": [182, 188, 189], "flex_attent": [182, 188, 189], "n_h": [182, 184], "num": [182, 184], "n_kv": 182, "h_d": [182, 184], "reset_cach": [182, 187, 188, 189, 198, 200], "setup_cach": [182, 187, 188, 189, 191, 192, 198, 200], "ep": 183, "root": [183, 249, 250], "squar": 183, "1910": 183, "07467": 183, "propos": [184, 278], "2104": 184, "09864": 184, "verfic": 184, "l80": 184, "init": [184, 238, 250, 280], "exceed": 184, "freq": 184, "recomput": [184, 278], "geometr": 184, "progress": [184, 269, 274, 278], "rotat": 184, "angl": 184, "basic": [185, 275], "tied_modul": 186, "lost": 186, "whose": [186, 205, 246, 251], "attributeerror": [186, 258], "attribut": [186, 205, 215, 223, 233], "attn": [187, 189, 191, 192, 193, 277, 279, 280], "multiheadattent": [187, 189, 277, 279], "ca_norm": 187, "mlp_norm": [187, 189], "ca_scal": 187, "mlp_scale": [187, 189], "convent": 187, "ff": [187, 189], "caches_are_en": [187, 188, 189, 191, 192, 193, 198, 200], "func": [187, 189, 200], "caches_are_setup": [187, 188, 189, 191, 192, 193, 198, 200], "token_sequ": 187, "embed_sequ": 187, "decoder_max_seq_len": [187, 188, 189, 191, 192, 193, 198, 200], "modulelist": 188, "output_hidden_st": [188, 198], "belong": [188, 233], "reduc": [188, 220, 266, 267, 268, 276, 277, 278, 279, 280], "statement": 188, "improv": [188, 213, 222, 236, 268, 275, 276, 277, 278], "readabl": [188, 273], "behaviour": [188, 198, 256], "alter": [188, 198], "common_util": [188, 191, 192, 193, 194], "disable_kv_cach": [188, 198], "chunked_output": 188, "last_hidden_st": 188, "chunk": [188, 195, 197, 213], "cewithchunkedoutputloss": [188, 198], "upcast": [188, 195, 197], "set_num_output_chunk": [188, 198], "num_chunk": [188, 195, 197], "s_e": [188, 198], "d_e": [188, 198], "arang": [188, 198], "prompt_length": [188, 198], "correspondingli": 188, "padded_prompt_length": 188, "m_": [188, 198], "realloc": [188, 198], "runtimeerror": [188, 215, 231, 235, 237, 242], "num_output_chunk": [188, 195, 197, 198], "transformercrossattentionlay": [188, 198, 200], "fusionlay": [188, 198], "sa_norm": 189, "sa_scal": 189, "token_pos_embed": 190, "pre_tile_pos_emb": 190, "post_tile_pos_emb": 190, "cls_project": 190, "vit": 190, "11929": 190, "convolut": 190, "flatten": 190, "downscal": 190, "800x400": 190, "400x400": 190, "_transform": 190, "whole": [190, 276], "n_token": 190, "101": 190, "pool": 190, "tiledtokenpositionalembed": 190, "tilepositionalembed": 190, "tile_pos_emb": 190, "even": [190, 256, 265, 271, 272, 275, 276, 277, 278, 280], "8x8": 190, "21": 190, "22": 190, "23": [190, 234], "24": [190, 274, 275], "25": [190, 273, 276], "26": 190, "27": [190, 273], "28": [190, 273], "29": [190, 280], "30": [190, 224, 279], "31": [190, 275], "33": 190, "34": 190, "35": [190, 280], "36": 190, "37": 190, "38": [190, 273], "39": 190, "41": 190, "43": 190, "44": 190, "45": 190, "46": 190, "47": 190, "48": [190, 273, 280], "49": 190, "50": [190, 224, 246, 273], "51": 190, "52": [190, 274], "53": 190, "54": 190, "55": [190, 274], "56": 190, "57": [190, 277, 280], "58": 190, "59": [190, 280], "60": 190, "61": [190, 273], "62": 190, "63": 190, "64": [190, 267, 277], "num_patches_per_til": 190, "emb_dim": 190, "constain": 190, "anim": 190, "max_n_img": 190, "n_channel": 190, "vision_util": 190, "tile_crop": 190, "800": 190, "patch_grid_s": 190, "rand": 190, "nch": 190, "tile_cropped_imag": 190, "batch_imag": 190, "unsqueez": 190, "batch_aspect_ratio": 190, "clip_vision_encod": 190, "cache_en": 191, "float32": [191, 192, 193, 235, 276], "1024": [191, 192, 193, 279], "temporarili": [192, 193, 205, 278], "enter": [192, 193], "overhead": [192, 220, 268, 278, 279], "untouch": [192, 272], "yield": [192, 193, 205], "caller": [192, 193, 205], "delete_kv_cach": 193, "offload_to_cpu": 194, "hook": [194, 251, 278, 280], "nf4": [194, 278, 280], "restor": 194, "higher": [194, 275, 276, 278, 279, 280], "offload": [194, 280], "increas": [194, 220, 234, 275, 276, 277, 278, 279], "peak": [194, 238, 244, 273, 275, 277, 280], "gpu": [194, 268, 271, 273, 274, 275, 276, 277, 278, 279, 280], "_register_state_dict_hook": 194, "mymodul": 194, "_after_": 194, "nf4tensor": [194, 280], "unquant": [194, 279, 280], "unus": 194, "ignore_index": [195, 196, 197, 276], "entropi": [195, 197, 276], "bf16": [195, 197, 235, 278, 280], "ce": [195, 276], "better": [195, 197, 223, 266, 272, 273, 276, 278, 279], "accuraci": [195, 197, 268, 273, 275, 276, 277, 278, 279, 280], "doubl": [195, 197, 280], "therefor": [195, 197, 280], "num_token": [195, 196, 197], "consider": [195, 197], "compute_cross_entropi": 195, "gain": [195, 268, 275], "won": [195, 272], "realiz": 195, "pull": [195, 267, 268, 271], "1390": 195, "loss_fn": [195, 197], "chunkedcrossentropyloss": 195, "output_chunk": [195, 197], "kullback": [196, 276], "leibler": [196, 276], "diverg": [196, 197, 219, 276], "jongwooko": [196, 276], "distillm": [196, 276], "17c0f98bc263b1861a02d5df578c84aea652ee65": 196, "student_logit": [196, 197, 276], "teacher_logit": [196, 197, 276], "student": [196, 197], "teacher": [196, 197, 273], "kl": [196, 197, 219, 276], "teacher_chunk": 197, "teacher_model": 197, "model_fus": [198, 199, 200, 201, 202], "deepfus": 198, "evolut": 198, "signatur": 198, "interchang": 198, "fusion_param": [198, 199, 200, 201, 202], "fusionembed": 198, "fusion_lay": [198, 200], "clip_vit_224": [198, 202], "feedforward": [198, 202], "register_fusion_modul": 198, "flamingo": [198, 200, 217], "Or": [198, 265], "strict": [198, 199, 200, 208, 277], "freez": [198, 273, 277], "fusion_vocab_s": 199, "necessit": 199, "rout": 199, "128": [199, 267, 275, 277, 278], "fusion_first": 200, "shot": [200, 273, 275, 279], "infus": 200, "interpret": 200, "enocd": 200, "isn": [200, 235, 271], "fused_lay": 200, "mark": [202, 272], "earli": 202, "peft": [203, 204, 205, 206, 207, 208, 209, 227, 267, 277, 280], "adapter_param": [203, 204, 205, 206, 207], "proj": 203, "loralinear": [203, 277, 280], "alpha": [204, 277, 278, 280], "use_bia": 204, "perturb": 204, "decomposit": [204, 277, 278], "matric": [204, 277, 280], "mapsto": 204, "w_0x": 204, "r": [204, 277], "bax": 204, "lora_a": [204, 277, 280], "lora_b": [204, 277, 280], "polici": [205, 219, 220, 221, 222, 223, 225, 236, 245, 252, 269], "neural": [205, 277, 280], "get_adapter_param": [207, 277], "base_miss": 208, "base_unexpect": 208, "lora_miss": 208, "lora_unexpect": 208, "validate_state_dict_for_lora": [208, 277], "unlik": 208, "reli": [208, 215, 273, 275], "unexpect": 208, "nonempti": 208, "full_model_state_dict_kei": 209, "lora_state_dict_kei": 209, "base_model_state_dict_kei": 209, "confirm": [209, 265], "lora_modul": 209, "complement": 209, "disjoint": 209, "overlap": [209, 278], "tiktokenbasetoken": 210, "light": 212, "sentencepieceprocessor": 212, "trim": 212, "whitespac": 212, "spm_model": [212, 272], "tokenized_text": [212, 213], "add_bo": [212, 213, 272], "trim_leading_whitespac": 212, "prefix": [212, 278], "unbatch": 212, "bos_id": [213, 215], "lightweight": [213, 272], "substr": 213, "repetit": 213, "speed": [213, 255, 275, 278, 279, 280], "identif": 213, "regex": 213, "absent": 213, "tt_model": 213, "tokenizer_json_path": 214, "heavili": 215, "concat": 215, "1788": 215, "2643": 215, "465": 215, "22137": 215, "join": 215, "satisfi": [215, 273], "loos": 216, "image_token_id": 217, "particip": [217, 218], "laid": 217, "fig": 217, "2204": 217, "14198": 217, "immedi": [217, 278], "until": [217, 278], "img3": 217, "equal": [217, 261], "gamma": [218, 222, 223], "lmbda": 218, "estim": [218, 219], "1506": 218, "02438": 218, "response_len": [218, 219], "receiv": 218, "discount": 218, "gae": 218, "lambda": 218, "logprob": [219, 223], "ref_logprob": 219, "kl_coeff": 219, "valid_score_idx": 219, "coeffici": [219, 221], "total_reward": 219, "kl_reward": 219, "beta": [220, 223], "label_smooth": [220, 223], "18290": 220, "intuit": [220, 222, 223], "dispref": 220, "dynam": [220, 279], "degener": 220, "occur": [220, 268], "naiv": 220, "trl": [220, 222, 223], "librari": [220, 222, 235, 254, 260, 264, 265, 266, 271, 278, 280], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 220, "dpo_train": [220, 222], "l844": 220, "2009": 220, "01325": 220, "regular": [220, 223, 278, 279, 280], "baselin": [220, 221, 273, 276, 277], "rather": [220, 278], "uncertainti": [220, 223], "policy_chosen_logp": [220, 222, 223], "policy_rejected_logp": [220, 222, 223], "reference_chosen_logp": [220, 222], "reference_rejected_logp": [220, 222], "chosen_reward": [220, 222, 223], "rejected_reward": [220, 222, 223], "value_clip_rang": 221, "value_coeff": 221, "proxim": [221, 269], "1707": 221, "06347": 221, "eqn": 221, "vwxyzjn": 221, "ccc19538e817e98a60d3253242ac15e2a562cb49": 221, "lm_human_preference_detail": 221, "train_policy_acceler": 221, "l719": 221, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 221, "ppo2": 221, "l68": 221, "l75": 221, "pi_old_logprob": 221, "pi_logprob": 221, "phi_old_valu": 221, "phi_valu": 221, "value_padding_mask": 221, "old": 221, "participag": 221, "five": 221, "policy_loss": 221, "value_loss": 221, "clipfrac": 221, "fraction": 221, "statist": [222, 278], "rso": 222, "hing": 222, "2309": 222, "06657": 222, "logist": 222, "regress": 222, "slic": 222, "10425": 222, "almost": [222, 277], "svm": 222, "counter": 222, "4dce042a3863db1d375358e8c8092b874b02934b": 222, "l1141": 222, "simpo": 223, "2405": 223, "14734": 223, "averag": [223, 276], "implicit": 223, "margin": 223, "bradlei": 223, "terri": 223, "larger": [223, 229, 273, 275, 276, 278], "win": 223, "lose": 223, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 223, "cpo_train": 223, "l603": 223, "pretti": [223, 273], "identitc": 223, "elimin": 223, "kind": 223, "ipoloss": 223, "fill_valu": 224, "sequence_length": 224, "stop_token_id": 224, "869": 224, "eos_mask": 224, "truncated_sequ": 224, "datatyp": [225, 278, 280], "denot": 225, "auto_wrap_polici": [225, 236, 252], "submodul": [225, 245], "obei": 225, "contract": 225, "get_fsdp_polici": 225, "modules_to_wrap": [225, 236, 245], "min_num_param": 225, "my_fsdp_polici": 225, "recurs": [225, 245, 249], "sum": [225, 276, 277], "p": [225, 231, 277, 279, 280], "numel": [225, 277], "1000": [225, 279], "stabl": [225, 243, 249, 254, 265, 278], "html": [225, 243, 249, 252, 254, 260, 264], "filename_format": 226, "max_filenam": 226, "concis": 226, "filenam": [226, 247], "file_": 226, "_of_": 226, "n_file": 226, "build_checkpoint_filenam": 226, "file_00001_of_00003": 226, "file_00002_of_00003": 226, "file_00003_of_00003": 226, "safe_seri": 227, "from_pretrain": 227, "0001_of_0003": 227, "0002_of_0003": 227, "todo": 227, "preserv": [227, 280], "weight_map": [227, 273], "convert_weight": 227, "_model_typ": [227, 230], "intermediate_checkpoint": [227, 228, 229], "adapter_onli": [227, 228, 229], "_weight_map": 227, "shard": [228, 275], "wip": 228, "qualnam": 230, "boundari": 230, "distinguish": 230, "llama3_vis": 230, "llama3_2_vision_decod": 230, "mistral_reward_7b": 230, "my_new_model": 230, "my_custom_state_dict_map": 230, "optim_map": 231, "bare": 231, "bone": 231, "optim_dict": [231, 233, 251], "cfg_optim": 231, "ckpt": 231, "optim_ckpt": 231, "placeholder_optim_dict": 231, "optiminbackwardwrapp": 231, "get_last_lr": 231, "rate": [231, 234, 237, 266, 274, 278], "schedul": [231, 234, 255, 274, 278], "get_optim_kei": 231, "arbitrari": [231, 277, 278], "optim_ckpt_map": 231, "set_lr_schedul": 231, "lr_schedul": [231, 234], "lrschedul": 231, "loadabl": 231, "step_lr_schedul": 231, "ac_mod": 232, "ac_opt": 232, "op": [232, 279], "ac": [232, 236], "optimizerinbackwardwrapp": [233, 237], "named_paramet": [233, 256], "num_warmup_step": 234, "num_training_step": 234, "num_cycl": [234, 255], "last_epoch": 234, "lambdalr": 234, "linearli": 234, "decreas": [234, 277, 278, 279, 280], "cosin": 234, "v4": 234, "src": 234, "l104": 234, "warmup": [234, 255], "phase": 234, "wave": 234, "half": [234, 278], "kernel": 235, "memory_efficient_fsdp_wrap": [236, 279], "maxim": [236, 245, 264, 266], "workload": [236, 268, 278, 279], "fullyshardeddataparallel": [236, 245], "fsdppolicytyp": [236, 245], "warpper": 237, "optimizer_in_backward": 237, "reset_stat": 238, "track": [238, 246], "alloc": [238, 244, 245, 275, 280], "reserv": [238, 244, 272, 280], "stat": [238, 244, 280], "int4": [239, 279], "4w": 239, "recogn": 239, "int8dynactint4weightquant": [239, 268, 279], "8da4w": [239, 279], "int8dynactint4weightqatquant": [239, 268, 279], "qat": [239, 264, 269], "exclud": 240, "aka": 241, "master": 243, "port": [243, 271], "address": [243, 276, 278], "hold": [243, 274], "peak_memory_act": 244, "peak_memory_alloc": 244, "peak_memory_reserv": 244, "get_memory_stat": 244, "hierarch": 245, "api_kei": 246, "experiment_kei": 246, "onlin": 246, "log_cod": 246, "comet": 246, "site": [246, 273], "ml": 246, "team": 246, "compar": [246, 249, 261, 273, 275, 276, 277, 279, 280], "sdk": 246, "uncategor": 246, "alphanumer": 246, "charact": 246, "get_or_cr": 246, "fresh": 246, "persist": 246, "hpo": 246, "sweep": 246, "server": 246, "offlin": 246, "auto": [246, 271], "creation": 246, "experimentconfig": 246, "project_nam": 246, "my_project": [246, 250], "my_workspac": 246, "my_metr": [246, 249, 250], "importerror": [246, 250], "termin": [246, 249, 250], "comet_api_kei": 246, "flush": [246, 247, 248, 249, 250], "ndarrai": [246, 247, 248, 249, 250], "scalar": [246, 247, 248, 249, 250], "record": [246, 247, 248, 249, 250, 255], "log_config": [246, 250], "payload": [246, 247, 248, 249, 250], "log_": 247, "unixtimestamp": 247, "thread": 247, "safe": 247, "organize_log": 249, "tensorboard": 249, "subdirectori": 249, "logdir": 249, "startup": 249, "tree": [249, 273, 275], "tfevent": 249, "encount": 249, "frontend": 249, "organ": [249, 271], "accordingli": [249, 279], "my_log_dir": 249, "view": [249, 276], "entiti": 250, "bias": [250, 277, 280], "usernam": 250, "my_ent": 250, "my_group": 250, "account": [250, 277, 280], "link": [250, 273, 275], "capecap": 250, "6053ofw0": 250, "torchtune_config_j67sb73v": 250, "soon": [251, 278], "readi": [251, 264, 272, 279], "grad": 251, "acwrappolicytyp": 252, "author": [252, 266, 274, 278, 280], "fsdp_adavnced_tutori": 252, "insid": 253, "contextmanag": 253, "debug_mod": 254, "pseudo": 254, "commonli": [254, 277, 278, 280], "numpi": 254, "determinist": 254, "global": [254, 278], "warn": 254, "nondeterminist": 254, "cudnn": 254, "set_deterministic_debug_mod": 254, "profile_memori": 255, "with_stack": 255, "record_shap": 255, "with_flop": 255, "wait_step": 255, "warmup_step": 255, "active_step": 255, "profil": 255, "layout": 255, "trace": 255, "profileract": 255, "gradient_accumul": 255, "sensibl": 255, "default_schedul": 255, "reduct": [255, 268, 277], "scope": 255, "flop": 255, "cycl": 255, "repeat": [255, 278], "model_named_paramet": 256, "force_overrid": 256, "concret": [256, 278], "vocab_dim": 256, "named_param": 257, "inplac": [258, 277], "too": [258, 268, 275], "handler": 260, "_log": 260, "__version__": 261, "generated_examples_python": 262, "galleri": [262, 270], "sphinx": 262, "000": [263, 270, 275], "execut": [263, 270], "generated_exampl": 263, "mem": [263, 270], "mb": [263, 270], "gentl": 264, "introduct": 264, "first_finetune_tutori": 264, "kd": 264, "torchvis": 265, "torchao": [265, 268, 273, 275, 279, 280], "latest": [265, 268, 274, 278, 280], "whl": 265, "cu121": 265, "cu118": 265, "cu124": 265, "And": [265, 273], "welcom": [265, 271], "greatest": [265, 274], "contributor": 265, "dev": 265, "commit": 265, "branch": 265, "therebi": [265, 278, 279, 280], "forc": [265, 276], "reinstal": 265, "opt": [265, 274], "suffix": 265, "On": [266, 277], "pointer": 266, "emphas": 266, "simplic": 266, "component": 266, "prove": 266, "democrat": 266, "zoo": 266, "varieti": [266, 277], "integr": [266, 273, 274, 275, 277, 279, 280], "fsdp2": 266, "excit": 266, "checkout": 266, "quickstart": 266, "attain": 266, "embodi": 266, "philosophi": 266, "usabl": 266, "composit": 266, "hard": 266, "outlin": 266, "unecessari": 266, "never": 266, "thoroughli": 266, "competit": 267, "grant": [267, 268, 274], "interest": [267, 268, 273, 276], "8b_lora_single_devic": [267, 271, 272, 275, 276, 278], "adjust": [267, 268, 276, 278, 279], "lever": [267, 268], "action": [267, 268], "degrad": [268, 278, 279, 280], "simul": [268, 278, 279], "compromis": 268, "blogpost": [268, 278], "qat_distribut": [268, 279], "8b_qat_ful": [268, 279], "least": [268, 275, 277, 279], "vram": [268, 275, 277, 278, 279], "80gb": [268, 279], "a100": 268, "h100": 268, "delai": 268, "fake": [268, 279], "empir": [268, 279], "potenti": [268, 277, 278], "fake_quant_after_n_step": [268, 279], "idea": [268, 276, 280], "roughli": 268, "total_step": 268, "futur": [268, 279], "plan": [268, 273], "un": 268, "groupsiz": [268, 279], "256": [268, 275, 279], "hackabl": [269, 274], "singularli": [269, 274], "technic": [269, 274], "awar": [269, 278, 279], "tracker": 269, "short": 271, "subcommand": 271, "anytim": 271, "symlink": 271, "wrote": 271, "readm": [271, 273, 275], "md": 271, "lot": [271, 273, 278], "recent": 271, "releas": [271, 275], "agre": 271, "term": [271, 278], "perman": 271, "eat": 271, "bandwith": 271, "storag": [271, 280], "00030": 271, "ootb": 271, "7b_full_low_memori": [271, 273, 274], "8b_full_single_devic": 271, "mini_full_low_memori": 271, "7b_full": [271, 273, 274], "13b_full": [271, 273, 274], "70b_full": 271, "edit": 271, "clobber": 271, "destin": 271, "lora_finetune_distribut": [271, 275, 277], "torchrun": 271, "launch": [271, 272, 274], "nproc": 271, "node": 271, "worker": 271, "nnode": [271, 277, 279], "minimum_nod": 271, "maximum_nod": 271, "fail": 271, "rdzv": 271, "rendezv": 271, "endpoint": 271, "8b_lora": [271, 275], "bypass": 271, "fancy_lora": 271, "8b_fancy_lora": 271, "nice": 272, "meet": 272, "overhaul": 272, "multiturn": 272, "untrain": 272, "accompani": 272, "who": 272, "influenti": 272, "hip": 272, "hop": 272, "artist": 272, "2pac": 272, "rakim": 272, "flavor": 272, "formatted_messag": 272, "nyou": 272, "nwho": 272, "why": [272, 274, 277], "518": 272, "25580": 272, "29962": 272, "3532": 272, "14816": 272, "29903": 272, "6778": 272, "_spm_model": 272, "piece_to_id": 272, "manual": [272, 280], "529": 272, "29879": 272, "29958": 272, "nhere": 272, "pure": 272, "mess": 272, "prime": 272, "strictli": 272, "ask": [272, 278], "though": 272, "robust": 272, "pretend": 272, "zuckerberg": 272, "seem": [272, 273], "good": [272, 277, 278], "altogeth": 272, "honor": 272, "copi": [272, 273, 274, 275, 279, 280], "custom_8b_lora_single_devic": 272, "favorit": [273, 277], "seemlessli": 273, "connect": [273, 279], "amount": 273, "natur": 273, "export": 273, "leverag": [273, 275, 280], "percentag": 273, "16gb": [273, 277], "rtx": 273, "3090": 273, "4090": 273, "hour": 273, "7b_qlora_single_devic": [273, 274, 280], "473": 273, "98": [273, 280], "gb": [273, 275, 277, 279, 280], "484": 273, "01": [273, 274], "fact": [273, 275, 277, 278], "third": 273, "But": [273, 277], "realli": 273, "eleuther_ev": [273, 275, 279], "eleuther_evalu": [273, 275, 279], "lm_eval": [273, 275], "custom_eval_config": [273, 275], "truthfulqa_mc2": [273, 275, 276, 277], "measur": [273, 275], "propens": [273, 275], "324": 273, "loglikelihood": 273, "195": 273, "121": 273, "197": 273, "acc": [273, 279], "388": 273, "489": 273, "great": [273, 278], "custom_generation_config": [273, 275], "kick": 273, "300": 273, "bai": 273, "area": 273, "92": 273, "exploratorium": 273, "san": 273, "francisco": 273, "magazin": 273, "awesom": 273, "bridg": 273, "cool": 273, "96": [273, 280], "sec": [273, 275], "83": 273, "99": [273, 277], "72": 273, "littl": 273, "int8_weight_onli": [273, 275], "int8_dynamic_activation_int8_weight": [273, 275], "ao": [273, 275], "quant_api": [273, 275], "quantize_": [273, 275], "int4_weight_onli": [273, 275], "previous": [273, 275, 277], "benefit": 273, "clone": [273, 277, 279, 280], "assumpt": 273, "new_dir": 273, "output_dict": 273, "sd_1": 273, "sd_2": 273, "dump": 273, "convert_hf_checkpoint": 273, "checkpoint_path": 273, "justin": 273, "school": 273, "math": 273, "ws": 273, "94": [273, 275], "bandwidth": [273, 275], "1391": 273, "84": 273, "thats": 273, "seamlessli": 273, "authent": [273, 274], "hopefulli": 273, "gave": 273, "minut": 274, "agreement": 274, "depth": 274, "principl": 274, "boilerpl": 274, "substanti": [274, 277], "custom_config": 274, "replic": 274, "lorafinetunerecipesingledevic": 274, "lora_finetune_output": 274, "log_1713194212": 274, "3697006702423096": 274, "25880": [274, 280], "83it": 274, "monitor": 274, "tqdm": 274, "e2": 274, "focu": 275, "theta": 275, "observ": [275, 279], "consum": [275, 280], "overal": [275, 276], "8b_qlora_single_devic": [275, 278], "coupl": [275, 277, 280], "meta_model_0": [275, 279], "122": 275, "sarah": 275, "busi": 275, "mum": 275, "young": 275, "children": 275, "live": 275, "north": 275, "east": 275, "england": 275, "135": 275, "88": 275, "138": 275, "346": 275, "09": 275, "139": 275, "broader": 275, "teach": [276, 277], "straight": [276, 277], "jump": [276, 277], "compress": 276, "transfer": 276, "capac": 276, "computation": 276, "expens": 276, "deploi": 276, "imit": 276, "diagram": 276, "aim": [276, 278], "minillm": 276, "forwardklloss": 276, "super": 276, "teacher_prob": 276, "student_logprob": 276, "log_softmax": 276, "prod_prob": 276, "omit": [276, 277, 278], "forwardklwithchunkedoutputloss": 276, "knowledge_distillation_single_devic": 276, "bit": [276, 277, 278, 279, 280], "alpaca_cleaned_dataset": 276, "hellaswag": [276, 279], "commonsense_qa": 276, "kd_ratio": 276, "teacher_checkpoint": 276, "00004": 276, "truthfulqa": [276, 277], "commonsens": 276, "constant": 276, "boost": 276, "graph": 276, "irrespect": 276, "3e": 276, "slightli": 276, "truthful_qa": 276, "wherea": 276, "unfamiliar": 277, "oppos": [277, 280], "momentum": [277, 278], "aghajanyan": 277, "et": 277, "al": 277, "hypothes": 277, "intrins": 277, "often": [277, 278], "eight": 277, "practic": 277, "blue": 277, "although": [277, 279], "rememb": 277, "approx": 277, "15m": 277, "65k": 277, "requires_grad": [277, 280], "frozen_out": [277, 280], "lora_out": [277, 280], "base_model": 277, "lora_model": 277, "lora_llama_2_7b": [277, 280], "alon": 277, "in_featur": [277, 279], "out_featur": [277, 279], "validate_missing_and_unexpected_for_lora": 277, "peft_util": 277, "set_trainable_param": 277, "lora_param": 277, "total_param": 277, "trainable_param": 277, "2f": 277, "6742609920": 277, "4194304": 277, "7b_lora": 277, "my_model_checkpoint_path": [277, 279, 280], "tokenizer_checkpoint": [277, 279, 280], "my_tokenizer_checkpoint_path": [277, 279, 280], "constraint": 277, "factori": 277, "benefici": 277, "impact": [277, 278], "minor": 277, "lora_experiment_1": 277, "smooth": [277, 280], "curv": [277, 280], "500": 277, "ran": 277, "footprint": [277, 279], "commod": 277, "cogniz": 277, "ax": 277, "parallel": 277, "475": 277, "87": 277, "508": 277, "86": 277, "504": 277, "04": 277, "514": 277, "lowest": 277, "absolut": 277, "4gb": 277, "tradeoff": 277, "salman": 278, "mohammadi": 278, "brief": 278, "glossari": 278, "struggl": 278, "constrain": [278, 279], "particularli": 278, "gradient_accumulation_step": 278, "throughput": 278, "cost": 278, "sebastian": 278, "raschka": 278, "fp16": 278, "sound": 278, "quot": 278, "aliv": 278, "region": 278, "enable_activation_checkpoint": 278, "bring": 278, "autograd": [278, 280], "saved_tensors_hook": 278, "cours": 278, "runtim": 278, "hide": 278, "later": 278, "brought": 278, "enable_activation_offload": 278, "dev20240907": 278, "total_batch_s": 278, "count": 278, "suppos": 278, "log_every_n_step": 278, "translat": 278, "frequent": 278, "slowli": 278, "num_devic": 278, "adamw8bit": 278, "pagedadamw": 278, "modern": 278, "converg": 278, "stateless": 278, "stochast": 278, "descent": 278, "sacrif": 278, "remov": 278, "optimizer_in_bwd": 278, "greatli": 278, "lora_": 278, "lora_llama3": 278, "_lora": 278, "firstli": 278, "secondli": 278, "affect": 278, "fashion": 278, "slower": [278, 280], "jointli": 278, "sens": 278, "novel": 278, "normalfloat": [278, 280], "8x": [278, 280], "worth": 278, "cast": [278, 279], "incur": [278, 279, 280], "penalti": 278, "qlora_": 278, "qlora_llama3_8b": 278, "_qlora": 278, "perplex": 279, "goal": 279, "ptq": 279, "kept": 279, "nois": 279, "henc": 279, "x_q": 279, "int8": 279, "zp": 279, "x_float": 279, "qmin": 279, "qmax": 279, "clamp": 279, "x_fq": 279, "dequant": 279, "proce": 279, "prepared_model": 279, "swap": 279, "int8dynactint4weightqatlinear": 279, "int8dynactint4weightlinear": 279, "train_loop": 279, "converted_model": 279, "qat_distributed_recipe_label": 279, "recov": 279, "modif": 279, "custom_8b_qat_ful": 279, "2000": 279, "led": 279, "presum": 279, "mutat": 279, "5gb": 279, "custom_quant": 279, "poorli": 279, "custom_eleuther_evalu": 279, "fullmodeltorchtunecheckpoint": 279, "max_seq_length": 279, "my_eleuther_evalu": 279, "stderr": 279, "word_perplex": 279, "9148": 279, "byte_perplex": 279, "5357": 279, "bits_per_byt": 279, "6189": 279, "5687": 279, "0049": 279, "acc_norm": 279, "7536": 279, "0043": 279, "portion": [279, 280], "74": 279, "048": 279, "190": 279, "7735": 279, "5598": 279, "6413": 279, "5481": 279, "0050": 279, "7390": 279, "0044": 279, "7251": 279, "4994": 279, "5844": 279, "5740": 279, "7610": 279, "outperform": 279, "importantli": 279, "characterist": 279, "187": 279, "958": 279, "halv": 279, "int4weightonlyquant": 279, "motiv": 279, "edg": 279, "smartphon": 279, "executorch": 279, "xnnpack": 279, "export_llama": 279, "use_sdpa_with_kv_cach": 279, "qmode": 279, "group_siz": 279, "get_bos_id": 279, "get_eos_id": 279, "output_nam": 279, "llama3_8da4w": 279, "pte": 279, "881": 279, "oneplu": 279, "709": 279, "tok": 279, "815": 279, "316": 279, "364": 279, "highli": 280, "vanilla": 280, "held": 280, "bespok": 280, "vast": 280, "major": 280, "normatfloat": 280, "deepdiv": 280, "distinct": 280, "de": 280, "counterpart": 280, "set_default_devic": 280, "qlora_linear": 280, "memory_alloc": 280, "177": 280, "152": 280, "del": 280, "empty_cach": 280, "lora_linear": 280, "081": 280, "344": 280, "qlora_llama2_7b": 280, "qlora_model": 280, "hood": 280, "essenti": 280, "reparametrize_as_dtype_state_dict_post_hook": 280, "149": 280, "9157477021217346": 280, "02": 280, "08": 280, "15it": 280, "nightli": 280, "200": 280, "hundr": 280, "228": 280, "8158286809921265": 280, "95it": 280, "exercis": 280, "linear_nf4": 280, "to_nf4": 280, "linear_weight": 280, "incom": 280}, "objects": {"torchtune.config": [[26, 0, 1, "", "instantiate"], [27, 0, 1, "", "log_config"], [28, 0, 1, "", "parse"], [29, 0, 1, "", "validate"]], "torchtune.data": [[30, 1, 1, "", "AlpacaToMessages"], [31, 1, 1, "", "ChatMLTemplate"], [32, 1, 1, "", "ChosenRejectedToMessages"], [33, 2, 1, "", "GrammarErrorCorrectionTemplate"], [34, 1, 1, "", "InputOutputToMessages"], [35, 1, 1, "", "Message"], [36, 1, 1, "", "OpenAIToMessages"], [37, 1, 1, "", "PromptTemplate"], [38, 1, 1, "", "PromptTemplateInterface"], [39, 2, 1, "", "QuestionAnswerTemplate"], [40, 2, 1, "", "Role"], [41, 1, 1, "", "ShareGPTToMessages"], [42, 2, 1, "", "SummarizeTemplate"], [43, 0, 1, "", "format_content_with_images"], [44, 0, 1, "", "left_pad_sequence"], [45, 0, 1, "", "load_image"], [46, 0, 1, "", "padded_collate"], [47, 0, 1, "", "padded_collate_dpo"], [48, 0, 1, "", "padded_collate_sft"], [49, 0, 1, "", "padded_collate_tiled_images_and_mask"], [50, 0, 1, "", "truncate"], [51, 0, 1, "", "validate_messages"]], "torchtune.data.Message": [[35, 3, 1, "", "contains_media"], [35, 4, 1, "", "from_dict"], [35, 4, 1, "", "get_media"], [35, 3, 1, "", "text_content"]], "torchtune.datasets": [[52, 1, 1, "", "ConcatDataset"], [53, 1, 1, "", "PackedDataset"], [54, 1, 1, "", "PreferenceDataset"], [55, 1, 1, "", "SFTDataset"], [56, 1, 1, "", "TextCompletionDataset"], [57, 0, 1, "", "alpaca_cleaned_dataset"], [58, 0, 1, "", "alpaca_dataset"], [59, 0, 1, "", "chat_dataset"], [60, 0, 1, "", "cnn_dailymail_articles_dataset"], [61, 0, 1, "", "grammar_dataset"], [62, 0, 1, "", "hh_rlhf_helpful_dataset"], [63, 0, 1, "", "instruct_dataset"], [66, 0, 1, "", "preference_dataset"], [67, 0, 1, "", "samsum_dataset"], [68, 0, 1, "", "slimorca_dataset"], [69, 0, 1, "", "stack_exchange_paired_dataset"], [70, 0, 1, "", "text_completion_dataset"], [71, 0, 1, "", "wikitext_dataset"]], "torchtune.datasets.multimodal": [[64, 0, 1, "", "llava_instruct_dataset"], [65, 0, 1, "", "the_cauldron_dataset"]], "torchtune.generation": [[72, 0, 1, "", "generate"], [73, 0, 1, "", "generate_next_token"], [74, 0, 1, "", "get_causal_mask_from_padding_mask"], [75, 0, 1, "", "get_position_ids_from_padding_mask"], [76, 0, 1, "", "sample"]], "torchtune.models.clip": [[77, 1, 1, "", "TilePositionalEmbedding"], [78, 1, 1, "", "TiledTokenPositionalEmbedding"], [79, 1, 1, "", "TokenPositionalEmbedding"], [80, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[77, 4, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[78, 4, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[79, 4, 1, "", "forward"]], "torchtune.models.code_llama2": [[81, 0, 1, "", "code_llama2_13b"], [82, 0, 1, "", "code_llama2_70b"], [83, 0, 1, "", "code_llama2_7b"], [84, 0, 1, "", "lora_code_llama2_13b"], [85, 0, 1, "", "lora_code_llama2_70b"], [86, 0, 1, "", "lora_code_llama2_7b"], [87, 0, 1, "", "qlora_code_llama2_13b"], [88, 0, 1, "", "qlora_code_llama2_70b"], [89, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[90, 0, 1, "", "gemma"], [91, 0, 1, "", "gemma_2b"], [92, 0, 1, "", "gemma_7b"], [93, 0, 1, "", "gemma_tokenizer"], [94, 0, 1, "", "lora_gemma"], [95, 0, 1, "", "lora_gemma_2b"], [96, 0, 1, "", "lora_gemma_7b"], [97, 0, 1, "", "qlora_gemma_2b"], [98, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[99, 1, 1, "", "Llama2ChatTemplate"], [100, 0, 1, "", "llama2"], [101, 0, 1, "", "llama2_13b"], [102, 0, 1, "", "llama2_70b"], [103, 0, 1, "", "llama2_7b"], [104, 0, 1, "", "llama2_reward_7b"], [105, 0, 1, "", "llama2_tokenizer"], [106, 0, 1, "", "lora_llama2"], [107, 0, 1, "", "lora_llama2_13b"], [108, 0, 1, "", "lora_llama2_70b"], [109, 0, 1, "", "lora_llama2_7b"], [110, 0, 1, "", "lora_llama2_reward_7b"], [111, 0, 1, "", "qlora_llama2_13b"], [112, 0, 1, "", "qlora_llama2_70b"], [113, 0, 1, "", "qlora_llama2_7b"], [114, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama3": [[115, 0, 1, "", "llama3"], [116, 0, 1, "", "llama3_70b"], [117, 0, 1, "", "llama3_8b"], [118, 0, 1, "", "llama3_tokenizer"], [119, 0, 1, "", "lora_llama3"], [120, 0, 1, "", "lora_llama3_70b"], [121, 0, 1, "", "lora_llama3_8b"], [122, 0, 1, "", "qlora_llama3_70b"], [123, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3_1": [[124, 0, 1, "", "llama3_1"], [125, 0, 1, "", "llama3_1_405b"], [126, 0, 1, "", "llama3_1_70b"], [127, 0, 1, "", "llama3_1_8b"], [128, 0, 1, "", "lora_llama3_1"], [129, 0, 1, "", "lora_llama3_1_405b"], [130, 0, 1, "", "lora_llama3_1_70b"], [131, 0, 1, "", "lora_llama3_1_8b"], [132, 0, 1, "", "qlora_llama3_1_405b"], [133, 0, 1, "", "qlora_llama3_1_70b"], [134, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.llama3_2": [[135, 0, 1, "", "llama3_2_1b"], [136, 0, 1, "", "llama3_2_3b"], [137, 0, 1, "", "lora_llama3_2_1b"], [138, 0, 1, "", "lora_llama3_2_3b"], [139, 0, 1, "", "qlora_llama3_2_1b"], [140, 0, 1, "", "qlora_llama3_2_3b"]], "torchtune.models.llama3_2_vision": [[141, 1, 1, "", "Llama3VisionEncoder"], [142, 1, 1, "", "Llama3VisionProjectionHead"], [143, 1, 1, "", "Llama3VisionTransform"], [144, 0, 1, "", "llama3_2_vision_11b"], [145, 0, 1, "", "llama3_2_vision_decoder"], [146, 0, 1, "", "llama3_2_vision_encoder"], [147, 0, 1, "", "llama3_2_vision_transform"], [148, 0, 1, "", "lora_llama3_2_vision_11b"], [149, 0, 1, "", "lora_llama3_2_vision_decoder"], [150, 0, 1, "", "lora_llama3_2_vision_encoder"], [151, 0, 1, "", "qlora_llama3_2_vision_11b"]], "torchtune.models.llama3_2_vision.Llama3VisionEncoder": [[141, 4, 1, "", "forward"]], "torchtune.models.llama3_2_vision.Llama3VisionProjectionHead": [[142, 4, 1, "", "forward"]], "torchtune.models.llama3_2_vision.Llama3VisionTransform": [[143, 4, 1, "", "decode"], [143, 4, 1, "", "tokenize_message"], [143, 4, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[152, 1, 1, "", "MistralChatTemplate"], [153, 0, 1, "", "lora_mistral"], [154, 0, 1, "", "lora_mistral_7b"], [155, 0, 1, "", "lora_mistral_classifier"], [156, 0, 1, "", "lora_mistral_reward_7b"], [157, 0, 1, "", "mistral"], [158, 0, 1, "", "mistral_7b"], [159, 0, 1, "", "mistral_classifier"], [160, 0, 1, "", "mistral_reward_7b"], [161, 0, 1, "", "mistral_tokenizer"], [162, 0, 1, "", "qlora_mistral_7b"], [163, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.phi3": [[164, 0, 1, "", "lora_phi3"], [165, 0, 1, "", "lora_phi3_mini"], [166, 0, 1, "", "phi3"], [167, 0, 1, "", "phi3_mini"], [168, 0, 1, "", "phi3_mini_tokenizer"], [169, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.qwen2": [[170, 0, 1, "", "lora_qwen2"], [171, 0, 1, "", "lora_qwen2_0_5b"], [172, 0, 1, "", "lora_qwen2_1_5b"], [173, 0, 1, "", "lora_qwen2_7b"], [174, 0, 1, "", "qwen2"], [175, 0, 1, "", "qwen2_0_5b"], [176, 0, 1, "", "qwen2_1_5b"], [177, 0, 1, "", "qwen2_7b"], [178, 0, 1, "", "qwen2_tokenizer"]], "torchtune.modules": [[179, 1, 1, "", "FeedForward"], [180, 1, 1, "", "Fp32LayerNorm"], [181, 1, 1, "", "KVCache"], [182, 1, 1, "", "MultiHeadAttention"], [183, 1, 1, "", "RMSNorm"], [184, 1, 1, "", "RotaryPositionalEmbeddings"], [185, 1, 1, "", "TanhGate"], [186, 1, 1, "", "TiedLinear"], [187, 1, 1, "", "TransformerCrossAttentionLayer"], [188, 1, 1, "", "TransformerDecoder"], [189, 1, 1, "", "TransformerSelfAttentionLayer"], [190, 1, 1, "", "VisionTransformer"]], "torchtune.modules.FeedForward": [[179, 4, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[180, 4, 1, "", "forward"]], "torchtune.modules.KVCache": [[181, 4, 1, "", "reset"], [181, 4, 1, "", "update"]], "torchtune.modules.MultiHeadAttention": [[182, 4, 1, "", "forward"], [182, 4, 1, "", "reset_cache"], [182, 4, 1, "", "setup_cache"]], "torchtune.modules.RMSNorm": [[183, 4, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[184, 4, 1, "", "forward"]], "torchtune.modules.TanhGate": [[185, 4, 1, "", "forward"]], "torchtune.modules.TransformerCrossAttentionLayer": [[187, 4, 1, "", "caches_are_enabled"], [187, 4, 1, "", "caches_are_setup"], [187, 4, 1, "", "forward"], [187, 4, 1, "", "reset_cache"], [187, 4, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoder": [[188, 4, 1, "", "caches_are_enabled"], [188, 4, 1, "", "caches_are_setup"], [188, 4, 1, "", "chunked_output"], [188, 4, 1, "", "forward"], [188, 4, 1, "", "reset_caches"], [188, 4, 1, "", "set_num_output_chunks"], [188, 4, 1, "", "setup_caches"]], "torchtune.modules.TransformerSelfAttentionLayer": [[189, 4, 1, "", "caches_are_enabled"], [189, 4, 1, "", "caches_are_setup"], [189, 4, 1, "", "forward"], [189, 4, 1, "", "reset_cache"], [189, 4, 1, "", "setup_caches"]], "torchtune.modules.VisionTransformer": [[190, 4, 1, "", "forward"]], "torchtune.modules.common_utils": [[191, 0, 1, "", "delete_kv_caches"], [192, 0, 1, "", "disable_kv_cache"], [193, 0, 1, "", "local_kv_cache"], [194, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[195, 1, 1, "", "CEWithChunkedOutputLoss"], [196, 1, 1, "", "ForwardKLLoss"], [197, 1, 1, "", "ForwardKLWithChunkedOutputLoss"]], "torchtune.modules.loss.CEWithChunkedOutputLoss": [[195, 4, 1, "", "compute_cross_entropy"], [195, 4, 1, "", "forward"]], "torchtune.modules.loss.ForwardKLLoss": [[196, 4, 1, "", "forward"]], "torchtune.modules.loss.ForwardKLWithChunkedOutputLoss": [[197, 4, 1, "", "forward"]], "torchtune.modules.model_fusion": [[198, 1, 1, "", "DeepFusionModel"], [199, 1, 1, "", "FusionEmbedding"], [200, 1, 1, "", "FusionLayer"], [201, 0, 1, "", "get_fusion_params"], [202, 0, 1, "", "register_fusion_module"]], "torchtune.modules.model_fusion.DeepFusionModel": [[198, 4, 1, "", "caches_are_enabled"], [198, 4, 1, "", "caches_are_setup"], [198, 4, 1, "", "forward"], [198, 4, 1, "", "reset_caches"], [198, 4, 1, "", "set_num_output_chunks"], [198, 4, 1, "", "setup_caches"]], "torchtune.modules.model_fusion.FusionEmbedding": [[199, 4, 1, "", "forward"], [199, 4, 1, "", "fusion_params"]], "torchtune.modules.model_fusion.FusionLayer": [[200, 4, 1, "", "caches_are_enabled"], [200, 4, 1, "", "caches_are_setup"], [200, 4, 1, "", "forward"], [200, 4, 1, "", "fusion_params"], [200, 4, 1, "", "reset_cache"], [200, 4, 1, "", "setup_caches"]], "torchtune.modules.peft": [[203, 1, 1, "", "AdapterModule"], [204, 1, 1, "", "LoRALinear"], [205, 0, 1, "", "disable_adapter"], [206, 0, 1, "", "get_adapter_params"], [207, 0, 1, "", "set_trainable_params"], [208, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [209, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[203, 4, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[204, 4, 1, "", "adapter_params"], [204, 4, 1, "", "forward"]], "torchtune.modules.tokenizers": [[210, 1, 1, "", "BaseTokenizer"], [211, 1, 1, "", "ModelTokenizer"], [212, 1, 1, "", "SentencePieceBaseTokenizer"], [213, 1, 1, "", "TikTokenBaseTokenizer"], [214, 0, 1, "", "parse_hf_tokenizer_json"], [215, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[210, 4, 1, "", "decode"], [210, 4, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[211, 4, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[212, 4, 1, "", "decode"], [212, 4, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[213, 4, 1, "", "decode"], [213, 4, 1, "", "encode"]], "torchtune.modules.transforms": [[216, 1, 1, "", "Transform"], [217, 1, 1, "", "VisionCrossAttentionMask"]], "torchtune.rlhf": [[218, 0, 1, "", "estimate_advantages"], [219, 0, 1, "", "get_rewards_ppo"], [224, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.rlhf.loss": [[220, 1, 1, "", "DPOLoss"], [221, 1, 1, "", "PPOLoss"], [222, 1, 1, "", "RSOLoss"], [223, 1, 1, "", "SimPOLoss"]], "torchtune.rlhf.loss.DPOLoss": [[220, 4, 1, "", "forward"]], "torchtune.rlhf.loss.PPOLoss": [[221, 4, 1, "", "forward"]], "torchtune.rlhf.loss.RSOLoss": [[222, 4, 1, "", "forward"]], "torchtune.rlhf.loss.SimPOLoss": [[223, 4, 1, "", "forward"]], "torchtune.training": [[225, 2, 1, "", "FSDPPolicyType"], [226, 1, 1, "", "FormattedCheckpointFiles"], [227, 1, 1, "", "FullModelHFCheckpointer"], [228, 1, 1, "", "FullModelMetaCheckpointer"], [229, 1, 1, "", "FullModelTorchTuneCheckpointer"], [230, 1, 1, "", "ModelType"], [231, 1, 1, "", "OptimizerInBackwardWrapper"], [232, 0, 1, "", "apply_selective_activation_checkpointing"], [233, 0, 1, "", "create_optim_in_bwd_wrapper"], [234, 0, 1, "", "get_cosine_schedule_with_warmup"], [235, 0, 1, "", "get_dtype"], [236, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [237, 0, 1, "", "get_lr"], [238, 0, 1, "", "get_memory_stats"], [239, 0, 1, "", "get_quantizer_mode"], [240, 0, 1, "", "get_unmasked_sequence_lengths"], [241, 0, 1, "", "get_world_size_and_rank"], [242, 0, 1, "", "init_distributed"], [243, 0, 1, "", "is_distributed"], [244, 0, 1, "", "log_memory_stats"], [245, 0, 1, "", "lora_fsdp_wrap_policy"], [251, 0, 1, "", "register_optim_in_bwd_hooks"], [252, 0, 1, "", "set_activation_checkpointing"], [253, 0, 1, "", "set_default_dtype"], [254, 0, 1, "", "set_seed"], [255, 0, 1, "", "setup_torch_profiler"], [256, 0, 1, "", "update_state_dict_for_classifier"], [257, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.training.FormattedCheckpointFiles": [[226, 4, 1, "", "build_checkpoint_filenames"]], "torchtune.training.FullModelHFCheckpointer": [[227, 4, 1, "", "load_checkpoint"], [227, 4, 1, "", "save_checkpoint"]], "torchtune.training.FullModelMetaCheckpointer": [[228, 4, 1, "", "load_checkpoint"], [228, 4, 1, "", "save_checkpoint"]], "torchtune.training.FullModelTorchTuneCheckpointer": [[229, 4, 1, "", "load_checkpoint"], [229, 4, 1, "", "save_checkpoint"]], "torchtune.training.OptimizerInBackwardWrapper": [[231, 4, 1, "", "get_last_lr"], [231, 4, 1, "", "get_optim_key"], [231, 4, 1, "", "load_state_dict"], [231, 4, 1, "", "set_lr_scheduler"], [231, 4, 1, "", "state_dict"], [231, 4, 1, "", "step_lr_scheduler"]], "torchtune.training.metric_logging": [[246, 1, 1, "", "CometLogger"], [247, 1, 1, "", "DiskLogger"], [248, 1, 1, "", "StdoutLogger"], [249, 1, 1, "", "TensorBoardLogger"], [250, 1, 1, "", "WandBLogger"]], "torchtune.training.metric_logging.CometLogger": [[246, 4, 1, "", "close"], [246, 4, 1, "", "log"], [246, 4, 1, "", "log_config"], [246, 4, 1, "", "log_dict"]], "torchtune.training.metric_logging.DiskLogger": [[247, 4, 1, "", "close"], [247, 4, 1, "", "log"], [247, 4, 1, "", "log_dict"]], "torchtune.training.metric_logging.StdoutLogger": [[248, 4, 1, "", "close"], [248, 4, 1, "", "log"], [248, 4, 1, "", "log_dict"]], "torchtune.training.metric_logging.TensorBoardLogger": [[249, 4, 1, "", "close"], [249, 4, 1, "", "log"], [249, 4, 1, "", "log_dict"]], "torchtune.training.metric_logging.WandBLogger": [[250, 4, 1, "", "close"], [250, 4, 1, "", "log"], [250, 4, 1, "", "log_config"], [250, 4, 1, "", "log_dict"]], "torchtune.utils": [[258, 0, 1, "", "batch_to_device"], [259, 0, 1, "", "get_device"], [260, 0, 1, "", "get_logger"], [261, 0, 1, "", "torch_version_ge"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:data", "3": "py:property", "4": "py:method"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "data", "Python data"], "3": ["py", "property", "Python property"], "4": ["py", "method", "Python method"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 7, 8, 21, 33, 39, 40, 42, 225, 264, 266, 271, 273, 275, 276, 277, 279, 280], "config": [0, 23, 24, 271, 274], "data": [1, 10, 33, 39, 40, 42, 272], "text": [1, 2, 13, 15, 19, 275], "templat": [1, 9, 11, 13, 18, 20, 272], "type": 1, "messag": [1, 12, 13, 35], "transform": [1, 5, 12, 13, 14, 216], "collat": 1, "helper": 1, "function": 1, "dataset": [2, 9, 10, 11, 15, 17, 19, 272], "imag": [2, 13, 15], "gener": [2, 3, 72, 273, 275], "builder": 2, "class": [2, 18, 24], "model": [4, 5, 14, 20, 25, 271, 273, 274, 275, 276, 277, 278, 279], "llama3": [4, 115, 272, 275, 276, 279], "2": [4, 276], "vision": [4, 5], "1": [4, 276], "llama2": [4, 100, 272, 273, 277, 280], "code": 4, "llama": 4, "qwen": 4, "phi": 4, "3": 4, "mistral": [4, 157], "gemma": [4, 90], "clip": 4, "modul": 5, "compon": [5, 23, 278], "build": [5, 265, 280], "block": 5, "loss": 5, "base": [5, 20], "token": [5, 13, 20, 272], "util": [5, 8], "peft": [5, 278], "fusion": 5, "rlhf": 6, "train": [7, 225, 268, 274], "checkpoint": [7, 21, 25, 273, 278], "reduc": 7, "precis": [7, 278], "distribut": [7, 268], "memori": [7, 277, 278, 280], "manag": 7, "schedul": 7, "metric": [7, 22, 25], "log": [7, 22, 25], "perform": [7, 277], "profil": 7, "miscellan": [7, 8], "chat": [9, 272], "exampl": [9, 11, 12, 14, 15, 17, 19], "format": [9, 11, 13, 15, 17, 19, 21], "load": [9, 11, 15, 17, 19, 20], "from": [9, 11, 15, 17, 19, 20, 272, 280], "hug": [9, 11, 15, 17, 19, 20, 273], "face": [9, 11, 15, 17, 19, 20, 273], "local": [9, 11, 15, 17, 19], "remot": [9, 11, 15], "specifi": 9, "convers": 9, "style": 9, "sharegpt": 9, "openai": 9, "renam": [9, 11], "column": [9, 11], "built": [9, 11, 15, 17, 18, 19, 271], "overview": [10, 21, 266, 269, 273, 278], "pipelin": 10, "instruct": [11, 265, 275], "configur": [12, 23], "custom": [12, 18, 272], "creat": [13, 14], "prompt": [13, 18, 20, 272], "access": [13, 275], "content": 13, "multimod": [14, 15], "us": [14, 18, 23, 24, 272, 273, 276, 280], "interleav": 15, "sampl": [16, 76], "pack": 16, "prefer": 17, "defin": 18, "via": [18, 265, 275], "dotpath": 18, "string": 18, "dictionari": 18, "prompttempl": [18, 37], "complet": 19, "json": 19, "txt": 19, "download": [20, 271, 273, 274], "file": 20, "set": 20, "max": 20, "sequenc": 20, "length": 20, "special": [20, 272], "handl": 21, "differ": 21, "hfcheckpoint": 21, "metacheckpoint": 21, "torchtunecheckpoint": 21, "intermedi": 21, "vs": 21, "final": 21, "lora": [21, 267, 273, 277, 278, 280], "put": [21, 280], "thi": 21, "all": [21, 23, 280], "togeth": [21, 280], "comet": 22, "logger": [22, 25], "about": 23, "where": 23, "do": 23, "paramet": [23, 278], "live": 23, "write": 23, "instanti": [23, 26], "referenc": 23, "other": [23, 273], "field": 23, "interpol": 23, "valid": [23, 29, 271], "your": [23, 24, 273, 274], "best": 23, "practic": 23, "airtight": 23, "public": 23, "api": 23, "onli": 23, "command": 23, "line": 23, "overrid": 23, "remov": 23, "what": [24, 266, 276, 277, 279, 280], "ar": 24, "recip": [24, 269, 271, 274, 276, 277, 279], "script": 24, "run": [24, 271, 273], "cli": [24, 271], "pars": [24, 28], "weight": 25, "bias": 25, "w": 25, "b": 25, "log_config": 27, "alpacatomessag": 30, "chatmltempl": 31, "chosenrejectedtomessag": 32, "grammarerrorcorrectiontempl": 33, "inputoutputtomessag": 34, "openaitomessag": 36, "prompttemplateinterfac": 38, "questionanswertempl": 39, "role": 40, "sharegpttomessag": 41, "summarizetempl": 42, "format_content_with_imag": 43, "left_pad_sequ": 44, "load_imag": 45, "padded_col": 46, "padded_collate_dpo": 47, "padded_collate_sft": 48, "padded_collate_tiled_images_and_mask": 49, "truncat": 50, "validate_messag": 51, "concatdataset": 52, "packeddataset": 53, "preferencedataset": 54, "sftdataset": 55, "textcompletiondataset": 56, "alpaca_cleaned_dataset": 57, "alpaca_dataset": 58, "chat_dataset": 59, "cnn_dailymail_articles_dataset": 60, "grammar_dataset": 61, "hh_rlhf_helpful_dataset": 62, "instruct_dataset": 63, "llava_instruct_dataset": 64, "the_cauldron_dataset": 65, "preference_dataset": 66, "samsum_dataset": 67, "slimorca_dataset": 68, "stack_exchange_paired_dataset": 69, "text_completion_dataset": 70, "wikitext_dataset": 71, "generate_next_token": 73, "get_causal_mask_from_padding_mask": 74, "get_position_ids_from_padding_mask": 75, "tilepositionalembed": 77, "tiledtokenpositionalembed": 78, "tokenpositionalembed": 79, "clip_vision_encod": 80, "code_llama2_13b": 81, "code_llama2_70b": 82, "code_llama2_7b": 83, "lora_code_llama2_13b": 84, "lora_code_llama2_70b": 85, "lora_code_llama2_7b": 86, "qlora_code_llama2_13b": 87, "qlora_code_llama2_70b": 88, "qlora_code_llama2_7b": 89, "gemma_2b": 91, "gemma_7b": 92, "gemma_token": 93, "lora_gemma": 94, "lora_gemma_2b": 95, "lora_gemma_7b": 96, "qlora_gemma_2b": 97, "qlora_gemma_7b": 98, "llama2chattempl": 99, "llama2_13b": 101, "llama2_70b": 102, "llama2_7b": 103, "llama2_reward_7b": 104, "llama2_token": 105, "lora_llama2": 106, "lora_llama2_13b": 107, "lora_llama2_70b": 108, "lora_llama2_7b": 109, "lora_llama2_reward_7b": 110, "qlora_llama2_13b": 111, "qlora_llama2_70b": 112, "qlora_llama2_7b": 113, "qlora_llama2_reward_7b": 114, "llama3_70b": 116, "llama3_8b": 117, "llama3_token": 118, "lora_llama3": 119, "lora_llama3_70b": 120, "lora_llama3_8b": 121, "qlora_llama3_70b": 122, "qlora_llama3_8b": 123, "llama3_1": 124, "llama3_1_405b": 125, "llama3_1_70b": 126, "llama3_1_8b": 127, "lora_llama3_1": 128, "lora_llama3_1_405b": 129, "lora_llama3_1_70b": 130, "lora_llama3_1_8b": 131, "qlora_llama3_1_405b": 132, "qlora_llama3_1_70b": 133, "qlora_llama3_1_8b": 134, "llama3_2_1b": 135, "llama3_2_3b": 136, "lora_llama3_2_1b": 137, "lora_llama3_2_3b": 138, "qlora_llama3_2_1b": 139, "qlora_llama3_2_3b": 140, "llama3visionencod": 141, "llama3visionprojectionhead": 142, "llama3visiontransform": 143, "llama3_2_vision_11b": 144, "llama3_2_vision_decod": 145, "llama3_2_vision_encod": 146, "llama3_2_vision_transform": 147, "lora_llama3_2_vision_11b": 148, "lora_llama3_2_vision_decod": 149, "lora_llama3_2_vision_encod": 150, "qlora_llama3_2_vision_11b": 151, "mistralchattempl": 152, "lora_mistr": 153, "lora_mistral_7b": 154, "lora_mistral_classifi": 155, "lora_mistral_reward_7b": 156, "mistral_7b": 158, "mistral_classifi": 159, "mistral_reward_7b": 160, "mistral_token": 161, "qlora_mistral_7b": 162, "qlora_mistral_reward_7b": 163, "lora_phi3": 164, "lora_phi3_mini": 165, "phi3": 166, "phi3_mini": 167, "phi3_mini_token": 168, "qlora_phi3_mini": 169, "lora_qwen2": 170, "lora_qwen2_0_5b": 171, "lora_qwen2_1_5b": 172, "lora_qwen2_7b": 173, "qwen2": [174, 276], "qwen2_0_5b": 175, "qwen2_1_5b": 176, "qwen2_7b": 177, "qwen2_token": 178, "feedforward": 179, "fp32layernorm": 180, "kvcach": 181, "multiheadattent": 182, "rmsnorm": 183, "rotarypositionalembed": 184, "tanhgat": 185, "tiedlinear": 186, "transformercrossattentionlay": 187, "transformerdecod": 188, "transformerselfattentionlay": 189, "visiontransform": 190, "delete_kv_cach": 191, "disable_kv_cach": 192, "local_kv_cach": 193, "reparametrize_as_dtype_state_dict_post_hook": 194, "cewithchunkedoutputloss": 195, "forwardklloss": 196, "forwardklwithchunkedoutputloss": 197, "deepfusionmodel": 198, "fusionembed": 199, "fusionlay": 200, "get_fusion_param": 201, "register_fusion_modul": 202, "adaptermodul": 203, "loralinear": 204, "disable_adapt": 205, "get_adapter_param": 206, "set_trainable_param": 207, "validate_missing_and_unexpected_for_lora": 208, "validate_state_dict_for_lora": 209, "basetoken": 210, "modeltoken": 211, "sentencepiecebasetoken": 212, "tiktokenbasetoken": 213, "parse_hf_tokenizer_json": 214, "tokenize_messages_no_special_token": 215, "visioncrossattentionmask": 217, "estimate_advantag": 218, "get_rewards_ppo": 219, "dpoloss": 220, "ppoloss": 221, "rsoloss": 222, "simpoloss": 223, "truncate_sequence_at_first_stop_token": 224, "fsdppolicytyp": 225, "formattedcheckpointfil": 226, "fullmodelhfcheckpoint": 227, "fullmodelmetacheckpoint": 228, "fullmodeltorchtunecheckpoint": 229, "modeltyp": 230, "optimizerinbackwardwrapp": 231, "apply_selective_activation_checkpoint": 232, "create_optim_in_bwd_wrapp": 233, "get_cosine_schedule_with_warmup": 234, "get_dtyp": 235, "get_full_finetune_fsdp_wrap_polici": 236, "get_lr": 237, "get_memory_stat": 238, "get_quantizer_mod": 239, "get_unmasked_sequence_length": 240, "get_world_size_and_rank": 241, "init_distribut": 242, "is_distribut": 243, "log_memory_stat": 244, "lora_fsdp_wrap_polici": 245, "cometlogg": 246, "disklogg": 247, "stdoutlogg": 248, "tensorboardlogg": 249, "wandblogg": 250, "register_optim_in_bwd_hook": 251, "set_activation_checkpoint": 252, "set_default_dtyp": 253, "set_se": 254, "setup_torch_profil": 255, "update_state_dict_for_classifi": 256, "validate_expected_param_dtyp": 257, "batch_to_devic": 258, "get_devic": 259, "get_logg": 260, "torch_version_g": 261, "comput": [263, 270], "time": [263, 270], "welcom": 264, "document": 264, "get": [264, 271, 275], "start": [264, 271], "tutori": 264, "instal": 265, "pre": 265, "requisit": 265, "pypi": 265, "git": 265, "clone": 265, "nightli": 265, "kei": 266, "concept": 266, "design": 266, "principl": 266, "singl": 267, "devic": [267, 279], "finetun": [267, 269, 273, 277, 279, 280], "quantiz": [268, 273, 275, 278, 279], "awar": 268, "qat": [268, 279], "list": 271, "copi": 271, "fine": [272, 274, 275, 276, 277, 278, 279, 280], "tune": [272, 274, 275, 276, 277, 278, 279, 280], "chang": 272, "when": 272, "should": 272, "i": 272, "end": 273, "workflow": 273, "7b": 273, "evalu": [273, 275, 279], "eleutherai": [273, 275], "s": [273, 275], "eval": [273, 275], "har": [273, 275], "speed": 273, "up": 273, "librari": 273, "upload": 273, "hub": 273, "first": 274, "llm": 274, "select": 274, "modifi": 274, "next": 274, "step": [274, 278], "meta": 275, "8b": [275, 276], "our": 275, "faster": 275, "distil": 276, "1b": 276, "knowledg": 276, "how": [276, 277], "doe": [276, 277], "work": [276, 277], "kd": 276, "ablat": 276, "studi": 276, "teacher": 276, "student": 276, "hyperparamet": 276, "learn": 276, "rate": 276, "ratio": 276, "5b": 276, "0": 276, "appli": [277, 279], "trade": 277, "off": 277, "optim": 278, "activ": 278, "offload": 278, "gradient": 278, "accumul": 278, "lower": [278, 279], "fuse": 278, "backward": 278, "pass": 278, "effici": 278, "low": 278, "rank": 278, "adapt": 278, "qlora": [278, 280], "option": 279, "save": 280, "deep": 280, "dive": 280}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})