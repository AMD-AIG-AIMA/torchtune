Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.Role", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.loss.IPOLoss", "generated/torchtune.modules.loss.PPOLoss", "generated/torchtune.modules.loss.RSOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.left_padded_collate", "generated/torchtune.modules.rlhf.padded_collate_dpo", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.loss.IPOLoss.rst", "generated/torchtune.modules.loss.PPOLoss.rst", "generated/torchtune.modules.loss.RSOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.left_padded_collate.rst", "generated/torchtune.modules.rlhf.padded_collate_dpo.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "torchtune.data.Role", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "left_padded_collate", "padded_collate_dpo", "truncate_sequence_at_first_stop_token", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 21, 23, 31, 32, 33, 36, 40, 44, 85, 111, 118, 119, 120, 192, 196, 197, 200, 202, 203, 204], "prompt": [1, 14, 17, 18, 19, 21, 23, 24, 25, 26, 29, 31, 33, 36, 37, 39, 40, 41, 42, 59, 69, 85, 103, 115, 128, 152, 166, 198, 199, 201], "chat": [1, 2, 15, 16, 19, 25, 26, 29, 37, 42, 69, 120], "includ": [1, 6, 7, 8, 15, 18, 49, 60, 69, 70, 86, 95, 108, 120, 138, 155, 159, 160, 164, 194, 196, 197, 198, 199, 200, 201, 202, 204], "some": [1, 6, 7, 16, 106, 140, 141, 192, 194, 196, 197, 198, 199, 200, 202, 203, 204], "specif": [1, 4, 7, 8, 10, 169, 197, 198, 199, 203, 204], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29, 31, 33, 36, 37, 40, 42, 69, 85, 156, 159, 160, 161, 162, 196, 197, 199, 200, 201, 202], "differ": [1, 7, 9, 30, 31, 33, 46, 47, 48, 103, 130, 133, 147, 149, 162, 189, 194, 196, 197, 199, 201, 202, 203, 204], "dataset": [1, 5, 7, 14, 17, 18, 20, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 133, 134, 194, 200, 201, 203], "model": [1, 2, 6, 7, 8, 10, 16, 21, 29, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 151, 152, 159, 160, 161, 162, 165, 166, 169, 171, 177, 183, 184, 192, 194, 197, 198, 204], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 23, 24, 26, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 61, 62, 71, 72, 73, 74, 89, 103, 109, 111, 120, 123, 128, 129, 130, 132, 133, 134, 136, 137, 140, 143, 149, 151, 153, 155, 156, 159, 160, 161, 163, 164, 165, 166, 180, 181, 183, 191, 193, 195, 196, 198, 199, 200, 201, 202, 203], "common": [1, 2, 4, 7, 152, 196, 197, 198, 201, 202, 203], "json": [1, 6, 25, 26, 29, 31, 33, 34, 37, 40, 44, 45, 89, 120, 151, 159, 196, 198, 199, 203], "messag": [1, 15, 16, 19, 21, 25, 26, 28, 29, 37, 59, 69, 85, 103, 115, 152, 193, 196, 197, 198], "miscellan": 1, "function": [1, 4, 6, 7, 8, 10, 12, 29, 47, 48, 49, 122, 123, 130, 131, 133, 135, 139, 142, 143, 147, 158, 159, 166, 167, 173, 177, 186, 194, 197, 198, 204], "us": [1, 2, 3, 4, 6, 9, 10, 12, 16, 19, 20, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 69, 70, 76, 86, 89, 90, 95, 98, 116, 120, 122, 123, 126, 127, 128, 129, 130, 131, 133, 135, 139, 142, 144, 145, 149, 150, 153, 154, 156, 158, 159, 160, 162, 163, 164, 166, 167, 168, 169, 171, 177, 178, 179, 180, 181, 186, 192, 193, 194, 196, 198, 200, 201, 202, 203], "modifi": [1, 7, 8, 9, 131, 194, 199, 201, 202, 203, 204], "For": [2, 5, 6, 7, 8, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 44, 45, 46, 47, 48, 49, 60, 64, 69, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 128, 130, 154, 155, 159, 164, 165, 172, 181, 184, 186, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204], "detail": [2, 6, 29, 31, 33, 34, 35, 37, 40, 42, 44, 45, 46, 47, 48, 49, 69, 110, 125, 130, 135, 158, 169, 177, 186, 196, 198, 199, 200, 201, 202, 203, 204], "usag": [2, 131, 162, 163, 187, 193, 196, 198, 199, 200, 201, 203, 204], "guid": [2, 7, 9, 194, 197, 198, 200, 202], "pleas": [2, 5, 46, 47, 48, 49, 56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 130, 158, 169, 177, 184, 193, 199, 201, 204], "see": [2, 5, 6, 9, 19, 21, 29, 31, 33, 34, 35, 37, 40, 42, 44, 45, 56, 57, 58, 67, 68, 69, 81, 82, 83, 84, 93, 94, 101, 102, 110, 113, 114, 121, 125, 130, 137, 158, 162, 164, 169, 170, 177, 181, 184, 186, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "our": [2, 6, 8, 194, 197, 198, 199, 200, 202, 203, 204], "tutori": [2, 6, 69, 184, 194, 197, 198, 199, 200, 201, 202, 203, 204], "support": [2, 3, 6, 8, 9, 10, 20, 21, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 64, 76, 90, 98, 104, 106, 116, 119, 120, 122, 124, 130, 136, 138, 154, 156, 160, 161, 163, 168, 171, 172, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "sever": 2, "wide": 2, "help": [2, 6, 19, 128, 130, 159, 164, 192, 193, 194, 196, 197, 198, 199, 200, 203, 204], "quickli": [2, 7, 34, 197, 198], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 23, 24, 29, 34, 47, 48, 49, 69, 130, 180, 181, 192, 193, 194, 196, 197, 198, 201, 202, 203, 204], "fine": [2, 6, 8, 9, 20, 32, 44, 69, 192, 194, 199, 202, 203], "tune": [2, 3, 6, 7, 8, 9, 12, 20, 32, 44, 69, 192, 193, 194, 196, 199, 202, 203, 204], "also": [2, 6, 7, 8, 9, 10, 30, 37, 40, 44, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 120, 122, 128, 138, 167, 169, 171, 177, 181, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204], "like": [2, 4, 6, 7, 8, 9, 29, 120, 130, 161, 193, 196, 197, 198, 199, 200, 202, 203], "These": [2, 4, 6, 7, 8, 10, 32, 130, 153, 164, 197, 198, 199, 200, 201, 202, 203, 204], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 17, 18, 19, 21, 23, 24, 28, 31, 32, 33, 36, 37, 39, 40, 41, 47, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 69, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 128, 130, 138, 139, 142, 143, 145, 147, 153, 155, 158, 159, 160, 162, 163, 165, 166, 168, 171, 175, 177, 187, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "especi": [2, 194, 196, 199], "specifi": [2, 6, 7, 8, 10, 37, 70, 76, 86, 90, 95, 98, 122, 128, 129, 158, 166, 169, 172, 177, 181, 184, 187, 196, 197, 198, 199, 200, 201, 203, 204], "yaml": [2, 7, 8, 10, 11, 12, 30, 37, 40, 44, 164, 181, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "config": [2, 6, 9, 10, 11, 12, 13, 30, 37, 40, 44, 122, 142, 159, 163, 164, 181, 187, 194, 197, 198, 199, 201, 202, 203, 204], "represent": [2, 202, 203, 204], "abov": [2, 3, 6, 131, 155, 175, 193, 199, 201, 202, 203, 204], "all": [3, 4, 8, 13, 30, 32, 37, 49, 89, 120, 122, 123, 128, 130, 131, 139, 154, 155, 159, 163, 164, 165, 175, 183, 189, 190, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203], "famili": [3, 6, 8, 36, 38, 42, 43, 45, 162, 194, 196, 201], "request": [3, 14, 168, 198, 199], "access": [3, 6, 7, 8, 30, 159, 165, 196, 198, 199, 200], "hug": [3, 6, 16, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 89, 120, 132, 151, 194, 196, 200, 201], "face": [3, 6, 16, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 89, 120, 132, 151, 194, 196, 200, 201], "To": [3, 6, 7, 8, 9, 32, 130, 159, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "download": [3, 6, 190, 193, 197, 198, 201, 202, 203, 204], "8b": [3, 88, 92, 94, 97, 100, 102, 117, 196, 197, 203], "meta": [3, 6, 19, 69, 85, 127, 159, 160, 196, 197, 199, 200], "hf": [3, 6, 115, 133, 134, 136, 159, 196, 197, 199, 200, 201], "token": [3, 6, 7, 8, 20, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 60, 63, 64, 69, 70, 75, 76, 85, 86, 89, 90, 95, 98, 103, 104, 106, 108, 110, 112, 115, 116, 118, 120, 122, 127, 128, 129, 130, 135, 145, 146, 148, 149, 150, 151, 152, 153, 166, 169, 182, 196, 198, 199, 200, 201, 202, 203, 204], "hf_token": 3, "70b": [3, 51, 54, 57, 72, 78, 82, 87, 91, 93, 96, 99, 101, 201], "ignor": [3, 6, 44, 115, 122, 123, 196], "pattern": [3, 150, 196], "origin": [3, 6, 35, 36, 131, 138, 197, 199, 201, 202, 203, 204], "consolid": [3, 6], "weight": [3, 6, 8, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 122, 131, 133, 137, 138, 142, 149, 159, 160, 161, 162, 172, 177, 181, 192, 196, 197, 199, 200, 201, 202, 203, 204], "you": [3, 6, 7, 8, 9, 10, 18, 19, 20, 24, 29, 31, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 130, 155, 162, 164, 166, 180, 181, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 69, 103, 126, 127, 130, 139, 149, 150, 154, 156, 158, 159, 162, 164, 169, 177, 180, 181, 184, 187, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "instead": [3, 6, 8, 32, 37, 40, 44, 49, 123, 125, 130, 138, 196, 201, 202, 203], "The": [3, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 29, 30, 31, 32, 33, 39, 41, 42, 43, 46, 47, 48, 49, 53, 54, 55, 59, 64, 65, 66, 69, 76, 77, 78, 79, 80, 85, 90, 91, 92, 98, 99, 100, 103, 104, 106, 115, 116, 117, 124, 126, 127, 130, 131, 132, 133, 134, 135, 136, 139, 144, 146, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 161, 164, 167, 168, 170, 172, 181, 185, 187, 188, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "reus": [3, 194], "llama3_token": [3, 166, 197, 201], "builder": [3, 6, 35, 38, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 105, 107, 109, 111, 113, 114, 117, 119, 121, 197, 198, 204], "class": [3, 7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 29, 30, 31, 32, 33, 34, 37, 40, 46, 47, 48, 59, 69, 74, 85, 103, 106, 110, 111, 115, 122, 123, 124, 125, 126, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 140, 141, 149, 150, 153, 159, 160, 161, 162, 163, 164, 178, 179, 180, 181, 197, 198, 200, 202, 204], "7b": [3, 6, 31, 33, 34, 36, 38, 40, 44, 45, 52, 55, 58, 62, 66, 73, 74, 79, 80, 83, 84, 105, 107, 109, 111, 114, 159, 160, 197, 200, 201, 202, 204], "2": [3, 6, 9, 28, 32, 42, 46, 47, 59, 69, 85, 103, 115, 122, 130, 134, 135, 146, 147, 148, 149, 150, 152, 154, 155, 156, 159, 160, 172, 182, 185, 186, 187, 188, 197, 199, 200, 201, 202, 203], "13b": [3, 6, 50, 53, 56, 71, 77, 81], "codellama": 3, "mini": [3, 115, 117, 118, 119, 120, 121], "4k": [3, 118, 119, 120], "microsoft": [3, 119, 120], "ai": [3, 109, 122, 181, 197, 201], "v0": 3, "mistralai": [3, 196], "size": [3, 6, 8, 10, 36, 39, 41, 47, 48, 49, 122, 125, 126, 127, 128, 130, 144, 145, 153, 154, 156, 157, 173, 175, 194, 196, 198, 199, 200, 201, 202, 203], "2b": [3, 61, 65], "googl": [3, 61, 62], "gguf": 3, "vision": [3, 49], "compon": [3, 6, 8, 13, 147, 194, 198, 200, 202, 204], "multimod": [3, 20], "encod": [3, 4, 49, 59, 69, 85, 103, 115, 133, 149, 150, 152, 153, 197], "perform": [4, 6, 32, 69, 123, 130, 139, 166, 194, 197, 199, 201, 203, 204], "direct": [4, 8, 133, 147, 193], "text": [4, 20, 29, 31, 32, 33, 34, 37, 38, 40, 44, 45, 69, 85, 103, 115, 149, 150, 152, 153, 197, 199, 203], "id": [4, 6, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 69, 85, 103, 115, 122, 127, 128, 129, 146, 147, 149, 150, 151, 152, 153, 159, 161, 166, 182, 197, 198, 199], "decod": [4, 60, 64, 70, 76, 85, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 128, 149, 150, 166, 197], "typic": [4, 7, 32, 34, 44, 120, 133, 198, 203, 204], "byte": [4, 150, 204], "pair": [4, 7, 14, 43, 134, 147, 150, 182, 198], "underli": [4, 103, 149, 204], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 131, 137, 140, 142, 156, 163, 164, 172, 193, 194, 198, 202, 204], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 23, 24, 25, 26, 27, 29, 31, 33, 34, 37, 38, 40, 44, 45, 48, 69, 103, 124, 131, 140, 141, 142, 143, 149, 152, 159, 160, 161, 163, 166, 174, 177, 186, 189, 196, 197, 198, 200, 202, 203], "preprocess": [4, 32, 130], "imag": [4, 20, 46, 47, 48, 49, 130, 153, 154, 155, 156, 157, 202], "algorithm": [4, 144, 186], "ppo": [4, 133, 135, 144, 145], "offer": 5, "allow": [5, 30, 142, 155, 180, 196, 203, 204], "seamless": 5, "transit": 5, "between": [5, 6, 134, 135, 145, 159, 162, 198, 199, 201, 202, 203, 204], "train": [5, 6, 8, 9, 19, 29, 30, 31, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 69, 122, 124, 127, 128, 129, 131, 132, 133, 159, 160, 161, 168, 171, 177, 187, 192, 194, 196, 197, 198, 199, 201, 202, 203, 204], "interoper": [5, 6, 8, 194, 199, 204], "rest": [5, 197, 204], "ecosystem": [5, 6, 8, 194, 199, 201, 204], "comprehens": 5, "overview": [5, 7, 9, 192, 200, 202, 204], "deep": [5, 6, 7, 8, 9, 194, 200, 201], "dive": [5, 6, 7, 8, 9, 194, 200, 201], "enabl": [5, 7, 8, 9, 30, 53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 83, 84, 91, 92, 93, 94, 99, 100, 101, 102, 105, 107, 113, 114, 117, 121, 138, 186, 187, 201, 202, 204], "work": [5, 6, 8, 164, 194, 196, 199, 201, 204], "set": [5, 6, 7, 8, 9, 20, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 44, 45, 70, 76, 85, 86, 90, 95, 98, 104, 106, 108, 110, 115, 116, 118, 122, 127, 128, 139, 141, 156, 158, 167, 169, 175, 177, 184, 185, 186, 187, 194, 196, 197, 199, 200, 201, 202, 203], "consumpt": [5, 30], "dure": [5, 6, 31, 32, 36, 39, 41, 122, 125, 127, 128, 129, 130, 131, 171, 197, 199, 201, 202, 203, 204], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 27, 30, 31, 32, 33, 42, 49, 128, 130, 133, 139, 161, 164, 167, 169, 181, 187, 194, 196, 197, 198, 199, 200, 201], "debug": [5, 6, 7, 8, 196], "finetun": [5, 6, 7, 8, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 192, 194, 200, 201], "job": [5, 9, 186, 200], "variou": [5, 18], "walk": [6, 8, 180, 194, 197, 198, 199, 200, 203, 204], "through": [6, 7, 8, 9, 49, 123, 130, 139, 194, 196, 197, 198, 199, 200, 203, 204], "design": [6, 8], "behavior": [6, 177, 197, 198], "associ": [6, 7, 8, 49, 60, 70, 86, 95, 108, 166, 199, 202], "util": [6, 7, 8, 9, 10, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 194, 199, 200, 201, 203, 204], "what": [6, 7, 9, 19, 21, 23, 39, 41, 130, 192, 197, 198, 199, 200, 201], "cover": [6, 7, 8, 9, 197, 199, 204], "how": [6, 7, 8, 9, 24, 130, 158, 184, 192, 196, 197, 198, 199, 200, 201, 203, 204], "we": [6, 7, 8, 9, 31, 32, 33, 34, 36, 38, 40, 44, 45, 69, 103, 122, 125, 127, 128, 130, 133, 138, 154, 155, 159, 160, 161, 166, 168, 172, 177, 183, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "them": [6, 7, 30, 31, 33, 40, 59, 69, 103, 115, 123, 130, 131, 152, 196, 197, 198, 199, 202, 203, 204], "scenario": [6, 30], "full": [6, 7, 8, 37, 40, 45, 56, 57, 58, 59, 67, 68, 69, 81, 82, 83, 84, 93, 94, 101, 102, 103, 113, 114, 115, 121, 142, 143, 152, 194, 196, 198, 199, 201, 202, 203], "compos": [6, 130], "which": [6, 7, 8, 30, 31, 32, 34, 36, 39, 41, 44, 53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 103, 104, 105, 106, 107, 116, 117, 122, 127, 128, 129, 130, 132, 142, 143, 149, 155, 159, 160, 161, 163, 168, 178, 181, 184, 194, 196, 197, 198, 199, 200, 202, 203, 204], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 123, 142, 159, 160, 161, 194, 197, 198, 199, 201, 204], "evalu": [6, 8, 192, 194, 200, 202, 204], "gener": [6, 8, 14, 17, 23, 24, 29, 31, 32, 33, 38, 42, 44, 69, 103, 139, 144, 185, 186, 187, 190, 192, 197, 198, 202, 203, 204], "each": [6, 8, 15, 18, 30, 32, 46, 47, 48, 49, 53, 54, 55, 59, 64, 65, 66, 69, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 103, 104, 105, 106, 107, 115, 116, 117, 122, 127, 128, 129, 130, 133, 134, 136, 142, 143, 144, 145, 147, 152, 153, 155, 157, 186, 187, 194, 196, 198, 199, 200, 202, 203], "make": [6, 7, 8, 9, 122, 129, 130, 194, 196, 197, 199, 200, 201, 202, 203, 204], "easi": [6, 8, 194, 198, 202], "understand": [6, 7, 8, 192, 194, 197, 198, 202, 204], "extend": [6, 8, 194], "befor": [6, 28, 31, 32, 33, 46, 47, 49, 60, 64, 122, 128, 129, 130, 138, 150, 159, 196, 199, 203], "let": [6, 7, 9, 196, 197, 198, 199, 200, 201, 202, 204], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 53, 54, 55, 59, 69, 76, 77, 78, 79, 80, 85, 90, 91, 92, 98, 99, 100, 103, 104, 105, 106, 107, 115, 116, 117, 120, 122, 125, 127, 128, 129, 130, 131, 133, 134, 136, 137, 140, 142, 143, 148, 150, 155, 158, 159, 160, 163, 167, 169, 171, 177, 180, 184, 185, 194, 196, 197, 198, 200, 202, 203, 204], "defin": [6, 7, 8, 29, 31, 33, 34, 37, 40, 44, 45, 123, 137, 138, 140, 145, 198, 200, 202], "concept": [6, 199, 200], "In": [6, 7, 8, 29, 47, 48, 49, 127, 130, 134, 138, 155, 158, 177, 180, 181, 197, 199, 201, 202, 203, 204], "ll": [6, 7, 8, 166, 172, 194, 197, 198, 199, 200, 201, 203, 204], "talk": 6, "about": [6, 8, 130, 133, 181, 194, 196, 197, 199, 200, 201, 202, 203, 204], "take": [6, 7, 8, 10, 123, 125, 130, 131, 147, 159, 161, 164, 167, 197, 198, 199, 200, 201, 202, 204], "close": [6, 8, 178, 179, 180, 181, 202], "look": [6, 7, 8, 165, 180, 193, 197, 198, 199, 200, 201, 202, 203], "veri": [6, 30, 128, 196, 199], "simpli": [6, 7, 32, 133, 134, 196, 197, 198, 199, 201, 204], "dictat": 6, "state_dict": [6, 131, 142, 159, 160, 161, 162, 163, 202, 204], "store": [6, 178, 181, 202, 204], "file": [6, 7, 8, 9, 10, 11, 12, 29, 31, 33, 34, 37, 40, 44, 45, 59, 69, 85, 89, 103, 115, 120, 149, 150, 151, 159, 160, 161, 164, 178, 181, 187, 191, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204], "disk": [6, 34, 178], "string": [6, 20, 36, 37, 38, 39, 41, 42, 43, 59, 69, 85, 103, 115, 137, 149, 150, 152, 167, 168, 172, 196, 198], "kei": [6, 7, 9, 14, 17, 23, 24, 25, 29, 31, 33, 40, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 128, 129, 141, 142, 143, 147, 159, 161, 163, 187, 196, 199, 200, 202, 204], "identifi": 6, "state": [6, 8, 130, 131, 133, 140, 141, 142, 143, 144, 159, 160, 161, 163, 165, 199, 201, 202, 204], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 37, 38, 40, 44, 45, 85, 115, 131, 140, 141, 142, 143, 146, 147, 150, 151, 159, 160, 161, 163, 165, 171, 174, 176, 182, 183, 198], "If": [6, 7, 13, 14, 17, 18, 20, 21, 23, 24, 25, 27, 28, 29, 31, 33, 36, 39, 40, 41, 42, 49, 70, 76, 85, 86, 90, 95, 98, 115, 122, 127, 128, 129, 130, 131, 138, 143, 155, 156, 159, 160, 161, 162, 163, 166, 167, 168, 169, 171, 172, 174, 180, 181, 186, 187, 189, 193, 196, 197, 198, 199, 200, 201, 202, 203], "don": [6, 7, 8, 181, 186, 196, 197, 198, 199, 200, 204], "t": [6, 7, 8, 134, 168, 181, 186, 196, 197, 198, 199, 200, 204], "match": [6, 31, 33, 40, 115, 143, 155, 193, 196, 198, 199, 201, 202], "up": [6, 8, 9, 31, 32, 33, 34, 36, 38, 40, 44, 45, 150, 153, 154, 156, 165, 187, 196, 197, 198, 200, 201, 202, 204], "exactli": [6, 143, 203], "those": [6, 162, 199, 201, 202], "definit": [6, 202], "either": [6, 143, 159, 166, 184, 196, 202, 203, 204], "run": [6, 7, 9, 12, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 123, 125, 128, 131, 159, 160, 161, 163, 165, 175, 180, 181, 183, 193, 194, 197, 198, 200, 201, 202, 203, 204], "explicit": 6, "error": [6, 7, 28, 125, 159, 186, 196], "load": [6, 8, 29, 30, 31, 32, 33, 34, 36, 38, 39, 41, 42, 43, 44, 45, 142, 159, 160, 161, 163, 164, 180, 197, 198, 199, 201, 202], "rais": [6, 10, 13, 21, 25, 28, 29, 31, 37, 42, 115, 122, 125, 128, 130, 142, 143, 147, 152, 159, 160, 161, 163, 168, 171, 174, 181, 186, 189], "an": [6, 7, 8, 9, 10, 14, 28, 29, 30, 31, 34, 39, 41, 44, 45, 46, 47, 48, 76, 90, 98, 104, 106, 110, 116, 122, 125, 128, 130, 133, 137, 139, 140, 141, 153, 154, 155, 156, 158, 159, 160, 161, 163, 167, 169, 181, 187, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "except": [6, 20, 21, 152, 198], "wors": 6, "silent": [6, 123], "succe": 6, "infer": [6, 19, 29, 60, 69, 108, 122, 125, 127, 128, 129, 167, 192, 197, 199, 200, 201, 203, 204], "expect": [6, 7, 10, 14, 17, 18, 23, 24, 29, 31, 33, 37, 40, 127, 143, 163, 181, 189, 197, 198, 202, 203], "addit": [6, 7, 8, 10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 69, 133, 142, 158, 159, 160, 161, 168, 169, 174, 177, 178, 180, 181, 184, 194, 197, 200, 202], "line": [6, 8, 14, 164, 196, 198, 200, 201], "need": [6, 7, 8, 9, 18, 29, 32, 42, 122, 123, 128, 130, 177, 180, 181, 183, 193, 196, 197, 198, 199, 200, 201, 202, 204], "shape": [6, 46, 47, 48, 49, 122, 125, 127, 128, 129, 130, 133, 134, 135, 136, 138, 144, 145, 146, 148, 153, 155, 157, 166, 187], "valu": [6, 7, 26, 42, 50, 51, 52, 60, 61, 62, 64, 70, 71, 72, 73, 74, 76, 86, 87, 88, 90, 95, 96, 97, 98, 104, 106, 108, 109, 110, 111, 116, 118, 122, 125, 126, 128, 129, 132, 135, 142, 144, 145, 147, 148, 159, 162, 163, 164, 166, 178, 179, 180, 181, 186, 196, 198, 200, 201, 202, 203], "two": [6, 7, 28, 47, 130, 148, 153, 155, 194, 199, 200, 201, 202, 203, 204], "popular": [6, 194, 198, 199], "llama2": [6, 7, 8, 10, 19, 29, 31, 33, 34, 36, 38, 40, 42, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 123, 128, 129, 162, 192, 194, 196, 200, 201, 203], "offici": [6, 19, 197, 200, 201], "implement": [6, 8, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 69, 103, 115, 123, 126, 127, 130, 132, 133, 134, 135, 136, 137, 138, 159, 172, 180, 194, 198, 202, 203, 204], "when": [6, 7, 8, 12, 30, 32, 34, 44, 69, 122, 127, 128, 129, 130, 131, 132, 142, 145, 154, 156, 166, 169, 180, 183, 196, 199, 201, 202, 203, 204], "llama": [6, 19, 29, 69, 85, 126, 127, 159, 160, 196, 197, 199, 200, 201, 202], "websit": 6, "get": [6, 7, 8, 9, 29, 69, 103, 168, 170, 171, 173, 193, 194, 197, 198, 199, 200, 202, 203], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 30, 32, 34, 44, 47, 48, 49, 74, 111, 122, 130, 142, 159, 160, 161, 162, 163, 165, 196, 197, 198, 199, 200, 201, 202, 204], "pth": [6, 199], "inspect": [6, 199, 202, 204], "content": [6, 15, 20, 25, 26, 29, 59, 69, 103, 115, 152, 197, 198], "easili": [6, 7, 194, 198, 202, 203, 204], "torch": [6, 7, 46, 47, 48, 124, 125, 128, 130, 131, 132, 133, 134, 135, 136, 144, 145, 146, 147, 148, 155, 156, 157, 161, 163, 165, 166, 167, 168, 171, 172, 174, 175, 182, 183, 184, 185, 186, 187, 188, 189, 199, 200, 201, 202, 204], "import": [6, 7, 10, 37, 40, 44, 130, 133, 180, 181, 197, 198, 199, 200, 201, 202, 203, 204], "00": [6, 191, 195, 200], "mmap": [6, 199], "true": [6, 7, 20, 30, 31, 32, 34, 35, 36, 37, 39, 40, 41, 44, 45, 49, 56, 57, 58, 59, 60, 64, 67, 68, 69, 81, 82, 83, 84, 85, 93, 94, 101, 102, 103, 113, 114, 115, 121, 122, 128, 129, 131, 135, 139, 144, 148, 149, 150, 152, 153, 155, 158, 159, 160, 161, 169, 171, 174, 175, 177, 180, 187, 188, 196, 197, 198, 199, 201, 202, 203, 204], "weights_onli": [6, 161], "map_loc": [6, 199], "cpu": [6, 8, 131, 168, 187, 193, 196, 199, 204], "tensor": [6, 46, 47, 48, 49, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 138, 144, 145, 146, 147, 148, 155, 156, 157, 159, 166, 178, 179, 180, 181, 182, 185, 202, 204], "item": 6, "print": [6, 9, 30, 36, 39, 41, 42, 59, 69, 85, 103, 115, 130, 149, 150, 152, 166, 188, 197, 198, 200, 202, 203, 204], "f": [6, 9, 36, 39, 41, 197, 199, 202, 204], "tok_embed": [6, 128], "32000": [6, 10, 202], "4096": [6, 10, 31, 33, 34, 36, 38, 40, 44, 45, 122, 127, 198, 202, 203], "len": [6, 30, 36, 39, 41, 128, 130], "292": 6, "contain": [6, 20, 25, 32, 34, 44, 59, 69, 85, 89, 103, 115, 120, 122, 125, 127, 128, 129, 137, 140, 141, 142, 144, 146, 147, 148, 150, 152, 154, 159, 160, 161, 163, 164, 165, 171, 176, 180, 182, 187, 197, 199, 201, 202], "input": [6, 14, 17, 18, 23, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 103, 115, 122, 123, 124, 126, 127, 128, 129, 130, 138, 146, 147, 149, 150, 153, 156, 157, 159, 161, 182, 186, 189, 197, 198, 202, 204], "embed": [6, 46, 47, 48, 49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 126, 127, 128, 130, 169, 197, 201, 203], "tabl": [6, 197, 199, 201, 204], "call": [6, 10, 20, 123, 130, 131, 142, 164, 178, 179, 180, 181, 183, 187, 197, 198, 202, 204], "layer": [6, 8, 49, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 70, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 111, 113, 114, 116, 117, 118, 121, 122, 128, 129, 130, 138, 142, 143, 158, 169, 194, 201, 202, 203, 204], "have": [6, 7, 10, 47, 48, 49, 122, 125, 130, 137, 143, 153, 155, 161, 163, 164, 169, 177, 180, 189, 193, 197, 198, 199, 200, 201, 202, 203, 204], "dim": [6, 122, 123, 126, 127, 128], "most": [6, 7, 154, 197, 200, 202, 204], "within": [6, 7, 10, 29, 32, 42, 46, 64, 76, 90, 98, 104, 106, 116, 123, 130, 166, 180, 186, 187, 196, 198, 202, 204], "hub": [6, 196, 198, 200], "default": [6, 7, 16, 20, 25, 26, 27, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 120, 122, 123, 126, 127, 128, 129, 131, 132, 133, 138, 142, 144, 145, 146, 147, 149, 150, 152, 159, 160, 161, 164, 166, 168, 173, 177, 178, 181, 182, 185, 186, 187, 193, 196, 197, 198, 199, 201, 202, 203, 204], "everi": [6, 8, 46, 47, 48, 123, 130, 180, 187, 193, 196, 204], "repo": [6, 159, 160, 162, 196, 199], "first": [6, 7, 10, 28, 32, 49, 125, 128, 130, 148, 159, 164, 192, 194, 197, 198, 199, 201, 202, 203, 204], "big": 6, "split": [6, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 150, 197, 198, 199, 203], "across": [6, 8, 30, 159, 180, 186, 199, 201, 203], "bin": [6, 196, 199], "correctli": [6, 8, 13, 142, 159, 193, 197, 200, 204], "piec": 6, "one": [6, 8, 28, 59, 69, 103, 115, 123, 130, 152, 155, 161, 197, 198, 199, 200, 201, 204], "pytorch_model": [6, 199], "00001": [6, 196], "00002": [6, 196], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 32, 38, 49, 64, 76, 90, 98, 104, 106, 116, 130, 138, 140, 142, 149, 159, 160, 161, 163, 164, 166, 168, 169, 171, 172, 177, 196, 198, 199, 200, 202, 203, 204], "doe": [6, 21, 25, 29, 32, 44, 60, 69, 108, 119, 122, 128, 129, 137, 152, 159, 161, 163, 164, 196, 197, 199, 203], "fewer": [6, 122], "sinc": [6, 7, 10, 123, 155, 156, 159, 161, 197, 199, 201, 203], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 23, 24, 31, 33, 34, 40, 42, 44, 45, 137, 141, 143, 150, 159, 160, 161, 162, 163, 164, 165, 166, 167, 178, 179, 180, 181, 189, 196, 197, 199, 201, 203], "caus": [6, 103, 149, 156], "try": [6, 7, 197, 199, 200, 201, 204], "same": [6, 7, 46, 47, 53, 54, 55, 59, 65, 66, 69, 77, 78, 79, 80, 91, 92, 99, 100, 103, 115, 117, 125, 129, 130, 135, 148, 152, 163, 164, 169, 181, 196, 197, 199, 201, 202, 203, 204], "As": [6, 7, 8, 9, 138, 194, 199, 204], "re": [6, 7, 161, 194, 197, 199, 200, 202], "care": [6, 123, 159, 161, 199, 201, 202], "end": [6, 8, 20, 34, 44, 85, 103, 150, 152, 192, 194, 197, 201, 202, 203], "number": [6, 8, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 128, 130, 132, 153, 154, 159, 160, 161, 166, 173, 186, 187, 196, 200, 202], "just": [6, 14, 194, 196, 197, 198, 200, 201, 202, 203], "save": [6, 8, 9, 131, 159, 160, 161, 163, 169, 177, 181, 192, 196, 197, 198, 199, 201, 202, 203], "less": [6, 42, 199, 200, 201, 204], "prone": 6, "manag": [6, 30, 139, 185, 197], "invari": 6, "accept": [6, 7, 42, 158, 198, 200, 204], "multipl": [6, 7, 8, 20, 29, 30, 122, 128, 129, 130, 138, 147, 154, 155, 178, 179, 180, 181, 187, 200, 201], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 197, 198, 199, 203], "worri": [6, 197, 200], "explicitli": [6, 137, 194, 202], "convert": [6, 25, 26, 29, 159, 182, 197, 199, 203, 204], "time": [6, 59, 60, 69, 103, 108, 115, 144, 152, 178, 180, 187, 196, 197, 198, 199, 201, 204], "produc": [6, 163, 203, 204], "back": [6, 28, 139, 159, 198, 202, 204], "form": [6, 7, 8, 28, 196], "One": [6, 203], "advantag": [6, 135, 144, 202], "being": [6, 159, 160, 161, 165, 167, 203, 204], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 25, 26, 32, 37, 40, 44, 53, 54, 55, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 110, 116, 117, 118, 122, 123, 130, 135, 137, 142, 143, 144, 157, 158, 164, 176, 178, 179, 180, 181, 193, 194, 198, 199, 200, 201, 202, 203, 204], "abl": [6, 8, 199, 200, 203], "post": [6, 130, 183, 187, 199, 201, 203, 204], "tool": [6, 20, 198, 199, 200], "quantiz": [6, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 138, 161, 172, 192, 200, 204], "eval": [6, 192, 194, 203], "without": [6, 7, 9, 14, 142, 155, 156, 193, 194, 197, 199, 202, 203], "code": [6, 8, 50, 51, 52, 53, 54, 55, 56, 57, 58, 128, 190, 194, 198, 200], "chang": [6, 7, 9, 14, 161, 193, 196, 199, 200, 201, 202, 203, 204], "OR": [6, 25], "convers": [6, 15, 16, 19, 21, 25, 26, 28, 29, 37, 42, 159, 161, 162, 194, 197, 198, 199, 202, 204], "script": [6, 9, 196, 198, 199, 200, 201], "wai": [6, 7, 29, 142, 196, 197, 198, 199, 200, 201], "surround": [6, 8, 194], "load_checkpoint": [6, 8, 159, 160, 161, 162], "save_checkpoint": [6, 8, 9, 159, 160, 161], "map": [6, 14, 17, 18, 23, 24, 25, 26, 29, 30, 31, 32, 33, 40, 85, 115, 141, 150, 151, 159, 163, 165, 178, 179, 180, 181, 183, 187, 197, 198, 199, 202], "exampl": [6, 7, 8, 9, 10, 12, 14, 17, 23, 24, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 44, 45, 49, 59, 69, 85, 103, 115, 122, 130, 133, 134, 136, 137, 139, 146, 147, 148, 149, 150, 152, 154, 155, 156, 157, 158, 159, 160, 162, 163, 166, 172, 180, 181, 182, 185, 188, 190, 191, 193, 195, 196, 197, 198, 199, 201, 202, 203, 204], "appli": [6, 8, 29, 31, 33, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 113, 114, 116, 117, 121, 122, 126, 127, 128, 129, 142, 143, 184, 194, 204], "permut": 6, "certain": [6, 7, 187, 197], "ensur": [6, 7, 13, 28, 42, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 159, 161, 168, 194, 198, 200], "behav": 6, "further": [6, 14, 130, 196, 198, 202, 203, 204], "illustr": [6, 201], "whilst": 6, "other": [6, 8, 10, 30, 134, 161, 164, 169, 187, 198, 200, 201, 202, 203], "phi3": [6, 115, 116, 117, 119, 120, 121, 162, 196], "own": [6, 177, 186, 196, 197, 198, 199, 201, 202], "found": [6, 7, 9, 126, 127, 159, 160, 161, 196, 202, 204], "folder": [6, 197], "three": [6, 8, 133, 134, 136, 200], "read": [6, 159, 160, 161, 194], "write": [6, 8, 14, 159, 160, 161, 178, 197, 198, 200], "compat": [6, 159, 161, 203], "transform": [6, 8, 29, 31, 33, 49, 53, 54, 55, 60, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 110, 116, 117, 118, 128, 129, 130, 132, 153, 154, 155, 156, 157, 184, 202, 203], "framework": [6, 8, 194], "mention": [6, 199, 204], "assum": [6, 14, 17, 18, 23, 24, 31, 33, 40, 122, 127, 128, 129, 132, 140, 150, 163, 165, 168, 177, 197, 199, 202], "checkpoint_dir": [6, 7, 159, 160, 161, 199, 201, 203], "necessari": [6, 42, 178, 179, 180, 181, 197, 202], "easiest": [6, 199, 200], "sure": [6, 7, 197, 199, 200, 201, 202, 203, 204], "everyth": [6, 8, 164, 194, 200], "follow": [6, 8, 20, 25, 26, 29, 32, 122, 132, 135, 153, 154, 161, 162, 163, 175, 181, 187, 192, 193, 196, 198, 199, 200, 201, 202, 203, 204], "flow": [6, 29, 31, 32, 33, 203, 204], "By": [6, 196, 202, 203, 204], "safetensor": [6, 159, 196], "output": [6, 18, 30, 31, 36, 39, 42, 49, 53, 54, 55, 60, 64, 70, 74, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 111, 116, 117, 122, 123, 124, 126, 127, 128, 129, 130, 138, 141, 142, 143, 153, 156, 161, 166, 169, 179, 187, 193, 196, 197, 198, 199, 200, 201, 202, 204], "dir": [6, 181, 193, 196, 199, 200, 201, 203], "output_dir": [6, 7, 159, 160, 161, 187, 199, 201, 202, 203, 204], "here": [6, 7, 9, 14, 16, 17, 23, 24, 39, 126, 127, 196, 197, 198, 199, 200, 201, 202, 203, 204], "argument": [6, 7, 10, 18, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 122, 158, 164, 169, 174, 178, 180, 181, 184, 196, 197, 198, 202, 203], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 128, 187, 196, 198, 199, 202, 204], "_component_": [6, 7, 9, 10, 30, 37, 40, 44, 187, 197, 198, 199, 201, 202, 203], "fullmodelhfcheckpoint": [6, 199], "directori": [6, 7, 159, 160, 161, 178, 180, 181, 187, 196, 197, 198, 199, 200, 201], "sort": [6, 159, 161], "so": [6, 7, 32, 130, 155, 159, 164, 193, 194, 197, 199, 200, 201, 202, 203, 204], "order": [6, 8, 159, 161, 180, 181, 200], "matter": [6, 159, 161, 196, 202], "checkpoint_fil": [6, 7, 9, 159, 160, 161, 199, 201, 202, 203, 204], "restart": [6, 196], "previou": [6, 32, 159, 160, 161], "more": [6, 7, 8, 29, 31, 33, 34, 35, 37, 40, 42, 44, 45, 69, 125, 127, 130, 142, 158, 161, 164, 181, 184, 186, 194, 196, 198, 199, 200, 201, 202, 203, 204], "next": [6, 32, 44, 49, 130, 153, 166, 201, 204], "section": [6, 8, 171, 192, 199, 201, 204], "recipe_checkpoint": [6, 159, 160, 161, 203], "null": [6, 7, 203], "usual": [6, 127, 148, 159, 181, 196, 199, 202], "model_typ": [6, 159, 160, 161, 199, 201, 203], "resume_from_checkpoint": [6, 159, 160, 161], "fals": [6, 7, 20, 25, 26, 29, 30, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 45, 49, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 121, 122, 128, 129, 138, 139, 142, 148, 149, 155, 159, 160, 161, 175, 187, 196, 197, 198, 199, 201, 202, 203, 204], "requir": [6, 7, 30, 34, 42, 44, 69, 147, 159, 161, 163, 172, 174, 175, 177, 180, 181, 186, 187, 193, 196, 197, 198, 200, 203, 204], "param": [6, 8, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 138, 140, 141, 143, 159, 177, 202, 203, 204], "directli": [6, 7, 8, 10, 37, 40, 44, 133, 158, 159, 196, 199, 200, 201, 202, 203, 204], "out": [6, 7, 8, 31, 36, 37, 39, 41, 153, 159, 160, 192, 194, 196, 197, 199, 200, 201, 202, 204], "case": [6, 8, 9, 20, 47, 48, 49, 130, 159, 163, 168, 172, 177, 178, 184, 194, 196, 197, 198, 199, 201, 202, 204], "discrep": [6, 159], "along": [6, 202], "github": [6, 10, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 122, 126, 127, 132, 133, 134, 135, 136, 142, 193, 198, 199, 200, 201], "repositori": [6, 19, 29, 31, 33, 34, 37, 40, 44, 45, 199, 200], "fullmodelmetacheckpoint": [6, 201, 203], "current": [6, 32, 60, 64, 76, 90, 98, 104, 106, 108, 116, 119, 122, 125, 127, 128, 129, 135, 160, 161, 169, 172, 173, 178, 180, 183, 186, 198, 200, 201, 203], "test": [6, 7, 8, 194, 197], "complet": [6, 8, 14, 32, 38, 44, 120, 134, 197, 198, 199, 200, 201], "written": [6, 7, 8, 159, 160, 178, 179, 180, 181, 194], "begin": [6, 32, 44, 69, 103, 130, 150, 197, 201, 204], "partit": [6, 159, 204], "ha": [6, 69, 103, 130, 137, 139, 140, 143, 148, 161, 163, 189, 197, 198, 199, 200, 201, 202, 204], "standard": [6, 17, 25, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 179, 194, 197, 199, 201], "key_1": [6, 161], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 199], "inform": [6, 181, 184, 194, 196, 199, 200], "subsequ": [6, 8, 130, 153], "recipe_st": [6, 159, 160, 161], "pt": [6, 9, 159, 160, 161, 199, 201, 203], "epoch": [6, 8, 9, 132, 159, 160, 161, 196, 197, 199, 200, 201, 203], "optim": [6, 7, 8, 30, 60, 69, 108, 119, 132, 133, 135, 136, 147, 161, 163, 165, 171, 183, 187, 197, 199, 200, 201, 202, 204], "etc": [6, 8, 159, 171, 200], "prevent": [6, 32, 133, 196], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 64, 128, 137, 163, 183, 186, 197, 198, 199, 202, 203, 204], "updat": [6, 7, 8, 125, 133, 135, 163, 187, 193, 197, 199, 200, 201, 202, 203, 204], "hf_model_0001_0": [6, 199], "hf_model_0002_0": [6, 199], "both": [6, 30, 143, 196, 199, 202, 203, 204], "adapt": [6, 137, 138, 139, 140, 141, 159, 160, 161, 197, 199, 202, 204], "merg": [6, 10, 11, 159, 199, 201, 204], "would": [6, 7, 9, 32, 128, 130, 134, 193, 197, 198, 199, 202, 204], "addition": [6, 149, 150, 186, 198, 202], "option": [6, 7, 8, 14, 17, 18, 23, 24, 27, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 48, 49, 53, 54, 55, 59, 64, 65, 66, 69, 70, 76, 77, 78, 79, 80, 85, 86, 89, 90, 91, 92, 95, 98, 99, 100, 103, 104, 105, 106, 107, 115, 116, 117, 120, 122, 127, 128, 129, 130, 131, 135, 142, 143, 144, 145, 149, 152, 155, 156, 159, 160, 161, 166, 167, 168, 170, 172, 178, 181, 186, 187, 193, 194, 196, 198, 199], "save_adapter_weights_onli": 6, "choos": [6, 202], "primari": [6, 7, 8, 200], "want": [6, 7, 8, 9, 10, 29, 154, 155, 166, 193, 196, 197, 198, 199, 200, 201, 202], "resum": [6, 8, 132, 159, 160, 161, 204], "initi": [6, 8, 12, 30, 32, 50, 51, 52, 61, 62, 71, 72, 73, 74, 87, 88, 96, 97, 109, 111, 133, 163, 174, 175, 200, 202, 204], "frozen": [6, 133, 202, 204], "base": [6, 10, 20, 31, 33, 42, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 113, 114, 116, 117, 118, 121, 127, 132, 133, 134, 136, 138, 139, 141, 142, 143, 145, 159, 164, 167, 169, 177, 178, 192, 197, 199, 200, 201, 202, 204], "well": [6, 7, 8, 194, 196, 198, 199, 201, 204], "learnt": [6, 197, 199], "someth": [6, 8, 9, 197, 199, 203], "NOT": [6, 60, 108], "refer": [6, 7, 8, 126, 127, 130, 133, 134, 135, 136, 139, 145, 194, 202, 203], "adapter_checkpoint": [6, 159, 160, 161], "adapter_0": [6, 199], "now": [6, 163, 165, 197, 198, 199, 200, 201, 202, 203, 204], "knowledg": 6, "creat": [6, 7, 10, 32, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 87, 88, 91, 92, 93, 94, 96, 97, 99, 100, 101, 102, 105, 107, 109, 111, 113, 114, 117, 119, 121, 125, 130, 132, 158, 159, 160, 161, 165, 178, 180, 196, 197, 198, 199, 204], "simpl": [6, 8, 14, 17, 23, 24, 130, 192, 198, 200, 202, 203, 204], "forward": [6, 8, 46, 47, 48, 122, 123, 124, 126, 127, 128, 129, 130, 133, 134, 135, 136, 138, 171, 187, 201, 202, 204], "modeltyp": [6, 159, 160, 161], "llama2_13b": [6, 77], "right": [6, 159, 199, 201, 202], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 142, 163, 202], "successfulli": [6, 196, 200], "vocab": [6, 10, 128, 201], "70": [6, 87], "x": [6, 46, 47, 48, 122, 123, 124, 126, 127, 128, 129, 130, 138, 166, 185, 202, 203, 204], "randint": 6, "0": [6, 8, 32, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 69, 70, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 122, 128, 130, 132, 133, 134, 135, 136, 138, 146, 147, 148, 152, 155, 166, 172, 180, 181, 182, 186, 188, 191, 195, 197, 198, 199, 200, 201, 202, 203, 204], "1": [6, 8, 32, 42, 46, 47, 59, 69, 70, 76, 85, 86, 90, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 106, 108, 110, 115, 116, 118, 122, 128, 130, 132, 133, 134, 135, 136, 146, 147, 149, 150, 152, 155, 156, 160, 162, 166, 175, 180, 181, 182, 185, 186, 196, 197, 199, 200, 202, 203, 204], "no_grad": 6, "6": [6, 32, 60, 64, 126, 130, 146, 147, 157, 182, 203, 204], "3989": 6, "9": [6, 130, 147, 199, 203, 204], "0531": 6, "3": [6, 32, 49, 85, 117, 119, 120, 130, 146, 147, 155, 156, 157, 162, 164, 170, 172, 182, 185, 196, 197, 199, 200, 201, 203, 204], "2375": 6, "5": [6, 7, 14, 130, 132, 133, 146, 147, 148, 155, 182, 199, 200, 201], "2822": 6, "4": [6, 7, 42, 49, 122, 130, 146, 147, 154, 172, 182, 188, 194, 196, 198, 199, 201, 202, 203, 204], "4872": 6, "7469": 6, "8": [6, 36, 39, 41, 53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 83, 84, 91, 92, 93, 94, 99, 100, 101, 102, 105, 107, 113, 114, 117, 121, 130, 146, 147, 199, 202, 203, 204], "6737": 6, "11": [6, 130, 147, 199, 203, 204], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 130, 135, 146, 147, 153, 182], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 130], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 20, 31, 33, 40, 42, 142, 152, 181, 196, 197, 198, 199, 200, 201, 202, 203], "find": [6, 8, 9, 133, 196, 199, 200, 202], "list": [6, 7, 15, 16, 19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 49, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 67, 68, 69, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 121, 130, 137, 138, 142, 143, 146, 147, 149, 150, 152, 153, 154, 155, 159, 160, 161, 164, 166, 170, 182, 197, 198, 199, 200, 201, 203], "hope": 6, "deeper": [6, 200], "insight": [6, 199], "happi": [6, 199], "thi": [7, 8, 9, 10, 17, 20, 24, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 60, 64, 69, 70, 76, 85, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 119, 120, 122, 123, 127, 128, 129, 130, 131, 132, 133, 134, 135, 137, 139, 142, 143, 145, 147, 149, 150, 152, 153, 158, 159, 160, 161, 163, 164, 166, 167, 168, 171, 175, 177, 178, 180, 181, 183, 184, 186, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "pars": [7, 10, 11, 151, 164, 197, 200], "effect": [7, 203], "cli": [7, 9, 11, 12, 193, 199, 200], "prerequisit": [7, 197, 198, 199, 200, 201, 202, 203, 204], "Be": [7, 197, 199, 200, 201, 202, 203, 204], "familiar": [7, 197, 199, 200, 201, 202, 203, 204], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 197, 198, 200], "instal": [7, 9, 175, 180, 181, 192, 196, 198, 199, 200, 201, 202, 203, 204], "fundament": [7, 203], "There": [7, 28, 47, 155, 197, 200, 201, 202], "entri": [7, 8, 200], "point": [7, 8, 25, 26, 152, 198, 199, 200, 201, 202, 203, 204], "locat": [7, 196, 198, 201, 202, 203, 204], "thei": [7, 8, 30, 49, 128, 130, 143, 164, 169, 196, 197, 198, 202, 203], "truth": [7, 199, 201], "reproduc": 7, "overridden": [7, 123, 164, 187], "quick": 7, "experiment": 7, "serv": [7, 152, 158, 198, 202], "particular": [7, 29, 30, 42, 158, 198, 202, 204], "seed": [7, 8, 9, 186, 200, 203], "shuffl": [7, 32, 203], "devic": [7, 8, 142, 163, 167, 168, 171, 196, 197, 199, 200, 201, 202], "cuda": [7, 167, 168, 171, 187, 193, 199, 204], "dtype": [7, 8, 125, 128, 131, 168, 185, 189, 199, 203, 204], "fp32": [7, 203, 204], "enable_fsdp": 7, "mani": [7, 32, 198, 199], "object": [7, 10, 11, 15, 16, 19, 21, 49, 122, 133, 158, 172, 197], "keyword": [7, 10, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 131, 197, 198], "loss": [7, 8, 20, 31, 36, 39, 41, 133, 134, 135, 136, 200, 202, 204], "subfield": 7, "dotpath": [7, 198], "wish": [7, 198], "exact": [7, 10, 199], "path": [7, 8, 9, 10, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 63, 69, 75, 85, 89, 103, 112, 115, 120, 149, 150, 151, 159, 160, 161, 164, 187, 196, 197, 198, 199, 201, 202], "normal": [7, 29, 32, 69, 103, 124, 126, 128, 129, 149, 197, 198, 202, 203, 204], "python": [7, 164, 170, 181, 186, 190, 196, 198, 199, 203], "alpaca_dataset": [7, 35, 198], "custom": [7, 8, 29, 31, 33, 37, 40, 44, 184, 194, 196, 200, 201, 202], "train_on_input": [7, 25, 26, 29, 30, 31, 35, 36, 37, 39, 40, 41, 42, 197, 198], "onc": [7, 139, 199, 200, 201, 202, 204], "ve": [7, 125, 196, 197, 198, 199, 201, 202], "instanc": [7, 10, 29, 30, 31, 76, 90, 98, 104, 106, 116, 123, 131, 140, 141, 202], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 37, 196, 199, 204], "under": [7, 187, 198, 204], "preced": [7, 10, 196, 201, 202], "actual": [7, 9, 14, 17, 23, 24, 29, 197, 203], "throw": 7, "notic": [7, 46, 47, 48, 130, 197, 198, 202], "miss": [7, 142, 143, 187, 202], "posit": [7, 10, 32, 46, 47, 48, 49, 60, 64, 104, 106, 108, 110, 116, 118, 122, 125, 127, 128, 129, 130, 201], "anoth": [7, 199], "handl": [7, 12, 30, 69, 103, 149, 150, 197, 199, 202, 204], "def": [7, 8, 9, 12, 158, 162, 197, 198, 202, 204], "dictconfig": [7, 8, 10, 11, 12, 13, 181, 187], "arg": [7, 10, 48, 124, 128, 131, 137, 164, 179, 187, 203], "tupl": [7, 10, 42, 48, 59, 69, 85, 103, 115, 125, 130, 131, 133, 134, 135, 136, 144, 145, 147, 148, 152, 154, 155, 156, 158, 164, 173, 187, 189], "kwarg": [7, 10, 124, 131, 137, 164, 174, 178, 179, 180, 181, 184, 187, 198], "str": [7, 10, 11, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 63, 69, 75, 85, 89, 103, 112, 115, 120, 131, 137, 138, 140, 141, 142, 143, 146, 147, 149, 150, 151, 159, 160, 161, 162, 163, 164, 167, 168, 170, 171, 172, 174, 176, 178, 179, 180, 181, 182, 186, 187, 188, 189, 197, 198], "mean": [7, 122, 126, 128, 129, 144, 177, 196, 197, 198, 200, 202, 203], "pass": [7, 10, 20, 29, 30, 31, 33, 34, 37, 38, 40, 44, 45, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 123, 131, 135, 139, 143, 150, 158, 161, 168, 169, 171, 174, 177, 180, 181, 184, 187, 196, 197, 198, 202, 203, 204], "add": [7, 9, 29, 32, 34, 44, 69, 85, 130, 150, 152, 161, 162, 164, 198, 199, 201, 202, 204], "d": [7, 20, 122, 125, 128, 196, 197, 202, 203], "llama2_token": [7, 197, 199], "tmp": [7, 163, 197, 200], "llama2token": [7, 75], "modeltoken": [7, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 152, 197, 198], "bool": [7, 20, 25, 26, 29, 31, 32, 34, 35, 36, 37, 39, 40, 41, 42, 44, 45, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 121, 128, 131, 138, 142, 143, 144, 148, 149, 150, 152, 155, 158, 159, 160, 161, 169, 171, 174, 175, 177, 180, 184, 187, 188, 197, 204], "max_seq_len": [7, 10, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 60, 64, 69, 70, 76, 85, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 122, 125, 127, 128, 146, 152, 197, 198, 203], "int": [7, 9, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 113, 114, 115, 116, 117, 118, 121, 122, 125, 126, 127, 128, 130, 132, 138, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 166, 169, 173, 177, 178, 179, 180, 181, 182, 184, 186, 187, 196, 197, 198, 202, 204], "512": [7, 35, 36, 49, 198, 204], "instructdataset": [7, 35, 36, 39, 40, 41, 198], "alreadi": [7, 162, 174, 177, 193, 196, 198, 199, 202], "overwrit": [7, 161, 193, 196], "duplic": [7, 8, 194, 196], "sometim": 7, "than": [7, 28, 42, 122, 125, 130, 133, 158, 161, 162, 188, 189, 197, 198, 199, 200, 201, 202, 204], "resolv": [7, 11, 200], "alpaca": [7, 14, 30, 35, 36, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 198], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 178, 179, 180, 181], "disklogg": 7, "log_dir": [7, 178, 180, 181], "conveni": [7, 8, 196], "verifi": [7, 167, 168, 169, 197, 200, 202], "properli": [7, 142, 175, 196], "experi": [7, 181, 192, 194, 197, 201, 202], "wa": [7, 47, 48, 49, 130, 142, 197, 202, 203, 204], "cp": [7, 193, 196, 197, 199, 200, 201, 203], "7b_lora_single_devic": [7, 199, 200, 202, 204], "my_config": [7, 196], "discuss": [7, 199, 200, 201, 202], "guidelin": 7, "while": [7, 8, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 123, 194, 199, 203, 204], "mai": [7, 9, 130, 169, 197, 198, 200, 202], "tempt": 7, "put": [7, 8, 200, 202, 203], "much": [7, 199, 201, 202, 203, 204], "give": [7, 198, 202], "maximum": [7, 27, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 60, 64, 70, 76, 85, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 127, 128, 146, 154, 155, 156, 196], "flexibl": [7, 30, 198], "switch": 7, "encourag": [7, 69, 202], "clariti": 7, "significantli": [7, 133], "easier": [7, 199, 200], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 161, 197, 200], "parent": [7, 196], "modul": [7, 10, 46, 47, 48, 49, 106, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 162, 165, 169, 177, 183, 184, 186, 198, 200, 202, 204], "__init__": [7, 8, 202, 204], "py": [7, 10, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 122, 125, 126, 127, 132, 133, 134, 135, 136, 196, 199, 201], "guarante": 7, "stabil": [7, 194, 203, 204], "underscor": 7, "_alpaca": 7, "collect": [7, 166, 200], "itself": 7, "via": [7, 9, 37, 40, 44, 138, 159, 202, 204], "k1": [7, 8], "v1": [7, 8, 45], "k2": [7, 8], "v2": [7, 8, 198], "lora_finetune_single_devic": [7, 196, 197, 199, 200, 201, 202, 204], "checkpoint": [7, 8, 131, 150, 159, 160, 161, 162, 163, 181, 184, 194, 196, 201, 202, 203, 204], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "assign": 7, "nest": 7, "dot": 7, "notat": [7, 122, 127, 128, 144, 145], "flag": [7, 8, 20, 31, 36, 39, 41, 158, 161, 169, 196, 204], "built": [7, 9, 43, 193, 197, 200, 204], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 29], "pytorch": [7, 8, 69, 128, 131, 142, 158, 175, 180, 184, 186, 187, 192, 193, 194, 199, 201, 202, 203, 204], "llama3": [7, 29, 42, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 116, 162, 166, 169, 192, 196, 198, 199], "8b_full": [7, 196, 198], "adamw": [7, 202], "lr": [7, 132], "2e": 7, "fuse": [7, 183, 203], "nproc_per_nod": [7, 198, 201, 202, 203], "full_finetune_distribut": [7, 196, 198, 199, 200], "core": [8, 194, 198, 200, 204], "i": [8, 19, 20, 21, 122, 128, 129, 130, 131, 141, 163, 166, 198, 199, 201, 203, 204], "structur": [8, 15, 16, 19, 21, 25, 26, 29, 37, 89, 120, 153, 197, 198, 199, 203], "new": [8, 38, 109, 125, 162, 178, 180, 197, 199, 200, 201, 202, 204], "user": [8, 15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 69, 70, 76, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 122, 152, 156, 197, 198, 200, 203], "thought": [8, 194, 200, 204], "target": [8, 194], "pipelin": [8, 194], "llm": [8, 192, 194, 198, 199, 201, 202], "eg": [8, 128, 159, 194], "meaning": [8, 194, 199], "featur": [8, 9, 193, 194, 199, 200], "fsdp": [8, 158, 163, 169, 177, 194, 200, 201], "activ": [8, 123, 171, 176, 184, 187, 194, 203, 204], "gradient": [8, 177, 183, 187, 194, 199, 201, 202, 204], "accumul": [8, 183, 187, 194], "mix": [8, 124, 196, 198, 199], "precis": [8, 124, 131, 168, 194, 200, 204], "given": [8, 10, 14, 17, 18, 23, 24, 28, 36, 38, 39, 41, 42, 43, 45, 134, 138, 139, 145, 166, 167, 168, 172, 177, 183, 188, 194, 202], "complex": 8, "becom": [8, 130, 134, 193, 198], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 128, 130, 162, 196, 198], "methodolog": 8, "reason": [8, 166, 199, 203], "possibl": [8, 32, 37, 154, 155, 196, 198], "trade": 8, "off": [8, 69, 103, 199, 203], "memori": [8, 30, 31, 32, 33, 34, 36, 38, 40, 44, 45, 131, 142, 169, 171, 176, 177, 187, 192, 194, 199, 200, 201, 203], "vs": [8, 134, 200], "qualiti": [8, 199, 202, 203], "believ": 8, "best": [8, 155, 197], "suit": [8, 200], "b": [8, 122, 125, 127, 128, 129, 138, 144, 145, 177, 181, 202, 204], "fit": [8, 29, 31, 32, 33, 34, 36, 38, 40, 44, 45, 130, 133, 134, 155, 156, 198], "solut": [8, 134], "result": [8, 49, 59, 69, 103, 115, 130, 152, 153, 187, 199, 201, 202, 203, 204], "meant": [8, 131, 163], "depend": [8, 9, 14, 159, 187, 196, 198, 199, 202, 204], "level": [8, 165, 170, 177, 194, 204], "expertis": 8, "routin": 8, "yourself": [8, 196, 201, 202], "exist": [8, 193, 196, 199, 200, 201, 204], "ad": [8, 46, 47, 48, 103, 110, 130, 149, 161, 162, 197, 198, 202, 203, 204], "ones": 8, "modular": [8, 194], "build": [8, 37, 40, 44, 49, 60, 70, 86, 95, 108, 110, 194, 201, 202], "block": [8, 32, 53, 54, 55, 60, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 116, 117, 142, 143, 194], "wandb": [8, 9, 181, 200], "log": [8, 11, 133, 134, 135, 136, 170, 171, 176, 178, 179, 180, 181, 199, 200, 201, 202, 204], "fulli": [8, 30], "nativ": [8, 192, 194, 202, 203, 204], "correct": [8, 17, 39, 126, 127, 128, 167, 194, 197, 198], "numer": [8, 194, 203], "pariti": [8, 194], "verif": 8, "extens": [8, 161, 194], "comparison": [8, 202, 204], "benchmark": [8, 186, 194, 199, 201, 202, 203], "limit": [8, 155, 156, 163, 198, 203], "hidden": [8, 49, 123, 130], "behind": 8, "100": [8, 31, 36, 39, 41, 42, 147, 166, 182, 202, 204], "prefer": [8, 23, 43, 133, 134, 135, 136, 147, 194, 196, 198], "over": [8, 132, 133, 134, 164, 194, 196, 199, 202, 204], "unnecessari": 8, "abstract": [8, 15, 18, 194, 200, 204], "No": [8, 161, 194], "inherit": [8, 164, 194, 198], "go": [8, 19, 21, 49, 59, 69, 103, 115, 130, 152, 194, 198, 199, 200, 204], "upon": [8, 30, 201], "figur": [8, 202, 204], "spectrum": 8, "decid": 8, "interact": [8, 192, 200], "start": [8, 9, 152, 162, 193, 194, 197, 198, 199, 200, 203], "avail": [8, 45, 164, 167, 168, 175, 194, 196, 199, 201, 202], "paradigm": 8, "consist": [8, 45, 200], "configur": [8, 31, 33, 36, 37, 38, 39, 40, 41, 42, 44, 45, 64, 76, 85, 90, 98, 104, 115, 116, 129, 194, 197, 200, 201, 202, 203, 204], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 165, 166, 167, 168, 169, 170, 171, 172, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204], "command": [8, 9, 164, 193, 196, 197, 198, 199, 200, 201, 202, 203, 204], "overrid": [8, 11, 12, 196, 199, 200, 201, 204], "togeth": [8, 32, 181, 200, 202, 203], "valid": [8, 28, 142, 143, 145, 189, 193, 199, 200], "environ": [8, 167, 175, 193, 196, 198, 199, 200, 203], "logic": [8, 162, 194, 200, 202], "api": [8, 9, 25, 56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 142, 196, 197, 200, 201, 204], "closer": [8, 202], "monolith": [8, 194], "trainer": [8, 133, 134, 136], "A": [8, 9, 25, 26, 30, 32, 49, 59, 69, 103, 115, 122, 128, 129, 130, 131, 133, 134, 135, 136, 138, 142, 144, 145, 146, 147, 148, 149, 150, 152, 153, 155, 158, 162, 163, 164, 171, 172, 176, 177, 182, 191, 192, 195, 196, 197, 202, 203, 204], "wrapper": [8, 124, 149, 150, 163, 165, 196, 202], "around": [8, 29, 69, 103, 124, 149, 150, 171, 196, 197, 199, 202, 203, 204], "extern": [8, 198], "primarili": [8, 30, 202], "eleutherai": [8, 45, 194, 202, 203], "har": [8, 194, 202, 203], "control": [8, 31, 36, 39, 41, 134, 139, 186, 199], "multi": [8, 29, 122, 142, 201], "stage": [8, 130], "distil": 8, "oper": [8, 130, 139, 186, 203], "turn": [8, 20, 28, 29, 197], "dataload": [8, 32, 36, 39, 41], "applic": [8, 122, 159, 160, 181], "clean": [8, 9, 35], "after": [8, 115, 122, 125, 126, 128, 129, 142, 148, 177, 178, 179, 180, 181, 197, 199, 201, 203, 204], "process": [8, 9, 49, 130, 131, 173, 174, 186, 198, 200, 203, 204], "group": [8, 122, 173, 174, 178, 179, 180, 181, 196, 201, 203], "init_process_group": [8, 174], "backend": [8, 196, 203], "gloo": 8, "els": [8, 164, 181, 194, 204], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "stuff": 8, "carri": 8, "relev": [8, 196, 199, 202], "interfac": [8, 15, 18, 30, 137, 198], "metric": [8, 200, 203], "logger": [8, 170, 176, 178, 179, 180, 181, 200], "self": [8, 9, 32, 53, 54, 55, 60, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 110, 116, 117, 118, 122, 128, 129, 137, 142, 143, 159, 162, 163, 198, 202, 204], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 158, 169, 177, 184, 197], "_model": [8, 163], "_setup_model": 8, "_token": [8, 198], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 163, 165, 183, 187, 204], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 133, 135, 186, 196, 201, 203], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 32], "batch": [8, 32, 36, 39, 41, 47, 122, 125, 127, 128, 130, 133, 134, 136, 144, 145, 146, 147, 182, 187, 194, 198, 200, 201, 202], "enumer": 8, "_autocast": 8, "logit": [8, 166], "label": [8, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 133, 147, 182], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 178, 179, 180, 181], "step": [8, 32, 128, 132, 144, 165, 178, 179, 180, 181, 183, 187, 192, 199, 202, 203, 204], "learn": [8, 30, 132, 134, 194, 197, 198, 200, 201, 202, 203, 204], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 23, 24, 27, 28, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 59, 69, 70, 76, 85, 86, 89, 90, 95, 98, 103, 115, 120, 122, 125, 127, 128, 129, 130, 135, 139, 141, 142, 143, 144, 145, 149, 152, 156, 159, 160, 161, 162, 166, 167, 168, 170, 172, 176, 178, 179, 180, 181, 183, 184, 185, 186, 187, 189, 197, 198, 199, 203], "fullfinetunerecip": 8, "wandblogg": [9, 202, 204], "workspac": 9, "seen": [9, 202, 204], "screenshot": 9, "below": [9, 14, 127, 158, 198, 201, 202, 204], "packag": [9, 180, 181, 193, 198], "pip": [9, 180, 181, 193, 199, 201], "Then": [9, 139, 200], "login": [9, 181, 196, 199], "project": [9, 49, 53, 54, 55, 60, 64, 70, 74, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 111, 116, 117, 122, 123, 130, 142, 143, 162, 169, 181, 192, 197, 202, 204], "grab": [9, 201], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 47, 48, 49, 130, 175, 197, 203], "exit": [9, 193, 196], "resourc": [9, 178, 179, 180, 181, 203], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 29, 31, 32, 33, 34, 40, 42, 44, 122, 127, 128, 129, 130, 136, 153, 166, 197, 199], "desir": [9, 29, 156, 185, 197], "suggest": 9, "approach": [9, 30, 198], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 199], "_output_dir": [9, 159, 160, 161], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 187], "type": [9, 10, 12, 20, 25, 26, 27, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 120, 122, 124, 125, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 138, 140, 144, 145, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 177, 182, 184, 185, 186, 187, 188, 189, 198, 199, 202, 203, 204], "descript": [9, 37, 42, 196], "whatev": 9, "metadata": [9, 203], "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": [9, 203], "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 25, 26, 29, 32, 36, 39, 41, 176, 198], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 69, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 91, 92, 93, 94, 99, 100, 101, 102, 109, 111, 113, 114, 117, 119, 120, 121, 122, 126, 127, 130, 132, 133, 134, 135, 136, 142, 144, 153, 158, 159, 160, 164, 170, 175, 180, 181, 184, 186, 193, 198, 199, 201], "com": [10, 53, 54, 55, 65, 66, 69, 77, 78, 79, 80, 85, 91, 92, 99, 100, 117, 122, 126, 127, 132, 133, 134, 135, 136, 142, 193, 199, 201], "facebookresearch": [10, 126], "blob": [10, 53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117, 120, 122, 126, 127, 132, 133, 134, 135, 136], "main": [10, 12, 69, 120, 122, 126, 127, 193, 199, 201], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 128, 130], "32": [10, 130, 201, 202, 203, 204], "num_head": [10, 49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 127, 128], "num_kv_head": [10, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125], "vocab_s": [10, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "must": [10, 30, 137, 164, 204], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 120, 122, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 182, 185, 186, 187, 188, 197, 198, 202, 204], "nn": [10, 122, 123, 125, 128, 129, 130, 131, 137, 139, 140, 141, 158, 165, 177, 183, 184, 189, 202, 204], "parsed_yaml": 10, "embed_dim": [10, 46, 47, 48, 49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 127, 129, 130, 202], "valueerror": [10, 21, 25, 28, 29, 31, 37, 42, 115, 122, 128, 130, 159, 160, 161, 168, 171, 186, 189], "recipe_nam": 11, "rank": [11, 53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 104, 105, 106, 107, 116, 117, 138, 173, 175, 186, 200, 202, 204], "zero": [11, 125, 126, 199, 201, 203], "displai": 11, "callabl": [12, 29, 31, 33, 128, 139, 158, 166, 169, 172, 177, 184], "With": [12, 199, 202, 203, 204], "my_recip": 12, "foo": 12, "bar": [12, 194, 200], "instanti": [13, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 86, 87, 88, 89, 90, 91, 92, 95, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 116, 117, 118, 119, 120, 163], "configerror": 13, "cannot": [13, 161, 201], "data": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 39, 40, 41, 42, 43, 44, 45, 69, 130, 133, 136, 171, 178, 179, 180, 181, 198, 199, 203, 204], "templat": [14, 17, 18, 23, 24, 29, 30, 31, 33, 36, 37, 39, 40, 41, 42, 69], "style": [14, 32, 35, 36, 37, 42, 204], "slightli": 14, "describ": [14, 69, 85, 184, 198], "task": [14, 24, 30, 38, 197, 198, 199, 201, 202, 203, 204], "context": [14, 16, 119, 139, 185, 187, 198], "respons": [14, 16, 59, 69, 103, 115, 133, 134, 136, 144, 145, 152, 198, 199, 200, 201], "appropri": [14, 16, 19, 20, 21, 30, 132, 159, 198, 204], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 198], "column_map": [14, 17, 18, 23, 24, 30, 31, 33, 40, 198], "placehold": [14, 17, 18, 23, 24, 31, 33, 40, 198], "column": [14, 17, 18, 23, 24, 31, 33, 34, 40, 44, 122, 128, 129, 197, 198, 203], "ident": [14, 17, 18, 21, 23, 24, 31, 32, 33, 40, 134, 199, 203], "poem": 14, "n": [14, 23, 59, 69, 103, 115, 122, 130, 152, 155, 191, 195, 196, 197, 198, 203], "nwrite": 14, "long": [14, 32, 150, 197, 198, 202], "where": [14, 17, 23, 24, 29, 36, 39, 41, 47, 69, 74, 103, 111, 122, 128, 130, 133, 134, 135, 138, 144, 147, 148, 149, 153, 155, 169, 177, 198], "me": 14, "tag": [15, 16, 19, 21, 29, 178, 179, 180, 181, 197], "system": [15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 69, 103, 115, 152, 197, 198], "assist": [15, 16, 19, 20, 22, 25, 26, 28, 29, 59, 69, 103, 115, 120, 152, 166, 197, 198], "role": [15, 20, 25, 26, 29, 59, 69, 103, 115, 152, 197, 198], "prepend": [15, 69, 85, 103, 149], "append": [15, 85, 103, 115, 149, 193, 198], "accord": [15, 21, 197], "openai": [16, 25, 37, 135, 198], "markup": 16, "languag": [16, 133, 138, 166, 202], "It": [16, 20, 21, 130, 133, 196, 197, 198, 204], "im_start": 16, "im_end": 16, "goe": [16, 139], "grammar": [17, 39, 198], "english": 17, "sentenc": [17, 32, 44, 103], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 202], "lazi": 17, "dog": [17, 153], "alwai": [18, 134, 164], "human": [19, 26, 133, 135, 136, 197], "pre": [19, 32, 44, 69, 130, 193, 197, 198], "taken": [19, 202, 204], "inst": [19, 21, 29, 69, 197, 198], "sy": [19, 69, 197, 198], "respect": [19, 30, 141, 155, 187, 197, 198], "honest": [19, 197, 198], "am": [19, 21, 197, 198, 199, 201], "pari": [19, 21, 24, 198], "capit": [19, 21, 23, 24, 198], "franc": [19, 21, 23, 24, 198], "known": [19, 21, 69, 103, 172, 198, 203], "its": [19, 21, 32, 106, 122, 127, 128, 129, 134, 183, 186, 196, 197, 198, 199, 201, 202], "stun": [19, 21, 198], "liter": [20, 22, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 142, 143], "ipython": [20, 22], "union": [20, 44, 45, 143, 178, 179, 180, 181, 184, 186], "mask": [20, 31, 32, 36, 39, 41, 59, 69, 85, 103, 115, 122, 128, 129, 135, 144, 152, 153, 197, 198], "eot": [20, 85], "repres": [20, 46, 47, 130, 147, 155, 197, 203], "individu": [20, 32, 171, 181, 184, 197, 198], "interleav": [20, 153], "tokenize_messag": [20, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 69, 85, 103, 115, 152, 197, 198], "attach": 20, "special": [20, 29, 69, 85, 89, 103, 115, 120, 130, 150, 151, 152, 153, 163, 198], "writer": 20, "dictionari": [20, 32, 146, 147, 171, 176, 178, 179, 180, 181, 182, 199], "hello": [20, 24, 59, 69, 85, 103, 115, 149, 150, 197, 199, 201], "world": [20, 59, 69, 85, 103, 115, 149, 150, 173, 175, 199], "whether": [20, 25, 26, 29, 31, 34, 36, 37, 39, 40, 41, 42, 44, 45, 53, 54, 55, 60, 64, 65, 66, 76, 77, 78, 79, 80, 85, 90, 91, 92, 98, 99, 100, 103, 104, 105, 106, 107, 115, 116, 117, 131, 138, 142, 143, 149, 150, 158, 168, 171, 197, 198], "calcul": [20, 122, 128, 130, 135, 144, 145, 155, 201], "correspond": [20, 135, 137, 140, 144, 147, 168, 200, 201, 203], "consecut": [20, 28, 153], "e": [20, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 122, 130, 131, 137, 141, 153, 155, 159, 163, 171, 187, 193, 199, 201, 202, 203, 204], "properti": [20, 164, 202], "contains_media": 20, "non": [20, 143, 145], "from_dict": [20, 197], "construct": [20, 153, 202], "text_cont": [20, 197], "mistral": [21, 29, 42, 103, 104, 105, 106, 107, 109, 110, 111, 112, 113, 114, 162, 196, 197, 199, 200], "llama2chatformat": [21, 69, 197, 198], "alia": [22, 158], "similar": [23, 38, 43, 44, 45, 133, 142, 198, 199, 201, 202, 204], "stackexchangedpair": 23, "question": [23, 197, 198, 199, 201], "answer": [23, 197, 199, 201], "nanswer": 23, "summar": [24, 41, 197, 198], "dialogu": [24, 41, 197], "summari": [24, 30, 41, 130, 171, 198], "dialog": 24, "did": [24, 201, 204], "know": [24, 197, 198, 199, 202], "adher": [25, 26], "could": [25, 202], "remain": [25, 26, 132, 202], "unmask": [25, 26], "sharegpt": [26, 37], "gpt": [26, 122, 199], "eos_id": [27, 85, 150, 152], "length": [27, 28, 30, 31, 32, 33, 34, 36, 38, 40, 42, 44, 45, 59, 60, 64, 69, 70, 76, 85, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 119, 122, 125, 127, 128, 144, 145, 146, 147, 150, 152, 153, 160, 182], "last": [27, 32, 44, 132, 145, 198], "replac": [27, 31, 36, 39, 41, 131, 202], "forth": [28, 198], "come": [28, 137, 202], "empti": [28, 196], "shorter": 28, "min": [28, 155, 202], "invalid": 28, "convert_to_messag": [29, 197], "chat_format": [29, 37, 42, 197, 198], "chatformat": [29, 37, 198], "load_dataset_kwarg": [29, 31, 33, 34, 37, 38, 40, 44, 45], "multiturn": [29, 197], "prepar": [29, 197, 203], "truncat": [29, 31, 32, 33, 34, 38, 40, 42, 44, 45, 59, 69, 85, 103, 115, 148, 150, 152, 198], "local": [29, 31, 33, 34, 37, 40, 44, 45, 89, 120, 181, 186, 193, 196, 197, 199, 200], "g": [29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 122, 130, 137, 153, 155, 159, 171, 187, 201, 202, 203, 204], "csv": [29, 31, 33, 34, 37, 40, 44, 45, 197, 198], "filepath": [29, 31, 33, 34, 37, 40, 44, 45], "data_fil": [29, 31, 33, 34, 37, 40, 44, 45, 197, 198], "load_dataset": [29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 197, 198], "huggingfac": [29, 31, 33, 34, 37, 38, 40, 44, 45, 111, 119, 120, 132, 133, 134, 136, 159, 160, 196, 199], "co": [29, 31, 33, 34, 37, 38, 40, 44, 45, 111, 119, 120, 159, 160, 199], "doc": [29, 31, 33, 34, 37, 38, 40, 44, 45, 69, 85, 158, 164, 170, 175, 180, 181, 186, 196, 198, 199], "en": [29, 31, 33, 34, 37, 38, 40, 44, 45, 203], "package_refer": [29, 31, 33, 34, 37, 38, 40, 44, 45], "loading_method": [29, 31, 33, 34, 37, 38, 40, 44, 45], "extra": [29, 193, 197, 202, 203, 204], "still": [29, 164, 202, 203, 204], "unless": 29, "concaten": [30, 59, 69, 103, 115, 147, 152], "sub": [30, 180], "unifi": [30, 111], "were": [30, 130, 135, 139, 197, 200, 203], "simplifi": [30, 133, 196, 202], "simultan": 30, "intern": [30, 164], "aggreg": 30, "transpar": 30, "index": [30, 32, 122, 127, 128, 129, 132, 145, 146, 147, 182, 193, 197, 199], "howev": [30, 120, 193], "constitu": 30, "might": [30, 196, 199], "larg": [30, 138, 187, 196, 204], "comput": [30, 70, 76, 86, 90, 95, 98, 122, 123, 127, 128, 133, 134, 136, 153, 154, 171, 186, 199, 203, 204], "cumul": 30, "maintain": [30, 204], "indic": [30, 31, 32, 49, 122, 127, 128, 129, 130, 135, 144, 148, 153, 158, 175, 197], "deleg": 30, "retriev": [30, 169], "lead": [30, 103, 149], "high": [30, 194, 202], "scale": [30, 53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 104, 105, 106, 107, 116, 117, 134, 138, 145, 155, 166, 202, 203, 204], "consid": [30, 47, 48, 49, 130], "strategi": 30, "stream": [30, 170], "demand": 30, "deriv": [30, 123, 128, 129], "dataset1": 30, "mycustomdataset": 30, "params1": 30, "dataset2": 30, "params2": 30, "concat_dataset": 30, "total": [30, 132, 135, 145, 173, 191, 195, 199, 201, 202], "data_point": 30, "1500": 30, "element": [30, 199], "accomplish": [30, 37, 40, 44], "instruct_dataset": [30, 198], "vicgal": [30, 198], "gpt4": [30, 198], "alpacainstructtempl": [30, 40, 198], "samsum": [30, 41, 198], "summarizetempl": [30, 197, 198], "focus": [30, 200], "enhanc": [30, 130, 204], "divers": 30, "machin": [30, 136, 167, 196, 199], "instructtempl": [31, 33, 198], "contribut": [31, 36, 39, 41, 135, 145], "variabl": [31, 33, 40, 162, 175, 198, 204], "disabl": [31, 33, 34, 38, 40, 44, 45, 139, 186, 203], "recommend": [31, 33, 34, 36, 38, 40, 44, 45, 180, 197, 199, 204], "highest": [31, 33, 34, 36, 38, 40, 44, 45], "sequenc": [31, 32, 33, 34, 36, 38, 40, 42, 44, 45, 59, 60, 64, 69, 70, 76, 85, 86, 90, 95, 98, 103, 104, 106, 108, 110, 115, 116, 118, 122, 125, 127, 128, 130, 145, 146, 147, 148, 150, 152, 153, 182, 197], "ds": [32, 42], "padding_idx": [32, 146, 147, 182], "max_pack": 32, "split_across_pack": [32, 44], "greedi": 32, "pack": [32, 35, 36, 37, 39, 40, 41, 42, 44, 45, 122, 127, 128, 129, 203], "done": [32, 142, 168, 177, 202, 203, 204], "outsid": [32, 186, 187, 202], "sampler": [32, 200], "part": [32, 136, 197, 204], "buffer": 32, "enough": [32, 197], "attent": [32, 49, 53, 54, 55, 60, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 110, 116, 117, 118, 119, 122, 125, 127, 128, 129, 142, 143, 153, 201, 202, 204], "lower": [32, 202], "triangular": 32, "cross": [32, 153], "attend": [32, 122, 128, 129, 153], "rel": [32, 122, 127, 128, 129, 133, 171, 202], "pad": [32, 130, 135, 145, 146, 147, 148, 155, 156, 182, 198], "max": [32, 42, 59, 69, 103, 115, 128, 130, 132, 150, 152, 196, 202], "wise": 32, "collat": [32, 182, 198], "made": [32, 37, 40, 44, 127, 199], "smaller": [32, 134, 199, 201, 202, 203, 204], "jam": 32, "vari": 32, "s1": [32, 69, 103, 149], "s2": [32, 69, 103, 149], "s3": 32, "s4": 32, "contamin": 32, "input_po": [32, 122, 125, 127, 128, 129], "matrix": 32, "causal": [32, 122, 128, 129], "continu": [32, 130, 198], "increment": 32, "move": [32, 44, 128], "entir": [32, 44, 177, 197, 204], "avoid": [32, 44, 126, 130, 131, 134, 186, 196, 203, 204], "add_eo": [34, 44, 59, 69, 85, 103, 115, 149, 150, 197], "freeform": [34, 44], "unstructur": [34, 44, 45], "corpu": [34, 38, 44, 45], "tabular": [34, 44], "txt": [34, 44, 178, 198, 200], "eo": [34, 44, 103, 115, 120, 149, 152, 197, 198], "yahma": [35, 40], "variant": [35, 39, 41], "version": [35, 64, 76, 90, 98, 104, 106, 116, 122, 166, 188, 193, 197, 201, 203, 204], "page": [35, 45, 193, 194, 196, 200, 201], "tatsu": 36, "lab": 36, "codebas": [36, 39, 41, 199], "anyth": [36, 38, 39, 41, 42, 43], "prior": [36, 37, 39, 40, 41, 42, 44, 45], "subset": [36, 38, 39, 41, 42, 43, 45, 64, 76, 90, 98, 104, 106, 116, 140], "10": [36, 38, 39, 41, 42, 43, 45, 130, 147, 182, 199, 201, 203, 204], "alpaca_d": 36, "batch_siz": [36, 39, 41, 122, 125, 128, 129, 133, 134, 136, 146, 148, 199, 203], "conversation_styl": [37, 198], "chatdataset": [37, 42, 197, 198], "friendli": [37, 40, 44, 166, 197], "check": [37, 46, 47, 48, 49, 128, 130, 142, 168, 175, 188, 192, 197, 199, 200, 202], "huggingfaceh4": 37, "no_robot": 37, "chatmlformat": 37, "2096": [37, 40, 44], "packeddataset": [37, 40, 44, 45, 198], "ccdv": 38, "cnn_dailymail": 38, "textcompletiondataset": [38, 44, 45, 198], "cnn": 38, "dailymail": 38, "articl": [38, 45], "extract": [38, 151], "highlight": [38, 204], "liweili": 39, "c4_200m": 39, "mirror": [39, 41], "llama_recip": [39, 41], "grammar_d": 39, "alpaca_clean": 40, "samsum_d": 41, "open": [42, 61, 62, 198, 199], "orca": 42, "slimorca": 42, "dedup": 42, "1024": [42, 43, 198, 203], "prescrib": 42, "least": [42, 201, 202, 203], "though": [42, 197], "351": 42, "82": 42, "391": 42, "221": 42, "220": 42, "193": 42, "12": [42, 130, 147, 193, 203], "471": 42, "lvwerra": [43, 198], "stack": [43, 130, 187, 198], "exchang": [43, 198], "preferencedataset": [43, 198], "stackexchangepair": 43, "allenai": [44, 198, 203], "c4": [44, 198, 203], "data_dir": [44, 198], "realnewslik": [44, 198], "wikitext_document_level": 45, "wikitext": [45, 203], "103": [45, 199], "wikipedia": 45, "clip": [46, 47, 48, 49, 130, 135], "max_num_til": [46, 47, 49, 130, 154], "tile": [46, 47, 48, 49, 130, 153, 154, 157], "patch": [46, 47, 48, 49, 130, 153], "document": [46, 47, 48, 49, 122, 134, 158, 169, 177, 196, 198], "vision_transform": [46, 47, 48, 49], "visiontransform": [46, 47, 48, 49], "divid": [46, 47, 48, 49, 130, 153, 154, 157], "dimension": [46, 47, 48, 49, 130], "aspect_ratio": [46, 47, 130], "bsz": [46, 47, 130, 166], "n_img": [46, 47, 130], "n_tile": [46, 47, 130], "n_token": [46, 47, 48, 130], "aspect": [46, 47, 194], "ratio": [46, 47, 133, 134, 135], "crop": [46, 47, 48, 49, 130, 157], "tile_s": [47, 48, 49, 130, 153, 154, 157], "patch_siz": [47, 48, 49, 130, 153], "local_token_positional_embed": 47, "equival": [47, 134, 136], "_position_embed": [47, 130], "tokenpositionalembed": [47, 130], "gate": [47, 162, 196, 200], "global_token_positional_embed": 47, "advanc": [47, 48, 49, 130, 198], "40": [47, 48, 49, 130, 153, 204], "400": [47, 48, 49, 130, 153, 157], "10x10": [47, 48, 49, 130, 153], "grid": [47, 48, 49, 130, 153], "k": [47, 122, 202], "th": 47, "cls_output_dim": [49, 130], "out_indic": [49, 130], "output_cls_project": 49, "in_channel": [49, 130], "transformerencoderlay": 49, "cl": [49, 130, 198], "head": [49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 127, 128, 162, 201], "intermedi": [49, 60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 130, 161, 184, 201, 204], "fourth": [49, 130], "determin": [49, 143, 155], "channel": [49, 130, 203], "code_llama2": [50, 51, 52, 53, 54, 55, 56, 57, 58, 196], "transformerdecod": [50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 113, 114, 116, 117, 118, 119, 121, 166, 202], "w": [50, 51, 52, 61, 62, 71, 72, 73, 74, 87, 88, 96, 97, 109, 111, 130, 156, 180, 181, 197, 199, 202, 204], "arxiv": [50, 51, 52, 56, 57, 58, 67, 68, 71, 72, 73, 74, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 122, 126, 127, 130, 133, 134, 135, 136, 144, 153], "org": [50, 51, 52, 56, 57, 58, 67, 68, 69, 71, 72, 73, 74, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 122, 126, 127, 130, 133, 134, 135, 136, 144, 153, 158, 164, 170, 175, 180, 184, 186, 193], "pdf": [50, 51, 52, 144, 153], "2308": [50, 51, 52], "12950": [50, 51, 52], "lora_attn_modul": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 142, 143, 202, 204], "q_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 122, 142, 143, 202, 203, 204], "k_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 122, 142, 143, 202, 203, 204], "v_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 122, 142, 143, 202, 203, 204], "output_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 122, 142, 143, 202, 203, 204], "apply_lora_to_mlp": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 142, 143, 202], "apply_lora_to_output": [53, 54, 55, 56, 57, 58, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 142, 143, 202], "lora_rank": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 202], "lora_alpha": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 202], "float": [53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 70, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 110, 113, 114, 116, 117, 118, 121, 122, 126, 132, 133, 134, 135, 136, 138, 144, 145, 166, 171, 176, 178, 179, 180, 181, 202, 203, 204], "16": [53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 83, 84, 91, 92, 93, 94, 99, 100, 101, 102, 105, 107, 113, 114, 117, 121, 130, 147, 202, 204], "lora_dropout": [53, 54, 55, 56, 57, 58, 64, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 98, 104, 106, 116], "05": [53, 54, 55, 56, 57, 58, 70, 76, 77, 78, 79, 80, 81, 82, 83, 84, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "quantize_bas": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 138, 204], "lora": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 138, 139, 142, 143, 159, 177, 192, 194, 197, 200, 201], "code_llama2_13b": 53, "tloen": [53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117], "8bb8579e403dc78e37fe81ffbb253c413007323f": [53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117], "l41": [53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117], "l43": [53, 54, 55, 65, 66, 77, 78, 79, 80, 91, 92, 99, 100, 117], "linear": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 82, 83, 84, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 104, 105, 106, 107, 113, 114, 116, 117, 121, 128, 137, 138, 142, 143, 202, 203, 204], "mlp": [53, 54, 55, 60, 64, 65, 66, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 110, 116, 117, 118, 128, 129, 142, 143, 201, 202], "final": [53, 54, 55, 60, 64, 70, 76, 77, 78, 79, 80, 86, 90, 91, 92, 95, 98, 99, 100, 104, 105, 106, 107, 108, 116, 117, 123, 128, 139, 142, 143, 199, 201, 202, 204], "low": [53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 104, 105, 106, 107, 116, 117, 138, 199, 202, 204], "approxim": [53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 104, 105, 106, 107, 116, 117, 138, 202], "factor": [53, 54, 55, 64, 65, 66, 76, 77, 78, 79, 80, 90, 91, 92, 98, 99, 100, 104, 105, 106, 107, 116, 117, 138, 144, 155, 199], "dropout": [53, 54, 55, 60, 64, 70, 76, 77, 78, 79, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 138, 202, 204], "probabl": [53, 54, 55, 64, 76, 77, 78, 79, 90, 98, 104, 106, 116, 133, 134, 135, 136, 138, 166, 199], "code_llama2_70b": 54, "code_llama2_7b": 55, "qlora": [56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 131, 192, 194, 201, 202], "per": [56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 125, 130, 131, 133, 145, 153, 154, 196, 203, 204], "paper": [56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 133, 134, 136, 153, 202, 204], "ab": [56, 57, 58, 67, 68, 71, 72, 73, 74, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 122, 126, 127, 130, 133, 134, 135, 136], "2305": [56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121, 122, 133, 136], "14314": [56, 57, 58, 67, 68, 81, 82, 83, 84, 93, 94, 101, 102, 113, 114, 121], "lora_code_llama2_13b": 56, "lora_code_llama2_70b": 57, "lora_code_llama2_7b": 58, "gemma": [59, 61, 62, 63, 64, 65, 66, 67, 68, 162], "sentencepiec": [59, 69, 103, 115, 149, 201], "pretrain": [59, 69, 85, 103, 115, 149, 150, 196, 197, 200, 202, 204], "spm_model": [59, 69, 103, 115, 149, 197], "tokenized_text": [59, 69, 85, 103, 115, 149, 150], "add_bo": [59, 69, 85, 103, 115, 149, 150, 197], "31587": [59, 69, 85, 103, 115, 149, 150], "29644": [59, 69, 85, 103, 115, 149, 150], "102": [59, 69, 85, 103, 115, 149, 150], "tokenizer_path": [59, 69, 103, 115], "separ": [59, 69, 103, 115, 152, 159, 197, 200, 201, 202, 204], "concat": [59, 69, 103, 115, 152], "1788": [59, 69, 103, 115, 152], "2643": [59, 69, 103, 115, 152], "13": [59, 69, 103, 115, 130, 147, 148, 152, 204], "1792": [59, 69, 103, 115, 152], "9508": [59, 69, 103, 115, 152], "465": [59, 69, 103, 115, 152], "22137": [59, 69, 103, 115, 152], "2933": [59, 69, 103, 115, 152], "join": [59, 69, 103, 115, 152], "attribut": [59, 69, 103, 115, 139, 152, 165], "head_dim": [60, 64, 122, 125, 128], "intermediate_dim": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "attn_dropout": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 128], "norm_ep": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "1e": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 126], "06": [60, 64, 126, 202], "rope_bas": [60, 64, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "10000": [60, 64, 104, 106, 108, 110, 116, 118, 127], "norm_embed": [60, 64], "gemmatransformerdecod": [60, 61, 62, 64, 65, 66, 67, 68], "transformerdecoderlay": [60, 70, 86, 95, 108, 128], "rm": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118], "norm": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 128, 129], "space": [60, 70, 86, 95, 108, 128], "slide": [60, 108, 119], "window": [60, 108, 119, 198], "vocabulari": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 202], "queri": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 128, 129, 201], "mha": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 128], "dimens": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122, 125, 127, 128, 130, 138, 201, 202, 204], "onto": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122], "scaled_dot_product_attent": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122], "epsilon": [60, 64, 70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 135], "rotari": [60, 64, 104, 106, 108, 110, 116, 118, 127, 201], "10_000": [60, 64, 104, 106, 108, 110, 118], "blog": [61, 62], "technolog": [61, 62], "develop": [61, 62, 204], "gemmatoken": 63, "becaus": [64, 125, 128, 130, 161, 196, 197, 203], "ti": 64, "gemma_2b": 65, "gemma_7b": 66, "lora_gemma_2b": 67, "lora_gemma_7b": 68, "card": [69, 85], "regist": [69, 85, 89, 115, 120, 123, 131, 183, 204], "uniqu": [69, 162], "strongli": 69, "beforehand": 69, "html": [69, 158, 164, 170, 175, 180, 184, 186, 192], "problem": [69, 103], "due": [69, 103, 149, 202, 204], "whitespac": [69, 103, 149], "slice": [69, 103], "gqa": [70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122], "mqa": [70, 76, 86, 90, 95, 98, 104, 106, 108, 110, 116, 118, 122], "kvcach": [70, 76, 86, 90, 95, 98, 116, 122, 128], "scale_hidden_dim_for_mlp": [70, 76, 86, 90, 95, 98], "2307": [71, 72, 73, 74], "09288": [71, 72, 73, 74], "classif": [74, 106, 110, 111, 162], "reward": [74, 80, 84, 107, 111, 114, 133, 134, 136, 144, 145, 162], "llama2_70b": 78, "llama2_7b": [79, 202], "classifi": [80, 106, 110, 111, 198], "llama2_reward_7b": [80, 162], "lora_llama2_13b": 81, "lora_llama2_70b": 82, "lora_llama2_7b": [83, 202], "lora_llama2_reward_7b": 84, "special_token": [85, 115, 150, 197], "tiktoken": [85, 150, 201], "left": [85, 115, 146, 202], "canon": [85, 89, 115, 120], "tt_model": [85, 150], "token_id": [85, 103, 150], "truncate_at_eo": [85, 150], "skip_special_token": [85, 150], "show": [85, 150, 153, 193, 196, 197, 202], "skip": [85, 150], "tokenize_head": 85, "tokenize_end": 85, "header": [85, 197], "eom": 85, "wether": 85, "500000": [86, 90, 95, 98], "special_tokens_path": [89, 120], "llama3token": [89, 197], "similarli": [89, 120, 198, 203], "llama3_70b": 91, "llama3_8b": [92, 166, 201, 203], "lora_llama3_70b": 93, "lora_llama3_8b": 94, "llama3_1": [96, 97, 98, 99, 100, 101, 102], "llama3_1_70b": 99, "llama3_1_8b": 100, "lora_llama3_1_70b": 101, "lora_llama3_1_8b": 102, "trim_leading_whitespac": [103, 149], "unbatch": [103, 149], "bo": [103, 120, 149, 152, 197, 198], "trim": [103, 149], "num_class": [106, 110], "announc": 109, "ray2333": 111, "feedback": [111, 133], "mistraltoken": [112, 197], "lora_mistral_7b": 113, "lora_mistral_reward_7b": 114, "ignore_system_prompt": 115, "phi3_mini": [117, 162], "ref": [119, 120, 181], "phi": [119, 120, 162], "128k": 119, "nor": 119, "phi3minitoken": 120, "tokenizer_config": 120, "spm": 120, "lm": [120, 135], "unk": 120, "augment": [120, 204], "endoftext": 120, "phi3minisentencepiecebasetoken": 120, "lora_phi3_mini": 121, "pos_embed": [122, 202, 203], "kv_cach": 122, "introduc": [122, 126, 138, 197, 198, 202, 203, 204], "13245v1": 122, "multihead": 122, "extrem": 122, "share": [122, 198, 199], "credit": 122, "lightn": 122, "lit": 122, "lit_gpt": 122, "v": [122, 128, 202], "q": [122, 202], "n_kv_head": 122, "rotarypositionalembed": [122, 202, 203], "cach": [122, 125, 127, 128, 193, 196], "rope": [122, 127], "seq_length": [122, 129, 166], "boolean": [122, 128, 129, 158], "softmax": [122, 128, 129], "row": [122, 128, 129, 155, 197], "j": [122, 128, 129], "seq_len": 122, "bigger": 122, "n_h": [122, 127], "num": [122, 127], "n_kv": 122, "kv": [122, 125, 128, 203], "emb": [122, 128], "h_d": [122, 127], "gate_proj": 123, "down_proj": 123, "up_proj": 123, "silu": 123, "feed": [123, 129], "network": [123, 139, 202, 204], "fed": [123, 197], "multipli": 123, "subclass": [123, 164], "although": [123, 202, 203], "afterward": 123, "former": 123, "hook": [123, 131, 183, 204], "latter": 123, "layernorm": 124, "standalon": 125, "past": 125, "expand": 125, "dpython": [125, 128, 131, 185, 189], "reset": [125, 128, 171], "k_val": 125, "v_val": 125, "assert": 125, "longer": [125, 198], "h": [125, 130, 156, 193, 196], "ep": 126, "root": [126, 180, 181], "squar": 126, "1910": 126, "07467": 126, "verfic": [126, 127], "small": [126, 199], "divis": [126, 157], "propos": 127, "2104": 127, "09864": 127, "l80": 127, "upto": 127, "init": [127, 171, 181, 204], "exceed": 127, "freq": 127, "recomput": 127, "geometr": 127, "progress": [127, 200], "rotat": 127, "angl": 127, "todo": 127, "effici": [127, 142, 169, 192, 194, 199, 200, 202, 203], "belong": [128, 165], "reduc": [128, 133, 194, 198, 202, 203, 204], "statement": 128, "improv": [128, 136, 150, 169, 201, 202], "readabl": [128, 199], "caches_are_en": 128, "At": 128, "arang": 128, "prompt_length": 128, "causal_mask": 128, "m_": 128, "seq": 128, "reset_cach": 128, "setup_cach": 128, "attn": [129, 202, 203, 204], "causalselfattent": [129, 202, 203], "sa_norm": 129, "mlp_norm": 129, "ff": 129, "token_pos_embed": 130, "pre_tile_pos_emb": 130, "post_tile_pos_emb": 130, "cls_project": 130, "vit": 130, "11929": 130, "convolut": 130, "flatten": 130, "treat": [130, 139, 164, 197], "downscal": [130, 155, 156], "800x400": 130, "400x400": 130, "_transform": 130, "clipimagetransform": 130, "broken": 130, "down": [130, 161, 198, 202, 204], "whole": 130, "num_til": [130, 157], "101": 130, "pool": 130, "tiledtokenpositionalembed": 130, "tilepositionalembed": 130, "tile_pos_emb": 130, "even": [130, 193, 196, 197, 198, 201, 202, 204], "8x8": 130, "14": [130, 147, 203, 204], "15": [130, 147, 169, 197, 199, 202, 204], "17": [130, 147, 202], "18": [130, 147, 201], "19": [130, 147, 204], "20": [130, 147, 148, 203], "21": 130, "22": 130, "23": [130, 132], "24": [130, 157, 200, 201], "25": [130, 199], "26": 130, "27": [130, 199], "28": [130, 199], "29": [130, 204], "30": [130, 148, 203], "31": [130, 201], "33": 130, "34": 130, "35": [130, 204], "36": 130, "37": 130, "38": [130, 199], "39": 130, "41": 130, "42": 130, "43": 130, "44": 130, "45": 130, "46": 130, "47": 130, "48": [130, 199, 204], "49": 130, "50": [130, 148, 157, 199], "51": 130, "52": [130, 200], "53": 130, "54": 130, "55": [130, 200], "56": 130, "57": [130, 202, 204], "58": 130, "59": [130, 204], "60": 130, "61": [130, 199], "62": 130, "63": 130, "64": [130, 202], "num_patches_per_til": 130, "emb_dim": 130, "greater": [130, 188], "constain": 130, "anim": [130, 198], "max_n_img": 130, "n_channel": 130, "hidden_st": 130, "vision_util": 130, "tile_crop": 130, "num_channel": 130, "image_s": [130, 156], "800": [130, 156], "patch_grid_s": 130, "random": [130, 186, 200], "rand": [130, 155, 156, 157], "nch": 130, "tile_cropped_imag": 130, "batch_imag": 130, "unsqueez": 130, "batch_aspect_ratio": 130, "clip_vision_encod": 130, "common_util": 131, "bfloat16": [131, 185, 199, 200, 201, 202, 203], "offload_to_cpu": 131, "nf4": [131, 204], "restor": 131, "higher": [131, 134, 201, 203, 204], "offload": [131, 204], "increas": [131, 132, 133, 201, 202, 203], "peak": [131, 171, 176, 199, 201, 202, 204], "gpu": [131, 196, 199, 200, 201, 202, 203, 204], "_register_state_dict_hook": 131, "m": [131, 166, 197, 203], "mymodul": 131, "_after_": 131, "nf4tensor": [131, 204], "unquant": [131, 203, 204], "unus": 131, "num_warmup_step": 132, "num_training_step": 132, "num_cycl": [132, 187], "last_epoch": 132, "lambdalr": 132, "rate": [132, 194, 200], "schedul": [132, 187, 200], "linearli": 132, "decreas": [132, 198, 202, 203, 204], "cosin": 132, "v4": 132, "src": 132, "l104": 132, "warmup": [132, 187], "phase": 132, "wave": 132, "half": 132, "lr_schedul": 132, "beta": 133, "label_smooth": 133, "dpo": [133, 134, 136, 139, 147], "18290": 133, "intuit": [133, 134, 136], "dispref": 133, "incorpor": [133, 198], "dynam": [133, 203], "degener": 133, "occur": 133, "naiv": 133, "trl": [133, 134, 136], "librari": [133, 134, 136, 164, 168, 170, 186, 192, 194, 196, 198, 204], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 133, "dpo_train": [133, 134, 136], "l844": 133, "retain": [133, 204], "2009": 133, "01325": 133, "polici": [133, 134, 135, 136, 139, 145, 158, 169, 177, 184], "align": [133, 197], "regular": [133, 134, 203, 204], "baselin": [133, 135, 199, 202], "rather": 133, "overhead": [133, 203], "temperatur": [133, 134, 136, 166, 199], "uncertainti": 133, "policy_chosen_logp": [133, 134, 136], "policy_rejected_logp": [133, 134, 136], "reference_chosen_logp": [133, 134, 136], "reference_rejected_logp": [133, 134, 136], "chosen": [133, 134, 136, 187, 198], "reject": [133, 134, 136, 198], "chosen_reward": [133, 134, 136], "rejected_reward": [133, 134, 136], "tau": 134, "optimis": 134, "ipo": 134, "2310": 134, "12036": 134, "pi": 134, "pi_ref": 134, "regress": [134, 136], "gap": 134, "likelihood": 134, "he": 134, "weaker": 134, "regularis": 134, "logprob": [134, 145], "word": [134, 203], "unlik": [134, 142], "toward": 134, "thu": [134, 203], "4dce042a3863db1d375358e8c8092b874b02934b": [134, 136], "l1143": 134, "reciproc": 134, "larger": [134, 161, 199, 201], "value_clip_rang": 135, "value_coeff": 135, "proxim": 135, "1707": 135, "06347": 135, "eqn": 135, "vwxyzjn": 135, "ccc19538e817e98a60d3253242ac15e2a562cb49": 135, "lm_human_preference_detail": 135, "train_policy_acceler": 135, "l719": 135, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 135, "ppo2": 135, "l68": 135, "l75": 135, "coeffici": [135, 145], "pi_old_logprob": 135, "pi_logprob": 135, "phi_old_valu": 135, "phi_valu": 135, "padding_mask": [135, 148], "value_padding_mask": 135, "old": 135, "predict": [135, 144, 145, 166], "participag": 135, "five": 135, "policy_loss": 135, "value_loss": 135, "clipfrac": 135, "fraction": 135, "gamma": [136, 144], "statist": 136, "rso": 136, "hing": 136, "2309": 136, "06657": 136, "logist": 136, "slic": 136, "10425": 136, "almost": [136, 202], "vector": [136, 197], "svm": 136, "counter": 136, "l1141": 136, "peft": [137, 138, 139, 140, 141, 142, 143, 159, 202, 204], "protocol": 137, "adapter_param": [137, 138, 139, 140, 141], "proj": 137, "in_dim": [137, 138, 202, 204], "out_dim": [137, 138, 202, 204], "bia": [137, 138, 202, 203, 204], "loralinear": [137, 202, 204], "alpha": [138, 202, 204], "use_bia": 138, "perturb": 138, "decomposit": [138, 202], "matric": [138, 177, 202, 204], "trainabl": [138, 141, 177, 202, 204], "mapsto": 138, "w_0x": 138, "r": [138, 202], "bax": 138, "lora_a": [138, 202, 204], "lora_b": [138, 202, 204], "temporarili": 139, "neural": [139, 202, 204], "caller": 139, "whose": [139, 183], "yield": 139, "get_adapter_param": [141, 202], "base_miss": 142, "base_unexpect": 142, "lora_miss": 142, "lora_unexpect": 142, "validate_state_dict_for_lora": [142, 202], "reli": [142, 152, 199, 201], "unexpect": 142, "strict": [142, 202], "pull": [142, 196], "120600": 142, "assertionerror": [142, 143, 147], "nonempti": 142, "full_model_state_dict_kei": 143, "lora_state_dict_kei": 143, "base_model_state_dict_kei": 143, "confirm": [143, 193], "lora_modul": 143, "complement": 143, "disjoint": 143, "overlap": 143, "rlhf": [144, 145, 146, 147, 148, 198], "lmbda": 144, "estim": [144, 145], "1506": 144, "02438": 144, "reponse_len": [144, 145], "receiv": [144, 197], "discount": 144, "gae": 144, "lambda": 144, "particip": [144, 153], "score": 145, "ref_logprob": 145, "kl_coeff": 145, "valid_score_idx": 145, "kl": 145, "response_len": 145, "total_reward": 145, "combin": [145, 154], "diverg": 145, "kl_reward": 145, "ignore_idx": [147, 182], "input_id": 147, "chosen_input_id": [147, 198], "rejected_input_id": [147, 198], "chosen_label": [147, 198], "rejected_label": [147, 198], "stop_token": [148, 166], "fill_valu": 148, "stop": [148, 166], "sequence_length": 148, "pad_id": 148, "been": [148, 169, 197, 203], "stop_token_id": 148, "869": 148, "eos_mask": 148, "truncated_sequ": 148, "light": 149, "sentencepieceprocessor": 149, "prefix": 149, "bos_id": [150, 152], "lightweight": [150, 197], "break": 150, "substr": 150, "repetit": 150, "speed": [150, 187, 201, 203, 204], "identif": 150, "regex": 150, "chunk": 150, "present": [150, 161], "absent": 150, "tokenizer_json_path": 151, "heavili": 152, "beggin": 152, "runtimeerror": [152, 163, 168, 174], "satisfi": [152, 155, 199], "image_token_id": 153, "laid": 153, "fig": 153, "flamingo": 153, "2204": 153, "14198": 153, "immedi": 153, "until": 153, "img1": 153, "img2": 153, "img3": 153, "cat": 153, "text_seq_len": 153, "image_seq_len": 153, "equal": [153, 157, 188], "resolut": [154, 155, 156], "1x1": 154, "1x2": 154, "2x1": 154, "side": [154, 155, 156], "height": [154, 155, 156], "width": [154, 155, 156, 203], "224": [154, 155, 156], "896": 154, "448": [154, 155, 156], "672": [154, 155], "possible_resolut": 155, "resize_to_max_canva": 155, "canva": 155, "resiz": [155, 156], "distort": [155, 156], "select": 155, "smallest": 155, "upscal": [155, 156], "2x": 155, "5x": 155, "canvas": 155, "condit": [155, 166, 175, 196, 198], "pick": 155, "lowest": [155, 202], "area": [155, 199], "minim": [155, 198, 200, 202, 203, 204], "200": [155, 157, 204], "300": [155, 156, 157, 199], "scale_height": 155, "1200": 155, "3600": 155, "2400": 155, "scale_width": 155, "7467": 155, "4933": 155, "scaling_factor": 155, "upscaling_opt": 155, "selected_scal": 155, "150528": 155, "100352": 155, "optimal_canva": 155, "target_s": 156, "resampl": 156, "interpolationmod": 156, "max_upscaling_s": 156, "exce": 156, "torchvis": 156, "nearest": 156, "nearest_exact": 156, "bilinear": 156, "bicub": 156, "1194": 156, "1344": 156, "stai": 156, "600": [156, 157], "500": [156, 202], "1000": [156, 158, 203], "488": 156, "channel_s": 157, "4x6": 157, "2x3": 157, "datatyp": [158, 204], "denot": 158, "integ": [158, 182, 186], "auto_wrap_polici": [158, 169, 184], "submodul": [158, 177], "obei": 158, "contract": 158, "get_fsdp_polici": 158, "modules_to_wrap": [158, 169, 177], "min_num_param": 158, "my_fsdp_polici": 158, "recurs": [158, 177, 180], "isinst": [158, 198], "sum": [158, 202], "p": [158, 163, 202, 203, 204], "numel": [158, 202], "functool": 158, "partial": 158, "stabl": [158, 175, 180, 186, 193], "safe_seri": 159, "from_pretrain": 159, "0001_of_0003": 159, "0002_of_0003": 159, "preserv": [159, 204], "weight_map": [159, 199], "convert_weight": 159, "_model_typ": [159, 162], "intermediate_checkpoint": [159, 160, 161], "adapter_onli": [159, 160, 161], "_weight_map": 159, "shard": [160, 201], "wip": 160, "qualnam": 162, "boundari": 162, "distinguish": 162, "mistral_reward_7b": 162, "qwen2": 162, "my_new_model": 162, "my_custom_state_dict_map": 162, "optim_map": 163, "bare": 163, "bone": 163, "distribut": [163, 167, 174, 175, 184, 186, 194, 196, 200, 201], "optim_dict": [163, 165, 183], "cfg_optim": 163, "ckpt": 163, "optim_ckpt": 163, "placeholder_optim_dict": 163, "optiminbackwardwrapp": 163, "get_optim_kei": 163, "arbitrari": [163, 202], "hyperparamet": [163, 194, 200, 202, 204], "optim_ckpt_map": 163, "loadabl": 163, "argpars": 164, "argumentpars": 164, "builtin": 164, "said": 164, "noth": 164, "consult": 164, "info": [164, 200], "parse_known_arg": 164, "namespac": 164, "act": 164, "precid": 164, "parse_arg": 164, "too": [164, 201], "optimizerinbackwardwrapp": 165, "top": [165, 204], "named_paramet": 165, "max_generated_token": 166, "top_k": [166, 199], "custom_generate_next_token": 166, "prune": [166, 204], "compil": [166, 199, 201, 204], "generate_next_token": 166, "hi": [166, 197], "my": [166, 196, 197, 198, 199, 201], "jeremi": 166, "float32": 168, "bf16": [168, 204], "inde": [168, 199], "kernel": 168, "isn": [168, 196], "hardwar": [168, 194, 198, 199, 202], "memory_efficient_fsdp_wrap": [169, 203], "maxim": [169, 177, 192, 194], "workload": [169, 203], "alongsid": 169, "ac": 169, "fullyshardeddataparallel": [169, 177], "fsdppolicytyp": [169, 177], "handler": 170, "reset_stat": 171, "track": 171, "alloc": [171, 176, 177, 201, 204], "reserv": [171, 176, 197, 204], "stat": [171, 176, 204], "int4": [172, 203], "4w": 172, "recogn": 172, "int8dynactint4weightquant": [172, 203], "8da4w": [172, 203], "int8dynactint4weightqatquant": [172, 203], "qat": [172, 192], "mode": [172, 199], "aka": 173, "master": 175, "port": [175, 196], "address": 175, "hold": [175, 200], "peak_memory_act": 176, "peak_memory_alloc": 176, "peak_memory_reserv": 176, "get_memory_stat": 176, "unit": [177, 194], "hierarch": 177, "requires_grad": [177, 202, 204], "filenam": 178, "log_": 178, "unixtimestamp": 178, "thread": 178, "safe": 178, "flush": [178, 179, 180, 181], "ndarrai": [178, 179, 180, 181], "scalar": [178, 179, 180, 181], "record": [178, 179, 180, 181, 187], "payload": [178, 179, 180, 181], "organize_log": 180, "tensorboard": 180, "subdirectori": 180, "compar": [180, 188, 199, 201, 202, 203, 204], "logdir": 180, "startup": 180, "tree": [180, 198, 199, 201], "tfevent": 180, "encount": 180, "frontend": 180, "organ": [180, 196], "accordingli": [180, 203], "my_log_dir": 180, "view": [180, 200], "my_metr": [180, 181], "termin": [180, 181], "entiti": 181, "bias": [181, 202, 204], "sent": 181, "usernam": 181, "my_project": 181, "my_ent": 181, "my_group": 181, "importerror": 181, "account": [181, 202, 204], "log_config": 181, "link": [181, 199, 201], "capecap": 181, "6053ofw0": 181, "torchtune_config_j67sb73v": 181, "longest": 182, "token_pair": 182, "soon": 183, "readi": [183, 192, 197, 203], "grad": 183, "achiev": [183, 199, 201, 202, 203, 204], "acwrappolicytyp": 184, "author": [184, 194, 200, 204], "fsdp_adavnced_tutori": 184, "insid": 185, "contextmanag": 185, "debug_mod": 186, "pseudo": 186, "commonli": [186, 202, 204], "numpi": 186, "determinist": 186, "global": [186, 198], "warn": 186, "nondeterminist": 186, "cudnn": 186, "set_deterministic_debug_mod": 186, "profile_memori": 187, "with_stack": 187, "record_shap": 187, "with_flop": 187, "wait_step": 187, "warmup_step": 187, "active_step": 187, "profil": 187, "layout": 187, "trace": 187, "profileract": 187, "gradient_accumul": 187, "sensibl": 187, "default_schedul": 187, "reduct": [187, 202], "iter": [187, 189, 204], "scope": 187, "flop": 187, "wait": 187, "cycl": 187, "repeat": 187, "against": [188, 203, 204], "__version__": 188, "named_param": 189, "generated_examples_python": 190, "zip": 190, "galleri": [190, 195], "sphinx": 190, "000": [191, 195, 201], "execut": [191, 195], "generated_exampl": 191, "mem": [191, 195], "mb": [191, 195], "topic": 192, "gentl": 192, "introduct": 192, "first_finetune_tutori": 192, "workflow": [192, 198, 200, 202], "requisit": 193, "proper": [193, 200], "host": [193, 196, 200], "latest": [193, 200, 204], "And": [193, 199], "ls": [193, 196, 199, 200, 201], "welcom": [193, 196], "greatest": [193, 200], "contributor": 193, "cd": [193, 199], "commit": 193, "branch": 193, "url": 193, "whl": 193, "therebi": [193, 203, 204], "forc": 193, "reinstal": 193, "opt": [193, 200], "suffix": 193, "cu121": 193, "On": [194, 202], "pointer": 194, "emphas": 194, "simplic": 194, "component": 194, "prove": 194, "democrat": 194, "box": [194, 204], "zoo": 194, "varieti": [194, 202], "techniqu": [194, 199, 200, 201, 202, 203], "integr": [194, 199, 200, 201, 202, 203, 204], "excit": 194, "checkout": 194, "quickstart": 194, "attain": 194, "better": [194, 197, 198, 199, 203], "chekckpoint": 194, "embodi": 194, "philosophi": 194, "usabl": 194, "composit": 194, "hard": [194, 198], "outlin": 194, "unecessari": 194, "never": 194, "thoroughli": 194, "short": 196, "subcommand": 196, "anytim": 196, "symlink": 196, "auto": 196, "wrote": 196, "readm": [196, 199, 201], "md": 196, "lot": [196, 199], "recent": 196, "releas": [196, 201], "agre": 196, "term": 196, "perman": 196, "eat": 196, "bandwith": 196, "storag": [196, 204], "00030": 196, "ootb": 196, "full_finetune_single_devic": [196, 198, 199, 200], "7b_full_low_memori": [196, 199, 200], "8b_full_single_devic": [196, 198], "mini_full_low_memori": 196, "7b_full": [196, 199, 200], "13b_full": [196, 199, 200], "70b_full": 196, "edit": 196, "clobber": 196, "destin": 196, "lora_finetune_distribut": [196, 201, 202], "torchrun": 196, "8b_lora_single_devic": [196, 197, 201], "launch": [196, 197, 200], "nproc": 196, "node": 196, "worker": 196, "nnode": [196, 202, 203], "minimum_nod": 196, "maximum_nod": 196, "fail": 196, "rdzv": 196, "rendezv": 196, "endpoint": 196, "8b_lora": [196, 201], "bypass": 196, "vice": 196, "versa": 196, "fancy_lora": 196, "8b_fancy_lora": 196, "sai": [196, 197, 200], "intend": 197, "nice": 197, "meet": 197, "overhaul": 197, "begin_of_text": 197, "start_header_id": 197, "end_header_id": 197, "eot_id": 197, "yet": [197, 199], "untrain": 197, "accompani": 197, "who": 197, "influenti": 197, "hip": 197, "hop": 197, "artist": 197, "2pac": 197, "rakim": 197, "c": 197, "na": 197, "flavor": [197, 198], "msg": 197, "formatted_messag": [197, 198], "nyou": [197, 198], "nwho": 197, "why": [197, 200, 202], "user_messag": 197, "518": 197, "25580": 197, "29962": 197, "3532": 197, "14816": 197, "29903": 197, "6778": 197, "_spm_model": 197, "piece_to_id": 197, "place": [197, 198], "manual": [197, 204], "529": 197, "29879": 197, "29958": 197, "nhere": 197, "128000": [197, 203], "128009": 197, "pure": 197, "That": 197, "won": 197, "mess": 197, "govern": 197, "prime": 197, "strictli": 197, "ask": 197, "untouch": 197, "nsummari": 197, "robust": 197, "onlin": 197, "forum": 197, "panda": 197, "pd": 197, "df": 197, "read_csv": 197, "your_fil": 197, "nrow": 197, "tolist": 197, "iloc": 197, "gp": 197, "commun": [197, 198, 199], "satellit": 197, "thing": [197, 204], "dataclass": 197, "message_convert": 197, "input_msg": 197, "output_msg": 197, "assistant_messag": 197, "But": [197, 199, 202], "mistralchatformat": 197, "custom_dataset": 197, "2048": 197, "honor": 197, "copi": [197, 199, 200, 201, 203, 204], "custom_8b_lora_single_devic": 197, "steer": 198, "wheel": 198, "publicli": 198, "great": [198, 199], "hood": [198, 204], "text_completion_dataset": [198, 203], "padded_col": 198, "upper": 198, "constraint": [198, 202], "slow": [198, 204], "signific": [198, 203], "speedup": [198, 199, 201], "my_data": 198, "fix": [198, 203], "goal": [198, 203], "agnost": 198, "respond": 198, "plant": 198, "miner": 198, "oak": 198, "copper": 198, "ore": 198, "eleph": 198, "customtempl": 198, "importlib": 198, "import_modul": 198, "mechan": 198, "search": 198, "often": [198, 202], "interpret": 198, "site": [198, 199], "runtim": 198, "pythonpath": 198, "chat_dataset": 198, "quit": [198, 204], "customchatformat": 198, "concatdataset": 198, "drive": 198, "rajpurkar": 198, "io": 198, "squad": 198, "explor": 198, "few": [198, 201, 202, 204], "adjust": [198, 203], "chosen_messag": 198, "transformed_sampl": 198, "key_chosen": 198, "rejected_messag": 198, "key_reject": 198, "c_mask": 198, "np": 198, "cross_entropy_ignore_idx": 198, "r_mask": 198, "stack_exchanged_paired_dataset": 198, "had": 198, "stackexchangedpairedtempl": 198, "response_j": 198, "response_k": 198, "rl": 198, "favorit": [199, 202], "seemlessli": 199, "beyond": [199, 204], "connect": [199, 203], "amount": 199, "natur": 199, "export": 199, "mobil": 199, "phone": 199, "leverag": [199, 201, 204], "plai": 199, "freez": [199, 202], "percentag": 199, "learnabl": 199, "keep": [199, 202], "16gb": [199, 202], "rtx": 199, "3090": 199, "4090": 199, "hour": 199, "7b_qlora_single_devic": [199, 200, 204], "473": 199, "98": [199, 204], "gb": [199, 201, 202, 203, 204], "484": 199, "01": [199, 200], "fact": [199, 201, 202], "third": 199, "realli": 199, "eleuther_ev": [199, 201, 203], "eleuther_evalu": [199, 201, 203], "lm_eval": [199, 201], "plan": 199, "custom_eval_config": [199, 201], "truthfulqa_mc2": [199, 201, 202], "measur": [199, 201], "propens": [199, 201], "shot": [199, 201, 203], "accuraci": [199, 201, 202, 203, 204], "324": 199, "loglikelihood": 199, "195": 199, "121": 199, "second": [199, 202, 204], "197": 199, "acc": [199, 203], "388": 199, "shown": [199, 203], "489": 199, "seem": 199, "custom_generation_config": [199, 201], "kick": 199, "interest": 199, "visit": 199, "bai": 199, "92": 199, "exploratorium": 199, "san": 199, "francisco": 199, "magazin": 199, "awesom": 199, "bridg": 199, "pretti": 199, "cool": 199, "96": [199, 204], "sec": [199, 201], "83": 199, "99": [199, 202], "72": 199, "littl": 199, "torchao": [199, 201, 203, 204], "int8_weight_onli": [199, 201], "int8_dynamic_activation_int8_weight": [199, 201], "ao": [199, 201], "quant_api": [199, 201], "_": [199, 201], "int4_weight_onli": [199, 201], "previous": [199, 201, 202], "benefit": 199, "doesn": 199, "fast": 199, "clone": [199, 202, 203, 204], "assumpt": 199, "new_dir": 199, "output_dict": 199, "sd_1": 199, "sd_2": 199, "dump": 199, "convert_hf_checkpoint": 199, "checkpoint_path": 199, "justin": 199, "school": 199, "math": 199, "teacher": 199, "ws": 199, "94": [199, 201], "bandwidth": [199, 201], "1391": 199, "84": 199, "thats": 199, "seamlessli": 199, "authent": [199, 200], "hopefulli": 199, "gave": 199, "grant": 200, "minut": 200, "agreement": 200, "altern": 200, "hackabl": 200, "singularli": 200, "technic": 200, "purpos": [200, 201], "depth": 200, "principl": 200, "boilerpl": 200, "substanti": [200, 202], "custom_config": 200, "replic": 200, "lorafinetunerecipesingledevic": 200, "lora_finetune_output": 200, "log_1713194212": 200, "3697006702423096": 200, "25880": [200, 204], "83it": 200, "monitor": 200, "tqdm": 200, "interv": 200, "e2": 200, "focu": 201, "128": [201, 202], "256": [201, 203], "theta": 201, "gain": 201, "basic": 201, "observ": [201, 203], "consum": [201, 204], "vram": [201, 202, 203], "overal": 201, "8b_qlora_single_devic": 201, "coupl": [201, 202, 204], "meta_model_0": [201, 203], "122": 201, "sarah": 201, "busi": 201, "mum": 201, "young": 201, "children": 201, "live": 201, "north": 201, "east": 201, "england": 201, "135": 201, "88": 201, "138": 201, "346": 201, "09": 201, "139": 201, "broader": 201, "teach": 202, "straight": 202, "unfamiliar": 202, "oppos": [202, 204], "momentum": 202, "relat": 202, "aghajanyan": 202, "et": 202, "al": 202, "hypothes": 202, "intrins": 202, "four": 202, "eight": 202, "practic": 202, "blue": 202, "rememb": 202, "approx": 202, "15m": 202, "8192": [202, 203], "65k": 202, "frozen_out": [202, 204], "lora_out": [202, 204], "omit": 202, "base_model": 202, "lora_model": 202, "lora_llama_2_7b": [202, 204], "alon": 202, "bit": [202, 203, 204], "in_featur": [202, 203], "out_featur": [202, 203], "inplac": 202, "feel": 202, "free": 202, "whenev": 202, "validate_missing_and_unexpected_for_lora": 202, "peft_util": 202, "set_trainable_param": 202, "fetch": 202, "lora_param": 202, "total_param": 202, "trainable_param": 202, "2f": 202, "6742609920": 202, "4194304": 202, "7b_lora": 202, "my_model_checkpoint_path": [202, 203, 204], "tokenizer_checkpoint": [202, 203, 204], "my_tokenizer_checkpoint_path": [202, 203, 204], "factori": 202, "benefici": 202, "impact": 202, "minor": 202, "good": 202, "lora_experiment_1": 202, "smooth": [202, 204], "curv": [202, 204], "ran": 202, "footprint": [202, 203], "commod": 202, "cogniz": 202, "ax": 202, "parallel": 202, "truthfulqa": 202, "475": 202, "87": 202, "508": 202, "86": 202, "504": 202, "04": 202, "514": 202, "absolut": 202, "4gb": 202, "tradeoff": 202, "potenti": 202, "awar": 203, "incur": [203, 204], "degrad": [203, 204], "perplex": 203, "simul": 203, "ultim": 203, "ptq": 203, "fake": 203, "kept": 203, "cast": 203, "nois": 203, "henc": 203, "x_q": 203, "int8": 203, "zp": 203, "x_float": 203, "qmin": 203, "qmax": 203, "round": 203, "clamp": 203, "x_fq": 203, "dequant": 203, "involv": 203, "insert": 203, "proce": 203, "prepared_model": 203, "swap": 203, "int8dynactint4weightqatlinear": 203, "int8dynactint4weightlinear": 203, "train_loop": 203, "converted_model": 203, "demonstr": 203, "recov": 203, "modif": 203, "8b_qat_ful": 203, "custom_8b_qat_ful": 203, "2000": 203, "fake_quant_after_n_step": 203, "issu": 203, "futur": 203, "empir": 203, "led": 203, "presum": 203, "80gb": 203, "qat_distribut": 203, "op": 203, "mutat": 203, "5gb": 203, "custom_quant": 203, "groupsiz": 203, "poorli": 203, "custom_eleuther_evalu": 203, "fullmodeltorchtunecheckpoint": 203, "hellaswag": 203, "max_seq_length": 203, "my_eleuther_evalu": 203, "filter": 203, "stderr": 203, "word_perplex": 203, "9148": 203, "byte_perplex": 203, "5357": 203, "bits_per_byt": 203, "6189": 203, "5687": 203, "0049": 203, "acc_norm": 203, "7536": 203, "0043": 203, "portion": [203, 204], "drop": 203, "74": 203, "048": 203, "190": 203, "7735": 203, "5598": 203, "6413": 203, "5481": 203, "0050": 203, "7390": 203, "0044": 203, "7251": 203, "4994": 203, "5844": 203, "5740": 203, "7610": 203, "outperform": 203, "importantli": 203, "characterist": 203, "187": 203, "958": 203, "halv": 203, "int4weightonlyquant": 203, "motiv": 203, "constrain": 203, "edg": 203, "smartphon": 203, "executorch": 203, "xnnpack": 203, "export_llama": 203, "use_sdpa_with_kv_cach": 203, "qmode": 203, "group_siz": 203, "get_bos_id": 203, "get_eos_id": 203, "128001": 203, "output_nam": 203, "llama3_8da4w": 203, "pte": 203, "881": 203, "oneplu": 203, "709": 203, "tok": 203, "815": 203, "316": 203, "364": 203, "highli": 204, "vanilla": 204, "held": 204, "therefor": 204, "bespok": 204, "normalfloat": 204, "8x": 204, "vast": 204, "major": 204, "normatfloat": 204, "doubl": 204, "themselv": 204, "deepdiv": 204, "idea": 204, "distinct": 204, "de": 204, "counterpart": 204, "set_default_devic": 204, "qlora_linear": 204, "memory_alloc": 204, "177": 204, "152": 204, "del": 204, "empty_cach": 204, "lora_linear": 204, "081": 204, "344": 204, "qlora_llama2_7b": 204, "qlora_model": 204, "essenti": 204, "reparametrize_as_dtype_state_dict_post_hook": 204, "slower": 204, "149": 204, "9157477021217346": 204, "02": 204, "08": 204, "15it": 204, "nightli": 204, "hundr": 204, "228": 204, "8158286809921265": 204, "95it": 204, "exercis": 204, "linear_nf4": 204, "to_nf4": 204, "linear_weight": 204, "autograd": 204, "incom": 204}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 4, 1, "", "Role"], [23, 1, 1, "", "StackExchangedPairedTemplate"], [24, 1, 1, "", "SummarizeTemplate"], [25, 0, 1, "", "get_openai_messages"], [26, 0, 1, "", "get_sharegpt_messages"], [27, 0, 1, "", "truncate"], [28, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 3, 1, "", "contains_media"], [20, 2, 1, "", "from_dict"], [20, 3, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[23, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[24, 2, 1, "", "format"]], "torchtune.datasets": [[29, 1, 1, "", "ChatDataset"], [30, 1, 1, "", "ConcatDataset"], [31, 1, 1, "", "InstructDataset"], [32, 1, 1, "", "PackedDataset"], [33, 1, 1, "", "PreferenceDataset"], [34, 1, 1, "", "TextCompletionDataset"], [35, 0, 1, "", "alpaca_cleaned_dataset"], [36, 0, 1, "", "alpaca_dataset"], [37, 0, 1, "", "chat_dataset"], [38, 0, 1, "", "cnn_dailymail_articles_dataset"], [39, 0, 1, "", "grammar_dataset"], [40, 0, 1, "", "instruct_dataset"], [41, 0, 1, "", "samsum_dataset"], [42, 0, 1, "", "slimorca_dataset"], [43, 0, 1, "", "stack_exchanged_paired_dataset"], [44, 0, 1, "", "text_completion_dataset"], [45, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[46, 1, 1, "", "TilePositionalEmbedding"], [47, 1, 1, "", "TiledTokenPositionalEmbedding"], [48, 1, 1, "", "TokenPositionalEmbedding"], [49, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[46, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[47, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[48, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[50, 0, 1, "", "code_llama2_13b"], [51, 0, 1, "", "code_llama2_70b"], [52, 0, 1, "", "code_llama2_7b"], [53, 0, 1, "", "lora_code_llama2_13b"], [54, 0, 1, "", "lora_code_llama2_70b"], [55, 0, 1, "", "lora_code_llama2_7b"], [56, 0, 1, "", "qlora_code_llama2_13b"], [57, 0, 1, "", "qlora_code_llama2_70b"], [58, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[59, 1, 1, "", "GemmaTokenizer"], [60, 0, 1, "", "gemma"], [61, 0, 1, "", "gemma_2b"], [62, 0, 1, "", "gemma_7b"], [63, 0, 1, "", "gemma_tokenizer"], [64, 0, 1, "", "lora_gemma"], [65, 0, 1, "", "lora_gemma_2b"], [66, 0, 1, "", "lora_gemma_7b"], [67, 0, 1, "", "qlora_gemma_2b"], [68, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[59, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[69, 1, 1, "", "Llama2Tokenizer"], [70, 0, 1, "", "llama2"], [71, 0, 1, "", "llama2_13b"], [72, 0, 1, "", "llama2_70b"], [73, 0, 1, "", "llama2_7b"], [74, 0, 1, "", "llama2_reward_7b"], [75, 0, 1, "", "llama2_tokenizer"], [76, 0, 1, "", "lora_llama2"], [77, 0, 1, "", "lora_llama2_13b"], [78, 0, 1, "", "lora_llama2_70b"], [79, 0, 1, "", "lora_llama2_7b"], [80, 0, 1, "", "lora_llama2_reward_7b"], [81, 0, 1, "", "qlora_llama2_13b"], [82, 0, 1, "", "qlora_llama2_70b"], [83, 0, 1, "", "qlora_llama2_7b"], [84, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[69, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[85, 1, 1, "", "Llama3Tokenizer"], [86, 0, 1, "", "llama3"], [87, 0, 1, "", "llama3_70b"], [88, 0, 1, "", "llama3_8b"], [89, 0, 1, "", "llama3_tokenizer"], [90, 0, 1, "", "lora_llama3"], [91, 0, 1, "", "lora_llama3_70b"], [92, 0, 1, "", "lora_llama3_8b"], [93, 0, 1, "", "qlora_llama3_70b"], [94, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[85, 2, 1, "", "decode"], [85, 2, 1, "", "tokenize_message"], [85, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[95, 0, 1, "", "llama3_1"], [96, 0, 1, "", "llama3_1_70b"], [97, 0, 1, "", "llama3_1_8b"], [98, 0, 1, "", "lora_llama3_1"], [99, 0, 1, "", "lora_llama3_1_70b"], [100, 0, 1, "", "lora_llama3_1_8b"], [101, 0, 1, "", "qlora_llama3_1_70b"], [102, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[103, 1, 1, "", "MistralTokenizer"], [104, 0, 1, "", "lora_mistral"], [105, 0, 1, "", "lora_mistral_7b"], [106, 0, 1, "", "lora_mistral_classifier"], [107, 0, 1, "", "lora_mistral_reward_7b"], [108, 0, 1, "", "mistral"], [109, 0, 1, "", "mistral_7b"], [110, 0, 1, "", "mistral_classifier"], [111, 0, 1, "", "mistral_reward_7b"], [112, 0, 1, "", "mistral_tokenizer"], [113, 0, 1, "", "qlora_mistral_7b"], [114, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[103, 2, 1, "", "decode"], [103, 2, 1, "", "encode"], [103, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[115, 1, 1, "", "Phi3MiniTokenizer"], [116, 0, 1, "", "lora_phi3"], [117, 0, 1, "", "lora_phi3_mini"], [118, 0, 1, "", "phi3"], [119, 0, 1, "", "phi3_mini"], [120, 0, 1, "", "phi3_mini_tokenizer"], [121, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[115, 2, 1, "", "decode"], [115, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[122, 1, 1, "", "CausalSelfAttention"], [123, 1, 1, "", "FeedForward"], [124, 1, 1, "", "Fp32LayerNorm"], [125, 1, 1, "", "KVCache"], [126, 1, 1, "", "RMSNorm"], [127, 1, 1, "", "RotaryPositionalEmbeddings"], [128, 1, 1, "", "TransformerDecoder"], [129, 1, 1, "", "TransformerDecoderLayer"], [130, 1, 1, "", "VisionTransformer"], [132, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[122, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[123, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[124, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[125, 2, 1, "", "reset"], [125, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[126, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[127, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[128, 2, 1, "", "caches_are_enabled"], [128, 2, 1, "", "forward"], [128, 2, 1, "", "reset_caches"], [128, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[129, 2, 1, "", "forward"]], "torchtune.modules.VisionTransformer": [[130, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[131, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[133, 1, 1, "", "DPOLoss"], [134, 1, 1, "", "IPOLoss"], [135, 1, 1, "", "PPOLoss"], [136, 1, 1, "", "RSOLoss"]], "torchtune.modules.loss.DPOLoss": [[133, 2, 1, "", "forward"]], "torchtune.modules.loss.IPOLoss": [[134, 2, 1, "", "forward"]], "torchtune.modules.loss.PPOLoss": [[135, 2, 1, "", "forward"]], "torchtune.modules.loss.RSOLoss": [[136, 2, 1, "", "forward"]], "torchtune.modules.peft": [[137, 1, 1, "", "AdapterModule"], [138, 1, 1, "", "LoRALinear"], [139, 0, 1, "", "disable_adapter"], [140, 0, 1, "", "get_adapter_params"], [141, 0, 1, "", "set_trainable_params"], [142, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [143, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[137, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[138, 2, 1, "", "adapter_params"], [138, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[144, 0, 1, "", "estimate_advantages"], [145, 0, 1, "", "get_rewards_ppo"], [146, 0, 1, "", "left_padded_collate"], [147, 0, 1, "", "padded_collate_dpo"], [148, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.tokenizers": [[149, 1, 1, "", "SentencePieceBaseTokenizer"], [150, 1, 1, "", "TikTokenBaseTokenizer"], [151, 0, 1, "", "parse_hf_tokenizer_json"], [152, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[149, 2, 1, "", "decode"], [149, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[150, 2, 1, "", "decode"], [150, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[153, 1, 1, "", "VisionCrossAttentionMask"], [154, 0, 1, "", "find_supported_resolutions"], [155, 0, 1, "", "get_canvas_best_fit"], [156, 0, 1, "", "resize_with_pad"], [157, 0, 1, "", "tile_crop"]], "torchtune.utils": [[158, 4, 1, "", "FSDPPolicyType"], [159, 1, 1, "", "FullModelHFCheckpointer"], [160, 1, 1, "", "FullModelMetaCheckpointer"], [161, 1, 1, "", "FullModelTorchTuneCheckpointer"], [162, 1, 1, "", "ModelType"], [163, 1, 1, "", "OptimizerInBackwardWrapper"], [164, 1, 1, "", "TuneRecipeArgumentParser"], [165, 0, 1, "", "create_optim_in_bwd_wrapper"], [166, 0, 1, "", "generate"], [167, 0, 1, "", "get_device"], [168, 0, 1, "", "get_dtype"], [169, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [170, 0, 1, "", "get_logger"], [171, 0, 1, "", "get_memory_stats"], [172, 0, 1, "", "get_quantizer_mode"], [173, 0, 1, "", "get_world_size_and_rank"], [174, 0, 1, "", "init_distributed"], [175, 0, 1, "", "is_distributed"], [176, 0, 1, "", "log_memory_stats"], [177, 0, 1, "", "lora_fsdp_wrap_policy"], [182, 0, 1, "", "padded_collate"], [183, 0, 1, "", "register_optim_in_bwd_hooks"], [184, 0, 1, "", "set_activation_checkpointing"], [185, 0, 1, "", "set_default_dtype"], [186, 0, 1, "", "set_seed"], [187, 0, 1, "", "setup_torch_profiler"], [188, 0, 1, "", "torch_version_ge"], [189, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[159, 2, 1, "", "load_checkpoint"], [159, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[160, 2, 1, "", "load_checkpoint"], [160, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[161, 2, 1, "", "load_checkpoint"], [161, 2, 1, "", "save_checkpoint"]], "torchtune.utils.OptimizerInBackwardWrapper": [[163, 2, 1, "", "get_optim_key"], [163, 2, 1, "", "load_state_dict"], [163, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[164, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[178, 1, 1, "", "DiskLogger"], [179, 1, 1, "", "StdoutLogger"], [180, 1, 1, "", "TensorBoardLogger"], [181, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[178, 2, 1, "", "close"], [178, 2, 1, "", "log"], [178, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[179, 2, 1, "", "close"], [179, 2, 1, "", "log"], [179, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[180, 2, 1, "", "close"], [180, 2, 1, "", "log"], [180, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[181, 2, 1, "", "close"], [181, 2, 1, "", "log"], [181, 2, 1, "", "log_config"], [181, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:property", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 22, 158, 192, 194, 196, 199, 201, 202, 203, 204], "config": [0, 7, 8, 196, 200], "data": [1, 5, 22, 197], "text": [1, 198, 201], "templat": [1, 197, 198], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 197, 198], "exampl": 2, "gener": [2, 166, 199, 201], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 196, 199, 200, 201, 202, 203], "llama3": [3, 86, 197, 201, 203], "1": 3, "llama2": [3, 70, 197, 199, 202, 204], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 108], "gemma": [3, 60], "clip": 3, "modul": 4, "compon": [4, 7], "build": [4, 193, 204], "block": 4, "base": 4, "token": [4, 197], "util": [4, 5, 158], "peft": 4, "loss": 4, "vision": 4, "transform": 4, "reinforc": 4, "learn": 4, "from": [4, 197, 204], "human": 4, "feedback": 4, "rlhf": 4, "checkpoint": [5, 6, 9, 199], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 198, 202, 204], "manag": 5, "perform": [5, 202], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 194, 199], "format": [6, 198], "handl": 6, "differ": 6, "hfcheckpoint": 6, "metacheckpoint": 6, "torchtunecheckpoint": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 199, 202, 204], "put": [6, 204], "thi": 6, "all": [6, 7, 204], "togeth": [6, 204], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 198], "us": [7, 8, 197, 199, 204], "instanti": [7, 10], "referenc": 7, "other": [7, 199], "field": 7, "interpol": 7, "valid": [7, 13, 196], "your": [7, 8, 199, 200], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 194, 202, 203, 204], "ar": 8, "recip": [8, 196, 200, 202, 203], "script": 8, "run": [8, 196, 199], "cli": [8, 196], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "role": 22, "stackexchangedpairedtempl": 23, "summarizetempl": 24, "get_openai_messag": 25, "get_sharegpt_messag": 26, "truncat": 27, "validate_messag": 28, "chatdataset": 29, "concatdataset": 30, "instructdataset": 31, "packeddataset": 32, "preferencedataset": 33, "textcompletiondataset": 34, "alpaca_cleaned_dataset": 35, "alpaca_dataset": 36, "chat_dataset": 37, "cnn_dailymail_articles_dataset": 38, "grammar_dataset": 39, "instruct_dataset": 40, "samsum_dataset": 41, "slimorca_dataset": 42, "stack_exchanged_paired_dataset": 43, "text_completion_dataset": 44, "wikitext_dataset": 45, "tilepositionalembed": 46, "tiledtokenpositionalembed": 47, "tokenpositionalembed": 48, "clip_vision_encod": 49, "code_llama2_13b": 50, "code_llama2_70b": 51, "code_llama2_7b": 52, "lora_code_llama2_13b": 53, "lora_code_llama2_70b": 54, "lora_code_llama2_7b": 55, "qlora_code_llama2_13b": 56, "qlora_code_llama2_70b": 57, "qlora_code_llama2_7b": 58, "gemmatoken": 59, "gemma_2b": 61, "gemma_7b": 62, "gemma_token": 63, "lora_gemma": 64, "lora_gemma_2b": 65, "lora_gemma_7b": 66, "qlora_gemma_2b": 67, "qlora_gemma_7b": 68, "llama2token": 69, "llama2_13b": 71, "llama2_70b": 72, "llama2_7b": 73, "llama2_reward_7b": 74, "llama2_token": 75, "lora_llama2": 76, "lora_llama2_13b": 77, "lora_llama2_70b": 78, "lora_llama2_7b": 79, "lora_llama2_reward_7b": 80, "qlora_llama2_13b": 81, "qlora_llama2_70b": 82, "qlora_llama2_7b": 83, "qlora_llama2_reward_7b": 84, "llama3token": 85, "llama3_70b": 87, "llama3_8b": 88, "llama3_token": 89, "lora_llama3": 90, "lora_llama3_70b": 91, "lora_llama3_8b": 92, "qlora_llama3_70b": 93, "qlora_llama3_8b": 94, "llama3_1": 95, "llama3_1_70b": 96, "llama3_1_8b": 97, "lora_llama3_1": 98, "lora_llama3_1_70b": 99, "lora_llama3_1_8b": 100, "qlora_llama3_1_70b": 101, "qlora_llama3_1_8b": 102, "mistraltoken": 103, "lora_mistr": 104, "lora_mistral_7b": 105, "lora_mistral_classifi": 106, "lora_mistral_reward_7b": 107, "mistral_7b": 109, "mistral_classifi": 110, "mistral_reward_7b": 111, "mistral_token": 112, "qlora_mistral_7b": 113, "qlora_mistral_reward_7b": 114, "phi3minitoken": 115, "lora_phi3": 116, "lora_phi3_mini": 117, "phi3": 118, "phi3_mini": 119, "phi3_mini_token": 120, "qlora_phi3_mini": 121, "causalselfattent": 122, "todo": [122, 129], "feedforward": 123, "fp32layernorm": 124, "kvcach": 125, "rmsnorm": 126, "rotarypositionalembed": 127, "transformerdecod": 128, "transformerdecoderlay": 129, "visiontransform": 130, "reparametrize_as_dtype_state_dict_post_hook": 131, "get_cosine_schedule_with_warmup": 132, "dpoloss": 133, "ipoloss": 134, "ppoloss": 135, "rsoloss": 136, "adaptermodul": 137, "loralinear": 138, "disable_adapt": 139, "get_adapter_param": 140, "set_trainable_param": 141, "validate_missing_and_unexpected_for_lora": 142, "validate_state_dict_for_lora": 143, "estimate_advantag": 144, "get_rewards_ppo": 145, "left_padded_col": 146, "padded_collate_dpo": 147, "truncate_sequence_at_first_stop_token": 148, "sentencepiecebasetoken": 149, "tiktokenbasetoken": 150, "parse_hf_tokenizer_json": 151, "tokenize_messages_no_special_token": 152, "visioncrossattentionmask": 153, "find_supported_resolut": 154, "get_canvas_best_fit": 155, "resize_with_pad": 156, "tile_crop": 157, "fsdppolicytyp": 158, "fullmodelhfcheckpoint": 159, "fullmodelmetacheckpoint": 160, "fullmodeltorchtunecheckpoint": 161, "modeltyp": 162, "optimizerinbackwardwrapp": 163, "tunerecipeargumentpars": 164, "create_optim_in_bwd_wrapp": 165, "get_devic": 167, "get_dtyp": 168, "get_full_finetune_fsdp_wrap_polici": 169, "get_logg": 170, "get_memory_stat": 171, "get_quantizer_mod": 172, "get_world_size_and_rank": 173, "init_distribut": 174, "is_distribut": 175, "log_memory_stat": 176, "lora_fsdp_wrap_polici": 177, "disklogg": 178, "stdoutlogg": 179, "tensorboardlogg": 180, "wandblogg": 181, "padded_col": 182, "register_optim_in_bwd_hook": 183, "set_activation_checkpoint": 184, "set_default_dtyp": 185, "set_se": 186, "setup_torch_profil": 187, "torch_version_g": 188, "validate_expected_param_dtyp": 189, "comput": [191, 195], "time": [191, 195], "welcom": 192, "document": 192, "get": [192, 196, 201], "start": [192, 196], "tutori": 192, "instal": 193, "instruct": [193, 198, 201], "via": [193, 201], "pypi": 193, "git": 193, "clone": 193, "nightli": 193, "kei": 194, "concept": 194, "design": 194, "principl": 194, "download": [196, 199, 200], "list": 196, "built": [196, 198], "copi": 196, "fine": [197, 198, 200, 201], "tune": [197, 198, 200, 201], "chat": [197, 198], "chang": 197, "prompt": 197, "special": 197, "when": 197, "should": 197, "i": 197, "custom": [197, 198], "hug": [198, 199], "face": [198, 199], "set": 198, "max": 198, "sequenc": 198, "length": 198, "sampl": 198, "pack": 198, "unstructur": 198, "corpu": 198, "multipl": 198, "local": 198, "remot": 198, "fulli": 198, "end": 199, "workflow": 199, "7b": 199, "finetun": [199, 202, 203, 204], "evalu": [199, 201, 203], "eleutherai": [199, 201], "s": [199, 201], "eval": [199, 201], "har": [199, 201], "speed": 199, "up": 199, "quantiz": [199, 201, 203], "librari": 199, "upload": 199, "hub": 199, "first": 200, "llm": 200, "select": 200, "modifi": 200, "train": 200, "next": 200, "step": 200, "meta": 201, "8b": 201, "access": 201, "our": 201, "faster": 201, "how": 202, "doe": 202, "work": 202, "appli": [202, 203], "trade": 202, "off": 202, "qat": 203, "lower": 203, "devic": 203, "option": 203, "qlora": 204, "save": 204, "deep": 204, "dive": 204}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})