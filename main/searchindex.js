Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "validate_messages", "ChatDataset", "InstructDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"support": [2, 6, 8, 9, 10, 20, 24, 25, 26, 27, 28, 29, 30, 31, 32, 50, 51, 61, 67, 70, 74, 87, 89, 90, 91, 92, 93, 94, 95], "sever": [2, 90], "wide": [2, 90], "us": [2, 4, 6, 9, 10, 11, 15, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 51, 52, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 69, 70, 75, 76, 77, 78, 82, 85, 86, 87, 90, 92, 93, 94], "help": [2, 6, 18, 56, 66, 68, 85, 86, 87, 89, 90, 91, 92, 93, 95], "quickli": [2, 7, 89, 90], "bootstrap": [2, 90], "your": [2, 5, 9, 10, 24, 77, 78, 85, 86, 87, 89, 90, 93, 94, 95], "fine": [2, 6, 8, 9, 85, 87, 91, 94], "tune": [2, 3, 6, 7, 8, 9, 11, 85, 86, 87, 91, 94, 95], "also": [2, 6, 7, 8, 9, 10, 28, 51, 56, 61, 69, 78, 86, 89, 90, 91, 92, 93, 94, 95], "common": [2, 4, 7, 89, 90, 93, 94], "format": [2, 5, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 66, 67, 89, 91, 92, 93, 94], "like": [2, 6, 7, 8, 9, 24, 86, 89, 90, 91, 92, 94], "chat": [2, 14, 15, 18, 19, 22, 24, 28, 32], "model": [2, 6, 7, 8, 10, 15, 20, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 66, 67, 80, 81, 85, 87, 89, 90, 95], "instruct": [2, 3, 13, 15, 17, 19, 20, 25, 26, 27, 30, 50, 85, 89, 92, 94, 95], "These": [2, 4, 6, 7, 8, 10, 68, 89, 90, 91, 92, 93, 94, 95], "ar": [2, 4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 56, 61, 66, 67, 70, 86, 87, 89, 90, 91, 92, 93, 94, 95], "especi": [2, 87, 91], "specifi": [2, 6, 7, 8, 10, 28, 51, 78, 81, 89, 90, 91, 92, 93, 95], "from": [2, 3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 48, 52, 56, 57, 59, 60, 62, 64, 66, 67, 68, 77, 78, 84, 86, 88, 90, 91, 92, 93, 94], "yaml": [2, 7, 8, 10, 11, 28, 30, 68, 78, 87, 89, 91, 92, 93, 94, 95], "config": [2, 6, 9, 10, 11, 12, 28, 30, 51, 66, 68, 78, 87, 89, 90, 91, 93, 94, 95], "represent": [2, 94, 95], "abov": [2, 6, 58, 86, 91, 93, 94, 95], "all": [3, 4, 8, 12, 24, 28, 51, 52, 56, 58, 65, 66, 68, 83, 85, 87, 88, 89, 91, 92, 93, 94], "famili": [3, 8, 26, 27, 32, 87, 93], "download": [3, 6, 83, 86, 89, 93, 94, 95], "meta": [3, 6, 18, 66, 67, 89, 91, 92], "llama": [3, 6, 18, 24, 28, 54, 55, 66, 67, 89, 91, 92, 93, 94], "8b": [3, 43, 44, 45, 89], "hf": [3, 6, 66, 89, 91, 92, 93], "token": [3, 6, 7, 8, 19, 24, 25, 26, 27, 28, 29, 30, 31, 32, 51, 55, 56, 57, 64, 65, 90, 91, 92, 93, 94, 95], "access_token": 3, "pre": [3, 18, 86, 89], "train": [3, 5, 6, 8, 9, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 51, 58, 59, 66, 67, 70, 80, 85, 87, 89, 90, 91, 93, 94, 95], "can": [3, 4, 6, 7, 8, 9, 10, 12, 19, 24, 25, 26, 27, 28, 30, 54, 55, 64, 66, 68, 77, 78, 81, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95], "hug": [3, 6, 24, 25, 26, 27, 28, 29, 30, 31, 32, 59, 87, 90, 92, 93], "face": [3, 6, 24, 25, 26, 27, 28, 29, 30, 31, 32, 59, 87, 90, 92, 93], "hub": [3, 6, 92], "follow": [3, 6, 8, 22, 24, 51, 59, 78, 85, 86, 90, 91, 92, 93, 94, 95], "command": [3, 8, 9, 68, 86, 89, 91, 92, 93, 94, 95], "2": [3, 6, 9, 23, 32, 51, 64, 66, 67, 79, 82, 89, 91, 92, 93, 94], "7b": [3, 6, 25, 26, 27, 30, 36, 39, 41, 47, 48, 66, 67, 89, 92, 93, 94, 95], "mini": [3, 50], "microsoft": [3, 50], "4k": [3, 50], "hf_token": 3, "ignor": [3, 6, 51, 52], "pattern": [3, 65], "ai": [3, 48, 51, 78, 89, 93], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 32, 51, 56, 59, 64, 65, 67, 77, 78, 79, 82, 89, 91, 92, 93, 94, 95], "googl": [3, 33], "2b": [3, 33], "offer": 5, "allow": [5, 77, 95], "seamless": 5, "transit": 5, "between": [5, 6, 66, 91, 93, 94, 95], "interoper": [5, 6, 8, 87, 91, 95], "rest": [5, 89, 95], "ecosystem": [5, 6, 8, 87, 91, 93, 95], "For": [5, 6, 7, 8, 24, 25, 26, 27, 28, 30, 51, 56, 68, 78, 81, 82, 86, 89, 90, 91, 92, 93, 94, 95], "comprehens": 5, "overview": [5, 7, 9, 92, 94, 95], "pleas": [5, 40, 41, 46, 49, 81, 86, 95], "see": [5, 6, 9, 18, 20, 28, 32, 40, 41, 46, 49, 53, 60, 68, 71, 78, 80, 81, 82, 86, 87, 89, 90, 91, 92, 93, 94, 95], "deep": [5, 6, 7, 8, 9, 87, 92, 93], "dive": [5, 6, 7, 8, 9, 87, 92, 93], "enabl": [5, 7, 8, 9, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 61, 80, 82, 93, 94, 95], "work": [5, 6, 8, 68, 87, 91, 93, 95], "set": [5, 6, 7, 8, 9, 25, 26, 27, 29, 30, 31, 32, 55, 56, 63, 69, 81, 82, 87, 89, 90, 91, 92, 93, 94], "consumpt": 5, "dure": [5, 6, 25, 26, 27, 29, 31, 51, 53, 55, 56, 57, 58, 89, 91, 93, 94, 95], "provid": [5, 6, 7, 8, 10, 15, 20, 24, 25, 32, 56, 68, 78, 87, 89, 90, 91, 92, 93], "debug": [5, 6, 7, 8], "finetun": [5, 6, 7, 8, 37, 38, 39, 44, 45, 74, 85, 87, 92, 93], "job": [5, 9, 82, 92], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 87, 92, 93], "walk": [6, 8, 77, 87, 89, 90, 91, 92, 95], "you": [6, 7, 8, 9, 10, 17, 18, 24, 25, 26, 27, 30, 68, 77, 78, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95], "through": [6, 7, 8, 9, 52, 87, 89, 90, 91, 92, 95], "design": [6, 8], "behavior": [6, 89, 90], "associ": [6, 7, 8, 91, 94], "util": [6, 7, 8, 9, 10, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 87, 91, 92, 93, 95], "what": [6, 7, 9, 18, 20, 29, 31, 85, 89, 90, 91, 92, 93], "cover": [6, 7, 8, 9, 89, 91, 95], "how": [6, 7, 8, 9, 81, 85, 89, 90, 91, 92, 93, 95], "we": [6, 7, 8, 9, 25, 26, 27, 30, 51, 53, 55, 56, 61, 64, 66, 67, 70, 87, 89, 90, 91, 92, 93, 94, 95], "them": [6, 7, 24, 25, 30, 32, 52, 58, 64, 89, 90, 91, 94, 95], "scenario": 6, "full": [6, 7, 8, 40, 41, 46, 49, 64, 87, 93, 94], "compos": 6, "compon": [6, 8, 12, 80, 87, 90, 92, 94, 95], "which": [6, 8, 25, 26, 27, 29, 31, 37, 38, 39, 44, 45, 47, 51, 55, 56, 57, 59, 64, 66, 67, 70, 75, 78, 81, 87, 89, 90, 91, 92, 93, 94, 95], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 21, 22, 24, 25, 28, 30, 58, 62, 63, 64, 66, 67, 82, 89, 90, 91, 92, 93, 94], "recip": [6, 7, 9, 10, 11, 52, 66, 67, 87, 89, 90, 91, 93, 95], "evalu": [6, 8, 85, 87, 92, 94, 95], "gener": [6, 8, 13, 16, 21, 24, 25, 32, 64, 82, 83, 85, 89, 90, 94, 95], "each": [6, 8, 14, 17, 37, 38, 39, 44, 45, 47, 51, 55, 56, 64, 65, 82, 87, 90, 91, 92, 93, 94], "make": [6, 7, 8, 9, 51, 57, 87, 91, 92, 93, 94, 95], "easi": [6, 8, 87, 94], "understand": [6, 7, 8, 85, 87, 89, 90, 94, 95], "extend": [6, 8, 87], "befor": [6, 23, 25, 56, 57, 61, 66, 91], "let": [6, 7, 9, 89, 90, 91, 92, 93, 94, 95], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 20, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 44, 45, 47, 51, 55, 56, 57, 58, 60, 62, 65, 66, 67, 69, 77, 80, 81, 87, 89, 90, 92, 94, 95], "defin": [6, 7, 8, 52, 60, 61, 62, 92, 94], "some": [6, 7, 15, 62, 63, 85, 87, 89, 91, 92, 94, 95], "concept": [6, 91, 92], "In": [6, 7, 8, 24, 55, 61, 77, 78, 89, 91, 93, 94, 95], "ll": [6, 7, 8, 65, 87, 89, 90, 91, 92, 93, 95], "talk": 6, "about": [6, 8, 66, 78, 87, 89, 91, 92, 93, 94, 95], "take": [6, 7, 8, 10, 52, 53, 58, 66, 68, 69, 89, 90, 91, 92, 93, 94, 95], "close": [6, 8, 75, 76, 77, 78, 94], "look": [6, 7, 8, 77, 86, 89, 90, 91, 92, 93, 94], "veri": [6, 56, 91], "simpli": [6, 7, 89, 90, 91, 93, 95], "dictat": 6, "state_dict": [6, 58, 66, 67, 94, 95], "store": [6, 75, 78, 94, 95], "file": [6, 7, 8, 9, 10, 11, 64, 65, 66, 67, 68, 75, 78, 80, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95], "disk": [6, 75], "weight": [6, 8, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 58, 60, 61, 66, 67, 78, 85, 89, 91, 92, 93, 94, 95], "string": [6, 24, 25, 26, 27, 28, 29, 30, 31, 32, 60, 64, 65, 69, 70], "kei": [6, 7, 9, 24, 25, 30, 32, 51, 53, 56, 63, 66, 91, 92, 94, 95], "identifi": 6, "state": [6, 8, 58, 62, 63, 66, 67, 91, 93, 94, 95], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 19, 21, 22, 24, 25, 28, 30, 58, 62, 63, 66, 67, 73], "If": [6, 7, 12, 13, 16, 17, 20, 21, 23, 24, 25, 26, 27, 29, 30, 31, 32, 51, 58, 61, 66, 67, 69, 70, 73, 77, 78, 82, 86, 89, 90, 91, 92, 93, 94], "don": [6, 7, 8, 78, 82, 89, 91, 92, 93, 95], "t": [6, 7, 8, 32, 65, 70, 78, 82, 89, 91, 92, 93, 95], "match": [6, 24, 25, 30, 32, 86, 91, 93, 94], "up": [6, 8, 9, 25, 26, 27, 30, 89, 90, 92, 93, 94, 95], "exactli": 6, "those": [6, 94], "definit": [6, 94], "either": [6, 66, 81, 94, 95], "run": [6, 7, 9, 11, 52, 53, 56, 58, 66, 67, 77, 78, 86, 87, 89, 92, 93, 94, 95], "explicit": 6, "error": [6, 7, 23, 66, 82], "load": [6, 8, 24, 25, 66, 67, 68, 77, 89, 91, 93, 94], "rais": [6, 10, 12, 20, 23, 28, 32, 51, 56, 66, 67, 70, 73, 78, 82], "an": [6, 7, 8, 9, 10, 13, 19, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 51, 56, 60, 62, 63, 66, 67, 78, 87, 89, 90, 91, 92, 93, 94, 95], "except": [6, 19, 20, 90], "wors": 6, "silent": [6, 52], "succe": 6, "infer": [6, 18, 24, 51, 53, 55, 56, 57, 85, 89, 91, 92, 93, 95], "expect": [6, 7, 10, 13, 16, 17, 21, 24, 25, 28, 30, 55, 78, 89, 90, 94], "addit": [6, 7, 8, 10, 24, 25, 28, 30, 66, 67, 70, 73, 75, 77, 78, 81, 87, 89, 92, 94], "line": [6, 8, 68, 92, 93], "need": [6, 7, 8, 9, 17, 24, 32, 51, 52, 56, 77, 78, 86, 89, 90, 91, 92, 93, 94, 95], "shape": [6, 51, 53, 55, 56, 57, 61], "valu": [6, 7, 22, 32, 33, 34, 35, 36, 42, 43, 48, 51, 53, 54, 56, 59, 66, 68, 75, 76, 77, 78, 82, 92, 93, 94], "two": [6, 7, 23, 87, 91, 92, 93, 94, 95], "popular": [6, 87, 90, 91], "llama2": [6, 7, 8, 10, 18, 22, 24, 25, 26, 27, 30, 32, 34, 35, 36, 37, 38, 39, 40, 41, 52, 56, 57, 64, 85, 87, 92, 93], "offici": [6, 18, 89, 92, 93], "implement": [6, 8, 24, 25, 26, 27, 28, 29, 30, 31, 32, 52, 54, 55, 59, 60, 61, 66, 77, 87, 94, 95], "when": [6, 7, 8, 11, 19, 56, 58, 59, 77, 91, 93, 94, 95], "websit": 6, "get": [6, 7, 8, 9, 24, 64, 70, 71, 72, 86, 87, 89, 90, 91, 92, 94], "access": [6, 7, 8, 66, 91, 92], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 20, 21, 22, 51, 66, 67, 89, 91, 92, 93, 94, 95], "pth": [6, 91, 93], "inspect": [6, 91, 94, 95], "content": [6, 19, 22, 24, 64, 89, 90], "easili": [6, 7, 87, 90, 94, 95], "torch": [6, 53, 56, 58, 59, 69, 70, 73, 80, 81, 82, 91, 92, 93, 94, 95], "import": [6, 7, 10, 28, 77, 78, 89, 90, 91, 92, 94, 95], "consolid": [6, 93], "00": [6, 84, 88, 92, 93], "mmap": [6, 91], "true": [6, 7, 19, 25, 26, 27, 28, 29, 31, 40, 41, 46, 49, 58, 64, 65, 66, 67, 73, 77, 89, 90, 91, 93, 94, 95], "weights_onli": 6, "map_loc": [6, 91], "cpu": [6, 8, 58, 70, 86, 91, 95], "tensor": [6, 51, 52, 53, 54, 55, 56, 57, 58, 61, 66, 75, 76, 77, 78, 79, 94, 95], "item": 6, "print": [6, 9, 26, 27, 29, 31, 32, 64, 89, 90, 92, 94, 95], "f": [6, 9, 26, 27, 29, 31, 89, 91, 94, 95], "tok_embed": [6, 56], "size": [6, 8, 10, 26, 27, 29, 31, 51, 53, 54, 55, 56, 57, 72, 87, 90, 91, 92, 93, 94], "32000": [6, 10, 94], "4096": [6, 10, 25, 26, 27, 30, 51, 55, 94], "len": [6, 26, 27, 29, 31, 56], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 23, 24, 25, 29, 31, 32, 37, 38, 39, 44, 45, 54, 55, 58, 59, 64, 65, 66, 68, 69, 70, 71, 78, 80, 82, 86, 87, 89, 91, 92, 93, 94, 95], "contain": [6, 19, 51, 53, 55, 56, 57, 60, 62, 63, 64, 65, 66, 67, 68, 77, 79, 89, 91, 93, 94], "includ": [6, 7, 8, 14, 17, 61, 66, 67, 68, 87, 89, 91, 92, 93, 94, 95], "input": [6, 13, 14, 17, 24, 25, 26, 27, 28, 29, 30, 32, 51, 52, 54, 55, 56, 57, 61, 64, 66, 79, 82, 89, 90, 94, 95], "embed": [6, 51, 53, 54, 55, 56, 89, 93], "tabl": [6, 89, 95], "call": [6, 10, 19, 52, 58, 68, 75, 76, 77, 78, 89, 94, 95], "layer": [6, 8, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 56, 57, 61, 87, 93, 94, 95], "have": [6, 7, 10, 51, 53, 60, 68, 77, 80, 86, 89, 90, 91, 92, 93, 94, 95], "dim": [6, 51, 52, 54, 55, 56, 57], "most": [6, 7, 65, 89, 92, 94, 95], "within": [6, 7, 10, 24, 32, 52, 77, 82, 91, 93, 94, 95], "default": [6, 7, 15, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 51, 52, 54, 55, 56, 57, 58, 59, 61, 64, 65, 66, 67, 68, 70, 75, 78, 79, 80, 82, 86, 91, 93, 94, 95], "everi": [6, 8, 52, 77, 86, 95], "repo": [6, 66, 67, 91], "first": [6, 7, 10, 23, 56, 65, 66, 68, 85, 87, 89, 91, 93, 94, 95], "big": [6, 91], "split": [6, 89, 90, 91], "across": [6, 8, 66, 77, 82, 91, 93], "bin": [6, 91], "To": [6, 7, 8, 9, 66, 86, 87, 89, 90, 91, 92, 93, 94, 95], "correctli": [6, 8, 12, 66, 86, 89, 92, 95], "piec": 6, "one": [6, 8, 23, 52, 64, 89, 90, 91, 92, 93, 95], "pytorch_model": [6, 91], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 19, 51, 55, 56, 57, 61, 62, 64, 67, 68, 70, 91, 92, 93, 94, 95], "doe": [6, 20, 24, 50, 60, 66, 68, 89, 91], "fewer": [6, 51], "sinc": [6, 7, 10, 52, 66, 89, 91, 93], "instead": [6, 8, 28, 30, 52, 53, 61, 91, 93, 94], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 21, 24, 25, 28, 30, 32, 60, 63, 65, 66, 67, 68, 69, 75, 76, 77, 78, 89, 91, 93], "caus": [6, 64], "try": [6, 7, 89, 91, 92, 93, 95], "same": [6, 7, 37, 38, 39, 44, 45, 51, 53, 57, 64, 68, 78, 89, 91, 93, 94, 95], "As": [6, 7, 8, 9, 61, 87, 91, 93, 95], "re": [6, 7, 65, 87, 89, 91, 92, 93, 94], "care": [6, 52, 66, 91, 93, 94], "end": [6, 8, 19, 65, 85, 87, 89, 93, 94], "number": [6, 8, 24, 25, 26, 27, 28, 30, 32, 51, 53, 56, 59, 66, 67, 72, 82, 92, 94], "just": [6, 13, 87, 89, 90, 92, 93, 94], "save": [6, 8, 9, 58, 66, 67, 78, 85, 89, 91, 93, 94], "less": [6, 32, 91, 92, 93, 95], "prone": 6, "manag": [6, 80, 89], "invari": 6, "accept": [6, 7, 32, 64, 92, 95], "multipl": [6, 7, 8, 19, 24, 61, 75, 76, 77, 78, 92, 93], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91], "worri": [6, 89, 92], "explicitli": [6, 60, 87, 94], "convert": [6, 22, 24, 66, 79, 89, 91, 95], "time": [6, 64, 75, 77, 89, 91, 93, 95], "produc": [6, 95], "back": [6, 23, 66, 94, 95], "origin": [6, 26, 27, 58, 61, 89, 90, 91, 93, 94, 95], "form": [6, 7, 8, 23], "One": [6, 91], "advantag": [6, 94], "being": [6, 66, 67, 69, 95], "should": [6, 7, 8, 14, 17, 18, 19, 20, 22, 28, 30, 37, 38, 39, 44, 45, 47, 51, 52, 60, 68, 75, 76, 77, 78, 86, 87, 90, 91, 92, 93, 94, 95], "abl": [6, 8, 90, 91, 92, 93], "post": [6, 95], "tool": [6, 91, 92], "quantiz": [6, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 61, 85, 92, 95], "eval": [6, 85, 87], "without": [6, 7, 9, 86, 87, 89, 90, 91, 94], "code": [6, 8, 56, 83, 87, 90, 92], "chang": [6, 7, 9, 13, 86, 90, 91, 92, 93, 94, 95], "OR": 6, "convers": [6, 14, 15, 18, 20, 22, 23, 24, 28, 32, 66, 87, 89, 90, 91, 93, 94, 95], "script": [6, 9, 91, 92, 93], "wai": [6, 7, 24, 89, 90, 91, 92, 93], "surround": [6, 8, 87], "load_checkpoint": [6, 8, 66, 67], "save_checkpoint": [6, 8, 9, 66, 67], "method": [6, 7, 8, 9, 11, 24, 25, 26, 27, 28, 29, 30, 31, 32, 58, 60, 62, 68, 86, 87, 91, 93, 94, 95], "convertor": 6, "avail": [6, 8, 68, 69, 70, 87, 91, 93, 94], "here": [6, 7, 9, 15, 29, 54, 55, 89, 90, 91, 92, 93, 94, 95], "three": [6, 8, 92], "hfcheckpoint": 6, "read": [6, 66, 67, 87], "write": [6, 8, 66, 67, 75, 89, 90, 92], "compat": [6, 66], "transform": [6, 8, 24, 25, 37, 38, 39, 44, 45, 47, 56, 57, 59, 81, 94], "framework": [6, 8, 87], "mention": [6, 91, 95], "assum": [6, 13, 16, 17, 21, 25, 30, 59, 62, 65, 70, 91, 94], "checkpoint_dir": [6, 7, 66, 67, 91, 93], "necessari": [6, 32, 75, 76, 77, 78, 89, 94], "json": [6, 66, 80, 91], "easiest": [6, 91, 92], "sure": [6, 7, 91, 92, 93, 94, 95], "everyth": [6, 8, 68, 87, 92], "flow": [6, 24, 25, 95], "By": [6, 93, 94, 95], "safetensor": 6, "output": [6, 17, 26, 27, 29, 32, 37, 38, 39, 44, 45, 47, 51, 52, 54, 55, 56, 57, 61, 63, 76, 80, 86, 89, 90, 91, 92, 93, 94, 95], "dir": [6, 78, 86, 91, 92, 93], "output_dir": [6, 7, 66, 67, 80, 91, 93, 94, 95], "argument": [6, 7, 10, 17, 24, 25, 28, 30, 32, 40, 41, 46, 49, 51, 68, 73, 75, 77, 78, 81, 89, 93, 94], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 56, 81, 91, 94, 95], "_component_": [6, 7, 9, 10, 28, 89, 90, 91, 93, 94], "fullmodelhfcheckpoint": [6, 91], "directori": [6, 7, 66, 67, 75, 77, 78, 91, 92, 93], "sort": [6, 66], "id": [6, 24, 25, 26, 27, 28, 30, 32, 64, 65, 66, 79, 89, 91], "so": [6, 7, 66, 68, 86, 87, 89, 91, 92, 93, 94, 95], "order": [6, 8, 66, 77, 78, 92], "matter": [6, 66, 94], "checkpoint_fil": [6, 7, 9, 66, 67, 91, 93, 94, 95], "restart": 6, "previou": [6, 66, 67], "more": [6, 7, 8, 28, 32, 53, 55, 68, 78, 80, 81, 82, 87, 90, 91, 92, 93, 94, 95], "next": [6, 93, 95], "section": [6, 8, 85, 91, 93, 95], "recipe_checkpoint": [6, 66, 67], "null": [6, 7], "usual": [6, 55, 66, 78, 91, 94], "model_typ": [6, 66, 67, 91, 93], "resume_from_checkpoint": [6, 66, 67], "fals": [6, 7, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 61, 64, 65, 66, 67, 80, 89, 91, 93, 94, 95], "requir": [6, 7, 32, 66, 77, 78, 82, 86, 89, 90, 92, 95], "param": [6, 8, 37, 38, 39, 44, 45, 61, 62, 63, 66, 94, 95], "directli": [6, 7, 8, 10, 28, 30, 66, 90, 91, 92, 93, 94, 95], "ensur": [6, 7, 12, 23, 32, 51, 66, 70, 87, 92], "out": [6, 7, 8, 24, 25, 26, 27, 28, 29, 31, 66, 67, 85, 87, 89, 91, 92, 93, 94, 95], "case": [6, 8, 9, 19, 51, 66, 70, 75, 81, 87, 89, 90, 91, 93, 94, 95], "discrep": [6, 66], "along": [6, 93, 94], "detail": [6, 28, 32, 53, 80, 82, 91, 92, 93, 94, 95], "found": [6, 7, 9, 54, 55, 94, 95], "metacheckpoint": 6, "github": [6, 10, 37, 38, 39, 44, 45, 51, 54, 55, 59, 86, 92], "repositori": [6, 18, 91, 92], "fullmodelmetacheckpoint": [6, 93], "torchtunecheckpoint": 6, "perform": [6, 52, 87, 89, 91, 93, 95], "current": [6, 50, 51, 55, 56, 57, 67, 72, 75, 77, 82, 91, 92, 93], "test": [6, 7, 8, 87, 89], "complet": [6, 8, 89, 90, 91, 92, 93], "written": [6, 7, 8, 66, 67, 75, 76, 77, 78, 87], "begin": [6, 64, 65, 89, 93, 95], "partit": [6, 95], "ha": [6, 60, 62, 64, 90, 91, 92, 93, 94, 95], "standard": [6, 76, 87, 89, 91, 93], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 91], "inform": [6, 19, 78, 81, 87, 91, 92, 93], "subsequ": [6, 8], "recipe_st": [6, 66, 67], "pt": [6, 9, 66, 67, 91, 93], "epoch": [6, 8, 9, 59, 66, 67, 89, 91, 92, 93], "optim": [6, 7, 8, 50, 59, 89, 91, 92, 93, 94, 95], "etc": [6, 8, 66, 92], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 19, 56, 60, 64, 66, 80, 82, 89, 90, 91, 94, 95], "updat": [6, 7, 8, 86, 89, 91, 92, 93, 94, 95], "hf_model_0001_0": [6, 91], "hf_model_0002_0": [6, 91], "both": [6, 91, 94, 95], "adapt": [6, 60, 61, 62, 63, 66, 67, 89, 91, 94, 95], "merg": [6, 10, 66, 91, 93, 95], "would": [6, 7, 9, 56, 86, 89, 90, 91, 94, 95], "our": [6, 8, 87, 89, 90, 91, 92, 94, 95], "tutori": [6, 81, 87, 89, 90, 91, 92, 93, 94, 95], "primari": [6, 7, 8, 92], "want": [6, 7, 8, 9, 10, 24, 86, 89, 91, 92, 93, 94], "resum": [6, 8, 59, 66, 67, 95], "initi": [6, 8, 11, 33, 34, 35, 36, 42, 43, 48, 73, 92, 94, 95], "frozen": [6, 94, 95], "base": [6, 10, 25, 32, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 55, 59, 61, 63, 66, 68, 75, 85, 89, 91, 92, 93, 94, 95], "well": [6, 7, 8, 87, 90, 91, 93, 95], "learnt": [6, 89, 91], "someth": [6, 8, 9, 89, 91], "NOT": 6, "refer": [6, 7, 8, 54, 55, 87, 94], "adapter_checkpoint": [6, 66, 67], "adapter_0": [6, 91], "now": [6, 64, 89, 90, 91, 92, 93, 94, 95], "knowledg": 6, "creat": [6, 7, 10, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 53, 59, 66, 67, 75, 77, 89, 90, 91, 93, 95], "simpl": [6, 8, 85, 92, 94, 95], "forward": [6, 8, 51, 52, 54, 55, 56, 57, 61, 93, 94, 95], "13b": [6, 34, 37, 40], "modeltyp": [6, 66, 67], "llama2_13b": [6, 37], "right": [6, 66, 91, 93, 94], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 94], "successfulli": [6, 92], "vocab": [6, 10, 56, 93], "70": [6, 42], "x": [6, 51, 52, 54, 55, 56, 57, 61, 94, 95], "randint": 6, "0": [6, 8, 37, 38, 39, 40, 41, 51, 56, 59, 61, 64, 77, 78, 79, 82, 84, 88, 89, 91, 92, 93, 94, 95], "no_grad": 6, "6": [6, 54, 79, 91, 95], "3989": 6, "9": [6, 91, 95], "0531": 6, "3": [6, 50, 65, 68, 71, 79, 89, 91, 92, 93, 95], "2375": 6, "5": [6, 59, 79, 80, 91, 92, 93], "2822": 6, "4": [6, 32, 51, 79, 87, 91, 93, 94, 95], "4872": 6, "7469": 6, "8": [6, 26, 27, 29, 31, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 91, 94, 95], "6737": 6, "11": [6, 91, 93, 95], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 79], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 24, 25, 30, 32, 78, 89, 90, 91, 92, 93, 94], "find": [6, 8, 9, 91, 92, 94], "list": [6, 7, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 60, 61, 64, 65, 66, 67, 68, 71, 74, 79, 89, 90, 92, 93], "builder": [6, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 89, 90, 95], "hope": 6, "deeper": [6, 92], "insight": [6, 91], "happi": [6, 91], "thi": [7, 8, 9, 10, 19, 24, 25, 26, 27, 28, 30, 32, 50, 51, 52, 55, 56, 57, 58, 59, 60, 64, 66, 67, 68, 69, 70, 75, 77, 78, 80, 81, 82, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95], "guid": [7, 9, 87, 89, 90, 92, 94], "pars": [7, 10, 65, 68, 89, 92], "effect": 7, "cli": [7, 9, 11, 86, 91, 92], "prerequisit": [7, 89, 90, 91, 92, 93, 94, 95], "Be": [7, 89, 91, 92, 93, 94, 95], "familiar": [7, 89, 91, 92, 93, 94, 95], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 86, 89, 90, 92], "instal": [7, 9, 77, 78, 85, 91, 92, 93, 94, 95], "fundament": 7, "There": [7, 14, 23, 89, 91, 92, 93, 94], "entri": [7, 8, 92], "point": [7, 8, 22, 90, 91, 92, 93, 94, 95], "locat": [7, 93, 94, 95], "thei": [7, 8, 19, 56, 68, 89, 90, 94], "truth": [7, 91, 93], "reproduc": 7, "overridden": [7, 52, 68], "quick": 7, "experiment": 7, "modifi": [7, 8, 9, 58, 87, 91, 93, 94, 95], "serv": [7, 90, 94], "particular": [7, 24, 32, 90, 94, 95], "seed": [7, 8, 9, 82, 92], "shuffl": 7, "devic": [7, 8, 69, 70, 89, 91, 92, 93, 94], "cuda": [7, 69, 70, 86, 91, 95], "dtype": [7, 8, 53, 58, 70, 74, 91, 95], "fp32": [7, 95], "enable_fsdp": 7, "mani": [7, 90, 91], "object": [7, 10, 14, 15, 18, 20, 51, 89, 90], "keyword": [7, 10, 24, 25, 28, 30, 32, 58, 89], "loss": [7, 8, 25, 26, 27, 29, 31, 92, 94, 95], "function": [7, 8, 10, 11, 24, 51, 52, 58, 69, 72, 82, 87, 89, 90, 95], "exampl": [7, 8, 9, 10, 11, 15, 18, 20, 25, 26, 27, 28, 29, 30, 31, 32, 51, 60, 64, 66, 67, 77, 78, 79, 83, 84, 86, 88, 89, 90, 91, 93, 94, 95], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 91], "path": [7, 8, 9, 10, 24, 25, 26, 27, 28, 29, 30, 31, 32, 64, 65, 66, 67, 68, 80, 89, 91, 93, 94], "normal": [7, 24, 54, 56, 57, 64, 89, 90, 94, 95], "python": [7, 65, 68, 71, 78, 82, 83, 91], "alpaca_dataset": [7, 26, 90], "custom": [7, 8, 24, 25, 28, 30, 81, 87, 91, 92, 93, 94], "train_on_input": [7, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 90], "onc": [7, 91, 92, 93, 94, 95], "ve": [7, 53, 65, 89, 90, 91, 93, 94], "instanc": [7, 10, 52, 58, 62, 63, 94], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 28, 91, 95], "under": [7, 91, 93, 95], "preced": [7, 10, 93, 94], "actual": [7, 9, 24, 89], "throw": 7, "notic": [7, 89, 90, 94], "miss": [7, 94], "posit": [7, 10, 51, 55, 56, 57, 93], "anoth": [7, 91], "handl": [7, 11, 19, 64, 89, 91, 94, 95], "def": [7, 8, 9, 11, 89, 90, 94, 95], "dictconfig": [7, 8, 10, 11, 12, 78], "arg": [7, 10, 56, 58, 60, 68, 76], "tupl": [7, 10, 32, 58, 64, 65, 68, 72, 79], "kwarg": [7, 10, 58, 60, 68, 73, 75, 76, 77, 78, 81], "str": [7, 10, 13, 16, 17, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 80, 82, 89], "mean": [7, 54, 89, 92, 94], "pass": [7, 10, 24, 25, 28, 30, 51, 52, 58, 70, 73, 77, 78, 81, 89, 94, 95], "add": [7, 9, 24, 65, 68, 90, 91, 93, 94, 95], "d": [7, 19, 51, 56, 57, 65, 89, 90, 94], "llama2_token": [7, 91], "tmp": [7, 89, 92, 93], "option": [7, 8, 13, 16, 17, 21, 24, 25, 28, 30, 32, 37, 38, 39, 44, 45, 47, 51, 55, 56, 57, 58, 64, 65, 66, 67, 69, 70, 71, 75, 78, 80, 82, 86, 87, 91], "bool": [7, 19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 58, 61, 64, 65, 66, 67, 73, 77, 80, 81, 95], "max_seq_len": [7, 10, 24, 25, 26, 27, 28, 30, 32, 51, 53, 55, 56, 64, 65, 89, 90], "int": [7, 9, 24, 25, 26, 27, 28, 30, 32, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 53, 54, 55, 56, 59, 61, 64, 65, 66, 67, 72, 75, 76, 77, 78, 79, 81, 82, 89, 90, 94, 95], "512": [7, 26, 27, 90, 95], "instructdataset": [7, 26, 27, 29, 30, 31, 90], "alreadi": [7, 73, 86, 91, 94], "overwrit": [7, 86], "duplic": [7, 8, 87], "sometim": 7, "than": [7, 23, 32, 51, 89, 91, 92, 93, 94, 95], "resolv": [7, 92], "alpaca": [7, 13, 26, 27, 37, 38, 39, 44, 45, 90], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 75, 76, 77, 78], "disklogg": 7, "log_dir": [7, 75, 77, 78], "conveni": [7, 8], "verifi": [7, 69, 70, 89, 92, 94], "properli": 7, "experi": [7, 78, 85, 87, 89, 93, 94], "wa": [7, 89, 91, 93, 94, 95], "cp": [7, 86, 89, 91, 92, 93], "7b_lora_single_devic": [7, 91, 92, 94, 95], "my_config": 7, "discuss": [7, 92, 94], "guidelin": 7, "while": [7, 8, 37, 38, 39, 44, 45, 52, 87, 91, 95], "mai": [7, 9, 80, 89, 90, 92, 94], "tempt": 7, "put": [7, 8, 92, 94], "much": [7, 91, 93, 94, 95], "give": [7, 94], "maximum": [7, 24, 25, 26, 27, 28, 30, 32, 51, 53, 55, 56, 65], "flexibl": [7, 90], "switch": 7, "encourag": [7, 94], "clariti": 7, "significantli": 7, "easier": [7, 91, 92], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 95], "expos": [7, 8, 89, 90, 92], "parent": 7, "modul": [7, 10, 32, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 81, 82, 89, 92, 94, 95], "__init__": [7, 8, 94, 95], "py": [7, 10, 37, 38, 39, 44, 45, 51, 53, 54, 55, 59, 91, 93], "guarante": 7, "stabil": [7, 87, 95], "underscor": 7, "_alpaca": 7, "collect": [7, 92], "differ": [7, 9, 24, 25, 64, 87, 89, 91, 93, 94, 95], "itself": 7, "via": [7, 9, 28, 61, 94, 95], "pair": [7, 79, 90], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 89, 91, 92, 93, 94, 95], "checkpoint": [7, 8, 58, 65, 66, 67, 78, 81, 87, 93, 94, 95], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 28, 30, 32, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 66, 67, 68, 75, 76, 77, 78, 89, 90, 92, 94, 95], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 51, 55, 56, 57], "core": [8, 87, 90, 92, 95], "i": [8, 18, 20, 58, 63, 65, 90, 91, 93, 95], "structur": [8, 14, 15, 18, 20, 24, 89, 90, 91], "new": [8, 48, 75, 77, 89, 91, 92, 93, 94, 95], "user": [8, 14, 15, 18, 19, 20, 22, 23, 24, 51, 64, 89, 90, 92], "thought": [8, 87, 92, 95], "target": [8, 87], "pipelin": [8, 87], "llm": [8, 85, 87, 90, 91, 94], "eg": [8, 56, 66, 87], "meaning": [8, 87, 91], "featur": [8, 9, 86, 87, 91, 92], "fsdp": [8, 87, 92, 93], "activ": [8, 52, 81, 87, 95], "gradient": [8, 87, 91, 93, 94, 95], "accumul": [8, 87], "mix": [8, 91], "precis": [8, 58, 70, 87, 92, 95], "appli": [8, 24, 25, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 54, 55, 56, 57, 81, 87, 95], "given": [8, 10, 17, 23, 61, 69, 70, 87, 94], "complex": 8, "becom": [8, 86, 90], "harder": 8, "anticip": 8, "architectur": [8, 18, 20, 56, 90], "methodolog": 8, "reason": [8, 91], "possibl": [8, 24, 28], "trade": 8, "off": [8, 64, 91], "memori": [8, 25, 26, 27, 30, 58, 85, 87, 90, 91, 92, 93], "vs": [8, 92], "qualiti": [8, 91, 94], "believ": 8, "best": [8, 89], "suit": [8, 92], "specif": [8, 10, 89, 90, 91, 95], "b": [8, 51, 55, 56, 57, 61, 78, 94, 95], "fit": [8, 24, 25, 26, 27, 30], "solut": 8, "result": [8, 64, 91, 93, 94, 95], "meant": [8, 58], "depend": [8, 9, 13, 91, 94, 95], "level": [8, 71, 87, 95], "expertis": 8, "routin": 8, "yourself": [8, 93, 94], "exist": [8, 86, 90, 91, 92, 93, 95], "ad": [8, 64, 89, 94, 95], "ones": 8, "modular": [8, 87], "build": [8, 28, 30, 87, 93, 94], "block": [8, 37, 38, 39, 44, 45, 47, 87], "wandb": [8, 9, 78, 92], "log": [8, 71, 75, 76, 77, 78, 91, 92, 93, 95], "fulli": 8, "nativ": [8, 85, 87, 94, 95], "pytorch": [8, 56, 58, 77, 80, 81, 82, 85, 86, 87, 93, 94, 95], "correct": [8, 16, 29, 54, 55, 56, 69, 87, 89, 90], "numer": [8, 87], "pariti": [8, 87], "verif": 8, "extens": [8, 87], "comparison": [8, 94, 95], "benchmark": [8, 82, 87, 91, 93, 94], "limit": 8, "hidden": [8, 52], "behind": 8, "100": [8, 25, 26, 27, 29, 31, 32, 79, 80, 94, 95], "flag": [8, 25, 26, 27, 29, 31, 95], "prefer": [8, 87, 90], "over": [8, 59, 68, 87, 90, 91, 93, 94, 95], "unnecessari": 8, "abstract": [8, 14, 17, 87, 92, 95], "No": [8, 87], "inherit": [8, 68, 87], "go": [8, 18, 20, 64, 87, 90, 91, 92, 95], "upon": [8, 93], "figur": [8, 94, 95], "spectrum": 8, "decid": 8, "interact": [8, 85, 92], "start": [8, 9, 65, 86, 87, 89, 90, 91, 92], "paradigm": 8, "consist": [8, 92], "configur": [8, 25, 26, 27, 28, 29, 30, 31, 32, 57, 87, 89, 92, 93, 94, 95], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 75, 76, 77, 78, 79, 80, 81, 82, 85, 87, 89, 91, 92, 93, 94, 95], "overrid": [8, 11, 91, 92, 93, 95], "togeth": [8, 78, 92, 94], "valid": [8, 23, 86, 91, 92], "environ": [8, 86, 91, 92], "logic": [8, 66, 87, 92, 94], "api": [8, 9, 40, 41, 46, 49, 89, 91, 92, 93, 95], "closer": [8, 94], "monolith": [8, 87], "trainer": [8, 72], "A": [8, 9, 22, 58, 61, 64, 65, 66, 68, 79, 84, 85, 88, 89, 91, 94, 95], "wrapper": [8, 64, 65, 94], "around": [8, 24, 64, 65, 80, 89, 91, 94, 95], "extern": 8, "primarili": [8, 94], "eleutherai": [8, 87, 94], "har": [8, 87, 94], "control": [8, 25, 26, 27, 29, 31, 82, 91], "multi": [8, 24, 51, 93], "stage": 8, "distil": 8, "oper": [8, 80, 82], "turn": [8, 19, 23, 24, 65, 89], "dataload": [8, 26, 27, 29, 31], "applic": [8, 51, 66, 67, 78], "clean": [8, 9, 26], "after": [8, 53, 54, 75, 76, 77, 78, 89, 95], "process": [8, 9, 58, 82, 92, 95], "group": [8, 51, 75, 76, 77, 78, 93], "init_process_group": [8, 73], "backend": 8, "gloo": 8, "els": [8, 68, 78, 87, 95], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 68, 90, 92, 93, 94], "stuff": 8, "carri": 8, "relev": [8, 19, 91, 94], "interfac": [8, 14, 17, 60], "metric": [8, 92], "logger": [8, 71, 75, 76, 77, 78, 92], "self": [8, 9, 37, 38, 39, 44, 45, 47, 51, 56, 57, 60, 90, 94, 95], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 80, 81, 89], "_model": 8, "_setup_model": 8, "_token": [8, 90], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 95], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 82, 93], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 26, 27, 29, 31, 51, 53, 55, 56, 57, 64, 79, 87, 90, 92, 93, 94], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 24, 25, 26, 27, 28, 30, 32, 79], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 75, 76, 77, 78], "step": [8, 56, 59, 65, 75, 76, 77, 78, 80, 85, 91, 94, 95], "learn": [8, 59, 87, 89, 90, 92, 93, 94, 95], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 20, 21, 23, 24, 25, 28, 30, 51, 55, 56, 57, 63, 64, 65, 66, 67, 69, 70, 71, 75, 76, 77, 78, 81, 82, 89, 91], "fullfinetunerecip": 8, "direct": [8, 86], "wandblogg": [9, 94, 95], "workspac": 9, "seen": [9, 94, 95], "screenshot": 9, "below": [9, 55, 90, 93, 94, 95], "packag": [9, 77, 78, 86], "pip": [9, 77, 78, 86, 91, 93], "Then": [9, 92], "login": [9, 78, 91], "built": [9, 86, 89, 90, 92, 95], "project": [9, 37, 38, 39, 44, 45, 47, 51, 52, 78, 85, 94, 95], "grab": [9, 93], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 89], "exit": [9, 86], "resourc": [9, 75, 76, 77, 78], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 30, 32, 89, 90, 91], "desir": [9, 24, 89], "suggest": 9, "approach": [9, 90], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 91], "_output_dir": [9, 66, 67], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 19, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 50, 51, 53, 54, 55, 56, 57, 58, 61, 62, 64, 65, 66, 67, 70, 71, 72, 73, 80, 81, 91, 94, 95], "descript": [9, 28, 32], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 19, 22, 24, 26, 27, 29, 31], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 24, 25, 28, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 48, 49, 50, 51, 54, 55, 59, 66, 67, 68, 71, 77, 78, 80, 81, 82, 86, 91], "com": [10, 37, 38, 39, 44, 45, 51, 54, 55, 59, 86], "facebookresearch": [10, 54, 55], "blob": [10, 37, 38, 39, 44, 45, 51, 54, 55, 59], "main": [10, 11, 51, 54, 55, 86, 91, 93], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 56], "32": [10, 93, 94, 95], "num_head": [10, 51, 53, 55, 56], "num_kv_head": [10, 51, 53], "vocab_s": 10, "must": [10, 24, 25, 26, 27, 28, 29, 30, 31, 32, 60, 65, 68, 95], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 50, 51, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 79, 80, 82, 89, 90, 94, 95], "nn": [10, 51, 52, 53, 56, 57, 58, 60, 62, 63, 81, 94, 95], "parsed_yaml": 10, "embed_dim": [10, 51, 55, 57, 94], "valueerror": [10, 20, 23, 28, 32, 51, 56, 66, 67, 70, 82], "callabl": [11, 24, 25, 56, 81], "With": [11, 91, 94, 95], "my_recip": 11, "foo": 11, "bar": [11, 87, 92], "instanti": [12, 33, 34, 35, 36, 37, 38, 39, 42, 43, 44, 45, 47, 48, 50], "configerror": 12, "cannot": [12, 93], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 75, 76, 77, 78, 90, 91, 95], "prompt": [13, 14, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 56, 64, 90, 91, 93], "templat": [13, 14, 16, 17, 21, 24, 25, 26, 27, 29, 30, 31, 32], "style": [13, 26, 27, 28, 32, 95], "slightli": 13, "classmethod": [13, 14, 15, 16, 17, 18, 19, 20, 21], "map": [13, 16, 17, 21, 22, 24, 25, 30, 32, 63, 66, 75, 76, 77, 78, 89, 91, 94], "column_map": [13, 16, 17, 21, 24, 25, 30, 32, 90], "placehold": [13, 14, 16, 17, 21, 24, 25, 30, 32], "column": [13, 16, 17, 21, 24, 25, 30, 32, 89], "ident": [13, 16, 17, 20, 21, 25, 30, 91], "role": [14, 19, 22, 24, 64, 89, 90], "system": [14, 15, 18, 19, 20, 22, 23, 24, 64, 89, 90], "assist": [14, 15, 18, 19, 22, 23, 24, 64, 89, 90], "messag": [14, 15, 18, 20, 22, 23, 24, 28, 64, 65, 86, 89, 90], "accord": [14, 20, 89], "openai": 15, "markup": 15, "languag": [15, 61, 94], "It": [15, 20, 89, 90, 95], "huggingfac": [15, 24, 25, 28, 30, 50, 59, 66, 67, 90, 91], "im_start": 15, "context": [15, 50, 80, 90], "im_end": 15, "goe": 15, "respons": [15, 64, 90, 91, 92, 93], "appropri": [15, 18, 20, 59, 90, 95], "tag": [15, 18, 20, 24, 65, 75, 76, 77, 78, 89], "grammar": [16, 29, 90], "sentenc": 16, "alwai": [17, 68], "human": [18, 22, 89], "taken": [18, 94, 95], "inst": [18, 20, 24, 89, 90], "sy": [18, 89, 90], "respect": [18, 63, 89, 90], "honest": [18, 89, 90], "am": [18, 20, 89, 90, 91, 93], "pari": [18, 20, 90], "capit": [18, 20, 90], "franc": [18, 20, 90], "known": [18, 20, 64, 90], "its": [18, 20, 82, 90, 91, 93, 94], "stun": [18, 20, 90], "liter": [19, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49], "mask": [19, 25, 26, 27, 29, 31, 51, 57, 64, 65, 89, 90], "ipython": 19, "eot": 19, "dataclass": [19, 89], "repres": [19, 89], "individu": [19, 78, 81, 89], "tiktoken": [19, 65, 93], "special": [19, 24, 65], "variabl": [19, 24, 25, 30, 32, 95], "writer": 19, "whether": [19, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 37, 38, 39, 44, 45, 47, 58, 61, 64, 65, 70, 89], "correspond": [19, 60, 62, 70, 92, 93], "consecut": [19, 23], "from_dict": [19, 89], "construct": [19, 94], "dictionari": [19, 75, 76, 77, 78, 91], "mistral": [20, 24, 47, 48, 49, 89, 91, 92], "llama2chatformat": [20, 32, 89, 90], "summar": [21, 31, 89, 90], "task": [21, 89, 90, 91, 93, 94, 95], "dialogu": [21, 31, 89], "dialog": 21, "adher": [22, 32], "sharegpt": [22, 28, 90], "gpt": [22, 51, 91], "remain": [22, 59, 94], "unmask": 22, "forth": 23, "come": [23, 60, 94], "empti": 23, "shorter": 23, "length": [23, 25, 26, 27, 30, 32, 50, 51, 53, 55, 56, 57, 64, 65, 67, 79], "min": [23, 94], "invalid": 23, "convert_to_messag": [24, 89], "chat_format": [24, 28, 32, 89, 90], "chatformat": [24, 28, 32, 90], "load_dataset_kwarg": [24, 25, 28, 30], "multiturn": [24, 89], "foreach": 24, "prepar": [24, 89], "truncat": [24, 25, 30, 32, 64, 65], "encod": [24, 25, 26, 27, 28, 29, 30, 31, 32, 64, 65, 89], "decod": [24, 25, 26, 27, 28, 29, 30, 31, 32, 56, 64, 65, 89], "anyth": [24, 25, 26, 27, 28, 29, 30, 31, 32], "load_dataset": [24, 25, 26, 27, 28, 29, 30, 31, 32, 89], "co": [24, 25, 28, 30, 50, 66, 67, 91], "doc": [24, 25, 28, 30, 68, 71, 77, 78, 80, 82, 91], "en": [24, 25, 28, 30], "package_refer": [24, 25, 28, 30], "loading_method": [24, 25, 28, 30], "text": [24, 64, 65, 89, 90, 91], "extra": [24, 86, 94, 95], "still": [24, 68, 94, 95], "llama3": [24, 42, 43, 44, 45, 46, 85], "where": [24, 26, 27, 29, 31, 51, 56, 61, 64, 90], "unless": 24, "check": [24, 28, 56, 70, 85, 89, 91, 92, 94], "instructtempl": [25, 90], "contribut": [25, 26, 27, 29, 31], "replac": [25, 26, 27, 29, 31, 58, 94], "disabl": [25, 30, 82], "recommend": [25, 26, 27, 30, 77, 89, 91, 95], "highest": [25, 26, 27, 30], "sequenc": [25, 26, 27, 30, 32, 51, 53, 55, 56, 57, 64, 65, 79, 89], "yahma": 26, "codebas": [26, 27, 29, 31, 91], "alpaca_d": [26, 27], "batch_siz": [26, 27, 29, 31, 51, 57, 91], "tatsu": [27, 90], "lab": [27, 90], "conversation_styl": [28, 90], "chatdataset": [28, 32, 89], "made": [28, 30, 55, 91], "friendli": [28, 30, 89], "huggingfaceh4": 28, "no_robot": 28, "chatmlformat": 28, "2096": 28, "accomplish": 28, "liweili": 29, "c4_200m": 29, "variant": [29, 31], "mirror": [29, 31], "llama_recip": [29, 31], "grammar_d": 29, "samsum": 31, "summari": 31, "samsum_d": 31, "_util": 32, "open": [32, 33, 90, 91], "orca": [32, 90], "slimorca": [32, 90], "dedup": [32, 90], "_chat_format": 32, "1024": [32, 90], "doesn": [32, 91], "prescrib": 32, "least": [32, 93, 94], "though": [32, 89], "max": [32, 56, 59, 64, 94], "ds": 32, "10": [32, 79, 91, 93, 95], "351": 32, "82": [32, 91], "391": 32, "221": 32, "220": 32, "193": 32, "12": [32, 86], "471": 32, "gemma": 33, "gemmatransformerdecod": 33, "w": [33, 34, 35, 36, 42, 43, 48, 77, 78, 89, 91, 94, 95], "blog": 33, "technolog": 33, "develop": [33, 95], "transformerdecod": [34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 94], "arxiv": [34, 35, 36, 40, 41, 46, 49, 51, 54, 55], "org": [34, 35, 36, 40, 41, 46, 49, 51, 54, 55, 68, 71, 77, 80, 81, 82, 86], "ab": [34, 35, 36, 40, 41, 46, 49, 55], "2307": [34, 35, 36], "09288": [34, 35, 36], "70b": [35, 38, 42, 44, 93], "lora_attn_modul": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94, 95], "q_proj": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 94, 95], "k_proj": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 94, 95], "v_proj": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 94, 95], "output_proj": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 94, 95], "apply_lora_to_mlp": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94], "apply_lora_to_output": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94], "lora_rank": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94], "lora_alpha": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94], "float": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 51, 54, 59, 61, 75, 76, 77, 78, 94, 95], "16": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 94, 95], "lora_dropout": [37, 38, 39, 40, 41], "05": [37, 38, 39, 40, 41], "quantize_bas": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 61, 95], "lora": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 61, 85, 87, 89, 92, 93], "tloen": [37, 38, 39, 44, 45], "8bb8579e403dc78e37fe81ffbb253c413007323f": [37, 38, 39, 44, 45], "l41": [37, 38, 39, 44, 45], "l43": [37, 38, 39, 44, 45], "linear": [37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 56, 60, 61, 94, 95], "attent": [37, 38, 39, 44, 45, 47, 50, 51, 53, 55, 56, 57, 93, 94, 95], "mlp": [37, 38, 39, 44, 45, 47, 56, 57, 93, 94], "final": [37, 38, 39, 44, 45, 47, 52, 56, 65, 91, 93, 94, 95], "rank": [37, 38, 39, 44, 45, 47, 61, 72, 82, 92, 94, 95], "low": [37, 38, 39, 44, 45, 47, 61, 91, 94, 95], "approxim": [37, 38, 39, 44, 45, 47, 61, 94], "scale": [37, 38, 39, 44, 45, 47, 61, 94, 95], "factor": [37, 38, 39, 44, 45, 47, 61, 91], "llama2_70b": 38, "llama2_7b": [39, 94], "qlora": [40, 41, 46, 49, 58, 85, 87, 93, 94], "per": [40, 41, 46, 49, 53, 58, 93, 95], "paper": [40, 41, 46, 49, 94, 95], "2305": [40, 41, 46, 49, 51], "14314": [40, 41, 46, 49], "lora_llama2_13b": 40, "lora_llama2_7b": [41, 94], "llama3_70b": 44, "llama3_8b": [45, 93], "lora_llama3_8b": 46, "announc": 48, "lora_mistral_7b": 49, "phi3": 50, "ref": [50, 78], "phi": 50, "128k": 50, "nor": 50, "slide": 50, "window": 50, "head_dim": [51, 53, 55, 56], "pos_embed": [51, 94], "kv_cach": 51, "kvcach": [51, 56], "attn_dropout": [51, 56], "head": [51, 53, 55, 56, 93], "queri": [51, 53, 56, 93], "gqa": 51, "introduc": [51, 54, 61, 89, 94, 95], "pdf": [51, 54], "13245v1": 51, "version": [51, 86, 93, 95], "multihead": 51, "mha": [51, 56], "n": [51, 64, 65, 84, 88, 89, 90], "extrem": 51, "share": [51, 90, 91], "mqa": 51, "credit": 51, "document": 51, "lightn": 51, "lit": 51, "lit_gpt": 51, "v": [51, 56, 94], "k": [51, 94], "q": [51, 94], "n_kv_head": 51, "dimens": [51, 53, 55, 56, 61, 93, 94, 95], "calcul": [51, 93], "e": [51, 58, 60, 63, 86, 91, 93, 94, 95], "g": [51, 60, 93, 94, 95], "rotarypositionalembed": [51, 94], "cach": [51, 53, 55, 86], "comput": [51, 52, 55, 56, 82, 91, 95], "rope": [51, 55], "dropout": [51, 61, 94, 95], "onto": 51, "scaled_dot_product_attent": 51, "input_po": [51, 55, 56, 57], "seq_length": [51, 57], "seq_len": [51, 55], "bigger": 51, "n_h": [51, 55], "num": [51, 55], "n_kv": 51, "kv": [51, 53, 56], "emb": [51, 56, 57], "h_d": [51, 55], "gate_proj": 52, "down_proj": 52, "up_proj": 52, "silu": 52, "feed": [52, 57], "network": [52, 94, 95], "deriv": [52, 56, 57], "fed": [52, 89], "multipli": 52, "subclass": [52, 68], "although": [52, 94], "afterward": 52, "former": 52, "regist": [52, 58, 95], "hook": [52, 58, 95], "latter": 52, "max_batch_s": 53, "standalon": 53, "past": 53, "becaus": [53, 56, 89, 91, 93], "expand": 53, "dpython": [53, 58], "ep": 54, "1e": 54, "06": [54, 94], "root": [54, 77, 78], "squar": 54, "1910": 54, "07467": 54, "verfic": [54, 55], "small": [54, 91], "avoid": [54, 58, 82, 95], "divis": 54, "zero": [54, 91, 93], "10000": 55, "rotari": [55, 93], "propos": 55, "2104": 55, "09864": 55, "l450": 55, "upto": 55, "init": [55, 78, 95], "exceed": 55, "freq": 55, "recomput": 55, "geometr": 55, "progress": [55, 92], "rotat": 55, "angl": 55, "bsz": 55, "todo": 55, "effici": [55, 85, 87, 91, 92, 94], "transformerdecoderlay": 56, "norm": [56, 57], "move": 56, "space": 56, "belong": 56, "reduc": [56, 87, 90, 94, 95], "statement": 56, "improv": [56, 91, 93, 94], "readabl": [56, 91], "At": 56, "arang": 56, "prompt_length": 56, "causal_mask": 56, "m_": 56, "seq": 56, "attn": [57, 94, 95], "causalselfattent": [57, 94], "sa_norm": 57, "mlp_norm": 57, "ff": 57, "common_util": 58, "bfloat16": [58, 91, 92, 93, 94], "offload_to_cpu": 58, "nf4": [58, 95], "restor": 58, "higher": [58, 93, 95], "offload": [58, 95], "increas": [58, 59, 93, 94], "peak": [58, 91, 93, 94, 95], "gpu": [58, 91, 92, 93, 94, 95], "usag": [58, 86, 91, 92, 93, 95], "_register_state_dict_hook": 58, "m": [58, 65, 89], "mymodul": 58, "_after_": 58, "nf4tensor": [58, 95], "unquant": [58, 91, 95], "unus": 58, "num_warmup_step": 59, "num_training_step": 59, "num_cycl": 59, "last_epoch": 59, "lambdalr": 59, "rate": [59, 87, 92], "schedul": [59, 80, 92], "linearli": 59, "lr": 59, "decreas": [59, 94, 95], "cosin": 59, "v4": 59, "23": [59, 93], "src": 59, "l104": 59, "warmup": [59, 80], "phase": 59, "total": [59, 72, 84, 88, 91, 93, 94], "wave": 59, "half": 59, "index": [59, 79, 86, 89, 91], "last": 59, "lr_schedul": 59, "peft": [60, 61, 62, 63, 94, 95], "protocol": 60, "adapter_param": [60, 61, 62, 63], "proj": 60, "in_dim": [60, 61, 94, 95], "out_dim": [60, 61, 94, 95], "bia": [60, 61, 94, 95], "loralinear": [60, 94, 95], "alpha": [61, 94, 95], "use_bia": 61, "larg": [61, 95], "perturb": 61, "decomposit": [61, 94], "matric": [61, 94, 95], "trainabl": [61, 63, 94, 95], "mapsto": 61, "w_0x": 61, "r": [61, 65, 94], "bax": 61, "probabl": [61, 91], "lora_a": [61, 94, 95], "lora_b": [61, 94, 95], "subset": 62, "get_adapter_param": [63, 94], "sentencepieceprocessor": 64, "pretrain": [64, 65, 89, 92, 94, 95], "non": 64, "spm_model": [64, 89], "tokenized_text": 64, "hello": [64, 89, 91, 93], "world": [64, 72, 91], "add_bo": [64, 65, 89], "add_eo": [64, 65, 89], "31587": 64, "29644": 64, "102": 64, "trim_leading_whitespac": 64, "prefix": 64, "unbatch": 64, "prepend": [64, 65], "bo": [64, 65, 89], "append": [64, 86], "eo": [64, 65, 89], "trim": 64, "lead": 64, "whitespac": 64, "underli": [64, 95], "sentencepiec": [64, 93], "s1": 64, "s2": 64, "due": [64, 94, 95], "tokenize_messag": [64, 65, 89, 90], "concaten": 64, "problem": 64, "slice": 64, "tokenizer_path": 64, "separ": [64, 66, 89, 92, 93, 94, 95], "concat": 64, "1788": 64, "2643": 64, "13": [64, 91, 93, 95], "1792": 64, "9508": 64, "465": 64, "22137": 64, "2933": 64, "join": 64, "attribut": 64, "llama3_tiktoken": 65, "p": [65, 94, 95], "l": 65, "all_special_token": 65, "bos_token": 65, "begin_of_text": [65, 89], "eos_token": 65, "end_of_text": 65, "start_header_id": [65, 89], "end_header_id": [65, 89], "step_id": 65, "eom_id": 65, "eot_id": [65, 89], "python_tag": 65, "identif": 65, "regex": 65, "element": [65, 91], "second": [65, 91, 93, 94, 95], "uniqu": 65, "256": [65, 90, 91, 93], "header": [65, 89], "token_id": 65, "truncate_at_eo": 65, "tokenize_head": 65, "few": [66, 90, 93, 94, 95], "0001_of_0003": 66, "0002_of_0003": 66, "preserv": [66, 95], "weight_map": [66, 91], "intermediate_checkpoint": [66, 67], "parit": 66, "_weight_map": 66, "shard": [67, 93], "wip": 67, "argpars": 68, "argumentpars": 68, "builtin": 68, "said": 68, "noth": 68, "treat": [68, 89], "consult": 68, "info": [68, 92], "librari": [68, 71, 82, 85, 87, 95], "html": [68, 71, 77, 80, 81, 82], "parse_known_arg": 68, "namespac": 68, "act": 68, "precid": 68, "parse_arg": 68, "intern": 68, "properti": [68, 94], "too": [68, 93], "availab": 69, "machin": [69, 91], "distribut": [69, 73, 81, 82, 87, 92, 93], "bf16": [70, 95], "request": [70, 90, 91], "inde": [70, 91], "kernel": 70, "runtimeerror": [70, 73], "float32": 70, "done": [70, 94, 95], "isn": 70, "hardwar": [70, 87, 91, 94], "stream": 71, "handler": 71, "aka": 72, "filenam": 75, "log_": 75, "unixtimestamp": 75, "txt": [75, 92], "thread": 75, "safe": 75, "flush": [75, 76, 77, 78], "union": [75, 76, 77, 78, 81, 82], "ndarrai": [75, 76, 77, 78], "scalar": [75, 76, 77, 78], "record": [75, 76, 77, 78], "payload": [75, 76, 77, 78], "organize_log": 77, "tensorboard": 77, "stabl": [77, 80, 82, 86], "subdirectori": 77, "sub": 77, "compar": [77, 91, 94, 95], "logdir": 77, "startup": 77, "recurs": 77, "tree": [77, 90, 91], "tfevent": 77, "encount": 77, "frontend": 77, "organ": 77, "accordingli": 77, "my_log_dir": 77, "view": [77, 91, 92], "my_metr": [77, 78], "termin": [77, 78], "entiti": 78, "bias": 78, "sent": 78, "usernam": 78, "my_project": 78, "my_ent": 78, "my_group": 78, "importerror": 78, "account": [78, 94, 95], "log_config": 78, "local": [78, 82, 86, 89, 91, 92], "link": [78, 91], "capecap": 78, "6053ofw0": 78, "torchtune_config_j67sb73v": 78, "padding_idx": 79, "ignore_idx": 79, "pad": 79, "longest": 79, "integ": [79, 82], "tokenpair": 79, "collat": 79, "token_pair": 79, "torchtune_perf_trac": 80, "contextmanag": 80, "wait": 80, "trace": 80, "speed": [80, 93, 95], "reduct": [80, 94], "auto_wrap_polici": 81, "acwrappolicytyp": 81, "polici": 81, "describ": [81, 90], "author": [81, 87, 92, 95], "intermedi": [81, 93, 95], "fsdp_adavnced_tutori": 81, "debug_mod": 82, "pseudo": 82, "random": [82, 92], "commonli": [82, 91, 94, 95], "numpi": 82, "own": [82, 89, 90, 91, 94], "determinist": 82, "global": 82, "warn": 82, "nondeterminist": 82, "addition": [82, 94], "cudnn": 82, "set_deterministic_debug_mod": 82, "algorithm": 82, "outsid": [82, 91, 93, 94], "generated_examples_python": 83, "zip": 83, "galleri": [83, 88], "sphinx": 83, "000": [84, 88, 93], "execut": [84, 88], "generated_exampl": 84, "mem": [84, 88], "mb": [84, 88], "topic": 85, "gentl": 85, "introduct": 85, "readi": [85, 89], "maxim": [85, 87], "workflow": [85, 90, 92, 94], "requisit": 86, "proper": [86, 92], "host": [86, 92], "page": [86, 87, 92, 93], "latest": [86, 92, 95], "confirm": 86, "And": [86, 91, 93], "h": 86, "ls": [86, 91, 92, 93], "welcom": 86, "show": [86, 89, 94], "greatest": [86, 92], "contributor": 86, "cd": [86, 91], "even": [86, 89, 93, 94, 95], "commit": 86, "branch": 86, "url": 86, "whl": 86, "therebi": [86, 95], "howev": 86, "forc": 86, "reinstal": 86, "opt": [86, 92], "suffix": 86, "cu121": 86, "On": [87, 94], "pointer": 87, "emphas": 87, "aspect": 87, "simplic": 87, "component": 87, "reus": 87, "high": [87, 94], "prove": 87, "democrat": 87, "box": [87, 95], "zoo": 87, "varieti": [87, 94], "techniqu": [87, 91, 92, 94], "integr": [87, 91, 92, 93, 94, 95], "excit": 87, "checkout": 87, "quickstart": 87, "attain": 87, "better": [87, 89, 90, 91], "chekckpoint": 87, "hyperparamet": [87, 92, 94, 95], "embodi": 87, "philosophi": 87, "usabl": 87, "composit": 87, "hard": 87, "outlin": 87, "unecessari": 87, "never": 87, "thoroughli": 87, "unit": 87, "know": [89, 90, 91, 93, 94], "align": 89, "intend": 89, "hi": 89, "nice": 89, "meet": 89, "overhaul": 89, "entir": [89, 95], "sai": [89, 90, 92], "accompani": 89, "who": 89, "influenti": 89, "hip": 89, "hop": 89, "artist": [89, 93], "2pac": 89, "rakim": 89, "c": 89, "na": 89, "flavor": [89, 90], "indic": [89, 90], "certain": 89, "msg": 89, "formatted_messag": [89, 90], "nyou": [89, 90], "nwho": 89, "sentencepiecetoken": 89, "why": [89, 92, 94], "user_messag": 89, "518": 89, "25580": 89, "29962": 89, "3532": 89, "14816": 89, "29903": 89, "6778": 89, "piece_to_id": 89, "reserv": [89, 95], "vector": 89, "place": 89, "manual": [89, 95], "529": 89, "29879": 89, "29958": 89, "tiktokentoken": 89, "nhere": 89, "_encode_special_token": 89, "128000": 89, "128009": 89, "part": [89, 95], "pure": 89, "That": 89, "won": [89, 91, 93], "mess": 89, "govern": 89, "prime": 89, "strictli": 89, "summarizetempl": 89, "lightweight": 89, "ask": 89, "untouch": 89, "nsummari": 89, "long": [89, 94], "robust": 89, "enough": 89, "csv": 89, "question": [89, 90, 91, 93], "answer": [89, 91, 93], "onlin": 89, "forum": 89, "panda": 89, "pd": 89, "df": 89, "read_csv": 89, "your_fil": 89, "nrow": 89, "tolist": 89, "row": 89, "iloc": 89, "gp": 89, "receiv": 89, "commun": [89, 91], "satellit": 89, "thing": [89, 95], "message_convert": 89, "input_msg": 89, "output_msg": 89, "assistant_messag": 89, "But": [89, 91, 93, 94], "were": [89, 92], "mistralchatformat": 89, "custom_dataset": 89, "2048": 89, "data_fil": 89, "honor": 89, "copi": [89, 91, 92, 93, 95], "8b_lora_single_devic": [89, 93], "my": [89, 91, 93], "launch": [89, 92], "custom_8b_lora_single_devic": 89, "15": [89, 91, 94, 95], "steer": 90, "wheel": 90, "publicli": 90, "great": [90, 91], "iter": [90, 95], "knob": 90, "tweak": 90, "footprint": [90, 94], "could": [90, 94], "achiev": [90, 91, 93, 94, 95], "fix": 90, "goal": 90, "agnost": 90, "condit": 90, "respond": 90, "alpacainstructtempl": 90, "further": [90, 94, 95], "classifi": 90, "anim": 90, "plant": 90, "miner": 90, "oak": 90, "copper": 90, "ore": 90, "eleph": 90, "instruct_dataset": 90, "mydataset": 90, "onthehub": 90, "customtempl": 90, "similar": [90, 91, 93, 94, 95], "quit": [90, 95], "similarli": 90, "chat_dataset": 90, "incorpor": 90, "advanc": 90, "preferencedataset": 90, "rlhf": 90, "adjust": 90, "chosen": 90, "reject": 90, "chosen_messag": 90, "transformed_sampl": 90, "key_chosen": 90, "rejected_messag": 90, "key_reject": 90, "chosen_input_id": 90, "c_mask": 90, "chosen_label": 90, "np": 90, "cross_entropy_ignore_idx": 90, "rejected_input_id": 90, "r_mask": 90, "rejected_label": 90, "purpos": [90, 92, 93], "stack_exchanged_paired_dataset": 90, "had": 90, "lvwerra": 90, "stack": 90, "exchang": 90, "stackexchangedpairedtempl": 90, "response_j": 90, "response_k": 90, "data_dir": 90, "rl": 90, "favorit": [91, 93, 94], "seemlessli": 91, "beyond": [91, 95], "connect": 91, "larger": [91, 93], "might": 91, "amount": 91, "natur": 91, "export": 91, "mobil": 91, "phone": 91, "leverag": [91, 93, 95], "mode": 91, "lot": 91, "plai": 91, "freez": [91, 94], "percentag": 91, "learnabl": 91, "keep": [91, 94], "16gb": [91, 94], "rtx": 91, "3090": 91, "4090": 91, "hour": 91, "full_finetune_single_devic": [91, 92], "7b_full_low_memori": [91, 92], "full_finetune_distribut": [91, 92], "7b_full": [91, 92], "13b_full": [91, 92], "7b_qlora_single_devic": [91, 92, 95], "473": 91, "98": [91, 95], "gb": [91, 93, 94, 95], "50": 91, "484": 91, "01": [91, 92], "fact": [91, 93, 94], "third": 91, "smaller": [91, 93, 94, 95], "realli": 91, "eleuther_ev": [91, 93], "eleuther_evalu": [91, 93], "lm_eval": [91, 93], "plan": 91, "custom_eval_config": [91, 93], "truthfulqa_mc2": [91, 93, 94], "measur": [91, 93], "propens": [91, 93], "shot": [91, 93], "accuraci": [91, 93, 94, 95], "baselin": [91, 94], "324": 91, "loglikelihood": 91, "195": 91, "121": 91, "27": 91, "197": 91, "acc": 91, "388": 91, "38": 91, "shown": 91, "489": 91, "48": [91, 95], "seem": 91, "custom_generation_config": [91, 93], "kick": 91, "top_k": 91, "300": 91, "temperatur": 91, "interest": 91, "site": 91, "visit": 91, "bai": 91, "area": 91, "92": [91, 93], "exploratorium": 91, "san": 91, "francisco": 91, "magazin": 91, "awesom": 91, "bridg": 91, "pretti": 91, "cool": 91, "96": [91, 95], "61": 91, "sec": [91, 93], "25": 91, "83": 91, "99": [91, 94], "72": 91, "littl": 91, "saw": 91, "took": [91, 93], "torchao": [91, 93, 95], "bit": [91, 93, 94, 95], "custom_quantization_config": [91, 93], "68": 91, "19": [91, 93, 95], "76": 91, "69": 91, "95": [91, 93], "67": 91, "4w": [91, 93], "unlik": [91, 93], "engin": [91, 93], "fullmodeltorchtunecheckpoint": [91, 93], "int4weightonlyquant": [91, 93], "groupsiz": [91, 93], "did": [91, 93, 95], "park": 91, "sit": 91, "top": [91, 95], "hill": 91, "beauti": 91, "62": [91, 93], "17": [91, 94], "85": 91, "compil": [91, 93, 95], "hood": [91, 95], "sped": 91, "almost": [91, 93, 94], "3x": [91, 93], "benefit": 91, "yet": 91, "fast": 91, "clone": [91, 94, 95], "assumpt": 91, "satisfi": 91, "new_dir": 91, "output_dict": 91, "sd_1": 91, "sd_2": 91, "dump": 91, "convert_hf_checkpoint": 91, "checkpoint_path": 91, "justin": 91, "school": 91, "math": 91, "teacher": 91, "ws": 91, "94": [91, 93], "103": 91, "28": 91, "bandwidth": [91, 93], "1391": 91, "84": 91, "thats": 91, "seamlessli": 91, "authent": [91, 92], "hopefulli": 91, "gave": 91, "gate": 92, "grant": 92, "minut": 92, "agreement": 92, "altern": 92, "hackabl": 92, "singularli": 92, "focus": 92, "technic": 92, "depth": 92, "principl": 92, "minim": [92, 94, 95], "boilerpl": 92, "hold": 92, "substanti": [92, 94], "custom_config": 92, "replic": 92, "lorafinetunerecipesingledevic": 92, "lora_finetune_output": 92, "log_1713194212": 92, "sampler": 92, "52": 92, "3697006702423096": 92, "25880": [92, 95], "24": [92, 93], "55": 92, "83it": 92, "monitor": 92, "tqdm": 92, "interv": 92, "e2": 92, "releas": 93, "focu": 93, "128": [93, 94], "theta": 93, "gain": 93, "illustr": 93, "basic": 93, "observ": 93, "18": 93, "consum": [93, 95], "vram": [93, 94], "overal": 93, "nproc_per_nod": [93, 94], "lora_finetune_distribut": [93, 94], "8b_lora": 93, "8b_qlora_single_devic": 93, "alloc": [93, 95], "coupl": [93, 94, 95], "llama3_token": 93, "122": 93, "sarah": 93, "busi": 93, "mum": 93, "young": 93, "children": 93, "live": 93, "north": 93, "east": 93, "england": 93, "135": 93, "88": 93, "138": 93, "346": 93, "09": 93, "139": 93, "31": 93, "been": 93, "far": 93, "drill": 93, "90": 93, "93": 93, "91": 93, "104": 93, "four": [93, 94], "again": 93, "jake": 93, "disciplin": 93, "passion": 93, "draw": 93, "paint": 93, "57": [93, 94, 95], "speedup": 93, "broader": 93, "teach": 94, "straight": 94, "jump": 94, "neural": [94, 95], "unfamiliar": 94, "oppos": [94, 95], "momentum": 94, "adamw": 94, "arbitrari": 94, "relat": 94, "aghajanyan": 94, "et": 94, "al": 94, "hypothes": 94, "intrins": 94, "lower": 94, "down": [94, 95], "often": 94, "eight": 94, "practic": 94, "imag": 94, "simplifi": 94, "left": 94, "blue": 94, "rememb": 94, "approx": 94, "15m": 94, "8192": 94, "65k": 94, "requires_grad": [94, 95], "frozen_out": [94, 95], "lora_out": [94, 95], "omit": 94, "base_model": 94, "choos": 94, "lora_model": 94, "lora_llama_2_7b": [94, 95], "alon": 94, "in_featur": 94, "out_featur": 94, "inplac": 94, "feel": 94, "free": 94, "strict": 94, "whenev": 94, "validate_state_dict_for_lora": 94, "peft_util": 94, "set_trainable_param": 94, "fetch": 94, "lora_param": 94, "total_param": 94, "sum": 94, "numel": 94, "trainable_param": 94, "2f": 94, "6742609920": 94, "4194304": 94, "nnode": 94, "7b_lora": 94, "my_model_checkpoint_path": [94, 95], "tokenizer_checkpoint": [94, 95], "my_tokenizer_checkpoint_path": [94, 95], "constraint": 94, "factori": 94, "benefici": 94, "impact": 94, "rel": 94, "minor": 94, "good": 94, "64": 94, "lora_experiment_1": 94, "smooth": [94, 95], "curv": [94, 95], "500": 94, "ran": 94, "commod": 94, "cogniz": 94, "ax": 94, "parallel": 94, "truthfulqa": 94, "previous": 94, "475": 94, "87": 94, "508": 94, "86": 94, "504": 94, "04": 94, "514": 94, "lowest": 94, "absolut": 94, "4gb": 94, "tradeoff": 94, "potenti": 94, "enhanc": 95, "maintain": 95, "highli": 95, "vanilla": 95, "held": 95, "therefor": 95, "bespok": 95, "normalfloat": 95, "8x": 95, "retain": 95, "vast": 95, "major": 95, "highlight": 95, "degrad": 95, "normatfloat": 95, "doubl": 95, "themselv": 95, "prune": 95, "deepdiv": 95, "idea": 95, "distinct": 95, "storag": 95, "datatyp": 95, "de": 95, "incur": 95, "counterpart": 95, "set_default_devic": 95, "qlora_linear": 95, "memory_alloc": 95, "177": 95, "152": 95, "byte": 95, "del": 95, "empty_cach": 95, "lora_linear": 95, "081": 95, "344": 95, "qlora_llama2_7b": 95, "qlora_model": 95, "essenti": 95, "reparametrize_as_dtype_state_dict_post_hook": 95, "stat": 95, "against": 95, "35": 95, "40": 95, "29": 95, "slow": 95, "slower": 95, "149": 95, "9157477021217346": 95, "02": 95, "08": 95, "14": 95, "15it": 95, "nightli": 95, "200": 95, "hundr": 95, "228": 95, "8158286809921265": 95, "59": 95, "95it": 95, "exercis": 95, "portion": 95, "augment": 95, "linear_nf4": 95, "to_nf4": 95, "linear_weight": 95, "autograd": 95, "regular": 95, "incom": 95}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "Message"], [20, 1, 1, "", "MistralChatFormat"], [21, 1, 1, "", "SummarizeTemplate"], [22, 0, 1, "", "sharegpt_to_llama2_messages"], [23, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.Message": [[19, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[20, 2, 1, "", "format"], [20, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[21, 2, 1, "", "format"]], "torchtune.datasets": [[24, 1, 1, "", "ChatDataset"], [25, 1, 1, "", "InstructDataset"], [26, 0, 1, "", "alpaca_cleaned_dataset"], [27, 0, 1, "", "alpaca_dataset"], [28, 0, 1, "", "chat_dataset"], [29, 0, 1, "", "grammar_dataset"], [30, 0, 1, "", "instruct_dataset"], [31, 0, 1, "", "samsum_dataset"], [32, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[33, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[34, 0, 1, "", "llama2_13b"], [35, 0, 1, "", "llama2_70b"], [36, 0, 1, "", "llama2_7b"], [37, 0, 1, "", "lora_llama2_13b"], [38, 0, 1, "", "lora_llama2_70b"], [39, 0, 1, "", "lora_llama2_7b"], [40, 0, 1, "", "qlora_llama2_13b"], [41, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[42, 0, 1, "", "llama3_70b"], [43, 0, 1, "", "llama3_8b"], [44, 0, 1, "", "lora_llama3_70b"], [45, 0, 1, "", "lora_llama3_8b"], [46, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[47, 0, 1, "", "lora_mistral_7b"], [48, 0, 1, "", "mistral_7b"], [49, 0, 1, "", "qlora_mistral_7b"]], "torchtune.models.phi3": [[50, 0, 1, "", "phi3_mini"]], "torchtune.modules": [[51, 1, 1, "", "CausalSelfAttention"], [52, 1, 1, "", "FeedForward"], [53, 1, 1, "", "KVCache"], [54, 1, 1, "", "RMSNorm"], [55, 1, 1, "", "RotaryPositionalEmbeddings"], [56, 1, 1, "", "TransformerDecoder"], [57, 1, 1, "", "TransformerDecoderLayer"], [59, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[51, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[52, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[54, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[55, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[56, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[57, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[58, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[60, 1, 1, "", "AdapterModule"], [61, 1, 1, "", "LoRALinear"], [62, 0, 1, "", "get_adapter_params"], [63, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[60, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[61, 2, 1, "", "adapter_params"], [61, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[64, 1, 1, "", "SentencePieceTokenizer"], [65, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[64, 2, 1, "", "decode"], [64, 2, 1, "", "encode"], [64, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[65, 2, 1, "", "decode"], [65, 2, 1, "", "encode"], [65, 2, 1, "", "tokenize_message"], [65, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[66, 1, 1, "", "FullModelHFCheckpointer"], [67, 1, 1, "", "FullModelMetaCheckpointer"], [68, 1, 1, "", "TuneRecipeArgumentParser"], [69, 0, 1, "", "get_device"], [70, 0, 1, "", "get_dtype"], [71, 0, 1, "", "get_logger"], [72, 0, 1, "", "get_world_size_and_rank"], [73, 0, 1, "", "init_distributed"], [74, 0, 1, "", "list_dtypes"], [79, 0, 1, "", "padded_collate"], [80, 0, 1, "", "profiler"], [81, 0, 1, "", "set_activation_checkpointing"], [82, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[66, 2, 1, "", "load_checkpoint"], [66, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[67, 2, 1, "", "load_checkpoint"], [67, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[68, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[75, 1, 1, "", "DiskLogger"], [76, 1, 1, "", "StdoutLogger"], [77, 1, 1, "", "TensorBoardLogger"], [78, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[75, 2, 1, "", "close"], [75, 2, 1, "", "log"], [75, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[76, 2, 1, "", "close"], [76, 2, 1, "", "log"], [76, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[77, 2, 1, "", "close"], [77, 2, 1, "", "log"], [77, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[78, 2, 1, "", "close"], [78, 2, 1, "", "log"], [78, 2, 1, "", "log_config"], [78, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 85, 87, 91, 93, 94, 95], "config": [0, 7, 8, 92], "data": [1, 5, 89], "instruct": [1, 86, 90, 93], "templat": [1, 89, 90], "chat": [1, 89, 90], "format": [1, 6, 90], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 89, 90], "exampl": 2, "gener": [2, 91, 93], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 91, 92, 93, 94], "llama3": [3, 89, 93], "llama2": [3, 89, 91, 94, 95], "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 86, 95], "block": 4, "token": [4, 89], "peft": 4, "util": [4, 5], "checkpoint": [5, 6, 9, 91], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 94, 95], "manag": 5, "perform": [5, 94], "profil": [5, 80], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 87, 91], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 91, 94, 95], "put": [6, 95], "thi": 6, "all": [6, 7, 95], "togeth": [6, 95], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 90], "us": [7, 8, 89, 91, 95], "instanti": [7, 10], "referenc": 7, "other": [7, 91], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 91, 92], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 87, 94, 95], "ar": 8, "recip": [8, 92, 94], "script": 8, "run": [8, 91], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "messag": 19, "mistralchatformat": 20, "summarizetempl": 21, "sharegpt_to_llama2_messag": 22, "validate_messag": 23, "chatdataset": 24, "instructdataset": 25, "alpaca_cleaned_dataset": 26, "alpaca_dataset": 27, "chat_dataset": 28, "grammar_dataset": 29, "instruct_dataset": 30, "samsum_dataset": 31, "slimorca_dataset": 32, "gemma_2b": 33, "llama2_13b": 34, "llama2_70b": 35, "llama2_7b": 36, "lora_llama2_13b": 37, "lora_llama2_70b": 38, "lora_llama2_7b": 39, "qlora_llama2_13b": 40, "qlora_llama2_7b": 41, "llama3_70b": 42, "llama3_8b": 43, "lora_llama3_70b": 44, "lora_llama3_8b": 45, "qlora_llama3_8b": 46, "lora_mistral_7b": 47, "mistral_7b": 48, "qlora_mistral_7b": 49, "phi3_mini": 50, "causalselfattent": 51, "todo": [51, 57], "feedforward": 52, "kvcach": 53, "rmsnorm": 54, "rotarypositionalembed": 55, "transformerdecod": 56, "transformerdecoderlay": 57, "reparametrize_as_dtype_state_dict_post_hook": 58, "get_cosine_schedule_with_warmup": 59, "adaptermodul": 60, "loralinear": 61, "get_adapter_param": 62, "set_trainable_param": 63, "sentencepiecetoken": 64, "tiktokentoken": 65, "fullmodelhfcheckpoint": 66, "fullmodelmetacheckpoint": 67, "tunerecipeargumentpars": 68, "get_devic": 69, "get_dtyp": 70, "get_logg": 71, "get_world_size_and_rank": 72, "init_distribut": 73, "list_dtyp": 74, "disklogg": 75, "stdoutlogg": 76, "tensorboardlogg": 77, "wandblogg": 78, "padded_col": 79, "set_activation_checkpoint": 81, "set_se": 82, "comput": [84, 88], "time": [84, 88], "welcom": 85, "document": 85, "get": [85, 93], "start": 85, "tutori": 85, "instal": 86, "via": [86, 93], "pypi": 86, "git": 86, "clone": 86, "nightli": 86, "kei": 87, "concept": 87, "design": 87, "principl": 87, "fine": [89, 90, 92, 93], "tune": [89, 90, 92, 93], "chang": 89, "from": [89, 95], "prompt": 89, "special": 89, "when": 89, "should": 89, "i": 89, "custom": [89, 90], "fulli": 90, "end": 91, "workflow": 91, "download": [91, 92], "7b": 91, "finetun": [91, 94, 95], "evalu": [91, 93], "eleutherai": [91, 93], "s": [91, 93], "eval": [91, 93], "har": [91, 93], "speed": 91, "up": 91, "quantiz": [91, 93], "librari": 91, "upload": 91, "hug": 91, "face": 91, "hub": 91, "first": 92, "llm": 92, "select": 92, "modifi": 92, "train": 92, "next": 92, "step": 92, "meta": 93, "8b": 93, "access": 93, "text": 93, "our": 93, "faster": 93, "how": 94, "doe": 94, "work": 94, "appli": 94, "trade": 94, "off": 94, "qlora": 95, "save": 95, "deep": 95, "dive": 95}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})