Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.Role", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "torchtune.data.Role", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "Llama3Tokenizer", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "MistralTokenizer", "lora_mistral_7b", "lora_mistral_classifier_7b", "mistral_7b", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "Phi3MiniTokenizer", "lora_phi3_mini", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 21, 23, 31, 32, 33, 35, 36, 40, 44, 74, 86, 92, 93, 155, 159, 160, 163, 165, 166], "prompt": [1, 14, 17, 18, 19, 21, 23, 24, 25, 26, 29, 31, 33, 35, 36, 37, 39, 40, 41, 42, 44, 55, 63, 74, 82, 90, 100, 115, 128, 161, 162, 164], "chat": [1, 2, 15, 16, 19, 25, 26, 29, 37, 42, 63, 93], "includ": [1, 6, 7, 8, 15, 18, 63, 93, 106, 117, 121, 122, 126, 157, 159, 160, 161, 162, 163, 164, 165, 166], "some": [1, 6, 7, 16, 108, 109, 155, 157, 159, 160, 161, 162, 163, 165, 166], "specif": [1, 4, 7, 8, 10, 131, 160, 161, 162, 166], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29, 31, 33, 35, 36, 37, 40, 42, 63, 74, 118, 121, 122, 123, 124, 159, 160, 162, 163, 164, 165], "differ": [1, 7, 9, 29, 30, 31, 33, 82, 112, 124, 145, 152, 157, 159, 160, 162, 164, 165, 166], "dataset": [1, 5, 7, 14, 17, 18, 20, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 157, 163, 164], "model": [1, 2, 6, 7, 8, 10, 16, 21, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 114, 115, 121, 122, 123, 124, 127, 128, 131, 133, 139, 146, 147, 155, 157, 160, 161, 166], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 56, 57, 64, 65, 66, 77, 82, 85, 86, 93, 96, 100, 101, 103, 105, 108, 111, 112, 114, 117, 118, 121, 122, 123, 125, 126, 127, 128, 142, 143, 146, 154, 156, 158, 159, 161, 162, 163, 164, 165], "common": [1, 2, 4, 7, 115, 159, 160, 161, 164, 165], "json": [1, 6, 25, 26, 77, 93, 114, 121, 159, 161, 162], "messag": [1, 15, 16, 19, 21, 25, 26, 28, 29, 37, 55, 63, 74, 82, 90, 115, 156, 159, 160, 161], "miscellan": 1, "function": [1, 4, 7, 8, 10, 12, 29, 95, 96, 102, 104, 107, 110, 111, 120, 121, 128, 129, 135, 139, 145, 149, 157, 160, 161, 166], "us": [1, 2, 4, 6, 9, 10, 12, 16, 19, 20, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 63, 77, 93, 95, 96, 98, 99, 100, 101, 102, 104, 107, 110, 112, 113, 116, 118, 120, 121, 122, 124, 125, 126, 128, 129, 130, 131, 133, 139, 140, 141, 142, 143, 149, 155, 156, 157, 159, 161, 163, 164, 165], "modifi": [1, 7, 8, 9, 102, 157, 162, 164, 165, 166], "For": [2, 5, 6, 7, 8, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 44, 45, 63, 95, 100, 116, 117, 121, 126, 127, 134, 143, 147, 149, 156, 159, 160, 161, 162, 163, 164, 165, 166], "detail": [2, 6, 37, 42, 63, 97, 120, 131, 139, 149, 159, 162, 163, 164, 165, 166], "usag": [2, 102, 124, 125, 150, 156, 159, 161, 162, 163, 164, 166], "guid": [2, 7, 9, 157, 160, 161, 163, 165], "pleas": [2, 5, 52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 120, 131, 139, 147, 156, 166], "see": [2, 5, 6, 9, 19, 21, 37, 42, 45, 52, 53, 54, 61, 62, 63, 71, 72, 73, 80, 81, 88, 89, 94, 97, 105, 120, 124, 126, 131, 132, 139, 143, 147, 149, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "our": [2, 6, 8, 157, 160, 161, 162, 163, 165, 166], "tutori": [2, 6, 63, 147, 157, 160, 161, 162, 163, 164, 165, 166], "support": [2, 6, 8, 9, 10, 20, 21, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 92, 93, 95, 106, 116, 118, 122, 123, 125, 130, 133, 157, 159, 160, 161, 162, 163, 164, 165, 166], "sever": 2, "wide": 2, "help": [2, 6, 19, 100, 121, 126, 155, 156, 157, 159, 160, 161, 162, 163, 164, 166], "quickli": [2, 7, 34, 160, 161], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 23, 24, 29, 34, 63, 142, 143, 155, 156, 157, 159, 160, 161, 164, 165, 166], "fine": [2, 6, 8, 9, 20, 32, 63, 155, 157, 162, 165], "tune": [2, 3, 6, 7, 8, 9, 12, 20, 32, 63, 155, 156, 157, 159, 162, 165, 166], "also": [2, 6, 7, 8, 9, 10, 37, 40, 44, 93, 95, 100, 106, 129, 131, 133, 139, 143, 156, 159, 160, 161, 162, 163, 164, 165, 166], "like": [2, 6, 7, 8, 9, 29, 93, 123, 156, 159, 160, 161, 162, 163, 165], "These": [2, 4, 6, 7, 8, 10, 32, 126, 160, 161, 162, 163, 164, 165, 166], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 17, 18, 19, 21, 23, 24, 28, 31, 32, 33, 35, 36, 37, 39, 40, 41, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 100, 106, 107, 110, 111, 117, 120, 121, 122, 124, 125, 127, 128, 130, 133, 137, 139, 145, 150, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "especi": [2, 157, 159, 162], "specifi": [2, 6, 7, 8, 10, 37, 95, 100, 101, 104, 120, 128, 131, 134, 139, 143, 147, 150, 159, 160, 161, 162, 163, 164, 166], "yaml": [2, 7, 8, 10, 11, 12, 37, 40, 44, 126, 143, 157, 159, 160, 161, 162, 163, 164, 165, 166], "config": [2, 6, 9, 10, 11, 12, 13, 37, 40, 44, 95, 110, 121, 125, 126, 143, 150, 157, 160, 161, 162, 164, 165, 166], "represent": [2, 165, 166], "abov": [2, 6, 102, 117, 137, 156, 162, 164, 165, 166], "all": [3, 4, 8, 13, 29, 30, 32, 37, 77, 93, 95, 96, 100, 102, 107, 116, 117, 121, 125, 126, 127, 137, 146, 152, 153, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165], "famili": [3, 8, 35, 36, 38, 42, 43, 45, 124, 157, 159, 164], "download": [3, 6, 153, 156, 160, 161, 164, 165, 166], "meta": [3, 6, 19, 63, 74, 121, 122, 159, 160, 162, 163], "8b": [3, 76, 79, 81, 91, 159, 160], "hf": [3, 6, 90, 104, 121, 159, 160, 162, 163, 164], "token": [3, 6, 7, 8, 20, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 58, 63, 67, 74, 77, 82, 87, 90, 93, 95, 99, 100, 101, 112, 113, 114, 115, 128, 131, 144, 159, 161, 162, 163, 164, 165, 166], "access_token": 3, "pre": [3, 19, 32, 63, 156, 160, 161], "train": [3, 5, 6, 8, 9, 19, 29, 30, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 63, 95, 99, 100, 101, 102, 103, 121, 122, 123, 130, 133, 139, 150, 155, 157, 159, 160, 161, 162, 164, 165, 166], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 44, 45, 63, 82, 98, 99, 107, 112, 113, 116, 118, 120, 121, 124, 126, 131, 139, 142, 143, 147, 150, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "hug": [3, 6, 16, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 77, 93, 103, 114, 157, 159, 163, 164], "face": [3, 6, 16, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 77, 93, 103, 114, 157, 159, 163, 164], "hub": [3, 6, 159, 161, 163], "follow": [3, 6, 8, 20, 25, 26, 29, 32, 95, 103, 116, 123, 124, 125, 137, 143, 150, 155, 156, 159, 161, 162, 163, 164, 165, 166], "command": [3, 8, 9, 126, 156, 159, 160, 161, 162, 163, 164, 165, 166], "2": [3, 6, 9, 28, 32, 42, 55, 63, 74, 82, 90, 95, 112, 113, 115, 116, 117, 118, 121, 122, 144, 145, 148, 149, 150, 151, 160, 162, 163, 164, 165], "7b": [3, 6, 31, 33, 34, 35, 36, 38, 40, 44, 45, 48, 51, 54, 57, 60, 66, 70, 73, 83, 84, 85, 86, 121, 122, 160, 163, 164, 165, 166], "codellama": 3, "mini": [3, 90, 91, 92, 93, 94], "microsoft": [3, 92, 93], "4k": [3, 92, 93], "hf_token": 3, "ignor": [3, 6, 90, 95, 96, 159], "pattern": [3, 113, 159], "ai": [3, 85, 95, 143, 160, 164], "mistralai": [3, 159], "v0": 3, "1": [3, 6, 8, 32, 42, 55, 63, 74, 82, 90, 95, 100, 103, 104, 112, 113, 115, 117, 118, 122, 124, 128, 137, 142, 143, 144, 145, 148, 149, 159, 160, 162, 163, 164, 165, 166], "size": [3, 6, 8, 10, 35, 36, 39, 41, 95, 97, 98, 99, 100, 116, 118, 119, 135, 137, 157, 159, 161, 162, 163, 164, 165], "2b": [3, 56, 59], "googl": [3, 56, 57], "perform": [4, 6, 32, 63, 96, 107, 128, 157, 160, 162, 164, 166], "direct": [4, 8, 104, 145, 156], "encod": [4, 55, 63, 74, 82, 90, 104, 112, 113, 115, 160], "text": [4, 20, 29, 32, 34, 37, 38, 44, 45, 63, 74, 82, 90, 112, 113, 160, 162], "id": [4, 6, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 63, 74, 82, 90, 95, 99, 100, 101, 112, 113, 114, 121, 123, 128, 144, 145, 160, 161, 162], "decod": [4, 74, 82, 90, 100, 112, 113, 128, 160], "typic": [4, 7, 32, 34, 44, 93, 104, 161, 166], "byte": [4, 113, 166], "pair": [4, 7, 14, 43, 113, 144, 145, 161], "underli": [4, 82, 112, 166], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 102, 105, 108, 110, 118, 125, 126, 134, 156, 157, 161, 162, 164, 165, 166], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 23, 24, 25, 26, 27, 29, 31, 33, 34, 37, 38, 40, 44, 45, 63, 82, 102, 108, 109, 110, 111, 112, 115, 121, 122, 123, 125, 128, 136, 139, 149, 152, 159, 160, 161, 162, 163, 164, 165], "preprocess": [4, 32], "imag": [4, 20, 116, 117, 118, 119, 165], "offer": 5, "allow": [5, 30, 110, 117, 142, 159, 166], "seamless": 5, "transit": 5, "between": [5, 6, 121, 124, 161, 162, 164, 165, 166], "interoper": [5, 6, 8, 157, 162, 166], "rest": [5, 160, 166], "ecosystem": [5, 6, 8, 157, 162, 164, 166], "comprehens": 5, "overview": [5, 7, 9, 155, 163, 165, 166], "deep": [5, 6, 7, 8, 9, 157, 163, 164], "dive": [5, 6, 7, 8, 9, 157, 163, 164], "enabl": [5, 7, 8, 9, 30, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 106, 149, 150, 164, 165, 166], "work": [5, 6, 8, 126, 157, 159, 162, 164, 166], "set": [5, 6, 7, 8, 9, 20, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 44, 45, 74, 90, 95, 99, 100, 107, 109, 118, 120, 129, 131, 137, 139, 147, 148, 149, 150, 157, 159, 160, 162, 163, 164, 165], "consumpt": [5, 30], "dure": [5, 6, 30, 31, 32, 35, 36, 39, 41, 95, 97, 99, 100, 101, 102, 133, 160, 162, 164, 165, 166], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 27, 29, 30, 31, 32, 33, 42, 100, 107, 123, 126, 129, 131, 143, 150, 157, 159, 160, 161, 162, 163, 164], "debug": [5, 6, 7, 8, 159], "finetun": [5, 6, 7, 8, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 155, 157, 163, 164], "job": [5, 9, 149, 163], "variou": [5, 18], "walk": [6, 8, 142, 157, 160, 161, 162, 163, 166], "you": [6, 7, 8, 9, 10, 18, 19, 20, 24, 29, 31, 33, 34, 35, 36, 38, 40, 44, 45, 117, 124, 126, 128, 142, 143, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "through": [6, 7, 8, 9, 96, 107, 157, 159, 160, 161, 162, 163, 166], "design": [6, 8], "behavior": [6, 139, 160, 161], "associ": [6, 7, 8, 128, 162, 165], "util": [6, 7, 8, 9, 10, 30, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 157, 162, 163, 164, 166], "what": [6, 7, 9, 19, 21, 23, 34, 39, 41, 155, 160, 161, 162, 163, 164], "cover": [6, 7, 8, 9, 160, 162, 166], "how": [6, 7, 8, 9, 24, 120, 147, 155, 159, 160, 161, 162, 163, 164, 166], "we": [6, 7, 8, 9, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 63, 82, 95, 97, 99, 100, 106, 116, 117, 121, 122, 123, 128, 130, 134, 139, 146, 157, 159, 160, 161, 162, 163, 164, 165, 166], "them": [6, 7, 29, 30, 31, 33, 40, 55, 63, 82, 90, 96, 102, 115, 159, 160, 161, 162, 165, 166], "scenario": [6, 30], "full": [6, 7, 8, 37, 40, 52, 53, 54, 55, 61, 62, 63, 71, 72, 73, 80, 81, 82, 88, 89, 90, 94, 110, 111, 115, 157, 159, 161, 164, 165], "compos": 6, "compon": [6, 8, 13, 145, 157, 161, 163, 165, 166], "which": [6, 7, 8, 30, 31, 32, 34, 35, 36, 39, 41, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 82, 83, 84, 91, 95, 99, 100, 101, 103, 110, 111, 112, 117, 121, 122, 123, 125, 130, 140, 143, 147, 157, 159, 160, 161, 162, 163, 164, 165, 166], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 96, 110, 121, 122, 123, 157, 160, 161, 162, 164, 166], "evalu": [6, 8, 155, 157, 163, 165, 166], "gener": [6, 8, 14, 17, 23, 24, 29, 31, 32, 33, 38, 42, 63, 82, 107, 148, 149, 150, 153, 155, 160, 161, 165, 166], "each": [6, 8, 15, 18, 30, 32, 49, 50, 51, 55, 59, 60, 63, 68, 69, 70, 78, 79, 82, 83, 84, 90, 91, 95, 99, 100, 101, 104, 110, 111, 115, 117, 119, 145, 149, 150, 157, 159, 161, 162, 163, 164, 165], "make": [6, 7, 8, 9, 95, 101, 157, 159, 162, 163, 164, 165, 166], "easi": [6, 8, 157, 161, 165], "understand": [6, 7, 8, 155, 157, 160, 161, 165, 166], "extend": [6, 8, 157], "befor": [6, 28, 31, 32, 33, 95, 100, 101, 106, 113, 121, 159, 162], "let": [6, 7, 9, 159, 160, 161, 162, 163, 164, 165, 166], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 50, 51, 55, 63, 68, 69, 70, 74, 78, 79, 82, 83, 84, 90, 91, 93, 95, 97, 99, 100, 101, 102, 104, 105, 108, 110, 111, 117, 120, 121, 122, 125, 129, 131, 133, 139, 142, 147, 148, 157, 159, 160, 161, 163, 165, 166], "defin": [6, 7, 8, 96, 105, 106, 108, 161, 163, 165], "concept": [6, 162, 163], "In": [6, 7, 8, 29, 99, 106, 117, 120, 139, 142, 143, 160, 162, 164, 165, 166], "ll": [6, 7, 8, 128, 134, 157, 160, 161, 162, 163, 164, 166], "talk": 6, "about": [6, 8, 104, 143, 157, 159, 160, 162, 163, 164, 165, 166], "take": [6, 7, 8, 10, 96, 97, 102, 121, 123, 126, 129, 145, 160, 161, 162, 163, 164, 165, 166], "close": [6, 8, 140, 141, 142, 143, 165], "look": [6, 7, 8, 127, 142, 156, 160, 161, 162, 163, 164, 165], "veri": [6, 30, 100, 159, 162], "simpli": [6, 7, 32, 159, 160, 161, 162, 164, 166], "dictat": 6, "state_dict": [6, 102, 110, 121, 122, 123, 124, 125, 165, 166], "store": [6, 30, 140, 143, 165, 166], "file": [6, 7, 8, 9, 10, 11, 12, 55, 63, 74, 77, 82, 90, 93, 112, 113, 114, 121, 122, 123, 126, 140, 143, 150, 154, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166], "disk": [6, 34, 140], "weight": [6, 8, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 102, 105, 106, 110, 112, 121, 122, 123, 124, 134, 139, 143, 155, 159, 160, 162, 163, 164, 165, 166], "string": [6, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 63, 74, 82, 90, 105, 112, 113, 115, 129, 130, 134, 159, 161], "kei": [6, 7, 9, 14, 17, 23, 24, 25, 29, 31, 33, 40, 95, 97, 100, 101, 109, 110, 111, 121, 123, 125, 145, 150, 159, 162, 163, 165, 166], "identifi": 6, "state": [6, 8, 102, 108, 109, 110, 111, 121, 122, 123, 125, 127, 162, 164, 165, 166], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 37, 38, 40, 44, 45, 74, 90, 102, 108, 109, 110, 111, 113, 114, 121, 122, 123, 125, 127, 133, 136, 138, 144, 145, 146, 161], "If": [6, 7, 13, 14, 17, 18, 20, 21, 23, 24, 25, 27, 28, 29, 31, 33, 35, 36, 39, 40, 41, 42, 74, 90, 95, 99, 100, 101, 102, 104, 106, 111, 117, 118, 121, 122, 123, 124, 125, 128, 129, 130, 131, 133, 134, 136, 142, 143, 149, 152, 156, 159, 160, 161, 162, 163, 164, 165], "don": [6, 7, 8, 143, 149, 159, 160, 161, 162, 163, 164, 166], "t": [6, 7, 8, 130, 143, 149, 159, 160, 161, 162, 163, 164, 166], "match": [6, 29, 31, 33, 40, 90, 111, 117, 156, 159, 161, 162, 164, 165], "up": [6, 8, 9, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 113, 116, 118, 127, 150, 159, 160, 161, 163, 164, 165, 166], "exactli": [6, 111], "those": [6, 124, 165], "definit": [6, 165], "either": [6, 111, 121, 128, 147, 159, 165, 166], "run": [6, 7, 9, 12, 96, 97, 100, 102, 121, 122, 123, 125, 127, 137, 142, 143, 146, 156, 157, 160, 161, 163, 164, 165, 166], "explicit": 6, "error": [6, 7, 28, 121, 149, 159], "load": [6, 8, 29, 30, 31, 32, 33, 34, 110, 121, 122, 123, 125, 126, 142, 160, 161, 162, 164, 165], "rais": [6, 10, 13, 21, 25, 28, 37, 42, 90, 95, 97, 100, 104, 110, 111, 121, 122, 123, 125, 130, 133, 136, 143, 145, 149, 152], "an": [6, 7, 8, 9, 10, 14, 28, 30, 34, 39, 41, 44, 45, 95, 100, 104, 105, 107, 108, 109, 116, 117, 118, 120, 121, 122, 123, 125, 129, 131, 143, 150, 157, 159, 160, 161, 162, 163, 164, 165, 166], "except": [6, 20, 21, 115, 161], "wors": 6, "silent": [6, 96], "succe": 6, "infer": [6, 19, 29, 63, 95, 97, 99, 100, 101, 129, 155, 160, 162, 163, 164, 166], "expect": [6, 7, 10, 14, 17, 18, 23, 24, 29, 31, 33, 37, 40, 99, 111, 125, 143, 152, 160, 161, 165], "addit": [6, 7, 8, 10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 63, 110, 120, 121, 122, 123, 130, 131, 136, 139, 140, 142, 143, 147, 150, 157, 160, 163, 165], "line": [6, 8, 14, 126, 159, 161, 163, 164], "need": [6, 7, 8, 9, 18, 29, 32, 42, 95, 96, 100, 139, 142, 143, 146, 156, 159, 160, 161, 162, 163, 164, 165, 166], "shape": [6, 95, 97, 99, 100, 101, 104, 106, 117, 119, 128, 150], "valu": [6, 7, 26, 42, 46, 47, 48, 56, 57, 64, 65, 66, 75, 76, 85, 86, 95, 97, 98, 100, 101, 103, 110, 121, 124, 125, 126, 128, 140, 141, 142, 143, 145, 149, 159, 161, 163, 164, 165], "two": [6, 7, 28, 117, 157, 162, 163, 164, 165, 166], "popular": [6, 157, 161, 162], "llama2": [6, 7, 8, 10, 19, 29, 31, 33, 34, 35, 36, 38, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 96, 100, 101, 124, 155, 157, 159, 163, 164], "offici": [6, 19, 160, 163, 164], "implement": [6, 8, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 63, 82, 90, 96, 98, 99, 103, 104, 105, 106, 121, 134, 142, 157, 161, 165, 166], "when": [6, 7, 8, 12, 30, 32, 34, 63, 95, 99, 100, 101, 102, 103, 110, 116, 118, 128, 131, 142, 146, 159, 162, 164, 165, 166], "llama": [6, 19, 29, 63, 74, 98, 99, 121, 122, 159, 160, 162, 163, 164, 165], "websit": 6, "get": [6, 7, 8, 9, 29, 63, 82, 130, 132, 133, 135, 156, 157, 160, 161, 162, 163, 165], "access": [6, 7, 8, 30, 121, 127, 159, 162, 163], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 30, 32, 34, 95, 110, 121, 122, 123, 125, 127, 159, 160, 161, 162, 163, 164, 165, 166], "pth": [6, 162], "inspect": [6, 162, 165, 166], "content": [6, 15, 20, 25, 26, 29, 55, 63, 82, 90, 115, 160, 161], "easili": [6, 7, 157, 161, 165, 166], "torch": [6, 7, 30, 97, 100, 102, 103, 104, 117, 118, 119, 123, 125, 127, 128, 129, 130, 133, 136, 137, 145, 146, 147, 148, 149, 150, 151, 152, 162, 163, 164, 165, 166], "import": [6, 7, 10, 37, 40, 44, 142, 143, 160, 161, 162, 163, 165, 166], "consolid": [6, 164], "00": [6, 154, 158, 163], "mmap": [6, 162], "true": [6, 7, 20, 31, 32, 34, 35, 36, 37, 39, 40, 41, 44, 52, 53, 54, 55, 61, 62, 63, 71, 72, 73, 74, 80, 81, 82, 88, 89, 90, 94, 95, 100, 101, 102, 107, 112, 113, 115, 117, 120, 121, 122, 123, 131, 133, 136, 137, 139, 142, 150, 151, 159, 160, 161, 162, 164, 165, 166], "weights_onli": [6, 123], "map_loc": [6, 162], "cpu": [6, 8, 102, 130, 150, 156, 159, 162, 166], "tensor": [6, 95, 96, 97, 98, 99, 100, 101, 102, 104, 106, 117, 118, 119, 121, 128, 140, 141, 142, 143, 144, 145, 148, 165, 166], "item": 6, "print": [6, 9, 30, 35, 36, 39, 41, 42, 55, 63, 74, 82, 90, 112, 113, 115, 128, 151, 160, 161, 163, 165, 166], "f": [6, 9, 35, 36, 39, 41, 160, 162, 165, 166], "tok_embed": [6, 100], "32000": [6, 10, 165], "4096": [6, 10, 31, 33, 34, 35, 36, 38, 40, 44, 45, 95, 99, 161, 165], "len": [6, 30, 35, 36, 39, 41, 100], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 29, 30, 31, 32, 33, 39, 41, 42, 43, 49, 50, 51, 55, 59, 60, 63, 68, 69, 70, 74, 78, 79, 82, 90, 91, 98, 99, 102, 103, 104, 107, 112, 113, 114, 115, 117, 118, 119, 120, 121, 123, 126, 129, 130, 132, 134, 143, 148, 150, 151, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "contain": [6, 20, 25, 30, 32, 34, 44, 55, 63, 74, 77, 82, 90, 93, 95, 97, 99, 100, 101, 105, 108, 109, 110, 113, 115, 116, 121, 122, 123, 125, 126, 127, 133, 138, 142, 144, 145, 150, 160, 162, 164, 165], "input": [6, 14, 17, 18, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 82, 90, 95, 96, 98, 99, 100, 101, 106, 112, 113, 118, 119, 121, 123, 144, 145, 149, 152, 160, 161, 165, 166], "embed": [6, 95, 97, 98, 99, 100, 131, 160, 164], "tabl": [6, 160, 166], "call": [6, 10, 20, 96, 102, 110, 126, 140, 141, 142, 143, 146, 150, 160, 161, 165, 166], "layer": [6, 8, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 100, 101, 106, 110, 111, 120, 131, 157, 164, 165, 166], "have": [6, 7, 10, 95, 97, 105, 111, 117, 123, 125, 126, 131, 139, 142, 152, 156, 160, 161, 162, 163, 164, 165, 166], "dim": [6, 95, 96, 98, 99, 100], "most": [6, 7, 116, 160, 163, 165, 166], "within": [6, 7, 10, 29, 32, 42, 96, 128, 142, 149, 150, 159, 161, 162, 164, 165, 166], "default": [6, 7, 16, 20, 25, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 59, 60, 63, 64, 65, 66, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 90, 91, 93, 95, 96, 98, 99, 100, 101, 102, 103, 104, 106, 110, 112, 113, 115, 121, 122, 123, 126, 128, 130, 135, 139, 140, 143, 144, 145, 148, 149, 150, 156, 159, 160, 161, 162, 164, 165, 166], "everi": [6, 8, 96, 142, 150, 156, 159, 166], "repo": [6, 121, 122, 124, 159, 162], "first": [6, 7, 10, 28, 32, 97, 100, 121, 126, 155, 157, 160, 161, 162, 164, 165, 166], "big": [6, 162], "split": [6, 32, 113, 160, 161, 162], "across": [6, 8, 30, 121, 142, 149, 162, 164], "bin": [6, 159, 162], "To": [6, 7, 8, 9, 32, 121, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "correctli": [6, 8, 13, 110, 121, 156, 160, 163, 166], "piec": 6, "one": [6, 8, 28, 55, 63, 82, 90, 96, 104, 115, 117, 123, 160, 161, 162, 163, 164, 166], "pytorch_model": [6, 162], "00001": [6, 159], "00002": [6, 159], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 32, 38, 106, 108, 110, 112, 122, 123, 125, 126, 128, 130, 131, 133, 134, 139, 159, 161, 162, 163, 164, 165, 166], "doe": [6, 21, 25, 29, 32, 63, 92, 95, 100, 101, 105, 115, 121, 123, 125, 126, 159, 160, 162], "fewer": [6, 95], "sinc": [6, 7, 10, 96, 117, 118, 121, 123, 160, 162, 164], "instead": [6, 8, 32, 37, 40, 44, 96, 97, 106, 159, 162, 164, 165], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 42, 44, 45, 105, 109, 111, 113, 121, 122, 123, 124, 125, 126, 127, 128, 129, 140, 141, 142, 143, 152, 159, 160, 162, 164], "caus": [6, 82, 112, 118], "try": [6, 7, 160, 162, 163, 164, 166], "same": [6, 7, 49, 50, 51, 55, 59, 60, 63, 68, 69, 70, 78, 79, 82, 90, 91, 97, 101, 115, 125, 126, 131, 143, 159, 160, 162, 164, 165, 166], "As": [6, 7, 8, 9, 106, 157, 162, 164, 166], "re": [6, 7, 123, 157, 160, 162, 163, 164, 165], "care": [6, 96, 121, 123, 162, 164, 165], "end": [6, 8, 20, 30, 34, 44, 74, 82, 113, 155, 157, 160, 164, 165], "number": [6, 8, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 95, 97, 100, 103, 116, 121, 122, 123, 128, 135, 149, 150, 159, 163, 165], "just": [6, 14, 157, 159, 160, 161, 163, 164, 165], "save": [6, 8, 9, 102, 121, 122, 123, 125, 131, 139, 143, 155, 159, 160, 161, 162, 164, 165], "less": [6, 42, 162, 163, 164, 166], "prone": 6, "manag": [6, 30, 107, 148, 160], "invari": 6, "accept": [6, 7, 42, 120, 161, 163, 166], "multipl": [6, 7, 8, 20, 29, 30, 95, 100, 101, 106, 116, 117, 140, 141, 142, 143, 145, 150, 163, 164], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 160, 161, 162], "worri": [6, 160, 163], "explicitli": [6, 105, 157, 165], "convert": [6, 25, 26, 29, 121, 144, 160, 162, 166], "time": [6, 55, 63, 82, 90, 115, 140, 142, 150, 159, 160, 161, 162, 164, 166], "produc": [6, 125, 166], "back": [6, 28, 107, 121, 161, 165, 166], "origin": [6, 35, 36, 102, 106, 160, 162, 164, 165, 166], "form": [6, 7, 8, 28, 159], "One": [6, 162], "advantag": [6, 165], "being": [6, 121, 122, 123, 127, 129, 166], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 25, 26, 32, 37, 40, 44, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 95, 96, 104, 105, 110, 111, 119, 120, 126, 138, 140, 141, 142, 143, 156, 157, 161, 162, 163, 164, 165, 166], "abl": [6, 8, 162, 163, 164], "post": [6, 146, 150, 166], "tool": [6, 20, 161, 162, 163], "quantiz": [6, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 106, 123, 134, 155, 163, 166], "eval": [6, 155, 157], "without": [6, 7, 9, 14, 110, 117, 118, 156, 157, 160, 162, 165], "code": [6, 8, 46, 47, 48, 49, 50, 51, 52, 53, 54, 100, 153, 157, 161, 163], "chang": [6, 7, 9, 14, 123, 156, 162, 163, 164, 165, 166], "OR": [6, 25], "convers": [6, 15, 16, 19, 21, 25, 26, 28, 29, 37, 42, 121, 123, 124, 157, 160, 161, 162, 164, 165, 166], "script": [6, 9, 159, 162, 163, 164], "wai": [6, 7, 29, 110, 159, 160, 161, 162, 163, 164], "surround": [6, 8, 157], "load_checkpoint": [6, 8, 121, 122, 123, 124], "save_checkpoint": [6, 8, 9, 121, 122, 123], "convertor": 6, "avail": [6, 8, 45, 126, 129, 130, 137, 157, 159, 162, 164, 165], "here": [6, 7, 9, 14, 16, 17, 23, 24, 39, 98, 99, 159, 160, 161, 162, 163, 164, 165, 166], "three": [6, 8, 104, 163], "hfcheckpoint": 6, "read": [6, 121, 122, 123, 157], "write": [6, 8, 14, 121, 122, 123, 140, 160, 161, 163], "compat": [6, 121, 123], "transform": [6, 8, 29, 31, 33, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 100, 101, 103, 116, 117, 118, 119, 147, 165], "framework": [6, 8, 157], "mention": [6, 162, 166], "assum": [6, 14, 17, 18, 23, 24, 31, 33, 40, 95, 99, 100, 101, 103, 108, 113, 125, 127, 130, 139, 162, 165], "checkpoint_dir": [6, 7, 121, 122, 123, 162, 164], "necessari": [6, 42, 140, 141, 142, 143, 160, 165], "easiest": [6, 162, 163], "sure": [6, 7, 162, 163, 164, 165, 166], "everyth": [6, 8, 126, 157, 163], "flow": [6, 29, 31, 32, 33, 166], "By": [6, 159, 164, 165, 166], "safetensor": [6, 121, 159], "output": [6, 18, 35, 36, 39, 42, 49, 50, 51, 68, 69, 70, 78, 79, 83, 84, 91, 95, 96, 98, 99, 100, 101, 106, 109, 110, 111, 118, 123, 128, 131, 141, 150, 156, 159, 160, 161, 162, 163, 164, 165, 166], "dir": [6, 143, 156, 159, 162, 163, 164], "output_dir": [6, 7, 121, 122, 123, 150, 162, 164, 165, 166], "argument": [6, 7, 10, 18, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 95, 120, 126, 131, 136, 140, 142, 143, 147, 159, 160, 161, 164, 165], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 100, 150, 159, 161, 162, 165, 166], "_component_": [6, 7, 9, 10, 37, 40, 44, 160, 161, 162, 164, 165], "fullmodelhfcheckpoint": [6, 162], "directori": [6, 7, 121, 122, 123, 140, 142, 143, 150, 159, 162, 163, 164], "sort": [6, 121, 123], "so": [6, 7, 32, 117, 121, 126, 156, 157, 160, 162, 163, 164, 165, 166], "order": [6, 8, 121, 123, 142, 143, 163], "matter": [6, 121, 123, 159, 165], "checkpoint_fil": [6, 7, 9, 121, 122, 123, 162, 164, 165, 166], "restart": [6, 159], "previou": [6, 32, 121, 122, 123], "more": [6, 7, 8, 37, 42, 63, 97, 99, 110, 120, 123, 126, 143, 147, 149, 157, 159, 161, 162, 163, 164, 165, 166], "next": [6, 32, 128, 164, 166], "section": [6, 8, 133, 155, 162, 164, 166], "recipe_checkpoint": [6, 121, 122, 123], "null": [6, 7], "usual": [6, 99, 121, 143, 159, 162, 165], "model_typ": [6, 121, 122, 123, 162, 164], "resume_from_checkpoint": [6, 121, 122, 123], "fals": [6, 7, 20, 25, 26, 29, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 88, 89, 90, 91, 94, 95, 100, 101, 106, 107, 110, 112, 117, 121, 122, 123, 137, 150, 159, 160, 161, 162, 164, 165, 166], "requir": [6, 7, 30, 34, 42, 44, 63, 121, 123, 125, 136, 137, 139, 142, 143, 145, 149, 150, 156, 159, 160, 161, 163, 166], "param": [6, 8, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 106, 108, 109, 111, 121, 139, 165, 166], "directli": [6, 7, 8, 10, 37, 40, 44, 120, 121, 159, 162, 163, 164, 165, 166], "ensur": [6, 7, 13, 28, 42, 95, 121, 123, 130, 157, 161, 163], "out": [6, 7, 8, 29, 31, 35, 36, 37, 39, 41, 121, 122, 155, 157, 159, 160, 162, 163, 164, 165, 166], "case": [6, 8, 9, 20, 121, 125, 130, 134, 139, 140, 147, 157, 159, 160, 161, 162, 164, 165, 166], "discrep": [6, 121], "along": [6, 164, 165], "found": [6, 7, 9, 98, 99, 159, 165, 166], "metacheckpoint": 6, "github": [6, 10, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 95, 98, 99, 103, 104, 110, 156, 161, 163], "repositori": [6, 19, 162, 163], "fullmodelmetacheckpoint": [6, 164], "torchtunecheckpoint": 6, "current": [6, 32, 92, 95, 97, 99, 100, 101, 122, 123, 131, 135, 140, 142, 146, 149, 162, 163, 164], "test": [6, 7, 8, 157, 160], "complet": [6, 8, 14, 32, 38, 93, 160, 161, 162, 163, 164], "written": [6, 7, 8, 121, 122, 140, 141, 142, 143, 157], "begin": [6, 32, 63, 82, 113, 160, 164, 166], "partit": [6, 121, 166], "ha": [6, 63, 82, 105, 107, 108, 111, 123, 125, 152, 161, 162, 163, 164, 165, 166], "standard": [6, 17, 25, 95, 141, 157, 160, 162, 164], "key_1": [6, 123], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 162], "inform": [6, 143, 147, 157, 159, 162, 163, 164], "subsequ": [6, 8], "recipe_st": [6, 121, 122, 123], "pt": [6, 9, 121, 122, 123, 162, 164], "epoch": [6, 8, 9, 103, 121, 122, 123, 159, 160, 162, 163, 164], "optim": [6, 7, 8, 30, 63, 92, 103, 104, 123, 125, 127, 133, 145, 146, 150, 160, 162, 163, 164, 165, 166], "etc": [6, 8, 121, 133, 163], "prevent": [6, 32, 159], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 63, 82, 100, 105, 125, 146, 149, 150, 160, 161, 162, 165, 166], "updat": [6, 7, 8, 97, 125, 150, 156, 160, 162, 163, 164, 165, 166], "hf_model_0001_0": [6, 162], "hf_model_0002_0": [6, 162], "both": [6, 30, 111, 159, 162, 165, 166], "adapt": [6, 105, 106, 107, 108, 109, 121, 122, 123, 160, 162, 165, 166], "merg": [6, 10, 11, 121, 162, 164, 166], "would": [6, 7, 9, 32, 100, 156, 160, 161, 162, 165, 166], "primari": [6, 7, 8, 163], "want": [6, 7, 8, 9, 10, 29, 116, 117, 128, 156, 159, 160, 161, 162, 163, 164, 165], "resum": [6, 8, 103, 121, 122, 123, 166], "initi": [6, 8, 12, 30, 32, 46, 47, 48, 56, 57, 64, 65, 66, 75, 76, 85, 86, 125, 136, 137, 163, 165, 166], "frozen": [6, 165, 166], "base": [6, 10, 20, 31, 33, 42, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 99, 103, 104, 106, 107, 109, 110, 111, 121, 126, 129, 131, 139, 140, 155, 160, 162, 163, 164, 165, 166], "well": [6, 7, 8, 157, 159, 161, 162, 164, 166], "learnt": [6, 160, 162], "someth": [6, 8, 9, 160, 162], "NOT": 6, "refer": [6, 7, 8, 98, 99, 104, 107, 157, 165], "adapter_checkpoint": [6, 121, 122, 123], "adapter_0": [6, 162], "now": [6, 125, 127, 160, 161, 162, 163, 164, 165, 166], "knowledg": 6, "creat": [6, 7, 10, 32, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 91, 92, 94, 97, 103, 120, 121, 122, 123, 127, 140, 142, 159, 160, 161, 162, 164, 166], "simpl": [6, 8, 14, 17, 23, 24, 155, 161, 163, 165, 166], "forward": [6, 8, 95, 96, 98, 99, 100, 101, 104, 106, 133, 150, 164, 165, 166], "13b": [6, 46, 49, 52, 64, 68, 71], "modeltyp": [6, 121, 122, 123], "llama2_13b": [6, 68], "right": [6, 121, 162, 164, 165], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 110, 125, 165], "successfulli": [6, 159, 163], "vocab": [6, 10, 100, 164], "70": [6, 75], "x": [6, 95, 96, 98, 99, 100, 101, 106, 128, 148, 165, 166], "randint": 6, "0": [6, 8, 32, 49, 50, 51, 52, 53, 54, 55, 63, 68, 69, 70, 71, 72, 73, 82, 90, 95, 100, 103, 104, 106, 115, 117, 128, 142, 143, 144, 145, 149, 151, 154, 158, 160, 161, 162, 163, 164, 165, 166], "no_grad": 6, "6": [6, 32, 98, 119, 144, 145, 162, 166], "3989": 6, "9": [6, 145, 162, 166], "0531": 6, "3": [6, 32, 74, 91, 92, 93, 117, 118, 119, 124, 126, 132, 144, 145, 148, 159, 160, 162, 163, 164, 166], "2375": 6, "5": [6, 7, 14, 103, 104, 117, 144, 145, 162, 163, 164], "2822": 6, "4": [6, 7, 42, 95, 116, 144, 145, 151, 157, 159, 161, 162, 164, 165, 166], "4872": 6, "7469": 6, "8": [6, 35, 36, 39, 41, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 145, 162, 165, 166], "6737": 6, "11": [6, 145, 162, 164, 166], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 144, 145], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 20, 29, 31, 33, 40, 42, 110, 115, 143, 159, 160, 161, 162, 163, 164, 165], "find": [6, 8, 9, 159, 162, 163, 165], "list": [6, 7, 15, 16, 19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 88, 89, 90, 91, 94, 105, 106, 110, 111, 112, 113, 115, 116, 117, 121, 122, 123, 126, 128, 132, 144, 145, 160, 161, 163, 164], "builder": [6, 38, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 91, 92, 94, 160, 161, 166], "hope": 6, "deeper": [6, 163], "insight": [6, 162], "happi": [6, 162], "thi": [7, 8, 9, 10, 17, 20, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 44, 45, 63, 74, 82, 90, 92, 93, 95, 96, 99, 100, 101, 102, 103, 105, 107, 110, 111, 112, 113, 115, 120, 121, 122, 123, 125, 126, 128, 129, 130, 133, 137, 139, 140, 142, 143, 145, 146, 147, 149, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166], "pars": [7, 10, 11, 114, 126, 160, 163], "effect": 7, "cli": [7, 9, 11, 12, 156, 162, 163], "prerequisit": [7, 160, 161, 162, 163, 164, 165, 166], "Be": [7, 160, 162, 163, 164, 165, 166], "familiar": [7, 160, 162, 163, 164, 165, 166], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 156, 160, 161, 163], "instal": [7, 9, 137, 142, 143, 155, 159, 162, 163, 164, 165, 166], "fundament": 7, "There": [7, 28, 117, 160, 162, 163, 164, 165], "entri": [7, 8, 163], "point": [7, 8, 25, 26, 115, 161, 162, 163, 164, 165, 166], "locat": [7, 159, 164, 165, 166], "thei": [7, 8, 30, 100, 111, 126, 131, 159, 160, 161, 165], "truth": [7, 162, 164], "reproduc": 7, "overridden": [7, 96, 126, 150], "quick": [7, 30], "experiment": 7, "serv": [7, 115, 120, 161, 165], "particular": [7, 29, 30, 42, 120, 161, 165, 166], "seed": [7, 8, 9, 149, 163], "shuffl": [7, 32], "devic": [7, 8, 110, 125, 129, 130, 133, 159, 160, 162, 163, 164, 165], "cuda": [7, 129, 130, 133, 150, 156, 162, 166], "dtype": [7, 8, 97, 100, 102, 130, 148, 152, 162, 166], "fp32": [7, 166], "enable_fsdp": 7, "mani": [7, 32, 161, 162], "object": [7, 10, 11, 15, 16, 19, 21, 95, 120, 134, 160], "keyword": [7, 10, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 102, 160, 161], "loss": [7, 8, 20, 31, 35, 36, 39, 41, 104, 163, 165, 166], "exampl": [7, 8, 9, 10, 12, 14, 17, 23, 24, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 55, 63, 74, 82, 90, 95, 104, 105, 107, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 128, 134, 142, 143, 144, 145, 148, 151, 153, 154, 156, 158, 159, 160, 161, 162, 164, 165, 166], "subfield": 7, "dotpath": 7, "wish": [7, 161], "exact": [7, 10, 162], "path": [7, 8, 9, 10, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 58, 63, 67, 74, 77, 82, 87, 90, 93, 112, 113, 114, 121, 122, 123, 126, 150, 159, 160, 161, 162, 164, 165], "normal": [7, 29, 32, 63, 82, 98, 100, 101, 112, 160, 161, 165, 166], "python": [7, 126, 132, 143, 149, 153, 159, 162], "alpaca_dataset": [7, 35, 161], "custom": [7, 8, 29, 31, 33, 37, 40, 44, 147, 157, 159, 162, 163, 164, 165], "train_on_input": [7, 25, 26, 29, 31, 35, 36, 37, 39, 40, 41, 42, 160, 161], "onc": [7, 107, 162, 163, 164, 165, 166], "ve": [7, 97, 160, 161, 162, 164, 165], "instanc": [7, 10, 30, 96, 102, 108, 109, 165], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 37, 159, 162, 166], "under": [7, 150, 161, 162, 164, 166], "preced": [7, 10, 159, 164, 165], "actual": [7, 9, 14, 17, 23, 24, 29, 160], "throw": 7, "notic": [7, 160, 161, 165], "miss": [7, 110, 111, 150, 165], "posit": [7, 10, 32, 95, 97, 99, 100, 101, 164], "anoth": [7, 162], "handl": [7, 12, 30, 63, 82, 112, 113, 160, 162, 165, 166], "def": [7, 8, 9, 12, 120, 124, 160, 161, 165, 166], "dictconfig": [7, 8, 10, 11, 12, 13, 143, 150], "arg": [7, 10, 100, 102, 105, 126, 141, 150], "tupl": [7, 10, 30, 42, 55, 63, 74, 82, 90, 97, 102, 104, 115, 116, 117, 118, 120, 126, 135, 144, 145, 150, 152], "kwarg": [7, 10, 102, 105, 126, 136, 140, 141, 142, 143, 147, 150, 161], "str": [7, 10, 11, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 58, 63, 67, 74, 77, 82, 87, 90, 93, 102, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 121, 122, 123, 125, 126, 129, 130, 132, 133, 134, 136, 138, 140, 141, 142, 143, 144, 145, 149, 150, 151, 152, 160, 161], "mean": [7, 95, 98, 100, 101, 139, 159, 160, 161, 163, 165], "pass": [7, 10, 20, 29, 30, 31, 33, 34, 37, 38, 40, 44, 45, 95, 96, 102, 107, 111, 113, 120, 123, 130, 131, 133, 136, 139, 142, 143, 147, 150, 159, 160, 161, 165, 166], "add": [7, 9, 29, 32, 34, 44, 63, 115, 123, 124, 126, 161, 162, 164, 165, 166], "d": [7, 20, 95, 97, 100, 159, 160, 165], "llama2_token": [7, 160, 162], "tmp": [7, 125, 160, 163, 164], "llama2token": [7, 67], "option": [7, 8, 14, 17, 18, 23, 24, 27, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 50, 51, 55, 59, 60, 63, 68, 69, 70, 74, 77, 78, 79, 82, 83, 84, 90, 91, 93, 95, 99, 100, 101, 102, 110, 111, 112, 115, 117, 118, 121, 122, 123, 128, 129, 130, 132, 134, 140, 143, 149, 150, 156, 157, 159, 161, 162], "modeltoken": [7, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 115, 160, 161], "bool": [7, 20, 25, 26, 29, 31, 32, 34, 35, 36, 37, 39, 40, 41, 42, 44, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 88, 89, 90, 91, 94, 102, 106, 110, 111, 112, 113, 115, 117, 120, 121, 122, 123, 131, 133, 136, 137, 139, 142, 147, 150, 151, 160, 166], "max_seq_len": [7, 10, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 63, 74, 82, 90, 95, 97, 99, 100, 115, 160, 161], "int": [7, 9, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 88, 89, 90, 91, 94, 95, 97, 98, 99, 100, 103, 106, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 128, 131, 135, 139, 140, 141, 142, 143, 144, 145, 147, 149, 150, 159, 160, 161, 165, 166], "512": [7, 35, 36, 161, 166], "instructdataset": [7, 35, 36, 39, 40, 41, 161], "alreadi": [7, 124, 136, 139, 156, 159, 161, 162, 165], "overwrit": [7, 123, 156], "duplic": [7, 8, 157, 159], "sometim": 7, "than": [7, 28, 42, 95, 97, 120, 123, 124, 151, 152, 160, 161, 162, 163, 164, 165, 166], "resolv": [7, 11, 163], "alpaca": [7, 14, 35, 36, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 161], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 140, 141, 142, 143], "disklogg": 7, "log_dir": [7, 140, 142, 143], "conveni": [7, 8, 159], "verifi": [7, 129, 130, 131, 160, 163, 165], "properli": [7, 110, 137, 159], "experi": [7, 143, 155, 157, 160, 164, 165], "wa": [7, 110, 160, 162, 164, 165, 166], "cp": [7, 156, 159, 160, 162, 163, 164], "7b_lora_single_devic": [7, 162, 163, 165, 166], "my_config": 7, "discuss": [7, 163, 165], "guidelin": 7, "while": [7, 8, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 96, 157, 162, 166], "mai": [7, 9, 131, 160, 161, 163, 165], "tempt": 7, "put": [7, 8, 163, 165], "much": [7, 162, 164, 165, 166], "give": [7, 161, 165], "maximum": [7, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 74, 95, 97, 99, 100, 116, 117, 118, 159], "flexibl": [7, 30, 161], "switch": 7, "encourag": [7, 63, 165], "clariti": 7, "significantli": 7, "easier": [7, 162, 163], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 123, 160, 163], "parent": [7, 159], "modul": [7, 10, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 124, 127, 131, 139, 146, 147, 149, 163, 165, 166], "__init__": [7, 8, 165, 166], "py": [7, 10, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 95, 97, 98, 99, 103, 104, 159, 162, 164], "guarante": 7, "stabil": [7, 157, 166], "underscor": 7, "_alpaca": 7, "collect": [7, 128, 163], "itself": 7, "via": [7, 9, 37, 40, 44, 106, 121, 165, 166], "k1": [7, 8], "v1": [7, 8, 45], "k2": [7, 8], "v2": [7, 8, 161], "lora_finetune_single_devic": [7, 159, 160, 162, 163, 164, 165, 166], "checkpoint": [7, 8, 102, 113, 121, 122, 123, 124, 125, 143, 147, 157, 159, 164, 165, 166], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 29, 30, 31, 32, 33, 34, 37, 40, 55, 63, 74, 82, 90, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 108, 109, 112, 113, 121, 122, 123, 124, 125, 126, 140, 141, 142, 143, 160, 161, 163, 165, 166], "assign": [7, 34], "nest": 7, "dot": 7, "notat": [7, 95, 99, 100], "certain": [7, 150, 160], "flag": [7, 8, 20, 31, 35, 36, 39, 41, 120, 123, 131, 159, 166], "built": [7, 9, 43, 156, 160, 163, 166], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 29], "pytorch": [7, 8, 63, 100, 102, 110, 120, 137, 142, 147, 149, 150, 155, 156, 157, 164, 165, 166], "llama3": [7, 29, 42, 74, 75, 76, 77, 78, 79, 80, 81, 124, 128, 131, 155, 159, 161], "8b_full": [7, 159, 161], "adamw": [7, 165], "lr": [7, 103], "2e": 7, "fuse": [7, 146], "nproc_per_nod": [7, 161, 164, 165], "full_finetune_distribut": [7, 159, 161, 162, 163], "core": [8, 157, 161, 163, 166], "i": [8, 19, 20, 21, 95, 100, 101, 102, 109, 125, 128, 161, 162, 164, 166], "structur": [8, 15, 16, 19, 21, 25, 26, 29, 37, 77, 93, 160, 161, 162], "new": [8, 38, 85, 97, 124, 140, 142, 160, 162, 163, 164, 165, 166], "user": [8, 15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 55, 63, 82, 90, 95, 115, 118, 160, 161, 163], "thought": [8, 157, 163, 166], "target": [8, 157], "pipelin": [8, 157], "llm": [8, 155, 157, 161, 162, 165], "eg": [8, 100, 121, 157], "meaning": [8, 157, 162], "featur": [8, 9, 156, 157, 162, 163], "fsdp": [8, 120, 125, 131, 139, 157, 163, 164], "activ": [8, 96, 133, 138, 147, 150, 157, 166], "gradient": [8, 139, 146, 150, 157, 162, 164, 165, 166], "accumul": [8, 146, 150, 157], "mix": [8, 159, 161, 162], "precis": [8, 102, 130, 157, 163, 166], "appli": [8, 29, 31, 33, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 98, 99, 100, 101, 110, 111, 147, 157, 166], "given": [8, 10, 14, 17, 18, 23, 24, 28, 106, 107, 128, 129, 130, 134, 139, 146, 151, 157, 165], "complex": 8, "becom": [8, 156, 161], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 100, 124, 159, 161], "methodolog": 8, "reason": [8, 128, 162], "possibl": [8, 29, 32, 37, 116, 117, 159, 161], "trade": 8, "off": [8, 63, 82, 162], "memori": [8, 30, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 102, 110, 131, 133, 138, 139, 150, 155, 157, 162, 163, 164], "vs": [8, 163], "qualiti": [8, 162, 165], "believ": 8, "best": [8, 117, 160], "suit": [8, 163], "b": [8, 95, 97, 99, 100, 101, 106, 139, 143, 165, 166], "fit": [8, 29, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 117, 118, 161], "solut": 8, "result": [8, 55, 63, 82, 90, 115, 150, 162, 164, 165, 166], "meant": [8, 102, 125], "depend": [8, 9, 14, 121, 150, 159, 161, 162, 165, 166], "level": [8, 127, 132, 139, 157, 166], "expertis": 8, "routin": 8, "yourself": [8, 159, 164, 165], "exist": [8, 156, 159, 162, 163, 164, 166], "ad": [8, 82, 112, 123, 124, 160, 165, 166], "ones": 8, "modular": [8, 157], "build": [8, 37, 40, 44, 157, 164, 165], "block": [8, 32, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 110, 111, 157], "wandb": [8, 9, 143, 163], "log": [8, 11, 104, 132, 133, 138, 140, 141, 142, 143, 162, 163, 164, 166], "fulli": [8, 30], "nativ": [8, 155, 157, 165, 166], "correct": [8, 17, 39, 98, 99, 100, 129, 157, 160, 161], "numer": [8, 157], "pariti": [8, 157], "verif": 8, "extens": [8, 123, 157], "comparison": [8, 165, 166], "benchmark": [8, 149, 157, 162, 164, 165], "limit": [8, 117, 118, 125, 161], "hidden": [8, 96], "behind": 8, "100": [8, 31, 35, 36, 39, 41, 42, 128, 144, 145, 165, 166], "prefer": [8, 23, 43, 104, 145, 157, 159, 161], "over": [8, 103, 126, 157, 162, 164, 165, 166], "unnecessari": 8, "abstract": [8, 15, 18, 157, 163, 166], "No": [8, 123, 157], "inherit": [8, 126, 157, 161], "go": [8, 19, 21, 55, 63, 82, 90, 115, 157, 161, 162, 163, 166], "upon": [8, 30, 164], "figur": [8, 165, 166], "spectrum": 8, "decid": 8, "interact": [8, 155, 163], "start": [8, 9, 30, 115, 124, 156, 157, 160, 161, 162, 163], "paradigm": 8, "consist": [8, 45, 163], "configur": [8, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 74, 90, 101, 157, 160, 163, 164, 165, 166], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 90, 91, 93, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 134, 136, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155, 157, 159, 160, 161, 162, 163, 164, 165, 166], "overrid": [8, 11, 12, 159, 162, 163, 164, 166], "togeth": [8, 32, 143, 163, 165], "valid": [8, 28, 110, 111, 152, 156, 162, 163], "environ": [8, 129, 137, 156, 159, 162, 163], "logic": [8, 124, 157, 163, 165], "api": [8, 9, 25, 52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 110, 159, 160, 162, 163, 164, 166], "closer": [8, 165], "monolith": [8, 157], "trainer": [8, 104], "A": [8, 9, 25, 26, 30, 32, 55, 63, 82, 90, 95, 100, 101, 102, 104, 106, 110, 112, 113, 115, 117, 120, 125, 126, 133, 134, 138, 139, 144, 145, 154, 155, 158, 159, 160, 162, 165, 166], "wrapper": [8, 112, 113, 125, 127, 159, 165], "around": [8, 29, 63, 82, 112, 113, 133, 159, 160, 162, 165, 166], "extern": [8, 161], "primarili": [8, 30, 165], "eleutherai": [8, 157, 165], "har": [8, 157, 165], "control": [8, 31, 35, 36, 39, 41, 107, 149, 162], "multi": [8, 29, 95, 110, 164], "stage": 8, "distil": 8, "oper": [8, 30, 107, 149], "turn": [8, 20, 28, 29, 160], "dataload": [8, 32, 35, 36, 39, 41], "applic": [8, 95, 121, 122, 143], "clean": [8, 9, 35], "after": [8, 90, 95, 97, 98, 100, 101, 110, 139, 140, 141, 142, 143, 160, 166], "process": [8, 9, 102, 135, 136, 149, 161, 163, 166], "group": [8, 95, 135, 136, 140, 141, 142, 143, 159, 164], "init_process_group": [8, 136], "backend": [8, 159], "gloo": 8, "els": [8, 126, 143, 157, 166], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 30, 123, 126, 131, 150, 161, 163, 164, 165], "stuff": 8, "carri": 8, "relev": [8, 159, 162, 165], "interfac": [8, 15, 18, 30, 105], "metric": [8, 163], "logger": [8, 132, 138, 140, 141, 142, 143, 163], "self": [8, 9, 32, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 95, 100, 101, 105, 110, 111, 121, 124, 125, 161, 165, 166], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 120, 131, 139, 147, 160], "_model": [8, 125], "_setup_model": 8, "_token": [8, 161], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 125, 127, 146, 150, 166], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 104, 149, 159, 164], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 32], "batch": [8, 32, 35, 36, 39, 41, 95, 97, 99, 100, 104, 144, 145, 150, 157, 161, 163, 164, 165], "enumer": 8, "_autocast": 8, "logit": [8, 128], "label": [8, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 104, 144, 145], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 140, 141, 142, 143], "step": [8, 32, 100, 103, 127, 140, 141, 142, 143, 146, 150, 155, 162, 165, 166], "learn": [8, 30, 103, 157, 160, 161, 163, 164, 165, 166], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 23, 24, 27, 28, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 55, 63, 74, 77, 82, 90, 93, 95, 97, 99, 100, 101, 107, 109, 110, 111, 112, 115, 118, 121, 122, 123, 124, 128, 129, 130, 132, 134, 138, 140, 141, 142, 143, 146, 147, 148, 149, 150, 152, 160, 161, 162], "fullfinetunerecip": 8, "wandblogg": [9, 165, 166], "workspac": 9, "seen": [9, 165, 166], "screenshot": 9, "below": [9, 14, 99, 120, 161, 164, 165, 166], "packag": [9, 142, 143, 156], "pip": [9, 142, 143, 156, 162, 164], "Then": [9, 107, 163], "login": [9, 143, 159, 162], "project": [9, 49, 50, 51, 68, 69, 70, 78, 79, 83, 84, 91, 95, 96, 110, 111, 131, 143, 155, 165, 166], "grab": [9, 164], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 137, 160], "exit": [9, 156, 159], "resourc": [9, 140, 141, 142, 143], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 29, 31, 32, 33, 34, 40, 42, 44, 95, 99, 100, 101, 128, 160, 162], "desir": [9, 29, 118, 148, 160], "suggest": 9, "approach": [9, 30, 161], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 162], "_output_dir": [9, 121, 122, 123], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 150], "type": [9, 10, 12, 20, 25, 26, 27, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 102, 104, 106, 108, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 139, 145, 147, 148, 149, 150, 151, 161, 162, 165, 166], "descript": [9, 37, 42, 159], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 25, 26, 29, 32, 35, 36, 39, 41, 138, 161], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 85, 86, 88, 89, 91, 92, 93, 94, 95, 98, 99, 103, 104, 110, 120, 121, 122, 126, 132, 137, 142, 143, 147, 149, 156, 161, 162], "com": [10, 49, 50, 51, 59, 60, 63, 68, 69, 70, 74, 78, 79, 91, 95, 98, 99, 103, 104, 110, 156], "facebookresearch": [10, 98, 99], "blob": [10, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91, 93, 95, 98, 99, 103, 104], "main": [10, 12, 63, 93, 95, 98, 99, 156, 162, 164], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 100], "32": [10, 164, 165, 166], "num_head": [10, 95, 97, 99, 100], "num_kv_head": [10, 95, 97], "vocab_s": 10, "must": [10, 30, 105, 126, 166], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 82, 83, 84, 85, 86, 87, 90, 91, 92, 93, 95, 97, 98, 99, 100, 101, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 144, 145, 148, 149, 150, 151, 160, 161, 165, 166], "nn": [10, 95, 96, 97, 100, 101, 102, 105, 107, 108, 109, 120, 127, 139, 146, 147, 152, 165, 166], "parsed_yaml": 10, "embed_dim": [10, 95, 99, 101, 165], "valueerror": [10, 21, 25, 28, 37, 42, 90, 95, 97, 100, 104, 121, 122, 123, 130, 133, 149, 152], "recipe_nam": 11, "rank": [11, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 106, 135, 137, 149, 163, 165, 166], "zero": [11, 97, 98, 162, 164], "displai": 11, "callabl": [12, 29, 31, 33, 100, 107, 120, 128, 131, 134, 139, 147], "With": [12, 162, 165, 166], "my_recip": 12, "foo": 12, "bar": [12, 157, 163], "instanti": [13, 46, 47, 48, 49, 50, 51, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 91, 92, 93, 125], "configerror": 13, "cannot": [13, 123, 164], "data": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 63, 133, 140, 141, 142, 143, 161, 162, 166], "templat": [14, 17, 18, 23, 24, 29, 31, 33, 35, 36, 37, 39, 40, 41, 42, 63], "style": [14, 32, 35, 36, 37, 42, 166], "slightli": 14, "describ": [14, 63, 74, 147, 161], "task": [14, 24, 30, 38, 160, 161, 162, 164, 165, 166], "further": [14, 159, 161, 165, 166], "context": [14, 16, 92, 107, 148, 150, 161], "respons": [14, 16, 55, 63, 82, 90, 104, 115, 161, 162, 163, 164], "appropri": [14, 16, 19, 20, 21, 30, 103, 121, 161, 166], "request": [14, 130, 161, 162], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 161], "map": [14, 17, 18, 23, 24, 25, 26, 29, 30, 31, 32, 33, 40, 74, 90, 109, 113, 114, 121, 125, 127, 140, 141, 142, 143, 146, 150, 160, 161, 162, 165], "column_map": [14, 17, 18, 23, 24, 29, 31, 33, 40, 161], "placehold": [14, 17, 18, 23, 24, 29, 31, 33, 40, 161], "column": [14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 44, 95, 100, 101, 160, 161], "ident": [14, 17, 18, 21, 23, 24, 31, 32, 33, 40, 162], "poem": 14, "n": [14, 23, 55, 63, 82, 90, 95, 115, 117, 154, 158, 160, 161], "nwrite": 14, "long": [14, 32, 113, 160, 165], "where": [14, 17, 23, 24, 29, 30, 35, 36, 39, 41, 63, 82, 95, 100, 106, 112, 117, 131, 139, 145, 161], "me": 14, "tag": [15, 16, 19, 21, 29, 140, 141, 142, 143, 160], "system": [15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 55, 63, 82, 90, 115, 160, 161], "assist": [15, 16, 19, 20, 22, 25, 26, 28, 29, 55, 63, 82, 90, 93, 115, 128, 160, 161], "role": [15, 20, 25, 26, 29, 55, 63, 82, 90, 115, 160, 161], "prepend": [15, 63, 74, 82, 112], "append": [15, 74, 82, 90, 112, 156], "accord": [15, 21, 160], "openai": [16, 25, 37, 161], "markup": 16, "languag": [16, 106, 128, 165], "It": [16, 20, 21, 159, 160, 161, 166], "im_start": 16, "im_end": 16, "goe": [16, 107], "grammar": [17, 39, 161], "english": 17, "sentenc": [17, 32, 82], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 165], "lazi": 17, "dog": 17, "alwai": [18, 126], "human": [19, 26, 160], "taken": [19, 165, 166], "inst": [19, 21, 29, 63, 160, 161], "sy": [19, 63, 160, 161], "respect": [19, 30, 109, 117, 150, 160, 161], "honest": [19, 160, 161], "am": [19, 21, 160, 161, 162, 164], "pari": [19, 21, 24, 161], "capit": [19, 21, 23, 24, 161], "franc": [19, 21, 23, 24, 161], "known": [19, 21, 63, 82, 134, 161], "its": [19, 21, 32, 95, 99, 100, 101, 146, 149, 159, 161, 162, 164, 165], "stun": [19, 21, 161], "liter": [20, 22, 49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 110, 111], "ipython": [20, 22], "union": [20, 111, 140, 141, 142, 143, 147, 149], "mask": [20, 31, 32, 35, 36, 39, 41, 55, 63, 74, 82, 90, 95, 100, 101, 115, 160, 161], "eot": [20, 74], "repres": [20, 117, 145, 160], "individu": [20, 32, 133, 143, 147, 160, 161], "interleav": 20, "tokenize_messag": [20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 55, 63, 74, 82, 90, 115, 160, 161], "attach": 20, "special": [20, 29, 63, 74, 77, 82, 90, 93, 113, 114, 115, 125, 161], "variabl": [20, 29, 30, 31, 33, 40, 137, 166], "writer": 20, "multimod": 20, "dictionari": [20, 32, 133, 138, 140, 141, 142, 143, 145, 162], "hello": [20, 24, 55, 63, 74, 82, 90, 112, 113, 160, 162, 164], "world": [20, 55, 63, 74, 82, 90, 112, 113, 135, 137, 162], "whether": [20, 25, 26, 29, 31, 34, 35, 36, 37, 39, 40, 41, 42, 44, 49, 50, 51, 59, 60, 68, 69, 70, 74, 78, 79, 82, 83, 84, 90, 91, 102, 106, 110, 111, 112, 113, 120, 130, 133, 160, 161], "calcul": [20, 95, 100, 117, 164], "correspond": [20, 105, 108, 130, 145, 163, 164], "consecut": [20, 28], "e": [20, 95, 102, 105, 109, 117, 121, 125, 133, 150, 156, 162, 164, 165, 166], "properti": [20, 126, 165], "contains_media": 20, "non": [20, 111], "from_dict": [20, 160], "construct": [20, 165], "text_cont": 20, "mistral": [21, 29, 42, 82, 83, 84, 85, 86, 87, 88, 89, 124, 159, 160, 162, 163], "llama2chatformat": [21, 63, 160, 161], "alia": [22, 120], "similar": [23, 38, 43, 45, 110, 161, 162, 164, 165, 166], "stackexchangedpair": 23, "question": [23, 160, 161, 162, 164], "answer": [23, 160, 162, 164], "nanswer": 23, "summar": [24, 41, 160, 161], "dialogu": [24, 41, 160], "summari": [24, 41, 133, 161], "dialog": 24, "did": [24, 162, 164, 166], "know": [24, 160, 161, 162, 164, 165], "adher": [25, 26], "could": [25, 165], "remain": [25, 26, 103, 165], "unmask": [25, 26], "sharegpt": [26, 37], "gpt": [26, 95, 162], "eos_id": [27, 113, 115], "length": [27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 40, 42, 44, 45, 55, 63, 74, 82, 90, 92, 95, 97, 99, 100, 113, 115, 122, 144, 145], "last": [27, 32, 103, 161], "replac": [27, 31, 35, 36, 39, 41, 102, 165], "forth": [28, 161], "come": [28, 105, 165], "empti": [28, 159], "shorter": 28, "min": [28, 117, 165], "invalid": 28, "convert_to_messag": [29, 160], "chat_format": [29, 37, 42, 160, 161], "chatformat": [29, 37, 161], "load_dataset_kwarg": [29, 31, 33, 34, 37, 38, 40, 44, 45], "multiturn": [29, 160], "prepar": [29, 160], "truncat": [29, 31, 32, 33, 34, 38, 40, 42, 44, 45, 55, 63, 74, 82, 90, 113, 115, 161], "anyth": [29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "load_dataset": [29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 160, 161], "huggingfac": [29, 31, 33, 34, 37, 38, 40, 44, 45, 86, 92, 93, 103, 104, 121, 122, 159, 162], "co": [29, 31, 33, 34, 37, 38, 40, 44, 45, 86, 92, 93, 121, 122, 162], "doc": [29, 31, 33, 34, 37, 38, 40, 44, 45, 63, 74, 120, 126, 132, 137, 142, 143, 149, 159, 162], "en": [29, 31, 33, 34, 37, 38, 40, 44, 45], "package_refer": [29, 31, 33, 34, 37, 38, 40, 44, 45], "loading_method": [29, 31, 33, 34, 37, 38, 40, 44, 45], "extra": [29, 156, 165, 166], "still": [29, 126, 165, 166], "unless": 29, "check": [29, 37, 100, 110, 130, 137, 151, 155, 160, 162, 163, 165], "concaten": [30, 55, 63, 82, 90, 115, 145], "sub": [30, 142], "unifi": [30, 86], "were": [30, 107, 160, 163], "simplifi": [30, 159, 165], "simultan": 30, "intern": [30, 126], "aggreg": 30, "transpar": 30, "index": [30, 32, 95, 99, 100, 101, 103, 144, 145, 156, 160, 162], "howev": [30, 93, 156], "constitu": 30, "might": [30, 159, 162], "larg": [30, 106, 150, 159, 166], "comput": [30, 95, 96, 99, 100, 104, 116, 133, 149, 162, 166], "cumul": 30, "maintain": [30, 166], "indic": [30, 32, 95, 99, 100, 101, 120, 137, 160], "deleg": 30, "retriev": [30, 131], "lead": [30, 82, 112], "high": [30, 157, 165], "scale": [30, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 106, 117, 128, 165, 166], "consid": 30, "strategi": 30, "stream": [30, 132], "demand": 30, "deriv": [30, 96, 100, 101], "_dataset": 30, "_len": 30, "total": [30, 103, 135, 154, 158, 162, 164, 165], "combin": [30, 116], "_index": 30, "lookup": 30, "dataset1": 30, "mycustomdataset": 30, "params1": 30, "dataset2": 30, "params2": 30, "concat_dataset": 30, "data_point": 30, "1500": 30, "element": [30, 162], "focus": [30, 163], "enhanc": [30, 166], "divers": 30, "machin": [30, 129, 159, 162], "instructtempl": [31, 33, 161], "contribut": [31, 35, 36, 39, 41], "disabl": [31, 33, 34, 38, 40, 44, 45, 107, 149], "recommend": [31, 33, 34, 35, 36, 38, 40, 44, 45, 142, 160, 162, 166], "highest": [31, 33, 34, 35, 36, 38, 40, 44, 45], "sequenc": [31, 32, 33, 34, 35, 36, 38, 40, 42, 44, 45, 55, 63, 74, 82, 90, 95, 97, 99, 100, 113, 115, 144, 145, 160], "ds": [32, 42], "padding_idx": [32, 144, 145], "max_pack": 32, "split_across_pack": 32, "greedi": 32, "pack": [32, 35, 36, 37, 39, 40, 41, 42, 44, 95, 99, 100, 101], "done": [32, 110, 130, 139, 165, 166], "outsid": [32, 149, 150, 162, 164, 165], "sampler": [32, 163], "part": [32, 160, 166], "buffer": 32, "enough": [32, 160], "attent": [32, 49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 92, 95, 97, 99, 100, 101, 110, 111, 164, 165, 166], "lower": [32, 165], "triangular": 32, "cross": 32, "attend": [32, 95, 100, 101], "rel": [32, 95, 99, 100, 101, 133, 165], "pad": [32, 117, 118, 128, 144, 145, 161], "max": [32, 42, 55, 63, 82, 90, 100, 103, 113, 115, 159, 165], "wise": 32, "collat": [32, 144, 161], "made": [32, 37, 40, 44, 99, 162], "smaller": [32, 162, 164, 165, 166], "jam": 32, "vari": 32, "s1": [32, 63, 82, 112], "s2": [32, 63, 82, 112], "s3": 32, "s4": 32, "contamin": 32, "input_po": [32, 95, 97, 99, 100, 101], "matrix": 32, "causal": [32, 95, 100, 101], "continu": [32, 161], "increment": 32, "move": [32, 100], "entir": [32, 139, 160, 166], "avoid": [32, 98, 102, 149, 159, 166], "add_eo": [34, 44, 55, 63, 74, 82, 90, 112, 113, 160], "freeform": [34, 44], "unstructur": [34, 45], "corpu": [34, 38, 45], "local": [34, 44, 77, 93, 143, 149, 156, 159, 160, 162, 163], "tabular": [34, 44], "eo": [34, 44, 82, 90, 93, 112, 115, 160, 161], "yahma": [35, 40], "codebas": [35, 36, 39, 41, 162], "prior": [35, 36, 37, 39, 40, 41, 42, 44], "alpaca_d": [35, 36], "batch_siz": [35, 36, 39, 41, 95, 97, 100, 101, 104, 162], "tatsu": 36, "lab": 36, "conversation_styl": [37, 161], "chatdataset": [37, 42, 160, 161], "friendli": [37, 40, 44, 128, 160], "huggingfaceh4": 37, "no_robot": 37, "chatmlformat": 37, "2096": [37, 40, 44], "accomplish": [37, 40, 44], "packeddataset": [37, 40, 44, 161], "ccdv": 38, "cnn_dailymail": 38, "textcompletiondataset": [38, 44, 45, 161], "cnn": 38, "dailymail": 38, "articl": [38, 45], "extract": [38, 114], "highlight": [38, 166], "liweili": 39, "c4_200m": 39, "variant": [39, 41], "mirror": [39, 41], "llama_recip": [39, 41], "grammar_d": 39, "alpaca_clean": 40, "alpacainstructtempl": [40, 161], "samsum": [41, 161], "samsum_d": 41, "open": [42, 56, 57, 161, 162], "orca": 42, "slimorca": 42, "dedup": 42, "1024": [42, 43, 161], "prescrib": 42, "least": [42, 164, 165], "though": [42, 160], "10": [42, 144, 145, 162, 164, 166], "351": 42, "82": [42, 162], "391": 42, "221": 42, "220": 42, "193": 42, "12": [42, 145, 156], "471": 42, "lvwerra": [43, 161], "stack": [43, 150, 161], "exchang": [43, 161], "preferencedataset": [43, 161], "stackexchangepair": 43, "textdataset": 44, "omit": [44, 165], "allenai": [44, 161], "c4": [44, 161], "data_dir": [44, 161], "realnewslik": [44, 161], "wikitext": 45, "subset": [45, 108], "103": [45, 162], "raw": 45, "wikipedia": 45, "page": [45, 156, 157, 159, 163, 164], "code_llama2": [46, 47, 48, 49, 50, 51, 52, 53, 54, 159], "transformerdecod": [46, 47, 48, 49, 50, 51, 52, 53, 54, 64, 65, 66, 68, 69, 70, 71, 72, 73, 75, 76, 78, 79, 80, 81, 83, 84, 85, 86, 88, 89, 91, 92, 94, 128, 165], "w": [46, 47, 48, 56, 57, 64, 65, 66, 75, 76, 85, 86, 118, 142, 143, 160, 162, 165, 166], "arxiv": [46, 47, 48, 52, 53, 54, 61, 62, 64, 65, 66, 71, 72, 73, 80, 81, 88, 89, 94, 95, 98, 99, 104], "org": [46, 47, 48, 52, 53, 54, 61, 62, 63, 64, 65, 66, 71, 72, 73, 80, 81, 88, 89, 94, 95, 98, 99, 104, 120, 126, 132, 137, 142, 147, 149, 156], "pdf": [46, 47, 48, 95, 98], "2308": [46, 47, 48], "12950": [46, 47, 48], "70b": [47, 50, 53, 65, 69, 72, 75, 78, 80, 164], "lora_attn_modul": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 110, 111, 165, 166], "q_proj": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 110, 111, 165, 166], "k_proj": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 110, 111, 165, 166], "v_proj": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 110, 111, 165, 166], "output_proj": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 110, 111, 165, 166], "apply_lora_to_mlp": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 110, 111, 165], "apply_lora_to_output": [49, 50, 51, 52, 53, 54, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 110, 111, 165], "lora_rank": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 165], "lora_alpha": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 165], "float": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 95, 98, 103, 104, 106, 128, 133, 138, 140, 141, 142, 143, 165, 166], "16": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 145, 165, 166], "lora_dropout": [49, 50, 51, 52, 53, 54, 68, 69, 70, 71, 72, 73], "05": [49, 50, 51, 52, 53, 54, 68, 69, 70, 71, 72, 73], "quantize_bas": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 106, 166], "lora": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 106, 107, 110, 111, 121, 139, 155, 157, 160, 163, 164], "code_llama2_13b": 49, "tloen": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91], "8bb8579e403dc78e37fe81ffbb253c413007323f": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91], "l41": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91], "l43": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 91], "linear": [49, 50, 51, 52, 53, 54, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 83, 84, 88, 89, 91, 94, 100, 105, 106, 110, 111, 165, 166], "mlp": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 100, 101, 110, 111, 164, 165], "final": [49, 50, 51, 68, 69, 70, 78, 79, 83, 84, 91, 96, 100, 107, 110, 111, 162, 164, 165, 166], "low": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 106, 162, 165, 166], "approxim": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 106, 165], "factor": [49, 50, 51, 59, 60, 68, 69, 70, 78, 79, 83, 84, 91, 106, 117, 162], "code_llama2_70b": 50, "code_llama2_7b": 51, "qlora": [52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 102, 155, 157, 164, 165], "per": [52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 97, 102, 116, 159, 164, 166], "paper": [52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 165, 166], "ab": [52, 53, 54, 61, 62, 64, 65, 66, 71, 72, 73, 80, 81, 88, 89, 94, 99, 104], "2305": [52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94, 95, 104], "14314": [52, 53, 54, 61, 62, 71, 72, 73, 80, 81, 88, 89, 94], "lora_code_llama2_13b": 52, "lora_code_llama2_70b": 53, "lora_code_llama2_7b": 54, "gemma": [55, 56, 57, 58, 59, 60, 61, 62, 124], "sentencepiec": [55, 63, 82, 90, 112, 164], "pretrain": [55, 63, 74, 82, 90, 112, 113, 159, 160, 163, 165, 166], "spm_model": [55, 63, 82, 90, 112, 160], "tokenized_text": [55, 63, 74, 82, 90, 112, 113], "add_bo": [55, 63, 74, 82, 90, 112, 113, 160], "31587": [55, 63, 74, 82, 90, 112, 113], "29644": [55, 63, 74, 82, 90, 112, 113], "102": [55, 63, 74, 82, 90, 112, 113], "tokenizer_path": [55, 63, 82, 90], "separ": [55, 63, 82, 90, 115, 121, 160, 163, 164, 165, 166], "concat": [55, 63, 82, 90, 115], "1788": [55, 63, 82, 90, 115], "2643": [55, 63, 82, 90, 115], "13": [55, 63, 82, 90, 115, 145, 162, 164, 166], "1792": [55, 63, 82, 90, 115], "9508": [55, 63, 82, 90, 115], "465": [55, 63, 82, 90, 115], "22137": [55, 63, 82, 90, 115], "2933": [55, 63, 82, 90, 115], "join": [55, 63, 82, 90, 115], "attribut": [55, 63, 82, 90, 107, 115, 127], "gemmatransformerdecod": [56, 57, 59, 60, 61, 62], "blog": [56, 57], "technolog": [56, 57], "develop": [56, 57, 166], "gemmatoken": 58, "gemma_2b": 59, "gemma_7b": 60, "lora_gemma_2b": 61, "lora_gemma_7b": 62, "card": [63, 74], "regist": [63, 74, 77, 90, 93, 96, 102, 146, 166], "uniqu": [63, 124], "strongli": 63, "beforehand": 63, "html": [63, 120, 126, 132, 137, 142, 147, 149, 155], "problem": [63, 82], "due": [63, 82, 112, 165, 166], "whitespac": [63, 82, 112], "slice": [63, 82], "2307": [64, 65, 66], "09288": [64, 65, 66], "llama2_70b": 69, "llama2_7b": [70, 165], "lora_llama2_13b": 71, "lora_llama2_70b": 72, "lora_llama2_7b": [73, 165], "special_token": [74, 90, 113], "tiktoken": [74, 113, 164], "left": [74, 90, 165], "canon": [74, 77, 90, 93], "tt_model": [74, 113], "token_id": [74, 82, 113], "truncate_at_eo": [74, 113], "tokenize_head": 74, "tokenize_end": 74, "header": [74, 160], "eom": 74, "special_tokens_path": [77, 93], "llama3token": [77, 160], "similarli": [77, 93, 150, 161], "llama3_70b": 78, "llama3_8b": [79, 128, 164], "lora_llama3_70b": 80, "lora_llama3_8b": 81, "trim_leading_whitespac": [82, 112], "unbatch": [82, 112], "bo": [82, 93, 112, 115, 160, 161], "trim": [82, 112], "classifi": [84, 86, 89, 161], "announc": 85, "ray2333": 86, "reward": [86, 104], "feedback": 86, "transformerclassifi": 86, "mistraltoken": [87, 160], "lora_mistral_7b": 88, "lora_mistral_classifier_7b": 89, "phi3": [90, 91, 92, 93, 94, 124, 159], "ignore_system_prompt": 90, "phi3_mini": [91, 124], "ref": [92, 93, 143], "phi": [92, 93, 124], "128k": 92, "nor": 92, "slide": 92, "window": [92, 161], "phi3minitoken": 93, "tokenizer_config": 93, "spm": 93, "lm": 93, "unk": 93, "augment": [93, 166], "endoftext": 93, "phi3minisentencepiecebasetoken": 93, "lora_phi3_mini": 94, "head_dim": [95, 97, 100], "pos_embed": [95, 165], "kv_cach": 95, "kvcach": [95, 100], "attn_dropout": [95, 100], "head": [95, 97, 99, 100, 124, 164], "queri": [95, 97, 100, 101, 164], "gqa": 95, "introduc": [95, 98, 106, 160, 161, 165, 166], "13245v1": 95, "version": [95, 128, 151, 156, 160, 164, 166], "multihead": 95, "mha": [95, 100], "extrem": 95, "share": [95, 161, 162], "mqa": 95, "credit": 95, "document": [95, 120, 131, 139, 159, 161], "lightn": 95, "lit": 95, "lit_gpt": 95, "v": [95, 100, 165], "k": [95, 165], "q": [95, 165], "n_kv_head": 95, "dimens": [95, 97, 99, 100, 106, 164, 165, 166], "g": [95, 105, 117, 121, 133, 150, 164, 165, 166], "rotarypositionalembed": [95, 165], "cach": [95, 97, 99, 100, 156, 159], "rope": [95, 99], "dropout": [95, 106, 165, 166], "onto": 95, "scaled_dot_product_attent": 95, "seq_length": [95, 101, 128], "boolean": [95, 100, 101, 120], "softmax": [95, 100, 101], "row": [95, 100, 101, 117, 160], "j": [95, 100, 101], "seq_len": 95, "bigger": 95, "n_h": [95, 99], "num": [95, 99], "n_kv": 95, "kv": [95, 97, 100], "emb": [95, 100], "h_d": [95, 99], "gate_proj": 96, "down_proj": 96, "up_proj": 96, "silu": 96, "feed": [96, 101], "network": [96, 107, 165, 166], "fed": [96, 160], "multipli": 96, "subclass": [96, 126], "although": [96, 165], "afterward": 96, "former": 96, "hook": [96, 102, 146, 166], "latter": 96, "standalon": 97, "past": 97, "becaus": [97, 100, 123, 159, 160, 162, 164], "expand": 97, "dpython": [97, 100, 102], "reset": [97, 100, 133], "k_val": 97, "v_val": 97, "h": [97, 118, 156, 159], "longer": [97, 161], "ep": 98, "1e": 98, "06": [98, 165], "root": [98, 142, 143], "squar": 98, "1910": 98, "07467": 98, "verfic": [98, 99], "small": [98, 162], "divis": [98, 119], "10000": 99, "rotari": [99, 164], "propos": 99, "2104": 99, "09864": 99, "l450": 99, "upto": 99, "init": [99, 133, 143, 166], "exceed": 99, "freq": 99, "recomput": 99, "geometr": 99, "progress": [99, 163], "rotat": 99, "angl": 99, "todo": 99, "effici": [99, 110, 131, 155, 157, 162, 163, 165], "transformerdecoderlay": 100, "norm": [100, 101], "space": 100, "belong": [100, 127], "reduc": [100, 157, 161, 165, 166], "statement": 100, "improv": [100, 113, 131, 162, 164, 165], "readabl": [100, 162], "At": 100, "arang": 100, "prompt_length": 100, "causal_mask": 100, "m_": 100, "seq": 100, "reset_cach": 100, "setup_cach": 100, "attn": [101, 165, 166], "causalselfattent": [101, 165], "sa_norm": 101, "mlp_norm": 101, "ff": 101, "common_util": 102, "bfloat16": [102, 148, 162, 163, 164, 165], "offload_to_cpu": 102, "nf4": [102, 166], "restor": 102, "higher": [102, 164, 166], "offload": [102, 166], "increas": [102, 103, 164, 165], "peak": [102, 133, 138, 162, 164, 165, 166], "gpu": [102, 159, 162, 163, 164, 165, 166], "_register_state_dict_hook": 102, "m": [102, 128, 160], "mymodul": 102, "_after_": 102, "nf4tensor": [102, 166], "unquant": [102, 162, 166], "unus": 102, "num_warmup_step": 103, "num_training_step": 103, "num_cycl": [103, 150], "last_epoch": 103, "lambdalr": 103, "rate": [103, 157, 163], "schedul": [103, 150, 163], "linearli": 103, "decreas": [103, 161, 165, 166], "cosin": 103, "v4": 103, "23": [103, 164], "src": 103, "l104": 103, "warmup": [103, 150], "phase": 103, "wave": 103, "half": 103, "lr_schedul": 103, "beta": 104, "label_smooth": 104, "loss_typ": 104, "sigmoid": 104, "dpo": [104, 107, 145], "18290": 104, "trl": 104, "librari": [104, 126, 130, 132, 149, 155, 157, 159, 161, 166], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 104, "dpo_train": 104, "l844": 104, "temperatur": [104, 128, 162], "uncertainti": 104, "hing": 104, "ipo": 104, "kto_pair": 104, "policy_chosen_logp": 104, "policy_rejected_logp": 104, "reference_chosen_logp": 104, "reference_rejected_logp": 104, "polici": [104, 107, 120, 131, 139, 147], "probabl": [104, 106, 128, 162], "chosen": [104, 150, 161], "reject": [104, 161], "chosen_reward": 104, "rejected_reward": 104, "unknown": 104, "peft": [105, 106, 107, 108, 109, 110, 111, 121, 165, 166], "protocol": 105, "adapter_param": [105, 106, 107, 108, 109], "proj": 105, "in_dim": [105, 106, 165, 166], "out_dim": [105, 106, 165, 166], "bia": [105, 106, 165, 166], "loralinear": [105, 165, 166], "alpha": [106, 165, 166], "use_bia": 106, "perturb": 106, "decomposit": [106, 165], "matric": [106, 139, 165, 166], "trainabl": [106, 109, 139, 165, 166], "mapsto": 106, "w_0x": 106, "r": [106, 165], "bax": 106, "lora_a": [106, 165, 166], "lora_b": [106, 165, 166], "temporarili": 107, "neural": [107, 165, 166], "treat": [107, 126, 160], "caller": 107, "whose": [107, 146], "yield": 107, "get_adapter_param": [109, 165], "base_miss": 110, "base_unexpect": 110, "lora_miss": 110, "lora_unexpect": 110, "validate_state_dict_for_lora": [110, 165], "unlik": [110, 162, 164], "reli": [110, 115], "unexpect": 110, "strict": [110, 165], "pull": [110, 159], "120600": 110, "assertionerror": [110, 111, 145], "nonempti": 110, "full_model_state_dict_kei": 111, "lora_state_dict_kei": 111, "base_model_state_dict_kei": 111, "confirm": [111, 156], "determin": [111, 117], "lora_modul": 111, "complement": 111, "disjoint": 111, "overlap": 111, "light": 112, "sentencepieceprocessor": 112, "addition": [112, 113, 149, 161, 165], "prefix": 112, "bos_id": [113, 115], "lightweight": [113, 160], "break": 113, "substr": 113, "repetit": 113, "speed": [113, 150, 164, 166], "identif": 113, "regex": 113, "chunk": 113, "present": [113, 123], "absent": 113, "tokenizer_json_path": 114, "heavili": 115, "max_num_til": 116, "tile_s": [116, 119], "resolut": [116, 117, 118], "divid": [116, 119], "tile": [116, 119], "1x1": 116, "1x2": 116, "2x1": 116, "side": [116, 117, 118], "height": [116, 117, 118], "width": [116, 117, 118], "224": [116, 117, 118], "896": 116, "448": [116, 117, 118], "672": [116, 117], "possible_resolut": 117, "resize_to_max_canva": 117, "canva": 117, "resiz": [117, 118], "distort": [117, 118], "select": 117, "smallest": 117, "upscal": [117, 118], "2x": 117, "5x": 117, "canvas": 117, "satisfi": [117, 162], "condit": [117, 128, 137, 159, 161], "pick": 117, "lowest": [117, 165], "area": [117, 162], "minim": [117, 161, 163, 165, 166], "downscal": [117, 118], "rand": [117, 118, 119], "200": [117, 119, 166], "300": [117, 118, 119, 162], "scale_height": 117, "1200": 117, "3600": 117, "2400": 117, "scale_width": 117, "7467": 117, "4933": 117, "scaling_factor": 117, "upscaling_opt": 117, "selected_scal": 117, "150528": 117, "100352": 117, "optimal_canva": 117, "target_s": 118, "resampl": 118, "interpolationmod": 118, "max_upscaling_s": 118, "exce": 118, "image_s": 118, "torchvis": 118, "nearest": 118, "nearest_exact": 118, "bilinear": 118, "bicub": 118, "800": 118, "1194": 118, "1344": 118, "stai": 118, "600": [118, 119], "500": [118, 165], "1000": [118, 120], "488": 118, "equal": [119, 151], "crop": 119, "num_til": 119, "channel_s": 119, "50": [119, 162], "4x6": 119, "24": [119, 163, 164], "400": 119, "2x3": 119, "datatyp": [120, 166], "denot": 120, "integ": [120, 144, 149], "auto_wrap_polici": [120, 131, 147], "submodul": [120, 139], "obei": 120, "contract": 120, "get_fsdp_polici": 120, "modules_to_wrap": [120, 131, 139], "min_num_param": 120, "my_fsdp_polici": 120, "recurs": [120, 139, 142], "isinst": [120, 161], "sum": [120, 165], "p": [120, 125, 165, 166], "numel": [120, 165], "functool": 120, "partial": 120, "stabl": [120, 137, 142, 149, 156], "safe_seri": 121, "from_pretrain": 121, "0001_of_0003": 121, "0002_of_0003": 121, "preserv": [121, 166], "weight_map": [121, 162], "convert_weight": 121, "_model_typ": [121, 124], "intermediate_checkpoint": [121, 122, 123], "_weight_map": 121, "shard": [122, 164], "wip": 122, "larger": [123, 162, 164], "down": [123, 161, 165, 166], "intermedi": [123, 147, 164, 166], "qualnam": 124, "boundari": 124, "distinguish": 124, "gate": [124, 159, 163], "my_new_model": 124, "my_custom_state_dict_map": 124, "mistral_reward": 124, "classif": 124, "mistral_classifi": 124, "optim_map": 125, "bare": 125, "bone": 125, "distribut": [125, 129, 136, 137, 147, 149, 157, 159, 163, 164], "optim_dict": [125, 127, 146], "cfg_optim": 125, "ckpt": 125, "optim_ckpt": 125, "placeholder_optim_dict": 125, "optiminbackwardwrapp": 125, "get_optim_kei": 125, "arbitrari": [125, 165], "hyperparamet": [125, 157, 163, 165, 166], "optim_ckpt_map": 125, "runtimeerror": [125, 130, 136], "loadabl": 125, "argpars": 126, "argumentpars": 126, "builtin": 126, "said": 126, "noth": 126, "consult": 126, "info": [126, 163], "parse_known_arg": 126, "namespac": 126, "act": 126, "precid": 126, "parse_arg": 126, "too": [126, 164], "optimizerinbackwardwrapp": 127, "top": [127, 162, 166], "named_paramet": 127, "max_generated_token": 128, "pad_id": 128, "top_k": [128, 162], "stop_token": 128, "custom_generate_next_token": 128, "bsz": 128, "predict": 128, "prune": [128, 166], "stop": 128, "compil": [128, 162, 164, 166], "generate_next_token": 128, "llama3_token": [128, 160, 164], "hi": [128, 160], "my": [128, 159, 160, 161, 162, 164], "jeremi": 128, "float32": 130, "bf16": [130, 166], "inde": [130, 162], "kernel": 130, "isn": [130, 159], "hardwar": [130, 157, 161, 162, 165], "memory_efficient_fsdp_wrap": 131, "maxim": [131, 139, 155, 157], "been": [131, 164], "workload": 131, "15": [131, 145, 160, 162, 165, 166], "alongsid": 131, "ac": 131, "fullyshardeddataparallel": [131, 139], "fsdppolicytyp": [131, 139], "handler": 132, "reset_stat": 133, "track": 133, "alloc": [133, 138, 139, 164, 166], "reserv": [133, 138, 160, 166], "stat": [133, 138, 166], "int4": 134, "4w": [134, 162, 164], "recogn": 134, "mode": [134, 162], "aka": 135, "master": 137, "port": [137, 159], "address": 137, "hold": [137, 163], "peak_memory_act": 138, "peak_memory_alloc": 138, "peak_memory_reserv": 138, "get_memory_stat": 138, "own": [139, 149, 159, 160, 161, 162, 165], "unit": [139, 157], "hierarch": 139, "requires_grad": [139, 165, 166], "filenam": 140, "log_": 140, "unixtimestamp": 140, "txt": [140, 161, 163], "thread": 140, "safe": 140, "flush": [140, 141, 142, 143], "ndarrai": [140, 141, 142, 143], "scalar": [140, 141, 142, 143], "record": [140, 141, 142, 143, 150], "payload": [140, 141, 142, 143], "organize_log": 142, "tensorboard": 142, "subdirectori": 142, "compar": [142, 151, 162, 165, 166], "logdir": 142, "startup": 142, "tree": [142, 161, 162], "tfevent": 142, "encount": 142, "frontend": 142, "organ": [142, 159], "accordingli": 142, "my_log_dir": 142, "view": [142, 162, 163], "my_metr": [142, 143], "termin": [142, 143], "entiti": 143, "bias": 143, "sent": 143, "usernam": 143, "my_project": 143, "my_ent": 143, "my_group": 143, "importerror": 143, "account": [143, 165, 166], "log_config": 143, "link": [143, 162], "capecap": 143, "6053ofw0": 143, "torchtune_config_j67sb73v": 143, "ignore_idx": [144, 145], "longest": 144, "token_pair": 144, "input_id": 145, "chosen_input_id": [145, 161], "chosen_label": [145, 161], "rejected_input_id": [145, 161], "rejected_label": [145, 161], "14": [145, 166], "17": [145, 162, 165], "18": [145, 164], "19": [145, 162, 164, 166], "20": 145, "soon": 146, "readi": [146, 155, 160], "grad": 146, "achiev": [146, 162, 164, 165, 166], "acwrappolicytyp": 147, "author": [147, 157, 163, 166], "fsdp_adavnced_tutori": 147, "insid": 148, "contextmanag": 148, "debug_mod": 149, "pseudo": 149, "random": [149, 163], "commonli": [149, 162, 165, 166], "numpi": 149, "determinist": 149, "global": [149, 161], "warn": 149, "nondeterminist": 149, "cudnn": 149, "set_deterministic_debug_mod": 149, "algorithm": 149, "profile_memori": 150, "with_stack": 150, "record_shap": 150, "with_flop": 150, "wait_step": 150, "warmup_step": 150, "active_step": 150, "profil": 150, "layout": 150, "trace": 150, "profileract": 150, "flop": 150, "wait": 150, "cycl": 150, "repeat": 150, "reduct": [150, 165], "iter": [150, 152, 166], "scope": 150, "gradient_accumul": 150, "sensibl": 150, "default_schedul": 150, "greater": 151, "against": [151, 166], "__version__": 151, "named_param": 152, "generated_examples_python": 153, "zip": 153, "galleri": [153, 158], "sphinx": 153, "000": [154, 158, 164], "execut": [154, 158], "generated_exampl": 154, "mem": [154, 158], "mb": [154, 158], "topic": 155, "gentl": 155, "introduct": 155, "first_finetune_tutori": 155, "workflow": [155, 161, 163, 165], "requisit": 156, "proper": [156, 163], "host": [156, 159, 163], "latest": [156, 163, 166], "And": [156, 162, 164], "ls": [156, 159, 162, 163, 164], "welcom": [156, 159], "show": [156, 159, 160, 165], "greatest": [156, 163], "contributor": 156, "cd": [156, 162], "even": [156, 159, 160, 161, 164, 165, 166], "commit": 156, "branch": 156, "url": 156, "whl": 156, "therebi": [156, 166], "forc": 156, "reinstal": 156, "opt": [156, 163], "suffix": 156, "cu121": 156, "On": [157, 165], "pointer": 157, "emphas": 157, "aspect": 157, "simplic": 157, "component": 157, "reus": 157, "prove": 157, "democrat": 157, "box": [157, 166], "zoo": 157, "varieti": [157, 165], "techniqu": [157, 162, 163, 165], "integr": [157, 162, 163, 164, 165, 166], "excit": 157, "checkout": 157, "quickstart": 157, "attain": 157, "better": [157, 160, 161, 162], "chekckpoint": 157, "embodi": 157, "philosophi": 157, "usabl": 157, "composit": 157, "hard": [157, 161], "outlin": 157, "unecessari": 157, "never": 157, "thoroughli": 157, "short": 159, "subcommand": 159, "anytim": 159, "symlink": 159, "auto": 159, "wrote": 159, "readm": 159, "md": 159, "lot": [159, 162], "recent": 159, "releas": [159, 164], "agre": 159, "term": 159, "perman": 159, "eat": 159, "bandwith": 159, "storag": [159, 166], "00030": 159, "ootb": 159, "full_finetune_single_devic": [159, 161, 162, 163], "7b_full_low_memori": [159, 162, 163], "8b_full_single_devic": [159, 161], "mini_full_low_memori": 159, "7b_full": [159, 162, 163], "13b_full": [159, 162, 163], "70b_full": 159, "edit": 159, "destin": 159, "lora_finetune_distribut": [159, 164, 165], "torchrun": 159, "8b_lora_single_devic": [159, 160, 164], "launch": [159, 160, 163], "nproc": 159, "node": 159, "worker": 159, "nnode": [159, 165], "minimum_nod": 159, "maximum_nod": 159, "fail": 159, "rdzv": 159, "rendezv": 159, "endpoint": 159, "8b_lora": [159, 164], "bypass": 159, "vice": 159, "versa": 159, "fancy_lora": 159, "8b_fancy_lora": 159, "sai": [159, 160, 163], "align": 160, "intend": 160, "nice": 160, "meet": 160, "overhaul": 160, "begin_of_text": 160, "start_header_id": 160, "end_header_id": 160, "eot_id": 160, "accompani": 160, "who": 160, "influenti": 160, "hip": 160, "hop": 160, "artist": [160, 164], "2pac": 160, "rakim": 160, "c": 160, "na": 160, "flavor": [160, 161], "msg": 160, "formatted_messag": [160, 161], "nyou": [160, 161], "nwho": 160, "why": [160, 163, 165], "user_messag": 160, "518": 160, "25580": 160, "29962": 160, "3532": 160, "14816": 160, "29903": 160, "6778": 160, "piece_to_id": 160, "vector": 160, "place": 160, "manual": [160, 166], "529": 160, "29879": 160, "29958": 160, "nhere": 160, "_encode_special_token": 160, "128000": 160, "128009": 160, "pure": 160, "That": 160, "won": [160, 162, 164], "mess": 160, "govern": 160, "prime": 160, "strictli": 160, "summarizetempl": [160, 161], "ask": 160, "untouch": 160, "nsummari": 160, "robust": 160, "csv": [160, 161], "onlin": 160, "forum": 160, "panda": 160, "pd": 160, "df": 160, "read_csv": 160, "your_fil": 160, "nrow": 160, "tolist": 160, "iloc": 160, "gp": 160, "receiv": 160, "commun": [160, 161, 162], "satellit": 160, "thing": [160, 166], "dataclass": 160, "message_convert": 160, "input_msg": 160, "output_msg": 160, "assistant_messag": 160, "But": [160, 162, 164, 165], "mistralchatformat": 160, "custom_dataset": 160, "2048": 160, "data_fil": [160, 161], "honor": 160, "copi": [160, 162, 163, 164, 166], "custom_8b_lora_single_devic": 160, "steer": 161, "wheel": 161, "publicli": 161, "great": [161, 162], "hood": [161, 162, 166], "text_completion_dataset": 161, "padded_col": 161, "upper": 161, "constraint": [161, 165], "slow": [161, 166], "signific": 161, "speedup": [161, 164], "my_data": 161, "instruct_dataset": 161, "fix": 161, "goal": 161, "agnost": 161, "respond": 161, "anim": 161, "plant": 161, "miner": 161, "oak": 161, "copper": 161, "ore": 161, "eleph": 161, "customtempl": 161, "cl": 161, "chat_dataset": 161, "quit": [161, 166], "incorpor": 161, "advanc": 161, "customchatformat": 161, "vicgal": 161, "gpt4": 161, "drive": 161, "rajpurkar": 161, "io": 161, "squad": 161, "explor": 161, "rlhf": 161, "few": [161, 164, 165, 166], "adjust": 161, "chosen_messag": 161, "transformed_sampl": 161, "key_chosen": 161, "rejected_messag": 161, "key_reject": 161, "c_mask": 161, "np": 161, "cross_entropy_ignore_idx": 161, "r_mask": 161, "stack_exchanged_paired_dataset": 161, "had": 161, "stackexchangedpairedtempl": 161, "response_j": 161, "response_k": 161, "rl": 161, "favorit": [162, 164, 165], "seemlessli": 162, "beyond": [162, 166], "connect": 162, "amount": 162, "natur": 162, "export": 162, "mobil": 162, "phone": 162, "leverag": [162, 164, 166], "plai": 162, "freez": [162, 165], "percentag": 162, "learnabl": 162, "keep": [162, 165], "16gb": [162, 165], "rtx": 162, "3090": 162, "4090": 162, "hour": 162, "7b_qlora_single_devic": [162, 163, 166], "473": 162, "98": [162, 166], "gb": [162, 164, 165, 166], "484": 162, "01": [162, 163], "fact": [162, 164, 165], "third": 162, "realli": 162, "eleuther_ev": [162, 164], "eleuther_evalu": [162, 164], "lm_eval": [162, 164], "plan": 162, "custom_eval_config": [162, 164], "truthfulqa_mc2": [162, 164, 165], "measur": [162, 164], "propens": [162, 164], "shot": [162, 164], "accuraci": [162, 164, 165, 166], "baselin": [162, 165], "324": 162, "loglikelihood": 162, "195": 162, "121": 162, "27": 162, "second": [162, 164, 165, 166], "197": 162, "acc": 162, "388": 162, "38": 162, "shown": 162, "489": 162, "48": [162, 166], "seem": 162, "custom_generation_config": [162, 164], "kick": 162, "interest": 162, "site": 162, "visit": 162, "bai": 162, "92": [162, 164], "exploratorium": 162, "san": 162, "francisco": 162, "magazin": 162, "awesom": 162, "bridg": 162, "pretti": 162, "cool": 162, "96": [162, 166], "61": 162, "sec": [162, 164], "25": 162, "83": 162, "99": [162, 165], "72": 162, "littl": 162, "saw": 162, "took": [162, 164], "torchao": [162, 164, 166], "bit": [162, 164, 165, 166], "custom_quantization_config": [162, 164], "68": 162, "76": 162, "69": 162, "95": [162, 164], "67": 162, "engin": [162, 164], "fullmodeltorchtunecheckpoint": [162, 164], "int4weightonlyquant": [162, 164], "groupsiz": [162, 164], "256": [162, 164], "park": 162, "sit": 162, "hill": 162, "beauti": 162, "62": [162, 164], "85": 162, "sped": 162, "almost": [162, 164, 165], "3x": [162, 164], "benefit": 162, "doesn": 162, "yet": 162, "fast": 162, "clone": [162, 165, 166], "assumpt": 162, "new_dir": 162, "output_dict": 162, "sd_1": 162, "sd_2": 162, "dump": 162, "convert_hf_checkpoint": 162, "checkpoint_path": 162, "justin": 162, "school": 162, "math": 162, "teacher": 162, "ws": 162, "94": [162, 164], "28": 162, "bandwidth": [162, 164], "1391": 162, "84": 162, "thats": 162, "seamlessli": 162, "authent": [162, 163], "hopefulli": 162, "gave": 162, "grant": 163, "minut": 163, "agreement": 163, "altern": 163, "hackabl": 163, "singularli": 163, "technic": 163, "purpos": [163, 164], "depth": 163, "principl": 163, "boilerpl": 163, "substanti": [163, 165], "custom_config": 163, "replic": 163, "lorafinetunerecipesingledevic": 163, "lora_finetune_output": 163, "log_1713194212": 163, "52": 163, "3697006702423096": 163, "25880": [163, 166], "55": 163, "83it": 163, "monitor": 163, "tqdm": 163, "interv": 163, "e2": 163, "focu": 164, "128": [164, 165], "theta": 164, "gain": 164, "illustr": 164, "basic": 164, "observ": 164, "consum": [164, 166], "vram": [164, 165], "overal": 164, "8b_qlora_single_devic": 164, "coupl": [164, 165, 166], "meta_model_0": 164, "122": 164, "sarah": 164, "busi": 164, "mum": 164, "young": 164, "children": 164, "live": 164, "north": 164, "east": 164, "england": 164, "135": 164, "88": 164, "138": 164, "346": 164, "09": 164, "139": 164, "31": 164, "far": 164, "drill": 164, "90": 164, "93": 164, "91": 164, "104": 164, "four": [164, 165], "again": 164, "jake": 164, "disciplin": 164, "passion": 164, "draw": 164, "paint": 164, "57": [164, 165, 166], "broader": 164, "teach": 165, "straight": 165, "unfamiliar": 165, "oppos": [165, 166], "momentum": 165, "relat": 165, "aghajanyan": 165, "et": 165, "al": 165, "hypothes": 165, "intrins": 165, "often": 165, "eight": 165, "practic": 165, "blue": 165, "rememb": 165, "approx": 165, "15m": 165, "8192": 165, "65k": 165, "frozen_out": [165, 166], "lora_out": [165, 166], "base_model": 165, "choos": 165, "lora_model": 165, "lora_llama_2_7b": [165, 166], "alon": 165, "in_featur": 165, "out_featur": 165, "inplac": 165, "feel": 165, "free": 165, "whenev": 165, "peft_util": 165, "set_trainable_param": 165, "fetch": 165, "lora_param": 165, "total_param": 165, "trainable_param": 165, "2f": 165, "6742609920": 165, "4194304": 165, "7b_lora": 165, "my_model_checkpoint_path": [165, 166], "tokenizer_checkpoint": [165, 166], "my_tokenizer_checkpoint_path": [165, 166], "factori": 165, "benefici": 165, "impact": 165, "minor": 165, "good": 165, "64": 165, "lora_experiment_1": 165, "smooth": [165, 166], "curv": [165, 166], "ran": 165, "footprint": 165, "commod": 165, "cogniz": 165, "ax": 165, "parallel": 165, "truthfulqa": 165, "previous": 165, "475": 165, "87": 165, "508": 165, "86": 165, "504": 165, "04": 165, "514": 165, "absolut": 165, "4gb": 165, "tradeoff": 165, "potenti": 165, "highli": 166, "vanilla": 166, "held": 166, "therefor": 166, "bespok": 166, "normalfloat": 166, "8x": 166, "retain": 166, "vast": 166, "major": 166, "degrad": 166, "normatfloat": 166, "doubl": 166, "themselv": 166, "deepdiv": 166, "idea": 166, "distinct": 166, "de": 166, "incur": 166, "counterpart": 166, "set_default_devic": 166, "qlora_linear": 166, "memory_alloc": 166, "177": 166, "152": 166, "del": 166, "empty_cach": 166, "lora_linear": 166, "081": 166, "344": 166, "qlora_llama2_7b": 166, "qlora_model": 166, "essenti": 166, "reparametrize_as_dtype_state_dict_post_hook": 166, "35": 166, "40": 166, "29": 166, "slower": 166, "149": 166, "9157477021217346": 166, "02": 166, "08": 166, "15it": 166, "nightli": 166, "hundr": 166, "228": 166, "8158286809921265": 166, "59": 166, "95it": 166, "exercis": 166, "portion": 166, "linear_nf4": 166, "to_nf4": 166, "linear_weight": 166, "autograd": 166, "regular": 166, "incom": 166}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 4, 1, "", "Role"], [23, 1, 1, "", "StackExchangedPairedTemplate"], [24, 1, 1, "", "SummarizeTemplate"], [25, 0, 1, "", "get_openai_messages"], [26, 0, 1, "", "get_sharegpt_messages"], [27, 0, 1, "", "truncate"], [28, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 3, 1, "", "contains_media"], [20, 2, 1, "", "from_dict"], [20, 3, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[23, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[24, 2, 1, "", "format"]], "torchtune.datasets": [[29, 1, 1, "", "ChatDataset"], [30, 1, 1, "", "ConcatDataset"], [31, 1, 1, "", "InstructDataset"], [32, 1, 1, "", "PackedDataset"], [33, 1, 1, "", "PreferenceDataset"], [34, 1, 1, "", "TextCompletionDataset"], [35, 0, 1, "", "alpaca_cleaned_dataset"], [36, 0, 1, "", "alpaca_dataset"], [37, 0, 1, "", "chat_dataset"], [38, 0, 1, "", "cnn_dailymail_articles_dataset"], [39, 0, 1, "", "grammar_dataset"], [40, 0, 1, "", "instruct_dataset"], [41, 0, 1, "", "samsum_dataset"], [42, 0, 1, "", "slimorca_dataset"], [43, 0, 1, "", "stack_exchanged_paired_dataset"], [44, 0, 1, "", "text_completion_dataset"], [45, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[46, 0, 1, "", "code_llama2_13b"], [47, 0, 1, "", "code_llama2_70b"], [48, 0, 1, "", "code_llama2_7b"], [49, 0, 1, "", "lora_code_llama2_13b"], [50, 0, 1, "", "lora_code_llama2_70b"], [51, 0, 1, "", "lora_code_llama2_7b"], [52, 0, 1, "", "qlora_code_llama2_13b"], [53, 0, 1, "", "qlora_code_llama2_70b"], [54, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[55, 1, 1, "", "GemmaTokenizer"], [56, 0, 1, "", "gemma_2b"], [57, 0, 1, "", "gemma_7b"], [58, 0, 1, "", "gemma_tokenizer"], [59, 0, 1, "", "lora_gemma_2b"], [60, 0, 1, "", "lora_gemma_7b"], [61, 0, 1, "", "qlora_gemma_2b"], [62, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[55, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[63, 1, 1, "", "Llama2Tokenizer"], [64, 0, 1, "", "llama2_13b"], [65, 0, 1, "", "llama2_70b"], [66, 0, 1, "", "llama2_7b"], [67, 0, 1, "", "llama2_tokenizer"], [68, 0, 1, "", "lora_llama2_13b"], [69, 0, 1, "", "lora_llama2_70b"], [70, 0, 1, "", "lora_llama2_7b"], [71, 0, 1, "", "qlora_llama2_13b"], [72, 0, 1, "", "qlora_llama2_70b"], [73, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[63, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[74, 1, 1, "", "Llama3Tokenizer"], [75, 0, 1, "", "llama3_70b"], [76, 0, 1, "", "llama3_8b"], [77, 0, 1, "", "llama3_tokenizer"], [78, 0, 1, "", "lora_llama3_70b"], [79, 0, 1, "", "lora_llama3_8b"], [80, 0, 1, "", "qlora_llama3_70b"], [81, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[74, 2, 1, "", "decode"], [74, 2, 1, "", "tokenize_message"], [74, 2, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[82, 1, 1, "", "MistralTokenizer"], [83, 0, 1, "", "lora_mistral_7b"], [84, 0, 1, "", "lora_mistral_classifier_7b"], [85, 0, 1, "", "mistral_7b"], [86, 0, 1, "", "mistral_classifier_7b"], [87, 0, 1, "", "mistral_tokenizer"], [88, 0, 1, "", "qlora_mistral_7b"], [89, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.mistral.MistralTokenizer": [[82, 2, 1, "", "decode"], [82, 2, 1, "", "encode"], [82, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[90, 1, 1, "", "Phi3MiniTokenizer"], [91, 0, 1, "", "lora_phi3_mini"], [92, 0, 1, "", "phi3_mini"], [93, 0, 1, "", "phi3_mini_tokenizer"], [94, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[90, 2, 1, "", "decode"], [90, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[95, 1, 1, "", "CausalSelfAttention"], [96, 1, 1, "", "FeedForward"], [97, 1, 1, "", "KVCache"], [98, 1, 1, "", "RMSNorm"], [99, 1, 1, "", "RotaryPositionalEmbeddings"], [100, 1, 1, "", "TransformerDecoder"], [101, 1, 1, "", "TransformerDecoderLayer"], [103, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[95, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[96, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[97, 2, 1, "", "reset"], [97, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[98, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[99, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[100, 2, 1, "", "forward"], [100, 2, 1, "", "reset_caches"], [100, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[101, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[102, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[104, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[104, 2, 1, "", "forward"]], "torchtune.modules.peft": [[105, 1, 1, "", "AdapterModule"], [106, 1, 1, "", "LoRALinear"], [107, 0, 1, "", "disable_adapter"], [108, 0, 1, "", "get_adapter_params"], [109, 0, 1, "", "set_trainable_params"], [110, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [111, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[105, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[106, 2, 1, "", "adapter_params"], [106, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[112, 1, 1, "", "SentencePieceBaseTokenizer"], [113, 1, 1, "", "TikTokenBaseTokenizer"], [114, 0, 1, "", "parse_hf_tokenizer_json"], [115, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[112, 2, 1, "", "decode"], [112, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[113, 2, 1, "", "decode"], [113, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[116, 0, 1, "", "find_supported_resolutions"], [117, 0, 1, "", "get_canvas_best_fit"], [118, 0, 1, "", "resize_with_pad"], [119, 0, 1, "", "tile_crop"]], "torchtune.utils": [[120, 4, 1, "", "FSDPPolicyType"], [121, 1, 1, "", "FullModelHFCheckpointer"], [122, 1, 1, "", "FullModelMetaCheckpointer"], [123, 1, 1, "", "FullModelTorchTuneCheckpointer"], [124, 1, 1, "", "ModelType"], [125, 1, 1, "", "OptimizerInBackwardWrapper"], [126, 1, 1, "", "TuneRecipeArgumentParser"], [127, 0, 1, "", "create_optim_in_bwd_wrapper"], [128, 0, 1, "", "generate"], [129, 0, 1, "", "get_device"], [130, 0, 1, "", "get_dtype"], [131, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [132, 0, 1, "", "get_logger"], [133, 0, 1, "", "get_memory_stats"], [134, 0, 1, "", "get_quantizer_mode"], [135, 0, 1, "", "get_world_size_and_rank"], [136, 0, 1, "", "init_distributed"], [137, 0, 1, "", "is_distributed"], [138, 0, 1, "", "log_memory_stats"], [139, 0, 1, "", "lora_fsdp_wrap_policy"], [144, 0, 1, "", "padded_collate"], [145, 0, 1, "", "padded_collate_dpo"], [146, 0, 1, "", "register_optim_in_bwd_hooks"], [147, 0, 1, "", "set_activation_checkpointing"], [148, 0, 1, "", "set_default_dtype"], [149, 0, 1, "", "set_seed"], [150, 0, 1, "", "setup_torch_profiler"], [151, 0, 1, "", "torch_version_ge"], [152, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[121, 2, 1, "", "load_checkpoint"], [121, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[122, 2, 1, "", "load_checkpoint"], [122, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[123, 2, 1, "", "load_checkpoint"], [123, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[124, 5, 1, "", "GEMMA"], [124, 5, 1, "", "LLAMA2"], [124, 5, 1, "", "LLAMA3"], [124, 5, 1, "", "MISTRAL"], [124, 5, 1, "", "MISTRAL_REWARD"], [124, 5, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[125, 2, 1, "", "get_optim_key"], [125, 2, 1, "", "load_state_dict"], [125, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[126, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[140, 1, 1, "", "DiskLogger"], [141, 1, 1, "", "StdoutLogger"], [142, 1, 1, "", "TensorBoardLogger"], [143, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[140, 2, 1, "", "close"], [140, 2, 1, "", "log"], [140, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[141, 2, 1, "", "close"], [141, 2, 1, "", "log"], [141, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[142, 2, 1, "", "close"], [142, 2, 1, "", "log"], [142, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[143, 2, 1, "", "close"], [143, 2, 1, "", "log"], [143, 2, 1, "", "log_config"], [143, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:property", "4": "py:data", "5": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"], "4": ["py", "data", "Python data"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 22, 120, 155, 157, 159, 162, 164, 165, 166], "config": [0, 7, 8, 159, 163], "data": [1, 5, 22, 160], "text": [1, 161, 164], "templat": [1, 160, 161], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 160, 161], "exampl": 2, "gener": [2, 128, 162, 164], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 159, 162, 163, 164, 165], "llama3": [3, 160, 164], "llama2": [3, 160, 162, 165, 166], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 156, 166], "block": 4, "base": 4, "token": [4, 160], "util": [4, 5, 120], "peft": 4, "loss": 4, "vision": 4, "transform": 4, "checkpoint": [5, 6, 9, 162], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 161, 165, 166], "manag": 5, "perform": [5, 165], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 157, 162], "format": [6, 161], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 162, 165, 166], "put": [6, 166], "thi": 6, "all": [6, 7, 166], "togeth": [6, 166], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 161], "us": [7, 8, 160, 162, 166], "instanti": [7, 10], "referenc": 7, "other": [7, 162], "field": 7, "interpol": 7, "valid": [7, 13, 159], "your": [7, 8, 162, 163], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 157, 165, 166], "ar": 8, "recip": [8, 159, 163, 165], "script": 8, "run": [8, 159, 162], "cli": [8, 159], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "role": 22, "stackexchangedpairedtempl": 23, "summarizetempl": 24, "get_openai_messag": 25, "get_sharegpt_messag": 26, "truncat": 27, "validate_messag": 28, "chatdataset": 29, "concatdataset": 30, "instructdataset": 31, "packeddataset": 32, "preferencedataset": 33, "textcompletiondataset": 34, "alpaca_cleaned_dataset": 35, "alpaca_dataset": 36, "chat_dataset": 37, "cnn_dailymail_articles_dataset": 38, "grammar_dataset": 39, "instruct_dataset": 40, "samsum_dataset": 41, "slimorca_dataset": 42, "stack_exchanged_paired_dataset": 43, "text_completion_dataset": 44, "wikitext_dataset": 45, "code_llama2_13b": 46, "code_llama2_70b": 47, "code_llama2_7b": 48, "lora_code_llama2_13b": 49, "lora_code_llama2_70b": 50, "lora_code_llama2_7b": 51, "qlora_code_llama2_13b": 52, "qlora_code_llama2_70b": 53, "qlora_code_llama2_7b": 54, "gemmatoken": 55, "gemma_2b": 56, "gemma_7b": 57, "gemma_token": 58, "lora_gemma_2b": 59, "lora_gemma_7b": 60, "qlora_gemma_2b": 61, "qlora_gemma_7b": 62, "llama2token": 63, "llama2_13b": 64, "llama2_70b": 65, "llama2_7b": 66, "llama2_token": 67, "lora_llama2_13b": 68, "lora_llama2_70b": 69, "lora_llama2_7b": 70, "qlora_llama2_13b": 71, "qlora_llama2_70b": 72, "qlora_llama2_7b": 73, "llama3token": 74, "llama3_70b": 75, "llama3_8b": 76, "llama3_token": 77, "lora_llama3_70b": 78, "lora_llama3_8b": 79, "qlora_llama3_70b": 80, "qlora_llama3_8b": 81, "mistraltoken": 82, "lora_mistral_7b": 83, "lora_mistral_classifier_7b": 84, "mistral_7b": 85, "mistral_classifier_7b": 86, "mistral_token": 87, "qlora_mistral_7b": 88, "qlora_mistral_classifier_7b": 89, "phi3minitoken": 90, "lora_phi3_mini": 91, "phi3_mini": 92, "phi3_mini_token": 93, "qlora_phi3_mini": 94, "causalselfattent": 95, "todo": [95, 101], "feedforward": 96, "kvcach": 97, "rmsnorm": 98, "rotarypositionalembed": 99, "transformerdecod": 100, "transformerdecoderlay": 101, "reparametrize_as_dtype_state_dict_post_hook": 102, "get_cosine_schedule_with_warmup": 103, "dpoloss": 104, "adaptermodul": 105, "loralinear": 106, "disable_adapt": 107, "get_adapter_param": 108, "set_trainable_param": 109, "validate_missing_and_unexpected_for_lora": 110, "validate_state_dict_for_lora": 111, "sentencepiecebasetoken": 112, "tiktokenbasetoken": 113, "parse_hf_tokenizer_json": 114, "tokenize_messages_no_special_token": 115, "find_supported_resolut": 116, "get_canvas_best_fit": 117, "resize_with_pad": 118, "tile_crop": 119, "fsdppolicytyp": 120, "fullmodelhfcheckpoint": 121, "fullmodelmetacheckpoint": 122, "fullmodeltorchtunecheckpoint": 123, "modeltyp": 124, "optimizerinbackwardwrapp": 125, "tunerecipeargumentpars": 126, "create_optim_in_bwd_wrapp": 127, "get_devic": 129, "get_dtyp": 130, "get_full_finetune_fsdp_wrap_polici": 131, "get_logg": 132, "get_memory_stat": 133, "get_quantizer_mod": 134, "get_world_size_and_rank": 135, "init_distribut": 136, "is_distribut": 137, "log_memory_stat": 138, "lora_fsdp_wrap_polici": 139, "disklogg": 140, "stdoutlogg": 141, "tensorboardlogg": 142, "wandblogg": 143, "padded_col": 144, "padded_collate_dpo": 145, "register_optim_in_bwd_hook": 146, "set_activation_checkpoint": 147, "set_default_dtyp": 148, "set_se": 149, "setup_torch_profil": 150, "torch_version_g": 151, "validate_expected_param_dtyp": 152, "comput": [154, 158], "time": [154, 158], "welcom": 155, "document": 155, "get": [155, 159, 164], "start": [155, 159], "tutori": 155, "instal": 156, "instruct": [156, 161, 164], "via": [156, 164], "pypi": 156, "git": 156, "clone": 156, "nightli": 156, "kei": 157, "concept": 157, "design": 157, "principl": 157, "download": [159, 162, 163], "list": 159, "built": [159, 161], "copi": 159, "fine": [160, 161, 163, 164], "tune": [160, 161, 163, 164], "chat": [160, 161], "chang": 160, "from": [160, 166], "prompt": 160, "special": 160, "when": 160, "should": 160, "i": 160, "custom": [160, 161], "hug": [161, 162], "face": [161, 162], "set": 161, "max": 161, "sequenc": 161, "length": 161, "sampl": 161, "pack": 161, "unstructur": 161, "corpu": 161, "multipl": 161, "local": 161, "remot": 161, "fulli": 161, "end": 162, "workflow": 162, "7b": 162, "finetun": [162, 165, 166], "evalu": [162, 164], "eleutherai": [162, 164], "s": [162, 164], "eval": [162, 164], "har": [162, 164], "speed": 162, "up": 162, "quantiz": [162, 164], "librari": 162, "upload": 162, "hub": 162, "first": 163, "llm": 163, "select": 163, "modifi": 163, "train": 163, "next": 163, "step": 163, "meta": 164, "8b": 164, "access": 164, "our": 164, "faster": 164, "how": 165, "doe": 165, "work": 165, "appli": 165, "trade": 165, "off": 165, "qlora": 166, "save": 166, "deep": 166, "dive": 166}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})