Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "MistralChatFormat", "SummarizeTemplate", "validate_messages", "alpaca_cleaned_dataset", "alpaca_dataset", "grammar_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_7b", "lora_llama2_13b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_8b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"all": [3, 4, 8, 12, 40, 41, 45, 47, 54, 55, 57, 72, 74, 76, 77, 79, 80, 81, 82], "from": [3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 38, 41, 45, 46, 48, 49, 51, 53, 55, 56, 57, 66, 67, 73, 75, 77, 78, 79, 80, 81, 82], "famili": [3, 8, 22, 23, 26, 76, 81], "tune": [3, 6, 7, 8, 9, 11, 74, 75, 76, 79, 82, 83], "download": [3, 6, 72, 75, 81, 82, 83], "meta": [3, 6, 18, 55, 56, 79, 80, 81], "llama": [3, 6, 18, 43, 44, 55, 56, 79, 80, 81, 82], "3": [3, 6, 54, 57, 60, 68, 79, 80, 81, 83], "8b": [3, 34, 35], "hf": [3, 6, 55, 79, 80, 81], "token": [3, 6, 7, 8, 22, 23, 24, 25, 26, 40, 44, 45, 46, 53, 54, 78, 79, 80, 81, 82, 83], "access_token": 3, "pre": [3, 18, 75], "train": [3, 5, 6, 8, 9, 18, 22, 23, 24, 25, 26, 40, 47, 48, 55, 56, 59, 69, 74, 76, 78, 79, 81, 82, 83], "can": [3, 4, 6, 7, 8, 9, 10, 12, 22, 23, 43, 44, 53, 55, 57, 66, 67, 74, 75, 76, 78, 79, 80, 81, 82, 83], "hug": [3, 6, 22, 23, 24, 25, 26, 48, 76, 78, 80, 81], "face": [3, 6, 22, 23, 24, 25, 26, 48, 76, 78, 80, 81], "hub": [3, 6, 80], "follow": [3, 6, 8, 40, 48, 67, 74, 75, 78, 79, 80, 81, 82, 83], "command": [3, 8, 9, 57, 75, 79, 80, 81, 82, 83], "2": [3, 6, 9, 21, 26, 40, 53, 55, 56, 68, 71, 79, 80, 81, 82], "7b": [3, 6, 22, 23, 29, 31, 37, 38, 55, 56, 80, 81, 82, 83], "ai": [3, 38, 40, 67, 81], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 26, 40, 45, 48, 53, 54, 56, 66, 67, 68, 71, 79, 80, 81, 82, 83], "googl": [3, 27], "2b": [3, 27], "ignor": [3, 6, 40, 41], "pattern": [3, 54], "These": [4, 6, 7, 8, 10, 57, 78, 79, 80, 81, 82, 83], "ar": [4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 35, 36, 37, 39, 45, 50, 55, 56, 59, 75, 76, 78, 79, 80, 81, 82, 83], "common": [4, 7, 78, 81, 82], "us": [4, 6, 9, 10, 11, 15, 18, 22, 23, 24, 25, 26, 40, 41, 43, 44, 45, 46, 47, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 67, 71, 74, 75, 76, 78, 80, 81, 82], "offer": 5, "allow": [5, 66, 83], "seamless": 5, "transit": 5, "between": [5, 6, 55, 79, 81, 82, 83], "format": [5, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 26, 55, 56, 79, 80, 81, 82], "interoper": [5, 6, 8, 76, 79, 83], "rest": [5, 83], "ecosystem": [5, 6, 8, 76, 79, 81, 83], "For": [5, 6, 7, 8, 22, 23, 40, 45, 57, 67, 71, 75, 78, 79, 80, 81, 82, 83], "comprehens": 5, "overview": [5, 7, 9, 80, 82, 83], "pleas": [5, 33, 36, 39, 75, 83], "see": [5, 6, 9, 18, 19, 33, 36, 39, 42, 49, 57, 60, 67, 69, 71, 75, 76, 78, 79, 80, 81, 82, 83], "deep": [5, 6, 7, 8, 9, 76, 80, 81], "dive": [5, 6, 7, 8, 9, 76, 80, 81], "enabl": [5, 7, 8, 9, 30, 31, 32, 33, 35, 36, 37, 39, 50, 69, 71, 81, 82, 83], "work": [5, 6, 8, 57, 76, 79, 81, 83], "set": [5, 6, 7, 8, 9, 22, 23, 24, 25, 26, 44, 45, 52, 58, 70, 71, 76, 78, 79, 80, 81, 82], "consumpt": 5, "dure": [5, 6, 22, 23, 24, 25, 40, 42, 44, 45, 46, 47, 79, 81, 82, 83], "provid": [5, 6, 7, 8, 10, 15, 19, 26, 45, 57, 67, 76, 78, 79, 80, 81], "debug": [5, 6, 7, 8], "your": [5, 9, 10, 66, 67, 74, 75, 76, 78, 81, 82, 83], "finetun": [5, 6, 7, 8, 30, 31, 32, 35, 63, 74, 76, 80, 81], "job": [5, 9, 71, 80], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 20, 22, 23, 24, 25, 26, 76, 80, 81], "walk": [6, 8, 66, 76, 78, 79, 80, 83], "you": [6, 7, 8, 9, 10, 17, 18, 22, 23, 57, 66, 67, 74, 75, 76, 78, 79, 80, 81, 82, 83], "through": [6, 7, 8, 9, 41, 76, 78, 79, 80, 83], "design": [6, 8], "behavior": [6, 78], "associ": [6, 7, 8, 79, 82], "util": [6, 7, 8, 9, 10, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 76, 79, 80, 81, 83], "what": [6, 7, 9, 18, 19, 24, 25, 74, 78, 79, 80, 81], "cover": [6, 7, 8, 9, 79, 83], "how": [6, 7, 8, 9, 74, 78, 79, 80, 81, 83], "we": [6, 7, 8, 9, 22, 23, 40, 42, 44, 45, 50, 53, 55, 56, 59, 76, 78, 79, 80, 81, 82, 83], "them": [6, 7, 41, 47, 53, 78, 79, 82, 83], "scenario": 6, "full": [6, 7, 8, 33, 36, 39, 53, 76, 81, 82], "compos": 6, "compon": [6, 8, 12, 69, 76, 78, 80, 82, 83], "which": [6, 8, 22, 23, 24, 25, 30, 31, 32, 35, 37, 40, 44, 45, 46, 48, 53, 55, 56, 59, 64, 67, 76, 78, 79, 80, 81, 82, 83], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 20, 47, 51, 52, 53, 55, 56, 71, 78, 79, 80, 81, 82], "recip": [6, 7, 9, 10, 11, 41, 55, 56, 76, 78, 79, 81, 83], "evalu": [6, 8, 74, 76, 80, 82, 83], "gener": [6, 8, 13, 16, 20, 26, 53, 71, 72, 74, 78, 82, 83], "each": [6, 8, 14, 17, 30, 31, 32, 35, 37, 40, 44, 45, 53, 54, 71, 76, 78, 79, 80, 81, 82], "support": [6, 8, 9, 10, 19, 22, 23, 24, 25, 26, 40, 50, 56, 59, 63, 76, 78, 79, 80, 81, 82, 83], "model": [6, 7, 8, 10, 15, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 55, 56, 69, 70, 74, 76, 78, 83], "make": [6, 7, 8, 9, 40, 46, 76, 79, 80, 81, 82, 83], "easi": [6, 8, 76, 82], "understand": [6, 7, 8, 74, 76, 78, 82, 83], "extend": [6, 8, 76], "befor": [6, 21, 45, 46, 50, 55, 79], "let": [6, 7, 9, 78, 79, 80, 81, 82, 83], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 19, 21, 22, 23, 24, 25, 26, 30, 31, 32, 35, 37, 40, 44, 45, 46, 47, 49, 51, 54, 55, 56, 58, 66, 69, 76, 78, 80, 82, 83], "defin": [6, 7, 8, 41, 49, 50, 51, 80, 82], "some": [6, 7, 15, 51, 52, 74, 76, 79, 80, 82, 83], "concept": [6, 79, 80], "In": [6, 7, 8, 44, 50, 66, 67, 79, 81, 82, 83], "ll": [6, 7, 8, 54, 76, 78, 79, 80, 81, 83], "talk": 6, "about": [6, 8, 55, 67, 76, 79, 80, 81, 82, 83], "take": [6, 7, 8, 10, 41, 42, 47, 55, 57, 58, 78, 79, 80, 81, 82, 83], "close": [6, 8, 64, 65, 66, 67, 82], "look": [6, 7, 8, 66, 75, 78, 79, 80, 81, 82], "veri": [6, 45, 79], "simpli": [6, 7, 78, 79, 81, 83], "dictat": 6, "state_dict": [6, 47, 55, 56, 82, 83], "store": [6, 64, 67, 82, 83], "file": [6, 7, 8, 9, 10, 11, 53, 54, 55, 56, 57, 64, 67, 69, 73, 76, 77, 78, 79, 80, 81, 82, 83], "disk": [6, 64], "weight": [6, 8, 30, 31, 32, 33, 35, 36, 37, 39, 40, 47, 49, 50, 55, 56, 67, 74, 79, 80, 81, 82, 83], "string": [6, 22, 23, 24, 25, 26, 49, 53, 54, 58, 59], "kei": [6, 7, 9, 40, 42, 45, 52, 55, 79, 80, 82, 83], "identifi": 6, "state": [6, 8, 47, 51, 52, 55, 56, 79, 81, 82, 83], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 20, 47, 51, 52, 55, 56, 62], "If": [6, 7, 12, 13, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 40, 47, 50, 55, 56, 58, 59, 62, 66, 67, 71, 75, 78, 79, 80, 81, 82], "identif": [6, 54], "don": [6, 7, 8, 67, 71, 79, 80, 81, 83], "t": [6, 7, 8, 26, 54, 59, 67, 71, 79, 80, 81, 83], "match": [6, 75, 79, 81, 82], "up": [6, 8, 9, 22, 23, 78, 80, 81, 82, 83], "exactli": 6, "those": [6, 82], "definit": [6, 82], "either": [6, 55, 82, 83], "run": [6, 7, 9, 11, 41, 42, 45, 47, 55, 56, 66, 67, 75, 76, 80, 81, 82, 83], "explicit": 6, "error": [6, 7, 21, 55, 71], "load": [6, 8, 55, 56, 57, 66, 79, 81, 82], "rais": [6, 10, 12, 19, 21, 26, 40, 45, 55, 56, 59, 62, 67, 71], "an": [6, 7, 8, 9, 10, 13, 21, 22, 23, 24, 25, 26, 40, 45, 49, 51, 52, 55, 56, 67, 76, 78, 79, 80, 81, 82, 83], "except": [6, 19, 78], "wors": 6, "silent": [6, 41], "succe": 6, "infer": [6, 18, 40, 42, 44, 45, 46, 74, 79, 80, 81, 83], "expect": [6, 7, 10, 13, 16, 17, 20, 44, 67, 78, 82], "addit": [6, 7, 8, 10, 55, 56, 59, 62, 64, 66, 67, 70, 76, 80, 82], "line": [6, 8, 57, 80, 81], "also": [6, 7, 8, 9, 10, 40, 45, 50, 58, 67, 75, 78, 79, 80, 81, 82, 83], "need": [6, 7, 8, 9, 17, 26, 40, 41, 45, 66, 67, 75, 78, 79, 80, 81, 82, 83], "shape": [6, 40, 42, 44, 45, 46, 50], "valu": [6, 7, 26, 27, 28, 29, 34, 38, 40, 42, 43, 45, 48, 55, 57, 64, 65, 66, 67, 71, 80, 81, 82], "two": [6, 7, 21, 76, 79, 80, 81, 82, 83], "popular": [6, 76, 78, 79], "llama2": [6, 7, 8, 10, 18, 22, 23, 26, 28, 29, 30, 31, 32, 33, 41, 45, 46, 53, 74, 76, 80, 81], "offici": [6, 18, 80, 81], "implement": [6, 8, 22, 23, 24, 25, 26, 41, 43, 44, 48, 49, 50, 55, 66, 76, 82, 83], "when": [6, 7, 8, 11, 45, 47, 48, 66, 79, 81, 82, 83], "websit": 6, "get": [6, 7, 8, 9, 53, 59, 60, 61, 75, 76, 78, 79, 80, 82], "access": [6, 7, 8, 55, 79, 80], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 19, 20, 40, 55, 56, 79, 80, 81, 82, 83], "pth": [6, 79, 81], "inspect": [6, 79, 82, 83], "content": [6, 53, 78], "easili": [6, 7, 76, 78, 82, 83], "torch": [6, 42, 45, 47, 48, 58, 59, 62, 69, 70, 71, 79, 80, 81, 82, 83], "import": [6, 7, 10, 66, 67, 78, 79, 80, 82, 83], "consolid": [6, 81], "00": [6, 73, 77, 80, 81], "mmap": [6, 79], "true": [6, 7, 22, 23, 24, 25, 32, 33, 36, 39, 47, 53, 54, 55, 56, 62, 66, 78, 79, 81, 82, 83], "weights_onli": 6, "map_loc": [6, 79], "cpu": [6, 8, 47, 59, 75, 79, 83], "tensor": [6, 40, 41, 42, 43, 44, 45, 46, 47, 50, 55, 64, 65, 66, 67, 68, 82, 83], "item": 6, "print": [6, 9, 22, 23, 24, 25, 26, 53, 78, 80, 82, 83], "f": [6, 9, 22, 23, 24, 25, 79, 82, 83], "tok_embed": [6, 45], "size": [6, 8, 10, 22, 23, 24, 25, 40, 42, 43, 44, 45, 46, 61, 76, 78, 79, 80, 81, 82], "32000": [6, 10, 82], "4096": [6, 10, 22, 23, 40, 44, 82], "len": [6, 22, 23, 24, 25, 45], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 30, 31, 32, 35, 43, 44, 47, 48, 53, 54, 55, 57, 58, 59, 60, 67, 69, 71, 75, 76, 79, 80, 81, 82, 83], "contain": [6, 40, 42, 44, 45, 46, 49, 51, 52, 53, 54, 55, 56, 57, 66, 68, 79, 81, 82], "includ": [6, 7, 8, 14, 17, 50, 55, 56, 57, 76, 79, 80, 81, 82, 83], "input": [6, 13, 14, 17, 22, 23, 24, 26, 40, 41, 43, 44, 45, 46, 50, 53, 55, 68, 71, 78, 82, 83], "embed": [6, 40, 42, 43, 44, 45, 81], "tabl": [6, 83], "call": [6, 10, 41, 47, 57, 64, 65, 66, 67, 82, 83], "layer": [6, 8, 30, 31, 32, 33, 35, 36, 37, 39, 40, 45, 46, 50, 76, 81, 82, 83], "have": [6, 7, 10, 40, 42, 49, 57, 66, 69, 75, 78, 79, 80, 81, 82, 83], "dim": [6, 40, 41, 43, 44, 45, 46], "most": [6, 7, 54, 80, 82, 83], "within": [6, 7, 10, 26, 41, 66, 71, 79, 81, 82, 83], "default": [6, 7, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 40, 41, 43, 44, 45, 46, 47, 48, 50, 53, 54, 55, 56, 57, 59, 64, 67, 68, 69, 71, 75, 79, 81, 82, 83], "everi": [6, 8, 41, 66, 75, 83], "config": [6, 9, 10, 11, 12, 40, 55, 57, 67, 76, 78, 79, 81, 82, 83], "repo": [6, 55, 56, 79], "first": [6, 7, 10, 21, 45, 54, 55, 57, 74, 76, 79, 81, 82, 83], "big": [6, 79], "split": [6, 78, 79], "across": [6, 8, 55, 66, 71, 79, 81], "bin": [6, 79], "To": [6, 7, 8, 9, 55, 75, 76, 78, 79, 80, 81, 82, 83], "correctli": [6, 8, 12, 55, 75, 80, 83], "piec": 6, "one": [6, 8, 21, 41, 53, 78, 79, 80, 81, 83], "pytorch_model": [6, 79], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 40, 44, 45, 46, 50, 51, 53, 56, 57, 59, 79, 80, 81, 82, 83], "doe": [6, 19, 49, 55, 57, 79], "fewer": [6, 40], "sinc": [6, 7, 10, 41, 55, 79, 81], "instead": [6, 8, 41, 42, 50, 79, 81, 82], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 20, 49, 52, 54, 55, 56, 57, 58, 64, 65, 66, 67, 79, 81], "caus": [6, 53], "try": [6, 7, 79, 80, 81, 83], "same": [6, 7, 30, 31, 32, 35, 40, 42, 46, 53, 57, 67, 79, 81, 82, 83], "As": [6, 7, 8, 9, 50, 76, 79, 81, 83], "re": [6, 7, 54, 76, 79, 80, 81, 82], "care": [6, 41, 55, 79, 81, 82], "like": [6, 7, 8, 9, 75, 78, 79, 80, 82], "end": [6, 8, 54, 74, 76, 81, 82], "number": [6, 8, 22, 23, 26, 40, 42, 45, 48, 55, 56, 61, 71, 80, 82], "just": [6, 13, 76, 78, 80, 81, 82], "save": [6, 8, 9, 47, 55, 56, 67, 74, 79, 81, 82], "less": [6, 26, 79, 80, 81, 83], "prone": 6, "manag": [6, 69], "invari": 6, "accept": [6, 7, 26, 53, 80, 83], "multipl": [6, 7, 8, 50, 64, 65, 66, 67, 80, 81], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 78, 79], "worri": [6, 80], "explicitli": [6, 49, 76, 82], "convert": [6, 55, 68, 79, 83], "time": [6, 53, 64, 66, 79, 81, 83], "produc": [6, 83], "back": [6, 21, 55, 82, 83], "origin": [6, 22, 23, 47, 50, 78, 79, 81, 82, 83], "form": [6, 7, 8, 21], "One": [6, 79], "advantag": [6, 82], "being": [6, 55, 56, 58, 83], "should": [6, 7, 8, 14, 17, 18, 19, 30, 31, 32, 35, 37, 40, 41, 49, 57, 64, 65, 66, 67, 75, 76, 78, 79, 80, 81, 82, 83], "abl": [6, 8, 78, 79, 80, 81], "fine": [6, 8, 9, 74, 76, 79, 82], "post": [6, 83], "tool": [6, 79, 80], "quantiz": [6, 30, 31, 32, 33, 35, 36, 37, 39, 50, 74, 80, 83], "eval": [6, 74, 76], "without": [6, 7, 9, 75, 76, 78, 79, 82], "code": [6, 8, 45, 72, 76, 78, 80], "chang": [6, 7, 9, 13, 75, 78, 79, 80, 81, 82, 83], "OR": 6, "convers": [6, 14, 15, 18, 19, 21, 26, 55, 76, 78, 79, 81, 82, 83], "script": [6, 9, 79, 80, 81], "wai": [6, 7, 78, 79, 80, 81], "surround": [6, 8, 76], "load_checkpoint": [6, 8, 55, 56], "save_checkpoint": [6, 8, 9, 55, 56], "method": [6, 7, 8, 9, 11, 22, 23, 24, 25, 26, 47, 49, 51, 57, 75, 76, 79, 81, 82, 83], "convertor": 6, "avail": [6, 8, 57, 58, 59, 76, 79, 81, 82], "here": [6, 7, 9, 15, 24, 43, 44, 78, 79, 80, 81, 82, 83], "three": [6, 8, 80], "hfcheckpoint": 6, "read": [6, 55, 56, 76], "write": [6, 8, 55, 56, 64, 78, 80], "compat": [6, 55], "transform": [6, 8, 30, 31, 32, 35, 37, 45, 46, 48, 82], "framework": [6, 8, 76], "mention": [6, 79, 83], "abov": [6, 47, 75, 79, 81, 82, 83], "assum": [6, 13, 16, 17, 20, 48, 51, 54, 59, 79, 82], "checkpoint_dir": [6, 7, 55, 56, 79, 81], "necessari": [6, 26, 64, 65, 66, 67, 82], "json": [6, 55, 69, 79], "easiest": [6, 79, 80], "sure": [6, 7, 79, 80, 81, 82, 83], "everyth": [6, 8, 57, 76, 80], "flow": [6, 83], "By": [6, 81, 82, 83], "safetensor": 6, "output": [6, 17, 22, 23, 24, 26, 30, 31, 32, 35, 37, 40, 41, 43, 44, 45, 46, 50, 52, 65, 69, 75, 78, 79, 80, 81, 82, 83], "dir": [6, 67, 75, 79, 80, 81], "output_dir": [6, 7, 55, 56, 69, 79, 81, 82, 83], "specifi": [6, 7, 8, 10, 40, 67, 78, 79, 80, 81, 83], "argument": [6, 7, 10, 17, 26, 33, 36, 39, 40, 57, 62, 64, 66, 67, 70, 82], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 45, 70, 79, 82, 83], "_component_": [6, 7, 9, 10, 78, 79, 81, 82], "fullmodelhfcheckpoint": [6, 79], "directori": [6, 7, 55, 56, 64, 66, 67, 79, 80, 81], "sort": [6, 55], "id": [6, 22, 23, 26, 53, 54, 55, 68, 79], "so": [6, 7, 55, 57, 75, 76, 79, 80, 81, 82, 83], "order": [6, 8, 55, 66, 67, 80], "matter": [6, 55, 82], "checkpoint_fil": [6, 7, 9, 55, 56, 79, 81, 82, 83], "restart": 6, "previou": [6, 55, 56], "more": [6, 7, 8, 42, 44, 57, 67, 69, 71, 76, 78, 79, 80, 81, 82, 83], "next": [6, 81, 83], "section": [6, 8, 74, 79, 81, 83], "recipe_checkpoint": [6, 55, 56], "null": [6, 7], "usual": [6, 44, 55, 67, 79, 82], "model_typ": [6, 55, 56, 79, 81], "resume_from_checkpoint": [6, 55, 56], "fals": [6, 7, 22, 23, 24, 25, 26, 30, 31, 32, 33, 35, 36, 37, 39, 40, 50, 53, 54, 55, 56, 69, 79, 81, 82, 83], "requir": [6, 7, 26, 55, 66, 67, 71, 75, 78, 80, 83], "param": [6, 8, 30, 31, 32, 35, 50, 51, 52, 55, 82, 83], "directli": [6, 7, 8, 10, 55, 78, 79, 80, 81, 82, 83], "help": [6, 18, 45, 55, 57, 74, 75, 76, 78, 79, 80, 81, 83], "ensur": [6, 7, 12, 21, 26, 40, 55, 59, 76, 80], "out": [6, 7, 8, 22, 23, 24, 25, 55, 56, 74, 76, 79, 80, 81, 82, 83], "case": [6, 8, 9, 40, 55, 59, 64, 76, 78, 79, 81, 82, 83], "discrep": [6, 55], "along": [6, 81, 82], "detail": [6, 42, 69, 71, 79, 80, 81, 82, 83], "found": [6, 7, 9, 43, 44, 82, 83], "metacheckpoint": 6, "github": [6, 10, 30, 31, 32, 35, 40, 43, 44, 48, 75, 80], "repositori": [6, 18, 79, 80], "fullmodelmetacheckpoint": [6, 81], "torchtunecheckpoint": 6, "perform": [6, 41, 76, 79, 81, 83], "current": [6, 40, 44, 45, 46, 56, 61, 64, 66, 71, 79, 80, 81], "test": [6, 7, 8, 75, 76], "complet": [6, 8, 78, 79, 80, 81], "written": [6, 7, 8, 55, 56, 64, 65, 66, 67, 76], "begin": [6, 53, 54, 81, 83], "partit": [6, 83], "ha": [6, 49, 51, 53, 78, 79, 80, 81, 82, 83], "standard": [6, 65, 76, 79, 81], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 79], "inform": [6, 67, 76, 79, 80, 81], "subsequ": [6, 8], "recipe_st": [6, 55, 56], "pt": [6, 9, 55, 56, 79, 81], "epoch": [6, 8, 9, 48, 55, 56, 79, 80, 81], "optim": [6, 7, 8, 48, 79, 80, 81, 82, 83], "etc": [6, 8, 55, 80], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 45, 49, 53, 55, 69, 71, 78, 79, 82, 83], "updat": [6, 7, 8, 75, 79, 80, 81, 82, 83], "hf_model_0001_0": [6, 79], "hf_model_0002_0": [6, 79], "both": [6, 79, 82, 83], "adapt": [6, 49, 50, 51, 52, 55, 56, 79, 82, 83], "merg": [6, 10, 55, 79, 81, 83], "would": [6, 7, 9, 45, 75, 78, 79, 82, 83], "our": [6, 8, 76, 78, 79, 80, 82, 83], "tutori": [6, 76, 78, 79, 80, 81, 82, 83], "primari": [6, 7, 8, 80], "want": [6, 7, 8, 9, 10, 75, 79, 80, 81, 82], "resum": [6, 8, 48, 55, 56, 83], "initi": [6, 8, 11, 27, 28, 29, 34, 38, 62, 80, 82, 83], "frozen": [6, 82, 83], "base": [6, 10, 26, 30, 31, 32, 33, 35, 36, 37, 39, 44, 48, 50, 52, 55, 57, 64, 74, 79, 80, 81, 82, 83], "well": [6, 7, 8, 76, 78, 79, 81, 83], "learnt": [6, 79], "someth": [6, 8, 9, 79], "NOT": 6, "refer": [6, 7, 8, 43, 44, 76, 82], "adapter_checkpoint": [6, 55, 56], "adapter_0": [6, 79], "now": [6, 53, 78, 79, 80, 81, 82, 83], "knowledg": 6, "creat": [6, 7, 10, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 48, 55, 56, 64, 66, 78, 79, 81, 83], "simpl": [6, 8, 74, 80, 82, 83], "forward": [6, 8, 40, 41, 43, 44, 45, 46, 50, 81, 82, 83], "13b": [6, 28, 30, 32], "modeltyp": [6, 55, 56], "llama2_13b": [6, 30, 32], "right": [6, 55, 79, 81, 82], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 82], "successfulli": [6, 80], "vocab": [6, 10, 45, 81], "70": 6, "x": [6, 40, 41, 43, 44, 45, 46, 50, 82, 83], "randint": 6, "0": [6, 8, 31, 33, 40, 45, 48, 50, 53, 66, 67, 68, 71, 73, 77, 79, 80, 81, 82, 83], "no_grad": 6, "6": [6, 43, 68, 79, 83], "3989": 6, "9": [6, 79, 83], "0531": 6, "2375": 6, "5": [6, 48, 68, 69, 79, 80, 81], "2822": 6, "4": [6, 26, 40, 68, 76, 79, 81, 82, 83], "4872": 6, "7469": 6, "8": [6, 22, 23, 24, 25, 30, 31, 32, 33, 35, 36, 37, 39, 79, 82, 83], "6737": 6, "11": [6, 79, 81, 83], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 68], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 67, 78, 79, 80, 81, 82], "find": [6, 8, 9, 79, 80, 82], "list": [6, 7, 14, 15, 18, 19, 21, 22, 23, 26, 30, 31, 32, 33, 35, 36, 37, 39, 49, 50, 53, 54, 55, 56, 57, 60, 63, 68, 78, 80, 81], "builder": [6, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 78, 83], "hope": 6, "deeper": [6, 80], "insight": [6, 79], "happi": [6, 79], "thi": [7, 8, 9, 10, 22, 23, 26, 40, 41, 44, 45, 46, 47, 48, 49, 53, 55, 56, 57, 58, 59, 64, 66, 67, 69, 71, 74, 75, 76, 78, 79, 80, 81, 82, 83], "guid": [7, 9, 76, 78, 80, 82], "yaml": [7, 8, 10, 11, 57, 67, 76, 79, 80, 81, 82, 83], "pars": [7, 10, 54, 57, 80], "effect": 7, "cli": [7, 9, 11, 75, 79, 80], "prerequisit": [7, 78, 79, 80, 81, 82, 83], "Be": [7, 79, 80, 81, 82, 83], "familiar": [7, 79, 80, 81, 82, 83], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 75, 78, 80], "instal": [7, 9, 66, 67, 74, 79, 80, 81, 82, 83], "fundament": 7, "There": [7, 14, 21, 79, 80, 81, 82], "entri": [7, 8, 80], "point": [7, 8, 78, 79, 80, 81, 82, 83], "locat": [7, 81, 82, 83], "thei": [7, 8, 45, 57, 78, 82], "truth": [7, 79, 81], "reproduc": 7, "overridden": [7, 41, 57], "quick": 7, "experiment": 7, "modifi": [7, 8, 9, 47, 76, 79, 81, 82, 83], "serv": [7, 78, 82], "particular": [7, 26, 78, 82, 83], "seed": [7, 8, 9, 71, 80], "shuffl": 7, "devic": [7, 8, 58, 59, 79, 80, 81, 82], "cuda": [7, 58, 59, 75, 79, 83], "dtype": [7, 8, 42, 47, 59, 63, 79, 83], "fp32": [7, 83], "enable_fsdp": 7, "mani": [7, 78, 79], "object": [7, 10, 14, 15, 18, 19, 40, 78], "keyword": [7, 10, 26, 47], "loss": [7, 8, 22, 23, 24, 25, 80, 82, 83], "function": [7, 8, 10, 11, 40, 41, 47, 58, 61, 71, 76, 78, 83], "exampl": [7, 8, 9, 10, 11, 15, 18, 19, 22, 23, 24, 25, 26, 40, 49, 53, 55, 56, 66, 67, 68, 72, 73, 75, 77, 78, 79, 81, 82, 83], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 79], "path": [7, 8, 9, 10, 22, 23, 24, 25, 26, 53, 54, 55, 56, 57, 69, 79, 81, 82], "normal": [7, 43, 45, 46, 53, 78, 82, 83], "python": [7, 54, 57, 60, 67, 71, 72, 79], "alpaca_dataset": [7, 22, 78], "custom": [7, 8, 76, 79, 80, 81, 82], "train_on_input": [7, 22, 23, 24, 25, 26, 78], "onc": [7, 79, 80, 81, 82, 83], "ve": [7, 42, 54, 78, 79, 81, 82], "instanc": [7, 10, 41, 47, 51, 52, 82], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 79, 83], "under": [7, 79, 81, 83], "preced": [7, 10, 81, 82], "actual": [7, 9], "throw": 7, "notic": [7, 78, 82], "miss": [7, 82], "posit": [7, 10, 40, 44, 45, 46, 81], "anoth": [7, 79], "handl": [7, 11, 53, 79, 82, 83], "def": [7, 8, 9, 11, 78, 82, 83], "dictconfig": [7, 8, 10, 11, 12, 67], "arg": [7, 10, 45, 47, 49, 57, 65], "tupl": [7, 10, 26, 47, 53, 54, 57, 61, 68], "kwarg": [7, 10, 47, 49, 57, 62, 64, 65, 66, 67, 70], "str": [7, 10, 13, 16, 17, 20, 22, 23, 24, 25, 26, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 69, 71], "mean": [7, 43, 80, 82], "pass": [7, 10, 40, 41, 47, 59, 62, 66, 67, 70, 82, 83], "add": [7, 9, 54, 57, 78, 79, 81, 82, 83], "d": [7, 40, 45, 46, 54, 78, 82], "llama2_token": [7, 79], "tmp": [7, 80, 81], "option": [7, 8, 13, 16, 17, 20, 30, 31, 32, 35, 37, 40, 44, 45, 46, 47, 53, 54, 55, 56, 58, 59, 60, 64, 67, 69, 70, 71, 75, 76, 79], "bool": [7, 22, 23, 24, 25, 26, 30, 31, 32, 33, 35, 36, 37, 39, 47, 50, 53, 54, 55, 56, 62, 66, 69, 83], "max_seq_len": [7, 10, 22, 23, 26, 40, 42, 44, 45, 53, 54, 78], "int": [7, 9, 22, 23, 26, 30, 31, 32, 33, 35, 36, 37, 39, 40, 42, 43, 44, 45, 48, 50, 53, 54, 55, 56, 61, 64, 65, 66, 67, 68, 71, 78, 82, 83], "512": [7, 22, 23, 78, 83], "instructdataset": [7, 22, 23, 24, 25, 78], "alreadi": [7, 62, 75, 79, 82], "overwrit": [7, 75], "duplic": [7, 8, 76], "sometim": 7, "than": [7, 21, 26, 40, 79, 80, 81, 82, 83], "resolv": [7, 80], "alpaca": [7, 13, 22, 23, 30, 31, 32, 35, 78], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 64, 65, 66, 67], "disklogg": 7, "log_dir": [7, 64, 66, 67], "conveni": [7, 8], "quickli": [7, 78], "verifi": [7, 58, 59, 80, 82], "properli": 7, "experi": [7, 67, 74, 76, 81, 82], "wa": [7, 79, 81, 82, 83], "cp": [7, 75, 79, 80, 81], "7b_lora_single_devic": [7, 79, 80, 82, 83], "my_config": 7, "discuss": [7, 80, 82], "guidelin": 7, "while": [7, 8, 30, 31, 32, 35, 41, 76, 79, 83], "mai": [7, 9, 69, 78, 80, 82], "tempt": 7, "put": [7, 8, 80, 82], "much": [7, 79, 81, 82, 83], "give": [7, 82], "maximum": [7, 22, 23, 26, 40, 42, 44, 45, 54], "flexibl": [7, 78], "switch": 7, "encourag": [7, 82], "clariti": 7, "significantli": 7, "easier": [7, 79, 80], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 83], "expos": [7, 8, 78, 80], "parent": 7, "modul": [7, 10, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 70, 71, 80, 82, 83], "__init__": [7, 8, 82, 83], "py": [7, 10, 30, 31, 32, 35, 40, 42, 43, 44, 48, 79, 81], "guarante": 7, "stabil": [7, 76, 83], "underscor": 7, "_alpaca": 7, "collect": [7, 80], "differ": [7, 9, 53, 76, 79, 81, 82, 83], "itself": 7, "via": [7, 9, 50, 82, 83], "pair": [7, 68, 78], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 79, 80, 81, 82, 83], "checkpoint": [7, 8, 47, 54, 55, 56, 67, 70, 76, 81, 82, 83], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 40, 41, 42, 43, 44, 45, 46, 49, 50, 51, 52, 53, 54, 55, 56, 57, 64, 65, 66, 67, 78, 80, 82, 83], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 40, 44, 45, 46], "core": [8, 76, 78, 80, 83], "i": [8, 18, 19, 47, 52, 54, 78, 79, 81, 83], "structur": [8, 14, 15, 18, 19, 78, 79], "new": [8, 38, 64, 66, 79, 80, 81, 82, 83], "user": [8, 14, 15, 18, 19, 21, 40, 53, 78, 80], "thought": [8, 76, 80, 83], "target": [8, 76], "pipelin": [8, 76], "llm": [8, 74, 76, 78, 79, 82], "eg": [8, 45, 55, 76], "meaning": [8, 76, 79], "featur": [8, 9, 75, 76, 79, 80], "fsdp": [8, 76, 80, 81], "activ": [8, 41, 70, 76, 83], "gradient": [8, 76, 79, 81, 82, 83], "accumul": [8, 76], "mix": [8, 79], "precis": [8, 47, 59, 76, 80, 83], "appli": [8, 30, 31, 32, 33, 35, 36, 37, 39, 40, 43, 44, 45, 46, 76, 83], "given": [8, 10, 17, 21, 50, 58, 59, 76, 82], "complex": 8, "becom": [8, 75, 78], "harder": 8, "anticip": 8, "architectur": [8, 18, 19, 45, 78], "methodolog": 8, "reason": [8, 79], "possibl": 8, "trade": 8, "off": [8, 53, 79], "memori": [8, 22, 23, 47, 74, 76, 78, 79, 80, 81], "vs": [8, 80], "qualiti": [8, 79, 82], "believ": 8, "best": 8, "suit": [8, 80], "specif": [8, 10, 78, 79, 83], "b": [8, 40, 44, 45, 46, 50, 67, 82, 83], "fit": [8, 22, 23], "solut": 8, "result": [8, 53, 79, 81, 82, 83], "meant": [8, 47], "depend": [8, 9, 13, 79, 82, 83], "level": [8, 60, 76, 83], "expertis": 8, "routin": 8, "yourself": [8, 81, 82], "exist": [8, 75, 78, 79, 80, 81, 83], "ad": [8, 53, 82, 83], "ones": 8, "modular": [8, 76], "build": [8, 76, 81, 82], "block": [8, 30, 31, 32, 35, 37, 76], "wandb": [8, 9, 67, 80], "log": [8, 60, 64, 65, 66, 67, 79, 80, 81, 83], "fulli": 8, "nativ": [8, 74, 76, 82, 83], "pytorch": [8, 45, 47, 66, 69, 71, 74, 75, 76, 81, 82, 83], "correct": [8, 16, 24, 43, 44, 45, 58, 76, 78], "numer": [8, 76], "pariti": [8, 76], "verif": 8, "extens": [8, 76], "comparison": [8, 82, 83], "benchmark": [8, 71, 76, 79, 81, 82], "limit": 8, "hidden": [8, 41], "behind": 8, "100": [8, 22, 23, 24, 25, 26, 68, 69, 82, 83], "flag": [8, 22, 23, 24, 25, 83], "prefer": [8, 76, 78], "over": [8, 48, 57, 76, 78, 79, 81, 82, 83], "unnecessari": 8, "abstract": [8, 14, 17, 76, 80, 83], "No": [8, 76], "inherit": [8, 57, 76], "go": [8, 18, 19, 53, 76, 78, 79, 80, 83], "upon": [8, 81], "figur": [8, 82, 83], "spectrum": 8, "decid": 8, "interact": [8, 74, 80], "start": [8, 9, 54, 75, 76, 78, 79, 80], "paradigm": 8, "consist": [8, 80], "configur": [8, 22, 23, 24, 25, 26, 46, 76, 80, 81, 82, 83], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 62, 64, 65, 66, 67, 68, 69, 70, 71, 74, 76, 79, 80, 81, 82, 83], "overrid": [8, 11, 79, 80, 81, 83], "togeth": [8, 67, 80, 82], "valid": [8, 21, 75, 79, 80], "environ": [8, 75, 79, 80], "logic": [8, 55, 76, 80, 82], "api": [8, 9, 33, 36, 39, 79, 80, 81, 83], "closer": [8, 82], "monolith": [8, 76], "trainer": [8, 61], "A": [8, 9, 47, 50, 53, 54, 55, 57, 68, 73, 74, 77, 79, 82, 83], "wrapper": [8, 53, 54, 82], "around": [8, 53, 54, 69, 79, 82, 83], "extern": 8, "primarili": [8, 82], "eleutherai": [8, 76, 82], "har": [8, 76, 82], "control": [8, 22, 23, 24, 25, 71, 79], "multi": [8, 40, 81], "stage": 8, "distil": 8, "oper": [8, 69, 71], "turn": [8, 21, 54], "dataload": [8, 22, 23, 24, 25], "applic": [8, 40, 55, 56, 67], "clean": [8, 9, 22], "after": [8, 42, 43, 64, 65, 66, 67, 83], "process": [8, 9, 47, 71, 80, 83], "group": [8, 40, 64, 65, 66, 67, 81], "init_process_group": [8, 62], "backend": 8, "gloo": 8, "els": [8, 57, 67, 76, 83], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 57, 78, 80, 81, 82], "stuff": 8, "carri": 8, "relev": [8, 79, 82], "interfac": [8, 14, 17, 49], "metric": [8, 80], "logger": [8, 60, 64, 65, 66, 67, 80], "self": [8, 9, 30, 31, 32, 35, 37, 40, 45, 46, 49, 78, 82, 83], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 69, 70], "_model": 8, "_setup_model": 8, "_token": [8, 78], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 83], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 71, 81], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 22, 23, 24, 25, 40, 42, 44, 45, 46, 53, 68, 76, 78, 80, 81, 82], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 22, 23, 26, 68], "total_training_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 64, 65, 66, 67], "step": [8, 45, 48, 54, 64, 65, 66, 67, 69, 74, 79, 82, 83], "learn": [8, 48, 76, 78, 80, 81, 82, 83], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 19, 20, 21, 40, 44, 45, 46, 52, 53, 54, 55, 56, 58, 59, 60, 64, 65, 66, 67, 70, 71, 79], "fullfinetunerecip": 8, "direct": [8, 75], "wandblogg": [9, 82, 83], "workspac": 9, "seen": [9, 82, 83], "screenshot": 9, "below": [9, 44, 78, 81, 82, 83], "packag": [9, 66, 67, 75], "pip": [9, 66, 67, 75, 79, 81], "Then": [9, 80], "login": [9, 67, 79], "built": [9, 75, 78, 80, 83], "project": [9, 30, 31, 32, 35, 37, 40, 41, 67, 74, 82, 83], "grab": [9, 81], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": 9, "exit": [9, 75], "resourc": [9, 64, 65, 66, 67], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 26, 78, 79], "desir": 9, "suggest": 9, "approach": [9, 78], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 79], "_output_dir": [9, 55, 56], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 40, 42, 43, 44, 45, 46, 47, 50, 51, 53, 54, 55, 56, 59, 60, 61, 62, 69, 79, 82, 83], "descript": 9, "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 22, 23, 24, 25], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 27, 28, 29, 30, 31, 32, 33, 35, 36, 38, 39, 40, 43, 44, 48, 55, 56, 57, 60, 66, 67, 69, 71, 75, 79], "com": [10, 30, 31, 32, 35, 40, 43, 44, 48, 75], "facebookresearch": [10, 43, 44], "blob": [10, 30, 31, 32, 35, 40, 43, 44, 48], "main": [10, 11, 40, 43, 44, 75, 79, 81], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 45], "32": [10, 81, 82, 83], "num_head": [10, 40, 42, 44, 45], "num_kv_head": [10, 40, 42], "vocab_s": 10, "must": [10, 22, 23, 24, 25, 26, 49, 54, 57, 83], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 40, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 58, 59, 60, 61, 62, 63, 68, 69, 71, 78, 82, 83], "nn": [10, 40, 41, 42, 45, 46, 47, 49, 51, 52, 70, 82, 83], "parsed_yaml": 10, "embed_dim": [10, 40, 44, 46, 82], "valueerror": [10, 19, 21, 26, 40, 45, 55, 56, 59, 71], "callabl": [11, 45], "With": [11, 79, 82, 83], "my_recip": 11, "foo": 11, "bar": [11, 76, 80], "instanti": [12, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38], "configerror": 12, "cannot": [12, 81], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 64, 65, 66, 67, 78, 79, 83], "prompt": [13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 45, 53, 78, 79, 81], "templat": [13, 14, 16, 17, 20, 22, 23, 24, 25, 26], "style": [13, 22, 23, 26, 83], "slightli": 13, "instruct": [13, 15, 17, 19, 22, 23, 74, 80, 81, 82, 83], "classmethod": [13, 14, 15, 16, 17, 18, 19, 20], "map": [13, 16, 17, 20, 52, 55, 64, 65, 66, 67, 79, 82], "column_map": [13, 16, 17, 20, 78], "placehold": [13, 14, 16, 17, 20], "column": [13, 16, 17, 20], "ident": [13, 16, 17, 19, 20, 79], "chat": [14, 15, 18, 26], "role": [14, 53, 78], "system": [14, 15, 18, 19, 21, 53, 78], "assist": [14, 15, 18, 21, 53, 78], "messag": [14, 15, 18, 19, 21, 53, 54, 75, 78], "accord": [14, 19], "openai": 15, "markup": 15, "languag": [15, 50, 82], "It": [15, 19, 78, 83], "huggingfac": [15, 48, 55, 56, 78, 79], "im_start": 15, "context": [15, 69, 78], "im_end": 15, "goe": 15, "respons": [15, 53, 78, 79, 80, 81], "appropri": [15, 18, 19, 48, 78, 83], "tag": [15, 18, 19, 54, 64, 65, 66, 67], "grammar": [16, 24, 78], "sentenc": 16, "alwai": [17, 57], "human": 18, "taken": [18, 82, 83], "inst": [18, 19, 78], "sy": [18, 78], "respect": [18, 52, 78], "honest": [18, 78], "am": [18, 19, 78, 79, 81], "pari": [18, 19, 78], "capit": [18, 19, 78], "franc": [18, 19, 78], "known": [18, 19, 53, 78], "its": [18, 19, 71, 78, 79, 81, 82], "stun": [18, 19, 78], "mistral": [19, 37, 38, 39, 79, 80], "llama2chatformat": [19, 78], "summar": [20, 25, 78], "task": [20, 78, 79, 81, 82, 83], "dialogu": [20, 25], "dialog": 20, "forth": 21, "consecut": 21, "come": [21, 49, 82], "empti": 21, "shorter": 21, "length": [21, 22, 23, 26, 40, 42, 44, 45, 46, 53, 54, 56, 68], "min": [21, 82], "invalid": 21, "yahma": 22, "codebas": [22, 23, 24, 25, 79], "where": [22, 23, 24, 25, 40, 45, 50, 53, 78], "mask": [22, 23, 24, 25, 40, 46, 53, 54, 78], "contribut": [22, 23, 24, 25], "replac": [22, 23, 24, 25, 47, 82], "encod": [22, 23, 24, 25, 26, 53, 54], "decod": [22, 23, 24, 25, 26, 45, 53, 54], "anyth": [22, 23, 24, 25, 26], "load_dataset": [22, 23, 24, 25, 26], "whether": [22, 23, 24, 25, 26, 30, 31, 32, 35, 37, 47, 50, 53, 54, 59], "recommend": [22, 23, 66, 79, 83], "highest": [22, 23], "sequenc": [22, 23, 26, 40, 42, 44, 45, 46, 53, 54, 68], "alpaca_d": [22, 23], "batch_siz": [22, 23, 24, 25, 40, 46, 79], "tatsu": [23, 78], "lab": [23, 78], "liweili": 24, "c4_200m": 24, "variant": [24, 25], "mirror": [24, 25], "llama_recip": [24, 25], "grammar_d": 24, "samsum": 25, "summari": 25, "samsum_d": 25, "open": [26, 27, 78, 79], "orca": [26, 78], "slimorca": [26, 78], "dedup": [26, 78], "1024": [26, 78], "chatdataset": 26, "adher": 26, "doesn": [26, 79], "prescrib": 26, "truncat": [26, 53, 54], "least": [26, 81, 82], "though": 26, "max": [26, 45, 48, 53, 82], "ds": 26, "10": [26, 68, 79, 81, 83], "351": 26, "82": [26, 79], "391": 26, "221": 26, "220": 26, "193": 26, "12": [26, 75], "471": 26, "gemma": 27, "transformerdecod": [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 82], "w": [27, 28, 29, 34, 38, 66, 67, 79, 82, 83], "blog": 27, "technolog": 27, "develop": [27, 83], "arxiv": [28, 29, 33, 36, 39, 40, 43, 44], "org": [28, 29, 33, 36, 39, 40, 43, 44, 57, 60, 66, 69, 71, 75], "ab": [28, 29, 33, 36, 39, 44], "2307": [28, 29], "09288": [28, 29], "lora_attn_modul": [30, 31, 32, 33, 35, 36, 37, 39, 82, 83], "liter": [30, 31, 32, 33, 35, 36, 37, 39], "q_proj": [30, 31, 32, 33, 35, 36, 37, 39, 40, 82, 83], "k_proj": [30, 31, 32, 33, 35, 36, 37, 39, 40, 82, 83], "v_proj": [30, 31, 32, 33, 35, 36, 37, 39, 40, 82, 83], "output_proj": [30, 31, 32, 33, 35, 36, 37, 39, 40, 82, 83], "apply_lora_to_mlp": [30, 31, 32, 33, 35, 36, 37, 39, 82], "apply_lora_to_output": [30, 31, 32, 33, 35, 36, 37, 39, 82], "lora_rank": [30, 31, 32, 33, 35, 36, 37, 39, 82], "lora_alpha": [30, 31, 32, 33, 35, 36, 37, 39, 82], "float": [30, 31, 32, 33, 35, 36, 37, 39, 40, 43, 48, 50, 64, 65, 66, 67, 82, 83], "16": [30, 31, 32, 33, 35, 36, 37, 39, 82, 83], "quantize_bas": [30, 31, 32, 33, 35, 36, 37, 39, 50, 83], "lora": [30, 31, 32, 33, 35, 36, 37, 39, 50, 74, 76, 80, 81], "tloen": [30, 31, 32, 35], "8bb8579e403dc78e37fe81ffbb253c413007323f": [30, 31, 32, 35], "l41": [30, 31, 32, 35], "l43": [30, 31, 32, 35], "linear": [30, 31, 32, 33, 35, 36, 37, 39, 45, 49, 50, 82, 83], "attent": [30, 31, 32, 35, 37, 40, 42, 44, 45, 46, 81, 82, 83], "mlp": [30, 31, 32, 35, 37, 45, 46, 81, 82], "final": [30, 31, 32, 35, 37, 41, 45, 54, 79, 81, 82, 83], "rank": [30, 31, 32, 35, 37, 50, 61, 71, 80, 82, 83], "low": [30, 31, 32, 35, 37, 50, 79, 82, 83], "approxim": [30, 31, 32, 35, 37, 50, 82], "scale": [30, 31, 32, 35, 37, 50, 82, 83], "factor": [30, 31, 32, 35, 37, 50, 79], "lora_dropout": [31, 33], "05": [31, 33], "llama2_7b": [31, 82], "qlora": [33, 36, 39, 47, 74, 76, 81, 82], "per": [33, 36, 39, 42, 47, 81, 83], "paper": [33, 36, 39, 82, 83], "2305": [33, 36, 39, 40], "14314": [33, 36, 39], "lora_llama2_7b": [33, 82], "llama3": [34, 35, 36, 74], "llama3_8b": 35, "lora_llama3_8b": 36, "announc": 38, "lora_mistral_7b": 39, "head_dim": [40, 42, 44, 45], "pos_embed": [40, 82], "kv_cach": 40, "kvcach": [40, 45], "attn_dropout": [40, 45], "head": [40, 42, 44, 45, 81], "queri": [40, 42, 45, 81], "gqa": 40, "introduc": [40, 43, 50, 82, 83], "pdf": [40, 43], "13245v1": 40, "version": [40, 75, 81, 83], "multihead": 40, "mha": [40, 45], "n": [40, 53, 54, 73, 77, 78], "extrem": 40, "share": [40, 78, 79], "mqa": 40, "credit": 40, "document": 40, "lightn": 40, "lit": 40, "gpt": [40, 79], "lit_gpt": 40, "v": [40, 45, 82], "k": [40, 82], "q": [40, 82], "n_kv_head": 40, "dimens": [40, 42, 44, 45, 50, 81, 82, 83], "calcul": [40, 81], "e": [40, 47, 49, 52, 75, 79, 81, 82, 83], "g": [40, 49, 81, 82, 83], "rotarypositionalembed": [40, 82], "cach": [40, 42, 44, 75], "comput": [40, 41, 44, 45, 71, 79, 83], "rope": [40, 44], "dropout": [40, 50, 82, 83], "onto": 40, "scaled_dot_product_attent": 40, "input_po": [40, 44, 45, 46], "seq_length": [40, 46], "seq_len": [40, 44], "bigger": 40, "n_h": [40, 44], "num": [40, 44], "n_kv": 40, "kv": [40, 42, 45], "emb": [40, 45, 46], "h_d": [40, 44], "gate_proj": 41, "down_proj": 41, "up_proj": 41, "silu": 41, "feed": [41, 46], "network": [41, 82, 83], "deriv": [41, 45, 46], "fed": 41, "multipli": 41, "subclass": [41, 57], "although": [41, 82], "afterward": 41, "former": 41, "regist": [41, 47, 83], "hook": [41, 47, 83], "latter": 41, "max_batch_s": 42, "standalon": 42, "past": 42, "becaus": [42, 45, 79, 81], "expand": 42, "dpython": [42, 47], "ep": 43, "1e": 43, "06": [43, 82], "root": [43, 66, 67], "squar": 43, "1910": 43, "07467": 43, "verfic": [43, 44], "small": [43, 79], "avoid": [43, 47, 71, 83], "divis": 43, "zero": [43, 79, 81], "10000": 44, "rotari": [44, 81], "propos": 44, "2104": 44, "09864": 44, "l450": 44, "upto": 44, "init": [44, 67, 83], "exceed": 44, "freq": 44, "recomput": 44, "geometr": 44, "progress": [44, 80], "rotat": 44, "angl": 44, "bsz": 44, "todo": 44, "made": [44, 79], "effici": [44, 74, 76, 79, 80, 82], "transformerdecoderlay": 45, "norm": [45, 46], "move": 45, "space": 45, "check": [45, 59, 74, 79, 80, 82], "belong": 45, "reduc": [45, 76, 78, 82, 83], "statement": 45, "improv": [45, 79, 81, 82], "readabl": [45, 79], "At": 45, "arang": 45, "prompt_length": 45, "causal_mask": 45, "m_": 45, "seq": 45, "attn": [46, 82, 83], "causalselfattent": [46, 82], "sa_norm": 46, "mlp_norm": 46, "ff": 46, "common_util": 47, "bfloat16": [47, 79, 80, 81, 82], "offload_to_cpu": 47, "nf4": [47, 83], "restor": 47, "higher": [47, 81, 83], "offload": [47, 83], "increas": [47, 48, 81, 82], "peak": [47, 79, 81, 82, 83], "gpu": [47, 79, 80, 81, 82, 83], "usag": [47, 75, 79, 80, 81, 83], "_register_state_dict_hook": 47, "m": [47, 54], "mymodul": 47, "_after_": 47, "nf4tensor": [47, 83], "unquant": [47, 79, 83], "unus": 47, "num_warmup_step": 48, "num_training_step": 48, "num_cycl": 48, "last_epoch": 48, "lambdalr": 48, "rate": [48, 76, 80], "schedul": [48, 69, 80], "linearli": 48, "lr": 48, "decreas": [48, 82, 83], "cosin": 48, "remain": [48, 82], "v4": 48, "23": [48, 81], "src": 48, "l104": 48, "warmup": [48, 69], "phase": 48, "total": [48, 61, 73, 77, 79, 81, 82], "wave": 48, "half": 48, "index": [48, 68, 75, 79], "last": 48, "lr_schedul": 48, "peft": [49, 50, 51, 52, 82, 83], "protocol": 49, "adapter_param": [49, 50, 51, 52], "correspond": [49, 51, 59, 80, 81], "proj": 49, "in_dim": [49, 50, 82, 83], "out_dim": [49, 50, 82, 83], "bia": [49, 50, 82, 83], "loralinear": [49, 82, 83], "alpha": [50, 82, 83], "use_bia": 50, "larg": [50, 83], "perturb": 50, "decomposit": [50, 82], "matric": [50, 82, 83], "trainabl": [50, 52, 82, 83], "mapsto": 50, "w_0x": 50, "r": [50, 54, 82], "bax": 50, "probabl": [50, 79], "lora_a": [50, 82, 83], "lora_b": [50, 82, 83], "subset": 51, "get_adapter_param": [52, 82], "sentencepieceprocessor": 53, "pretrain": [53, 54, 80, 82, 83], "non": 53, "spm_model": 53, "tokenized_text": 53, "hello": [53, 79, 81], "world": [53, 61, 79], "add_bo": [53, 54], "add_eo": [53, 54], "31587": 53, "29644": 53, "102": 53, "text": [53, 54, 78, 79], "trim_leading_whitespac": 53, "prefix": 53, "unbatch": 53, "prepend": [53, 54], "bo": [53, 54], "append": [53, 75], "eo": [53, 54], "trim": 53, "lead": 53, "whitespac": 53, "underli": [53, 83], "sentencepiec": [53, 81], "s1": 53, "s2": 53, "due": [53, 82, 83], "tokenize_messag": [53, 54, 78], "concaten": 53, "problem": 53, "slice": 53, "tokenizer_path": 53, "separ": [53, 55, 80, 81, 82, 83], "concat": 53, "1788": 53, "2643": 53, "13": [53, 79, 81, 83], "1792": 53, "9508": 53, "465": 53, "22137": 53, "2933": 53, "join": 53, "attribut": 53, "llama3_tiktoken": 54, "p": [54, 82, 83], "l": 54, "all_special_token": 54, "bos_token": 54, "begin_of_text": 54, "eos_token": 54, "end_of_text": 54, "start_header_id": 54, "end_header_id": 54, "step_id": 54, "eom_id": 54, "eot_id": 54, "python_tag": 54, "tiktoken": [54, 81], "regex": 54, "special": 54, "element": [54, 79], "second": [54, 79, 81, 82, 83], "uniqu": 54, "256": [54, 78, 79, 81], "header": 54, "token_id": 54, "truncate_at_eo": 54, "tokenize_head": 54, "co": [55, 56, 79], "few": [55, 78, 81, 82, 83], "0001_of_0003": 55, "0002_of_0003": 55, "preserv": [55, 83], "weight_map": [55, 79], "intermediate_checkpoint": [55, 56], "parit": 55, "_weight_map": 55, "shard": [56, 81], "wip": 56, "argpars": 57, "argumentpars": 57, "builtin": 57, "said": 57, "noth": 57, "treat": 57, "still": [57, 82, 83], "consult": 57, "doc": [57, 60, 66, 67, 69, 71, 79], "info": [57, 80], "librari": [57, 60, 71, 74, 76, 83], "html": [57, 60, 66, 69, 71], "parse_known_arg": 57, "namespac": 57, "act": 57, "precid": 57, "parse_arg": 57, "intern": 57, "properti": [57, 82], "too": [57, 81], "availab": 58, "machin": [58, 79], "distribut": [58, 62, 70, 71, 76, 80, 81], "bf16": [59, 83], "request": [59, 78, 79], "inde": [59, 79], "kernel": 59, "runtimeerror": [59, 62], "float32": 59, "done": [59, 82, 83], "isn": 59, "hardwar": [59, 76, 79, 82], "stream": 60, "handler": 60, "aka": 61, "filenam": 64, "log_": 64, "unixtimestamp": 64, "txt": [64, 80], "thread": 64, "safe": 64, "flush": [64, 65, 66, 67], "union": [64, 65, 66, 67, 71], "ndarrai": [64, 65, 66, 67], "scalar": [64, 65, 66, 67], "record": [64, 65, 66, 67], "payload": [64, 65, 66, 67], "dictionari": [64, 65, 66, 67, 79], "organize_log": 66, "tensorboard": 66, "stabl": [66, 69, 71, 75], "subdirectori": 66, "sub": 66, "compar": [66, 79, 82, 83], "logdir": 66, "startup": 66, "recurs": 66, "tree": [66, 78, 79], "tfevent": 66, "encount": 66, "frontend": 66, "organ": 66, "accordingli": 66, "my_log_dir": 66, "view": [66, 79, 80], "my_metr": [66, 67], "termin": [66, 67], "entiti": 67, "bias": 67, "ref": 67, "sent": 67, "usernam": 67, "individu": 67, "my_project": 67, "my_ent": 67, "my_group": 67, "importerror": 67, "account": [67, 82, 83], "log_config": 67, "local": [67, 71, 75, 79, 80], "link": [67, 79], "capecap": 67, "6053ofw0": 67, "torchtune_config_j67sb73v": 67, "padding_idx": 68, "ignore_idx": 68, "pad": 68, "longest": 68, "integ": [68, 71], "tokenpair": 68, "collat": 68, "token_pair": 68, "torchtune_perf_trac": 69, "contextmanag": 69, "wait": 69, "trace": 69, "speed": [69, 81, 83], "reduct": [69, 82], "auto_wrap_polici": 70, "polici": 70, "debug_mod": 71, "pseudo": 71, "random": [71, 80], "commonli": [71, 79, 82, 83], "numpi": 71, "own": [71, 78, 79, 82], "determinist": 71, "global": 71, "warn": 71, "nondeterminist": 71, "addition": [71, 82], "cudnn": 71, "disabl": 71, "set_deterministic_debug_mod": 71, "algorithm": 71, "outsid": [71, 79, 81, 82], "generated_examples_python": 72, "zip": 72, "galleri": [72, 77], "sphinx": 72, "000": [73, 77, 81], "execut": [73, 77], "generated_exampl": 73, "mem": [73, 77], "mb": [73, 77], "topic": 74, "gentl": 74, "introduct": 74, "readi": 74, "maxim": [74, 76], "workflow": [74, 78, 80, 82], "requisit": 75, "proper": [75, 80], "host": [75, 80], "page": [75, 76, 80, 81], "latest": [75, 80, 83], "confirm": 75, "And": [75, 79, 81], "h": 75, "ls": [75, 79, 80, 81], "welcom": 75, "show": [75, 82], "greatest": [75, 80], "contributor": 75, "cd": [75, 79], "even": [75, 81, 82, 83], "commit": 75, "branch": 75, "extra": [75, 82, 83], "url": 75, "whl": 75, "therebi": [75, 83], "howev": 75, "forc": 75, "reinstal": 75, "opt": [75, 80], "suffix": 75, "cu121": 75, "On": [76, 82], "pointer": 76, "author": [76, 80, 83], "emphas": 76, "aspect": 76, "simplic": 76, "component": 76, "reus": 76, "high": [76, 82], "prove": 76, "democrat": 76, "box": [76, 83], "zoo": 76, "varieti": [76, 82], "techniqu": [76, 79, 80, 82], "integr": [76, 79, 80, 81, 82, 83], "excit": 76, "checkout": 76, "quickstart": 76, "attain": 76, "better": [76, 78, 79], "chekckpoint": 76, "hyperparamet": [76, 80, 82, 83], "embodi": 76, "philosophi": 76, "especi": [76, 79], "usabl": 76, "composit": 76, "hard": 76, "outlin": 76, "unecessari": 76, "never": 76, "thoroughli": 76, "unit": 76, "know": [78, 79, 81, 82], "steer": 78, "wheel": 78, "publicli": 78, "great": [78, 79], "sever": 78, "wide": 78, "bootstrap": 78, "indic": 78, "iter": [78, 83], "knob": 78, "tweak": 78, "sai": [78, 80], "footprint": [78, 82], "could": [78, 82], "achiev": [78, 79, 81, 82, 83], "fix": 78, "goal": 78, "flavor": 78, "agnost": 78, "condit": 78, "respond": 78, "alpacainstructtempl": 78, "describ": 78, "further": [78, 82, 83], "classifi": 78, "anim": 78, "plant": 78, "miner": 78, "oak": 78, "copper": 78, "ore": 78, "eleph": 78, "instructtempl": 78, "instruct_dataset": 78, "mydataset": 78, "onthehub": 78, "customtempl": 78, "similar": [78, 79, 81, 82, 83], "chatformat": 78, "quit": [78, 83], "similarli": 78, "chat_dataset": 78, "conversation_styl": 78, "sharegpt": 78, "chat_format": 78, "formatted_messag": 78, "nyou": 78, "incorpor": 78, "advanc": 78, "preferencedataset": 78, "rlhf": 78, "adjust": 78, "chosen": 78, "reject": 78, "chosen_messag": 78, "transformed_sampl": 78, "key_chosen": 78, "rejected_messag": 78, "key_reject": 78, "chosen_input_id": 78, "c_mask": 78, "chosen_label": 78, "np": 78, "cross_entropy_ignore_idx": 78, "rejected_input_id": 78, "r_mask": 78, "rejected_label": 78, "purpos": [78, 80, 81], "stack_exchanged_paired_dataset": 78, "had": 78, "lvwerra": 78, "stack": 78, "exchang": 78, "stackexchangedpairedtempl": 78, "question": [78, 79, 81], "response_j": 78, "response_k": 78, "data_dir": 78, "rl": 78, "favorit": [79, 81, 82], "commun": 79, "seemlessli": 79, "beyond": [79, 83], "connect": 79, "larger": [79, 81], "might": 79, "amount": 79, "natur": 79, "export": 79, "mobil": 79, "phone": 79, "leverag": [79, 81, 83], "mode": 79, "lot": 79, "plai": 79, "freez": [79, 82], "percentag": 79, "learnabl": 79, "keep": [79, 82], "16gb": [79, 82], "rtx": 79, "3090": 79, "4090": 79, "hour": 79, "full_finetune_single_devic": [79, 80], "7b_full_low_memori": [79, 80], "full_finetune_distribut": [79, 80], "7b_full": [79, 80], "13b_full": [79, 80], "7b_qlora_single_devic": [79, 80, 83], "473": 79, "98": [79, 83], "gb": [79, 81, 82, 83], "50": 79, "484": 79, "01": [79, 80], "fact": [79, 81, 82], "third": 79, "smaller": [79, 81, 82, 83], "But": [79, 81, 82], "realli": 79, "eleuther_ev": [79, 81], "eleuther_evalu": [79, 81], "lm_eval": [79, 81], "plan": 79, "copi": [79, 80, 81, 83], "custom_eval_config": [79, 81], "truthfulqa_mc2": [79, 81, 82], "measur": [79, 81], "propens": [79, 81], "answer": [79, 81], "shot": [79, 81], "accuraci": [79, 81, 82, 83], "baselin": [79, 82], "324": 79, "loglikelihood": 79, "195": 79, "121": 79, "27": 79, "197": 79, "acc": 79, "388": 79, "38": 79, "shown": 79, "489": 79, "48": [79, 83], "seem": 79, "custom_generation_config": [79, 81], "kick": 79, "top_k": 79, "300": 79, "temperatur": 79, "interest": 79, "site": 79, "visit": 79, "bai": 79, "area": 79, "92": [79, 81], "exploratorium": 79, "san": 79, "francisco": 79, "magazin": 79, "awesom": 79, "bridg": 79, "pretti": 79, "cool": 79, "96": [79, 83], "61": 79, "sec": [79, 81], "25": 79, "83": 79, "99": [79, 82], "15": [79, 82, 83], "72": 79, "littl": 79, "saw": 79, "took": [79, 81], "torchao": [79, 81, 83], "bit": [79, 81, 82, 83], "custom_quantization_config": [79, 81], "68": 79, "19": [79, 81, 83], "76": 79, "69": 79, "95": [79, 81], "67": 79, "4w": [79, 81], "unlik": [79, 81], "won": [79, 81], "engin": [79, 81], "fullmodeltorchtunecheckpoint": [79, 81], "int4weightonlyquant": [79, 81], "groupsiz": [79, 81], "did": [79, 81, 83], "park": 79, "sit": 79, "top": [79, 83], "hill": 79, "beauti": 79, "62": [79, 81], "17": [79, 82], "85": 79, "compil": [79, 81, 83], "hood": [79, 83], "sped": 79, "almost": [79, 81, 82], "3x": [79, 81], "benefit": 79, "yet": 79, "fast": 79, "clone": [79, 82, 83], "assumpt": 79, "satisfi": 79, "new_dir": 79, "output_dict": 79, "sd_1": 79, "sd_2": 79, "dump": 79, "convert_hf_checkpoint": 79, "checkpoint_path": 79, "my": [79, 81], "justin": 79, "school": 79, "math": 79, "teacher": 79, "ws": 79, "94": [79, 81], "103": 79, "28": 79, "bandwidth": [79, 81], "1391": 79, "84": 79, "thats": 79, "seamlessli": 79, "authent": [79, 80], "hopefulli": 79, "gave": 79, "launch": 80, "gate": 80, "grant": 80, "minut": 80, "agreement": 80, "altern": 80, "hackabl": 80, "singularli": 80, "focus": 80, "technic": 80, "depth": 80, "why": [80, 82], "principl": 80, "minim": [80, 82, 83], "boilerpl": 80, "hold": 80, "substanti": [80, 82], "custom_config": 80, "replic": 80, "lorafinetunerecipesingledevic": 80, "lora_finetune_output": 80, "log_1713194212": 80, "sampler": 80, "52": 80, "3697006702423096": 80, "25880": [80, 83], "24": [80, 81], "55": 80, "83it": 80, "were": 80, "monitor": 80, "tqdm": 80, "interv": 80, "e2": 80, "releas": 81, "128": [81, 82], "intermedi": [81, 83], "theta": 81, "gain": 81, "illustr": 81, "basic": 81, "8b_lora_single_devic": 81, "observ": 81, "18": 81, "consum": [81, 83], "vram": [81, 82], "nproc_per_nod": [81, 82], "lora_finetune_distribut": [81, 82], "8b_lora": 81, "8b_qlora_single_devic": 81, "alloc": [81, 83], "coupl": [81, 82, 83], "llama3_token": 81, "122": 81, "sarah": 81, "busi": 81, "mum": 81, "young": 81, "children": 81, "live": 81, "north": 81, "east": 81, "england": 81, "135": 81, "88": 81, "138": 81, "346": 81, "09": 81, "139": 81, "31": 81, "been": 81, "far": 81, "drill": 81, "90": 81, "93": 81, "91": 81, "104": 81, "meta_model_0": 81, "four": [81, 82], "again": 81, "jake": 81, "disciplin": 81, "artist": 81, "passion": 81, "draw": 81, "paint": 81, "57": [81, 82, 83], "speedup": 81, "broader": 81, "teach": 82, "straight": 82, "jump": 82, "neural": [82, 83], "unfamiliar": 82, "oppos": [82, 83], "momentum": 82, "adamw": 82, "arbitrari": 82, "relat": 82, "aghajanyan": 82, "et": 82, "al": 82, "hypothes": 82, "intrins": 82, "lower": 82, "down": [82, 83], "often": 82, "eight": 82, "practic": 82, "imag": 82, "simplifi": 82, "represent": [82, 83], "left": 82, "blue": 82, "rememb": 82, "approx": 82, "15m": 82, "8192": 82, "65k": 82, "requires_grad": [82, 83], "frozen_out": [82, 83], "lora_out": [82, 83], "omit": 82, "construct": 82, "base_model": 82, "choos": 82, "lora_model": 82, "lora_llama_2_7b": [82, 83], "alon": 82, "in_featur": 82, "out_featur": 82, "inplac": 82, "feel": 82, "free": 82, "strict": 82, "whenev": 82, "validate_state_dict_for_lora": 82, "peft_util": 82, "set_trainable_param": 82, "fetch": 82, "lora_param": 82, "total_param": 82, "sum": 82, "numel": 82, "trainable_param": 82, "2f": 82, "6742609920": 82, "4194304": 82, "nnode": 82, "7b_lora": 82, "my_model_checkpoint_path": [82, 83], "tokenizer_checkpoint": [82, 83], "my_tokenizer_checkpoint_path": [82, 83], "constraint": 82, "factori": 82, "benefici": 82, "long": 82, "impact": 82, "rel": 82, "minor": 82, "good": 82, "64": 82, "lora_experiment_1": 82, "smooth": [82, 83], "curv": [82, 83], "500": 82, "ran": 82, "commod": 82, "cogniz": 82, "ax": 82, "parallel": 82, "truthfulqa": 82, "previous": 82, "475": 82, "87": 82, "508": 82, "86": 82, "504": 82, "04": 82, "514": 82, "lowest": 82, "absolut": 82, "4gb": 82, "tradeoff": 82, "potenti": 82, "enhanc": 83, "maintain": 83, "highli": 83, "part": 83, "vanilla": 83, "held": 83, "therefor": 83, "bespok": 83, "normalfloat": 83, "8x": 83, "retain": 83, "vast": 83, "major": 83, "highlight": 83, "degrad": 83, "normatfloat": 83, "doubl": 83, "themselv": 83, "prune": 83, "deepdiv": 83, "idea": 83, "distinct": 83, "storag": 83, "datatyp": 83, "de": 83, "incur": 83, "counterpart": 83, "set_default_devic": 83, "qlora_linear": 83, "memory_alloc": 83, "177": 83, "152": 83, "byte": 83, "del": 83, "empty_cach": 83, "lora_linear": 83, "081": 83, "344": 83, "qlora_llama2_7b": 83, "qlora_model": 83, "essenti": 83, "reparametrize_as_dtype_state_dict_post_hook": 83, "entir": 83, "stat": 83, "reserv": 83, "against": 83, "35": 83, "40": 83, "29": 83, "slow": 83, "slower": 83, "149": 83, "9157477021217346": 83, "02": 83, "08": 83, "14": 83, "15it": 83, "thing": 83, "nightli": 83, "200": 83, "hundr": 83, "228": 83, "8158286809921265": 83, "59": 83, "95it": 83, "exercis": 83, "manual": 83, "portion": 83, "augment": 83, "linear_nf4": 83, "to_nf4": 83, "linear_weight": 83, "autograd": 83, "regular": 83, "incom": 83, "variabl": 83}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "MistralChatFormat"], [20, 1, 1, "", "SummarizeTemplate"], [21, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.MistralChatFormat": [[19, 2, 1, "", "format"], [19, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[20, 2, 1, "", "format"]], "torchtune.datasets": [[22, 0, 1, "", "alpaca_cleaned_dataset"], [23, 0, 1, "", "alpaca_dataset"], [24, 0, 1, "", "grammar_dataset"], [25, 0, 1, "", "samsum_dataset"], [26, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[27, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[28, 0, 1, "", "llama2_13b"], [29, 0, 1, "", "llama2_7b"], [30, 0, 1, "", "lora_llama2_13b"], [31, 0, 1, "", "lora_llama2_7b"], [32, 0, 1, "", "qlora_llama2_13b"], [33, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[34, 0, 1, "", "llama3_8b"], [35, 0, 1, "", "lora_llama3_8b"], [36, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[37, 0, 1, "", "lora_mistral_7b"], [38, 0, 1, "", "mistral_7b"], [39, 0, 1, "", "qlora_mistral_7b"]], "torchtune.modules": [[40, 1, 1, "", "CausalSelfAttention"], [41, 1, 1, "", "FeedForward"], [42, 1, 1, "", "KVCache"], [43, 1, 1, "", "RMSNorm"], [44, 1, 1, "", "RotaryPositionalEmbeddings"], [45, 1, 1, "", "TransformerDecoder"], [46, 1, 1, "", "TransformerDecoderLayer"], [48, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[40, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[41, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[43, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[44, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[45, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[46, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[47, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[49, 1, 1, "", "AdapterModule"], [50, 1, 1, "", "LoRALinear"], [51, 0, 1, "", "get_adapter_params"], [52, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[49, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[50, 2, 1, "", "adapter_params"], [50, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[53, 1, 1, "", "SentencePieceTokenizer"], [54, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[53, 2, 1, "", "decode"], [53, 2, 1, "", "encode"], [53, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[54, 2, 1, "", "decode"], [54, 2, 1, "", "encode"], [54, 2, 1, "", "tokenize_message"], [54, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[55, 1, 1, "", "FullModelHFCheckpointer"], [56, 1, 1, "", "FullModelMetaCheckpointer"], [57, 1, 1, "", "TuneRecipeArgumentParser"], [58, 0, 1, "", "get_device"], [59, 0, 1, "", "get_dtype"], [60, 0, 1, "", "get_logger"], [61, 0, 1, "", "get_world_size_and_rank"], [62, 0, 1, "", "init_distributed"], [63, 0, 1, "", "list_dtypes"], [68, 0, 1, "", "padded_collate"], [69, 0, 1, "", "profiler"], [70, 0, 1, "", "set_activation_checkpointing"], [71, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[55, 2, 1, "", "load_checkpoint"], [55, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[56, 2, 1, "", "load_checkpoint"], [56, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[57, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[64, 1, 1, "", "DiskLogger"], [65, 1, 1, "", "StdoutLogger"], [66, 1, 1, "", "TensorBoardLogger"], [67, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[64, 2, 1, "", "close"], [64, 2, 1, "", "log"], [64, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[65, 2, 1, "", "close"], [65, 2, 1, "", "log"], [65, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[66, 2, 1, "", "close"], [66, 2, 1, "", "log"], [66, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[67, 2, 1, "", "close"], [67, 2, 1, "", "log"], [67, 2, 1, "", "log_config"], [67, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 74, 76, 79, 81, 82, 83], "config": [0, 7, 8, 80], "data": [1, 5], "dataset": [2, 78], "model": [3, 4, 9, 79, 80, 81, 82], "llama3": [3, 81], "llama2": [3, 79, 82, 83], "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 75, 83], "block": 4, "token": 4, "peft": 4, "util": [4, 5], "checkpoint": [5, 6, 9, 79], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 82, 83], "manag": 5, "perform": [5, 82], "profil": [5, 69], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 76, 79], "format": [6, 78], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 79, 82, 83], "put": [6, 83], "thi": 6, "all": [6, 7, 83], "togeth": [6, 83], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 78], "us": [7, 8, 79, 83], "instanti": [7, 10], "referenc": 7, "other": [7, 79], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 79, 80], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 76, 82, 83], "ar": 8, "recip": [8, 80, 82], "script": 8, "class": 8, "run": [8, 79], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "mistralchatformat": 19, "summarizetempl": 20, "validate_messag": 21, "alpaca_cleaned_dataset": 22, "alpaca_dataset": 23, "grammar_dataset": 24, "samsum_dataset": 25, "slimorca_dataset": 26, "gemma_2b": 27, "llama2_13b": 28, "llama2_7b": 29, "lora_llama2_13b": 30, "lora_llama2_7b": 31, "qlora_llama2_13b": 32, "qlora_llama2_7b": 33, "llama3_8b": 34, "lora_llama3_8b": 35, "qlora_llama3_8b": 36, "lora_mistral_7b": 37, "mistral_7b": 38, "qlora_mistral_7b": 39, "causalselfattent": 40, "todo": [40, 46], "feedforward": 41, "kvcach": 42, "rmsnorm": 43, "rotarypositionalembed": 44, "transformerdecod": 45, "transformerdecoderlay": 46, "reparametrize_as_dtype_state_dict_post_hook": 47, "get_cosine_schedule_with_warmup": 48, "adaptermodul": 49, "loralinear": 50, "get_adapter_param": 51, "set_trainable_param": 52, "sentencepiecetoken": 53, "tiktokentoken": 54, "fullmodelhfcheckpoint": 55, "fullmodelmetacheckpoint": 56, "tunerecipeargumentpars": 57, "get_devic": 58, "get_dtyp": 59, "get_logg": 60, "get_world_size_and_rank": 61, "init_distribut": 62, "list_dtyp": 63, "disklogg": 64, "stdoutlogg": 65, "tensorboardlogg": 66, "wandblogg": 67, "padded_col": 68, "set_activation_checkpoint": 70, "set_se": 71, "comput": [73, 77], "time": [73, 77], "welcom": 74, "document": 74, "get": [74, 81], "start": 74, "tutori": 74, "instal": 75, "instruct": [75, 78], "via": [75, 81], "pypi": 75, "git": 75, "clone": 75, "nightli": 75, "kei": 76, "concept": 76, "design": 76, "principl": 76, "fine": [78, 80, 81], "tune": [78, 80, 81], "custom": 78, "templat": 78, "chat": 78, "fulli": 78, "end": 79, "workflow": 79, "download": [79, 80], "7b": 79, "finetun": [79, 82, 83], "evalu": [79, 81], "eleutherai": [79, 81], "s": [79, 81], "eval": [79, 81], "har": [79, 81], "gener": [79, 81], "speed": 79, "up": 79, "quantiz": [79, 81], "librari": 79, "upload": 79, "hug": 79, "face": 79, "hub": 79, "first": 80, "llm": 80, "select": 80, "modifi": 80, "train": 80, "next": 80, "step": 80, "8b": 81, "access": 81, "text": 81, "our": 81, "faster": 81, "how": 82, "doe": 82, "work": 82, "appli": 82, "trade": 82, "off": 82, "qlora": 83, "save": 83, "deep": 83, "dive": 83, "from": 83}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})