Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.Qwen2Tokenizer", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.left_padded_collate", "generated/torchtune.modules.rlhf.loss.DPOLoss", "generated/torchtune.modules.rlhf.loss.IPOLoss", "generated/torchtune.modules.rlhf.loss.PPOLoss", "generated/torchtune.modules.rlhf.loss.RSOLoss", "generated/torchtune.modules.rlhf.loss.SimPOLoss", "generated/torchtune.modules.rlhf.padded_collate_dpo", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.get_inscribed_size", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.CometLogger", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.Qwen2Tokenizer.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.left_padded_collate.rst", "generated/torchtune.modules.rlhf.loss.DPOLoss.rst", "generated/torchtune.modules.rlhf.loss.IPOLoss.rst", "generated/torchtune.modules.rlhf.loss.PPOLoss.rst", "generated/torchtune.modules.rlhf.loss.RSOLoss.rst", "generated/torchtune.modules.rlhf.loss.SimPOLoss.rst", "generated/torchtune.modules.rlhf.padded_collate_dpo.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.get_inscribed_size.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.CometLogger.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Llama2ChatFormat", "Message", "MistralChatFormat", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralChatTemplate", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "Qwen2Tokenizer", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "left_padded_collate", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "padded_collate_dpo", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "get_inscribed_size", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 15, 17, 18, 22, 26, 39, 40, 42, 45, 49, 53, 95, 114, 123, 130, 131, 132, 140, 141, 142, 220, 224, 225, 228, 230, 231, 232], "prompt": [1, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 34, 37, 39, 41, 42, 45, 46, 48, 49, 50, 51, 52, 68, 78, 79, 95, 114, 115, 127, 134, 150, 177, 193, 226, 227, 229], "chat": [1, 2, 16, 17, 18, 23, 24, 31, 33, 34, 37, 42, 46, 78, 79, 132, 134], "includ": [1, 6, 8, 9, 16, 22, 27, 28, 42, 58, 69, 79, 80, 96, 105, 120, 132, 139, 156, 172, 181, 186, 187, 191, 222, 224, 225, 226, 227, 228, 229, 230, 232], "some": [1, 6, 8, 17, 18, 118, 158, 159, 220, 222, 224, 225, 226, 227, 228, 230, 231, 232], "specif": [1, 4, 8, 9, 11, 41, 42, 48, 50, 51, 173, 196, 225, 226, 227, 231, 232], "format": [1, 2, 5, 15, 16, 17, 22, 24, 25, 26, 33, 37, 39, 41, 42, 45, 46, 48, 49, 50, 51, 52, 78, 79, 95, 114, 134, 173, 182, 183, 186, 187, 188, 189, 224, 225, 227, 228, 229, 230], "differ": [1, 8, 10, 38, 39, 55, 56, 57, 115, 152, 165, 170, 174, 189, 217, 222, 224, 225, 227, 229, 230, 231, 232], "dataset": [1, 5, 8, 15, 19, 21, 22, 23, 25, 31, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 165, 166, 222, 228, 229, 231], "model": [1, 2, 6, 7, 8, 9, 11, 17, 18, 19, 21, 25, 26, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 172, 173, 176, 177, 178, 186, 187, 188, 189, 192, 193, 196, 198, 204, 205, 211, 212, 220, 222, 225, 226, 232], "from": [1, 2, 3, 6, 7, 8, 9, 10, 11, 15, 22, 23, 24, 25, 31, 34, 37, 38, 39, 40, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 70, 71, 78, 81, 82, 83, 84, 99, 115, 121, 123, 132, 134, 140, 141, 142, 143, 145, 150, 151, 152, 154, 155, 158, 161, 165, 166, 168, 169, 174, 176, 179, 181, 182, 183, 186, 187, 188, 190, 191, 192, 193, 205, 208, 209, 211, 219, 221, 223, 224, 226, 227, 228, 229, 230, 231], "common": [1, 2, 4, 8, 177, 224, 225, 226, 229, 230, 231], "json": [1, 6, 23, 31, 33, 34, 37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 99, 132, 134, 143, 176, 186, 224, 226, 227, 231], "schema": 1, "convers": [1, 6, 16, 17, 19, 24, 26, 31, 33, 34, 36, 37, 41, 42, 46, 51, 186, 188, 189, 222, 225, 226, 227, 230, 232], "list": [1, 6, 8, 16, 17, 19, 24, 25, 26, 27, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 58, 62, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 79, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 138, 152, 155, 156, 160, 161, 164, 170, 172, 173, 174, 175, 177, 179, 180, 181, 186, 187, 188, 191, 193, 197, 205, 210, 225, 226, 227, 228, 229, 231], "miscellan": 1, "us": [1, 2, 3, 4, 6, 7, 10, 11, 13, 17, 18, 21, 24, 25, 27, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 78, 79, 80, 86, 96, 99, 100, 105, 109, 128, 132, 134, 135, 139, 143, 144, 145, 148, 149, 150, 151, 152, 153, 157, 160, 162, 163, 165, 167, 169, 174, 175, 179, 180, 183, 185, 186, 187, 189, 190, 191, 193, 194, 195, 196, 198, 204, 205, 206, 207, 208, 209, 214, 220, 221, 222, 224, 226, 228, 229, 230, 231], "modifi": [1, 8, 9, 10, 153, 222, 227, 229, 230, 231, 232], "For": [2, 5, 6, 8, 9, 19, 23, 27, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 69, 73, 79, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 150, 152, 180, 181, 186, 191, 192, 199, 205, 209, 212, 214, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232], "detail": [2, 6, 37, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 79, 122, 147, 152, 167, 185, 196, 204, 214, 224, 226, 227, 228, 229, 230, 231, 232], "usag": [2, 153, 189, 190, 215, 221, 224, 226, 227, 228, 229, 231, 232], "guid": [2, 7, 8, 10, 169, 205, 222, 225, 226, 228, 230], "pleas": [2, 5, 20, 29, 32, 55, 56, 57, 58, 65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 152, 185, 196, 204, 212, 221, 227, 229, 232], "see": [2, 5, 6, 7, 10, 20, 24, 26, 29, 32, 37, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 53, 54, 65, 66, 67, 76, 77, 78, 79, 91, 92, 93, 94, 103, 104, 112, 113, 114, 122, 125, 126, 133, 134, 147, 152, 155, 185, 189, 191, 196, 197, 204, 205, 209, 212, 214, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "our": [2, 6, 9, 222, 225, 226, 227, 228, 230, 231, 232], "tutori": [2, 6, 79, 212, 222, 225, 226, 227, 228, 229, 230, 231, 232], "support": [2, 3, 6, 7, 9, 10, 11, 25, 26, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 73, 86, 100, 109, 114, 116, 118, 128, 131, 132, 135, 144, 146, 152, 156, 168, 180, 183, 187, 188, 190, 195, 198, 199, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "sever": 2, "wide": 2, "help": [2, 6, 24, 78, 150, 152, 186, 191, 205, 220, 221, 222, 224, 225, 226, 227, 228, 231, 232], "quickli": [2, 8, 27, 43, 225, 226], "bootstrap": 2, "your": [2, 5, 7, 10, 11, 15, 27, 37, 43, 56, 57, 58, 79, 152, 205, 208, 209, 220, 221, 222, 224, 225, 226, 229, 230, 231, 232], "fine": [2, 6, 7, 9, 10, 25, 40, 41, 42, 53, 79, 220, 222, 227, 230, 231], "tune": [2, 3, 6, 7, 8, 9, 10, 13, 25, 40, 41, 42, 53, 79, 220, 221, 222, 224, 227, 230, 231, 232], "also": [2, 6, 7, 8, 9, 10, 11, 38, 46, 49, 53, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 132, 135, 139, 144, 150, 156, 169, 194, 196, 198, 204, 205, 209, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232], "like": [2, 4, 6, 7, 8, 9, 10, 37, 132, 152, 188, 221, 224, 225, 226, 227, 228, 230, 231], "These": [2, 4, 6, 8, 9, 11, 40, 41, 152, 179, 191, 225, 226, 227, 228, 229, 230, 231, 232], "ar": [2, 4, 6, 7, 8, 10, 11, 15, 16, 22, 23, 24, 26, 27, 28, 31, 36, 39, 40, 41, 42, 45, 46, 48, 49, 50, 51, 52, 56, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 78, 79, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 150, 151, 152, 156, 157, 160, 161, 163, 170, 179, 181, 185, 186, 187, 189, 190, 192, 193, 195, 198, 202, 204, 215, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "especi": [2, 222, 224, 227], "specifi": [2, 6, 8, 9, 11, 21, 41, 42, 46, 80, 86, 96, 100, 105, 109, 135, 139, 144, 150, 151, 185, 193, 196, 199, 204, 209, 212, 215, 224, 225, 226, 227, 228, 229, 231, 232], "yaml": [2, 8, 9, 11, 12, 13, 38, 46, 49, 53, 191, 209, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "config": [2, 6, 7, 10, 11, 12, 13, 14, 38, 46, 49, 53, 144, 160, 186, 190, 191, 205, 209, 215, 222, 225, 226, 227, 229, 230, 231, 232], "represent": [2, 230, 231, 232], "abov": [2, 3, 6, 153, 181, 202, 221, 227, 229, 230, 231, 232], "all": [3, 4, 9, 14, 27, 38, 40, 41, 42, 46, 58, 99, 132, 134, 143, 144, 145, 150, 152, 153, 157, 178, 180, 181, 186, 190, 191, 192, 202, 211, 217, 218, 220, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231], "famili": [3, 6, 9, 45, 47, 51, 52, 54, 189, 222, 224, 229], "request": [3, 15, 195, 226, 227], "access": [3, 6, 8, 9, 38, 186, 192, 224, 226, 227, 228], "hug": [3, 6, 17, 18, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 99, 132, 143, 154, 176, 222, 224, 228, 229], "face": [3, 6, 17, 18, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 99, 132, 143, 154, 176, 222, 224, 228, 229], "To": [3, 6, 8, 9, 10, 40, 152, 186, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "download": [3, 6, 218, 221, 225, 226, 229, 230, 231, 232], "8b": [3, 98, 102, 104, 108, 111, 113, 129, 224, 225, 231], "meta": [3, 6, 24, 78, 79, 95, 149, 186, 187, 224, 225, 227, 228], "hf": [3, 6, 127, 165, 166, 168, 186, 224, 225, 227, 228, 229], "token": [3, 6, 8, 9, 25, 35, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 68, 69, 72, 73, 79, 80, 85, 86, 95, 96, 99, 100, 105, 109, 115, 116, 118, 120, 122, 124, 127, 128, 130, 132, 134, 135, 139, 143, 144, 149, 150, 151, 152, 163, 164, 167, 171, 172, 173, 174, 175, 176, 177, 179, 193, 196, 210, 224, 226, 227, 228, 229, 230, 231, 232], "hf_token": 3, "70b": [3, 60, 63, 66, 82, 88, 92, 97, 101, 103, 107, 110, 112, 229], "ignor": [3, 6, 53, 127, 144, 145, 224], "pattern": [3, 175, 224], "origin": [3, 6, 44, 45, 153, 156, 225, 227, 229, 230, 231, 232], "consolid": [3, 6], "weight": [3, 6, 9, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 144, 153, 155, 156, 160, 165, 174, 186, 187, 188, 189, 199, 204, 209, 220, 224, 225, 227, 228, 229, 230, 231, 232], "you": [3, 6, 7, 8, 9, 10, 11, 22, 24, 25, 27, 37, 39, 41, 42, 43, 45, 47, 48, 49, 50, 51, 52, 53, 54, 78, 152, 181, 189, 191, 193, 205, 208, 209, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "can": [3, 4, 6, 7, 8, 9, 10, 11, 14, 25, 27, 28, 38, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 79, 115, 148, 149, 152, 157, 174, 175, 180, 182, 183, 185, 186, 189, 191, 196, 204, 205, 208, 209, 212, 215, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "instead": [3, 6, 9, 40, 41, 46, 49, 53, 58, 134, 145, 147, 152, 156, 169, 224, 229, 230, 231], "405b": [3, 106], "The": [3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 18, 22, 24, 25, 26, 36, 37, 38, 39, 40, 41, 42, 48, 50, 52, 55, 56, 57, 58, 62, 63, 64, 68, 73, 74, 75, 79, 86, 87, 88, 89, 90, 95, 100, 101, 102, 109, 110, 111, 115, 116, 118, 127, 128, 129, 134, 135, 136, 137, 138, 146, 148, 149, 152, 153, 154, 157, 162, 164, 165, 166, 167, 168, 169, 172, 173, 174, 175, 176, 177, 179, 181, 182, 183, 184, 185, 186, 188, 191, 194, 195, 197, 199, 205, 209, 213, 215, 216, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "reus": [3, 222], "llama3_token": [3, 193, 225, 229], "builder": [3, 6, 44, 47, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 97, 98, 101, 102, 103, 104, 106, 107, 108, 110, 111, 112, 113, 117, 119, 121, 123, 125, 126, 129, 131, 133, 136, 137, 138, 140, 141, 142, 225, 226, 232], "class": [3, 8, 10, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 37, 38, 39, 40, 41, 42, 43, 46, 49, 55, 56, 57, 68, 78, 79, 84, 95, 114, 115, 118, 122, 123, 127, 134, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155, 156, 158, 159, 165, 166, 167, 168, 169, 172, 173, 174, 175, 178, 179, 186, 187, 188, 189, 190, 191, 205, 206, 207, 208, 209, 225, 226, 228, 230, 232], "7b": [3, 6, 39, 43, 45, 47, 49, 53, 54, 61, 64, 67, 71, 75, 83, 84, 89, 90, 93, 94, 117, 119, 121, 123, 126, 138, 142, 186, 187, 225, 228, 229, 230, 232], "13b": [3, 6, 59, 62, 65, 81, 87, 91], "codellama": 3, "size": [3, 6, 9, 11, 45, 48, 50, 56, 57, 58, 134, 144, 147, 148, 149, 150, 151, 152, 162, 163, 179, 180, 182, 183, 184, 200, 202, 222, 224, 226, 227, 228, 229, 230, 231], "0": [3, 6, 9, 40, 58, 62, 63, 64, 65, 66, 67, 68, 69, 73, 79, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 100, 105, 109, 115, 116, 118, 120, 122, 127, 128, 130, 134, 135, 136, 137, 138, 139, 140, 141, 144, 150, 152, 154, 156, 164, 165, 166, 167, 168, 169, 170, 171, 177, 181, 193, 199, 205, 208, 209, 210, 214, 216, 219, 223, 225, 226, 227, 228, 229, 230, 231, 232], "5b": [3, 136, 137, 140, 141], "qwen2": [3, 134, 135, 136, 137, 138, 140, 141, 142, 143, 189], "exampl": [3, 6, 7, 8, 9, 10, 11, 13, 15, 19, 23, 27, 38, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 58, 68, 79, 95, 115, 127, 134, 144, 152, 155, 157, 164, 165, 166, 168, 169, 170, 171, 174, 175, 177, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 193, 199, 205, 208, 209, 210, 213, 216, 218, 219, 221, 223, 224, 225, 226, 227, 229, 230, 231, 232], "output": [3, 6, 21, 22, 38, 39, 41, 42, 45, 48, 50, 51, 58, 62, 63, 64, 69, 73, 80, 84, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 123, 128, 129, 135, 138, 139, 144, 145, 146, 148, 149, 150, 151, 152, 156, 159, 160, 161, 179, 182, 183, 188, 193, 196, 207, 215, 221, 224, 225, 226, 227, 228, 229, 230, 232], "dir": [3, 6, 209, 221, 224, 227, 228, 229, 231], "tmp": [3, 8, 190, 225, 228], "none": [3, 9, 10, 12, 14, 15, 19, 21, 22, 23, 31, 35, 36, 37, 39, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 68, 72, 79, 80, 85, 86, 95, 96, 99, 100, 105, 109, 115, 124, 127, 132, 134, 143, 144, 147, 149, 150, 151, 152, 157, 159, 160, 161, 162, 163, 167, 174, 177, 182, 183, 186, 187, 188, 189, 193, 194, 195, 197, 199, 203, 205, 206, 207, 208, 209, 211, 212, 213, 214, 215, 217, 224, 225, 226, 227, 231], "mini": [3, 127, 129, 130, 131, 132, 133], "4k": [3, 130, 131, 132], "microsoft": [3, 131, 132], "ai": [3, 41, 42, 121, 144, 209, 225, 229], "v0": [3, 114], "mistralai": [3, 224], "2b": [3, 70, 74], "googl": [3, 70, 71], "gguf": 3, "vision": [3, 42, 58], "compon": [3, 6, 9, 14, 41, 42, 170, 222, 226, 228, 230, 232], "multimod": [3, 25, 42], "encod": [3, 4, 42, 58, 68, 79, 95, 115, 127, 134, 165, 169, 172, 174, 175, 177, 179, 225], "perform": [4, 6, 40, 79, 145, 152, 157, 169, 178, 193, 222, 225, 227, 229, 231, 232], "direct": [4, 9, 165, 170, 221], "text": [4, 25, 27, 28, 37, 39, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 79, 95, 115, 127, 134, 172, 174, 175, 177, 179, 225, 227, 231], "id": [4, 6, 37, 39, 40, 43, 45, 46, 47, 49, 53, 54, 79, 95, 115, 127, 134, 144, 149, 150, 151, 164, 170, 172, 173, 174, 175, 176, 177, 179, 186, 188, 193, 205, 210, 225, 226, 227], "decod": [4, 69, 73, 80, 86, 95, 96, 100, 105, 109, 115, 116, 118, 120, 122, 127, 128, 130, 134, 135, 139, 150, 172, 174, 175, 193, 225], "typic": [4, 8, 40, 41, 42, 43, 53, 132, 165, 169, 226, 231, 232], "byte": [4, 134, 175, 232], "pair": [4, 8, 15, 52, 166, 170, 175, 210, 226], "underli": [4, 115, 174, 232], "helper": 4, "method": [4, 6, 8, 9, 10, 13, 37, 39, 41, 43, 45, 46, 47, 49, 52, 53, 54, 134, 153, 155, 158, 160, 172, 173, 183, 190, 191, 199, 221, 222, 226, 230, 232], "ani": [4, 6, 8, 9, 11, 13, 14, 15, 22, 27, 33, 34, 35, 37, 39, 41, 42, 43, 46, 47, 49, 53, 54, 57, 79, 115, 146, 153, 158, 159, 160, 161, 172, 173, 174, 177, 186, 187, 188, 190, 193, 201, 204, 205, 214, 217, 224, 225, 226, 228, 230, 231], "function": [4, 6, 8, 9, 11, 13, 37, 56, 57, 58, 144, 145, 152, 153, 157, 160, 161, 165, 167, 170, 185, 186, 193, 194, 200, 204, 214, 222, 225, 226, 232], "preprocess": [4, 40, 152], "imag": [4, 25, 42, 55, 56, 57, 58, 152, 179, 180, 181, 182, 183, 184, 230], "algorithm": [4, 162, 169, 214], "ppo": [4, 162, 163, 165, 167], "offer": 5, "allow": [5, 38, 160, 181, 208, 224, 231, 232], "seamless": 5, "transit": 5, "between": [5, 6, 41, 134, 163, 166, 167, 169, 186, 189, 205, 226, 227, 229, 230, 231, 232], "train": [5, 6, 7, 9, 10, 19, 21, 24, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 78, 79, 144, 146, 149, 150, 151, 153, 154, 165, 169, 186, 187, 188, 195, 198, 204, 205, 215, 220, 222, 224, 225, 226, 227, 229, 230, 231, 232], "interoper": [5, 6, 9, 222, 227, 232], "rest": [5, 225, 232], "ecosystem": [5, 6, 9, 222, 227, 229, 232], "comprehens": 5, "overview": [5, 8, 10, 220, 228, 230, 232], "deep": [5, 6, 7, 8, 9, 10, 222, 228, 229], "dive": [5, 6, 7, 8, 9, 10, 222, 228, 229], "enabl": [5, 7, 8, 9, 10, 38, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 87, 88, 89, 90, 91, 92, 93, 94, 101, 102, 103, 104, 110, 111, 112, 113, 117, 119, 125, 126, 129, 133, 136, 137, 138, 140, 141, 156, 214, 215, 229, 230, 232], "work": [5, 6, 9, 191, 222, 224, 227, 229, 232], "set": [5, 6, 7, 8, 9, 10, 25, 39, 40, 43, 45, 47, 48, 49, 50, 51, 53, 54, 80, 86, 95, 96, 100, 105, 109, 116, 118, 120, 122, 127, 128, 130, 134, 135, 139, 144, 149, 150, 157, 159, 182, 183, 185, 194, 196, 202, 204, 205, 212, 213, 214, 215, 222, 224, 225, 227, 228, 229, 230, 231], "consumpt": [5, 38], "dure": [5, 6, 39, 40, 45, 48, 50, 51, 144, 147, 149, 150, 151, 152, 153, 169, 198, 225, 227, 229, 230, 231, 232], "provid": [5, 6, 8, 9, 11, 15, 17, 18, 23, 26, 35, 38, 39, 40, 58, 150, 152, 157, 165, 188, 191, 194, 196, 205, 209, 215, 222, 224, 225, 226, 227, 228, 229], "debug": [5, 6, 8, 9, 205, 224], "finetun": [5, 6, 8, 9, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138, 220, 222, 228, 229], "job": [5, 10, 214, 228], "variou": [5, 22], "walk": [6, 9, 208, 222, 225, 226, 227, 228, 231, 232], "through": [6, 7, 8, 9, 10, 41, 58, 145, 152, 157, 222, 224, 225, 226, 227, 228, 231, 232], "design": [6, 9, 169], "behavior": [6, 204, 225, 226], "associ": [6, 8, 9, 58, 69, 80, 96, 105, 120, 139, 193, 205, 227, 230], "util": [6, 7, 8, 9, 10, 11, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 222, 227, 228, 229, 231, 232], "what": [6, 7, 8, 10, 24, 26, 41, 42, 48, 50, 78, 114, 152, 220, 225, 226, 227, 228, 229], "cover": [6, 7, 8, 9, 10, 225, 227, 232], "how": [6, 7, 8, 9, 10, 152, 185, 205, 212, 220, 224, 225, 226, 227, 228, 229, 231, 232], "we": [6, 7, 8, 9, 10, 39, 40, 41, 42, 43, 45, 47, 49, 53, 54, 79, 115, 134, 144, 147, 149, 150, 152, 156, 165, 169, 180, 181, 186, 187, 188, 193, 195, 199, 204, 211, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "them": [6, 8, 38, 39, 49, 68, 79, 115, 127, 145, 152, 153, 177, 224, 225, 226, 227, 230, 231, 232], "scenario": [6, 38, 41, 42], "full": [6, 8, 9, 20, 29, 32, 41, 46, 49, 54, 65, 66, 67, 68, 76, 77, 79, 91, 92, 93, 94, 103, 104, 112, 113, 115, 125, 126, 127, 133, 160, 161, 177, 222, 224, 226, 227, 229, 230, 231], "compos": [6, 152], "which": [6, 8, 9, 38, 39, 40, 43, 45, 48, 50, 51, 53, 62, 63, 64, 72, 73, 74, 75, 85, 86, 87, 88, 89, 90, 95, 99, 100, 101, 102, 109, 110, 111, 114, 115, 116, 117, 118, 119, 124, 128, 129, 132, 135, 136, 137, 138, 144, 149, 150, 151, 152, 154, 160, 161, 174, 181, 186, 187, 188, 190, 195, 206, 209, 212, 222, 224, 225, 226, 227, 228, 230, 231, 232], "plug": 6, "recip": [6, 7, 8, 10, 11, 12, 13, 145, 160, 186, 187, 188, 222, 225, 226, 227, 229, 232], "evalu": [6, 9, 220, 222, 228, 230, 232], "gener": [6, 9, 15, 37, 39, 40, 47, 53, 79, 115, 134, 157, 162, 205, 213, 214, 215, 218, 220, 225, 226, 230, 231, 232], "each": [6, 9, 16, 22, 27, 28, 38, 40, 41, 42, 55, 56, 57, 58, 62, 63, 64, 68, 73, 74, 75, 79, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 115, 116, 117, 118, 119, 127, 128, 129, 135, 136, 137, 138, 144, 149, 150, 151, 152, 160, 161, 162, 163, 165, 166, 168, 169, 170, 177, 179, 181, 184, 214, 215, 222, 224, 226, 227, 228, 230, 231], "make": [6, 7, 8, 9, 10, 144, 151, 152, 222, 224, 225, 227, 228, 229, 230, 231, 232], "easi": [6, 9, 222, 226, 230], "understand": [6, 8, 9, 220, 222, 225, 226, 230, 232], "extend": [6, 9, 222], "befor": [6, 27, 36, 39, 40, 55, 56, 58, 69, 73, 144, 150, 151, 152, 156, 175, 186, 205, 224, 227, 231], "let": [6, 8, 10, 224, 225, 226, 227, 228, 229, 230, 232], "s": [6, 8, 9, 10, 11, 13, 15, 16, 17, 18, 23, 24, 26, 31, 33, 34, 36, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 62, 63, 64, 68, 78, 79, 86, 87, 88, 89, 90, 95, 100, 101, 102, 109, 110, 111, 114, 115, 116, 117, 118, 119, 127, 128, 129, 132, 135, 138, 139, 144, 147, 149, 150, 151, 152, 153, 155, 158, 160, 161, 165, 166, 168, 169, 171, 175, 181, 185, 186, 187, 190, 194, 196, 198, 204, 205, 208, 212, 213, 222, 224, 225, 226, 228, 230, 231, 232], "defin": [6, 8, 9, 27, 37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 145, 155, 156, 158, 163, 226, 228, 230], "concept": [6, 227, 228], "In": [6, 8, 9, 37, 56, 57, 58, 149, 152, 156, 166, 181, 185, 204, 208, 209, 225, 227, 229, 230, 231, 232], "ll": [6, 8, 9, 193, 199, 222, 225, 226, 227, 228, 229, 231, 232], "talk": 6, "about": [6, 9, 152, 165, 169, 205, 209, 222, 224, 225, 227, 228, 229, 230, 231, 232], "take": [6, 8, 9, 11, 41, 42, 145, 147, 152, 153, 170, 186, 188, 191, 194, 225, 226, 227, 228, 229, 230, 232], "close": [6, 9, 205, 206, 207, 208, 209, 230], "look": [6, 8, 9, 192, 208, 221, 225, 226, 227, 228, 229, 230, 231], "veri": [6, 38, 150, 224, 227], "simpli": [6, 8, 23, 40, 42, 165, 166, 224, 225, 226, 227, 229, 232], "dictat": 6, "state_dict": [6, 153, 160, 186, 187, 188, 189, 190, 230, 232], "store": [6, 41, 42, 205, 206, 209, 230, 232], "file": [6, 7, 8, 9, 10, 11, 12, 13, 37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 68, 79, 95, 99, 115, 127, 132, 134, 143, 174, 175, 176, 186, 187, 188, 191, 206, 209, 215, 219, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232], "disk": [6, 43, 206], "string": [6, 25, 27, 45, 46, 47, 68, 79, 95, 115, 127, 134, 155, 172, 174, 175, 177, 194, 195, 199, 205, 224, 226], "kei": [6, 8, 10, 15, 33, 37, 39, 41, 42, 49, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 150, 151, 159, 160, 161, 169, 170, 186, 188, 190, 205, 215, 224, 227, 228, 230, 232], "identifi": [6, 205], "state": [6, 9, 152, 153, 158, 159, 160, 161, 162, 165, 186, 187, 188, 190, 192, 227, 229, 230, 232], "dict": [6, 8, 9, 10, 11, 15, 19, 21, 22, 23, 25, 27, 31, 33, 34, 37, 39, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 95, 127, 134, 153, 158, 159, 160, 161, 164, 170, 172, 173, 175, 176, 178, 186, 187, 188, 190, 192, 198, 201, 203, 205, 210, 211, 226], "If": [6, 8, 14, 15, 22, 23, 25, 26, 31, 33, 35, 36, 37, 39, 41, 42, 45, 48, 49, 50, 51, 52, 58, 80, 86, 95, 96, 100, 105, 109, 127, 135, 139, 144, 149, 150, 151, 152, 153, 156, 161, 181, 182, 183, 186, 187, 188, 189, 190, 193, 194, 195, 196, 198, 199, 201, 205, 208, 209, 214, 215, 217, 221, 224, 225, 226, 227, 228, 229, 230, 231], "don": [6, 8, 9, 209, 214, 224, 225, 226, 227, 228, 232], "t": [6, 8, 9, 166, 195, 209, 214, 224, 225, 226, 227, 228, 232], "match": [6, 39, 49, 127, 161, 181, 205, 221, 224, 226, 227, 229, 230], "up": [6, 7, 9, 10, 39, 40, 43, 45, 47, 49, 53, 54, 134, 175, 179, 180, 183, 192, 205, 215, 224, 225, 226, 228, 229, 230, 232], "exactli": [6, 161, 231], "those": [6, 189, 227, 229, 230], "definit": [6, 230], "either": [6, 41, 42, 161, 186, 193, 205, 212, 224, 230, 231, 232], "run": [6, 7, 8, 10, 13, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 134, 135, 139, 145, 147, 150, 153, 186, 187, 188, 190, 192, 202, 205, 208, 209, 211, 221, 222, 225, 226, 228, 229, 230, 231, 232], "explicit": 6, "error": [6, 8, 20, 36, 134, 147, 186, 214, 224], "load": [6, 9, 37, 38, 39, 40, 41, 42, 43, 45, 47, 48, 50, 51, 52, 53, 54, 160, 186, 187, 188, 190, 191, 208, 225, 226, 227, 229, 230], "rais": [6, 11, 14, 26, 33, 36, 37, 39, 46, 127, 135, 144, 147, 150, 152, 160, 161, 170, 177, 186, 187, 188, 190, 195, 198, 201, 205, 209, 214, 217], "an": [6, 7, 8, 9, 10, 11, 15, 36, 37, 38, 39, 43, 48, 50, 53, 54, 55, 56, 57, 86, 100, 109, 116, 118, 122, 128, 134, 135, 136, 137, 140, 141, 144, 147, 150, 152, 155, 157, 158, 159, 165, 179, 180, 181, 182, 183, 185, 186, 187, 188, 190, 194, 196, 205, 209, 215, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "except": [6, 25, 26, 114, 177, 226], "wors": 6, "silent": [6, 145], "succe": 6, "infer": [6, 24, 37, 42, 69, 78, 79, 120, 144, 147, 149, 150, 151, 194, 220, 225, 227, 228, 229, 231, 232], "expect": [6, 8, 11, 15, 19, 21, 22, 23, 31, 37, 39, 41, 42, 46, 48, 49, 50, 51, 52, 149, 161, 190, 205, 209, 217, 225, 226, 230, 231], "addit": [6, 8, 9, 11, 37, 39, 41, 42, 43, 46, 47, 49, 53, 54, 79, 114, 160, 165, 185, 186, 187, 188, 195, 196, 201, 204, 205, 206, 208, 209, 212, 222, 225, 228, 230], "line": [6, 7, 9, 15, 191, 224, 226, 228, 229], "need": [6, 7, 8, 9, 10, 22, 27, 37, 40, 42, 144, 145, 150, 152, 169, 204, 205, 208, 209, 211, 221, 224, 225, 226, 227, 228, 229, 230, 232], "shape": [6, 55, 56, 57, 58, 144, 147, 149, 150, 151, 152, 156, 162, 163, 164, 165, 166, 167, 168, 169, 171, 179, 181, 184, 193, 215], "valu": [6, 8, 31, 34, 59, 60, 61, 69, 70, 71, 73, 80, 81, 82, 83, 84, 86, 96, 97, 98, 100, 105, 106, 107, 108, 109, 116, 118, 120, 121, 122, 123, 128, 130, 135, 139, 140, 141, 142, 144, 147, 148, 150, 151, 154, 160, 162, 163, 167, 170, 171, 186, 189, 190, 191, 193, 205, 206, 207, 208, 209, 214, 224, 226, 228, 229, 230, 231], "two": [6, 8, 21, 36, 56, 152, 171, 179, 181, 222, 227, 228, 229, 230, 231, 232], "popular": [6, 222, 226, 227], "llama2": [6, 8, 9, 11, 24, 37, 39, 41, 42, 43, 45, 47, 49, 53, 54, 59, 60, 61, 62, 63, 64, 65, 66, 67, 78, 79, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 145, 150, 151, 189, 220, 222, 224, 228, 229, 231], "offici": [6, 24, 78, 134, 225, 228, 229], "implement": [6, 9, 37, 39, 41, 43, 45, 46, 47, 49, 52, 53, 54, 68, 79, 115, 127, 145, 148, 149, 152, 154, 155, 156, 165, 166, 167, 168, 169, 172, 173, 186, 199, 208, 222, 226, 230, 231, 232], "when": [6, 8, 9, 13, 38, 40, 41, 42, 43, 53, 79, 134, 144, 149, 150, 151, 152, 153, 154, 160, 163, 180, 182, 183, 193, 196, 208, 211, 224, 227, 229, 230, 231, 232], "llama": [6, 24, 37, 78, 79, 95, 148, 149, 186, 187, 224, 225, 227, 228, 229, 230], "websit": 6, "get": [6, 7, 8, 9, 10, 37, 41, 42, 79, 115, 134, 195, 197, 198, 200, 205, 221, 222, 225, 226, 227, 228, 230, 231], "singl": [6, 8, 11, 15, 16, 17, 22, 24, 26, 33, 34, 38, 40, 41, 42, 43, 53, 56, 57, 58, 72, 84, 85, 95, 99, 123, 124, 132, 134, 144, 152, 160, 186, 187, 188, 189, 190, 192, 224, 225, 226, 227, 228, 229, 230, 232], "pth": [6, 227], "inspect": [6, 227, 230, 232], "content": [6, 16, 19, 23, 25, 27, 28, 31, 33, 34, 37, 41, 42, 68, 79, 115, 127, 177, 225, 226], "easili": [6, 8, 222, 226, 230, 231, 232], "torch": [6, 8, 55, 56, 57, 144, 146, 147, 150, 151, 152, 153, 154, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 181, 183, 184, 188, 190, 192, 193, 194, 195, 198, 199, 201, 202, 210, 211, 212, 213, 214, 215, 216, 217, 227, 228, 229, 230, 232], "import": [6, 8, 11, 46, 49, 53, 152, 165, 205, 208, 209, 225, 226, 227, 228, 229, 230, 231, 232], "00": [6, 219, 223, 228], "mmap": [6, 227], "true": [6, 8, 25, 38, 39, 40, 43, 44, 45, 46, 48, 49, 50, 51, 53, 54, 58, 65, 66, 67, 68, 69, 73, 76, 77, 79, 91, 92, 93, 94, 95, 103, 104, 112, 113, 115, 125, 126, 127, 133, 134, 144, 150, 151, 153, 157, 162, 167, 171, 174, 175, 177, 179, 181, 185, 186, 187, 188, 196, 198, 201, 202, 204, 205, 208, 215, 216, 224, 225, 226, 227, 229, 230, 231, 232], "weights_onli": [6, 188], "map_loc": [6, 227], "cpu": [6, 9, 153, 195, 215, 221, 224, 227, 232], "tensor": [6, 55, 56, 57, 58, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 156, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 181, 183, 184, 186, 193, 205, 206, 207, 208, 209, 210, 213, 230, 232], "item": 6, "print": [6, 10, 38, 45, 48, 50, 51, 68, 79, 95, 115, 127, 134, 152, 174, 175, 177, 193, 216, 225, 226, 228, 230, 231, 232], "f": [6, 10, 45, 48, 50, 225, 227, 230, 232], "tok_embed": [6, 150], "32000": [6, 11, 230], "4096": [6, 11, 39, 43, 45, 47, 49, 53, 54, 144, 149, 226, 230, 231], "len": [6, 38, 45, 48, 50, 150, 152], "292": 6, "contain": [6, 19, 21, 25, 33, 40, 41, 42, 43, 53, 68, 79, 95, 99, 115, 127, 132, 134, 143, 144, 147, 149, 150, 151, 155, 158, 159, 160, 162, 164, 170, 171, 175, 177, 180, 186, 187, 188, 190, 191, 192, 198, 203, 208, 210, 215, 225, 227, 229, 230], "input": [6, 15, 21, 22, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 51, 53, 54, 55, 56, 57, 58, 72, 85, 95, 99, 115, 124, 127, 132, 135, 139, 144, 145, 146, 148, 149, 150, 151, 152, 156, 164, 170, 174, 175, 179, 183, 184, 186, 188, 210, 214, 217, 225, 226, 230, 232], "embed": [6, 55, 56, 57, 58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 148, 149, 150, 152, 196, 225, 229, 231], "tabl": [6, 225, 227, 229, 232], "call": [6, 11, 25, 27, 41, 42, 114, 144, 145, 152, 153, 160, 191, 205, 206, 207, 208, 209, 211, 215, 225, 226, 230, 232], "layer": [6, 9, 58, 62, 63, 64, 65, 66, 67, 69, 73, 74, 75, 76, 77, 80, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 100, 101, 102, 103, 104, 105, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 122, 123, 125, 126, 128, 129, 130, 133, 135, 136, 137, 138, 139, 144, 150, 151, 152, 156, 160, 161, 185, 196, 222, 229, 230, 231, 232], "have": [6, 8, 11, 21, 41, 56, 57, 58, 144, 147, 152, 155, 161, 169, 179, 181, 188, 190, 191, 196, 204, 208, 217, 221, 225, 226, 227, 228, 229, 230, 231, 232], "dim": [6, 144, 145, 148, 149, 150], "most": [6, 8, 27, 180, 225, 228, 230, 232], "within": [6, 8, 11, 37, 40, 55, 73, 86, 100, 109, 116, 118, 128, 135, 145, 152, 182, 193, 208, 214, 215, 224, 226, 230, 232], "hub": [6, 41, 42, 224, 226, 228], "default": [6, 8, 17, 18, 19, 21, 23, 25, 31, 33, 34, 35, 37, 39, 40, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 58, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 148, 149, 150, 151, 153, 154, 156, 160, 162, 163, 164, 165, 169, 170, 174, 175, 177, 186, 187, 188, 191, 193, 195, 200, 204, 205, 206, 209, 210, 213, 214, 215, 221, 224, 225, 226, 227, 229, 230, 231, 232], "everi": [6, 9, 55, 56, 57, 145, 152, 208, 215, 221, 224, 232], "2": [6, 10, 36, 40, 51, 55, 56, 68, 79, 95, 114, 115, 127, 134, 144, 152, 164, 166, 167, 169, 170, 171, 174, 175, 177, 180, 181, 182, 183, 186, 187, 199, 210, 213, 214, 215, 216, 225, 227, 228, 229, 230, 231], "repo": [6, 186, 187, 189, 224, 227], "first": [6, 8, 11, 36, 40, 58, 147, 150, 152, 171, 186, 191, 220, 222, 225, 226, 227, 229, 230, 231, 232], "big": 6, "split": [6, 37, 38, 39, 40, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 134, 175, 225, 226, 227, 231], "across": [6, 9, 38, 186, 208, 214, 227, 229, 231], "bin": [6, 224, 227], "correctli": [6, 9, 14, 160, 186, 221, 225, 228, 232], "piec": 6, "one": [6, 9, 21, 36, 68, 79, 115, 127, 134, 145, 152, 177, 181, 182, 188, 205, 225, 226, 227, 228, 229, 232], "pytorch_model": [6, 227], "00001": [6, 224], "00002": [6, 224], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 7, 10, 25, 40, 41, 42, 47, 58, 73, 86, 100, 109, 114, 116, 118, 128, 135, 152, 156, 158, 160, 174, 186, 187, 188, 190, 191, 193, 195, 196, 198, 199, 204, 224, 226, 227, 228, 230, 231, 232], "doe": [6, 26, 33, 37, 40, 53, 69, 79, 114, 120, 131, 144, 150, 151, 155, 177, 186, 188, 190, 191, 224, 225, 227, 231], "fewer": [6, 144], "sinc": [6, 8, 11, 41, 42, 145, 181, 182, 183, 186, 188, 225, 227, 229, 231], "mismatch": 6, "name": [6, 7, 8, 10, 12, 15, 19, 21, 22, 23, 31, 39, 43, 48, 49, 50, 51, 52, 53, 54, 155, 159, 161, 175, 186, 187, 188, 189, 190, 191, 192, 193, 194, 205, 206, 207, 208, 209, 217, 224, 225, 227, 229, 231], "caus": [6, 115, 174, 183], "try": [6, 8, 225, 227, 228, 229, 232], "same": [6, 8, 27, 55, 56, 62, 63, 64, 68, 74, 75, 79, 87, 88, 89, 90, 101, 102, 110, 111, 115, 127, 129, 136, 137, 138, 147, 151, 152, 167, 169, 171, 177, 190, 191, 196, 209, 224, 225, 227, 229, 230, 231, 232], "As": [6, 8, 9, 10, 156, 222, 227, 232], "re": [6, 8, 169, 188, 222, 225, 227, 228, 230], "care": [6, 145, 186, 188, 227, 229, 230], "end": [6, 9, 25, 43, 53, 95, 115, 134, 175, 177, 220, 222, 225, 229, 230, 231], "number": [6, 9, 37, 39, 40, 43, 45, 46, 47, 49, 53, 54, 55, 56, 58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 150, 152, 154, 179, 180, 186, 187, 188, 193, 200, 214, 215, 224, 228, 230], "just": [6, 15, 222, 224, 225, 226, 228, 229, 230, 231], "save": [6, 9, 10, 153, 186, 187, 188, 190, 196, 204, 209, 220, 224, 225, 226, 227, 229, 230, 231], "less": [6, 182, 227, 228, 229, 232], "prone": 6, "manag": [6, 38, 157, 205, 213, 225], "invari": 6, "accept": [6, 8, 185, 226, 228, 232], "multipl": [6, 8, 9, 25, 37, 38, 42, 144, 150, 151, 152, 156, 170, 180, 181, 205, 206, 207, 208, 209, 215, 228, 229], "sourc": [6, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 225, 226, 227, 231], "worri": [6, 225, 228], "explicitli": [6, 155, 222, 230], "convert": [6, 19, 21, 23, 31, 33, 34, 37, 41, 42, 48, 50, 51, 134, 186, 210, 225, 227, 231, 232], "time": [6, 68, 69, 79, 115, 120, 127, 162, 177, 206, 208, 215, 224, 225, 226, 227, 229, 232], "produc": [6, 190, 231, 232], "back": [6, 36, 157, 186, 226, 230, 232], "form": [6, 8, 9, 36, 41, 42, 182, 224], "One": [6, 231], "advantag": [6, 162, 167, 230], "being": [6, 42, 186, 187, 188, 192, 194, 231, 232], "should": [6, 8, 9, 15, 16, 22, 23, 24, 25, 26, 27, 31, 33, 34, 40, 46, 49, 53, 62, 63, 64, 73, 74, 75, 78, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 114, 116, 117, 118, 119, 120, 122, 128, 129, 130, 134, 135, 136, 137, 138, 139, 144, 145, 152, 155, 160, 161, 162, 167, 184, 185, 191, 203, 205, 206, 207, 208, 209, 221, 222, 226, 227, 228, 229, 230, 231, 232], "abl": [6, 9, 227, 228, 231], "post": [6, 152, 211, 215, 227, 229, 231, 232], "tool": [6, 25, 27, 42, 114, 205, 226, 227, 228], "quantiz": [6, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 156, 188, 199, 220, 228, 232], "eval": [6, 220, 222, 231], "without": [6, 8, 10, 15, 160, 181, 183, 221, 222, 225, 227, 230, 231], "code": [6, 9, 59, 60, 61, 62, 63, 64, 65, 66, 67, 150, 205, 218, 222, 226, 228], "chang": [6, 7, 8, 10, 15, 19, 21, 188, 221, 224, 227, 228, 229, 230, 231, 232], "OR": [6, 33], "script": [6, 10, 224, 226, 227, 228, 229], "wai": [6, 8, 37, 41, 42, 160, 224, 225, 226, 227, 228, 229], "surround": [6, 9, 222], "load_checkpoint": [6, 9, 186, 187, 188, 189], "save_checkpoint": [6, 9, 10, 186, 187, 188], "map": [6, 15, 19, 21, 22, 23, 27, 31, 33, 34, 37, 38, 39, 40, 48, 49, 50, 51, 52, 95, 127, 159, 175, 176, 186, 190, 192, 205, 206, 207, 208, 209, 211, 215, 225, 226, 227, 230], "appli": [6, 9, 37, 39, 41, 42, 62, 63, 64, 65, 66, 67, 69, 73, 74, 75, 76, 77, 79, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 100, 101, 102, 103, 104, 105, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 125, 126, 128, 129, 133, 134, 135, 136, 137, 138, 139, 144, 148, 149, 150, 151, 160, 161, 212, 222, 232], "permut": 6, "certain": [6, 8, 215, 225], "ensur": [6, 8, 14, 36, 41, 42, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 186, 188, 195, 222, 226, 228], "behav": 6, "further": [6, 15, 152, 169, 224, 226, 230, 231, 232], "illustr": [6, 229], "whilst": 6, "other": [6, 9, 11, 21, 27, 38, 166, 188, 191, 196, 215, 226, 228, 229, 230, 231], "phi3": [6, 127, 128, 129, 131, 132, 133, 189, 224], "own": [6, 27, 204, 214, 224, 225, 226, 227, 229, 230], "found": [6, 7, 8, 10, 148, 149, 186, 187, 188, 224, 230, 232], "folder": [6, 225], "three": [6, 9, 41, 42, 165, 166, 168, 169, 228], "read": [6, 186, 187, 188, 222], "write": [6, 9, 15, 186, 187, 188, 206, 225, 226, 228], "compat": [6, 186, 188, 231], "transform": [6, 9, 19, 21, 23, 37, 39, 41, 42, 48, 50, 51, 58, 62, 63, 64, 69, 73, 74, 75, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 122, 128, 129, 130, 134, 135, 136, 137, 138, 139, 150, 151, 152, 154, 179, 180, 181, 182, 183, 184, 212, 230, 231], "framework": [6, 9, 222], "mention": [6, 227, 232], "assum": [6, 15, 22, 23, 31, 39, 48, 49, 50, 51, 52, 144, 149, 150, 151, 154, 158, 175, 190, 192, 195, 204, 225, 227, 230], "checkpoint_dir": [6, 8, 186, 187, 188, 227, 229, 231], "necessari": [6, 41, 42, 205, 206, 207, 208, 209, 225, 230], "easiest": [6, 227, 228], "sure": [6, 8, 225, 227, 228, 229, 230, 231, 232], "everyth": [6, 9, 191, 222, 228], "follow": [6, 9, 23, 25, 27, 31, 33, 34, 37, 40, 41, 42, 134, 144, 154, 167, 179, 180, 188, 189, 190, 202, 209, 215, 220, 221, 224, 226, 227, 228, 229, 230, 231, 232], "flow": [6, 37, 39, 40, 231, 232], "By": [6, 134, 224, 230, 231, 232], "safetensor": [6, 186, 224], "output_dir": [6, 8, 186, 187, 188, 215, 227, 229, 230, 231, 232], "here": [6, 7, 8, 10, 15, 17, 18, 48, 148, 149, 224, 225, 226, 227, 228, 229, 230, 231, 232], "argument": [6, 8, 11, 20, 22, 29, 32, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 144, 185, 191, 196, 201, 205, 206, 208, 209, 212, 224, 225, 226, 230, 231], "snippet": 6, "explain": 6, "setup": [6, 8, 9, 144, 150, 151, 215, 224, 226, 227, 230, 232], "_component_": [6, 7, 8, 10, 11, 38, 46, 49, 53, 215, 225, 226, 227, 229, 230, 231], "fullmodelhfcheckpoint": [6, 227], "directori": [6, 8, 186, 187, 188, 206, 208, 209, 215, 224, 225, 226, 227, 228, 229], "sort": [6, 186, 188], "so": [6, 8, 40, 152, 181, 186, 191, 221, 222, 225, 227, 228, 229, 230, 231, 232], "order": [6, 7, 9, 186, 188, 208, 209, 228], "matter": [6, 186, 188, 224, 230], "checkpoint_fil": [6, 8, 10, 186, 187, 188, 227, 229, 230, 231, 232], "restart": [6, 224], "previou": [6, 40, 186, 187, 188], "more": [6, 8, 9, 27, 37, 39, 41, 42, 43, 44, 46, 48, 49, 50, 51, 52, 53, 54, 79, 134, 147, 149, 152, 160, 185, 188, 191, 205, 209, 212, 214, 222, 224, 226, 227, 228, 229, 230, 231, 232], "next": [6, 40, 53, 58, 152, 179, 193, 229, 232], "section": [6, 9, 198, 220, 227, 229, 232], "recipe_checkpoint": [6, 186, 187, 188, 231], "null": [6, 8, 231], "usual": [6, 149, 171, 186, 209, 224, 227, 230], "model_typ": [6, 186, 187, 188, 227, 229, 231], "resume_from_checkpoint": [6, 186, 187, 188], "fals": [6, 8, 19, 21, 23, 25, 31, 33, 34, 37, 38, 39, 40, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 58, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 125, 126, 127, 128, 129, 133, 134, 135, 136, 137, 138, 139, 144, 150, 151, 156, 157, 160, 171, 174, 181, 186, 187, 188, 202, 215, 224, 225, 226, 227, 229, 230, 231, 232], "requir": [6, 8, 38, 41, 42, 43, 53, 79, 134, 170, 186, 188, 190, 199, 201, 202, 204, 205, 208, 209, 214, 215, 221, 224, 225, 226, 228, 231, 232], "param": [6, 9, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138, 156, 158, 159, 161, 186, 204, 230, 231, 232], "directli": [6, 8, 9, 11, 41, 42, 46, 49, 53, 165, 185, 186, 224, 227, 228, 229, 230, 231, 232], "out": [6, 8, 9, 39, 45, 46, 48, 50, 51, 179, 186, 187, 220, 222, 224, 225, 227, 228, 229, 230, 232], "case": [6, 9, 10, 25, 27, 56, 57, 58, 152, 186, 190, 195, 199, 204, 206, 212, 222, 224, 225, 226, 227, 229, 230, 232], "discrep": [6, 186], "along": [6, 182, 230], "github": [6, 11, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 134, 136, 137, 138, 144, 148, 149, 154, 160, 165, 166, 167, 168, 169, 221, 226, 227, 228, 229], "repositori": [6, 24, 37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 78, 227, 228], "fullmodelmetacheckpoint": [6, 229, 231], "current": [6, 40, 69, 73, 86, 100, 109, 116, 118, 120, 128, 131, 135, 144, 147, 149, 150, 151, 167, 187, 188, 196, 199, 200, 206, 208, 211, 214, 226, 228, 229, 231], "test": [6, 8, 9, 222, 225], "complet": [6, 9, 15, 40, 47, 53, 132, 166, 225, 226, 227, 228, 229], "written": [6, 8, 9, 186, 187, 205, 206, 207, 208, 209, 222], "begin": [6, 40, 53, 79, 115, 134, 152, 175, 225, 229, 232], "partit": [6, 186, 232], "ha": [6, 79, 115, 152, 155, 157, 158, 161, 171, 188, 190, 217, 225, 226, 227, 228, 229, 230, 232], "standard": [6, 20, 33, 41, 42, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 207, 222, 225, 227, 229], "key_1": [6, 188], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 227], "inform": [6, 134, 205, 209, 212, 222, 224, 227, 228], "subsequ": [6, 9, 152, 179], "recipe_st": [6, 186, 187, 188], "pt": [6, 10, 186, 187, 188, 227, 229, 231], "epoch": [6, 9, 10, 154, 186, 187, 188, 224, 225, 227, 228, 229, 231], "optim": [6, 8, 9, 38, 41, 69, 79, 120, 131, 154, 165, 167, 168, 169, 170, 188, 190, 192, 198, 211, 215, 225, 227, 228, 229, 230, 232], "etc": [6, 9, 186, 198, 228], "prevent": [6, 40, 165, 224], "flood": 6, "overwritten": 6, "note": [6, 8, 22, 73, 134, 150, 155, 190, 211, 214, 225, 226, 227, 230, 231, 232], "updat": [6, 8, 9, 27, 147, 165, 167, 178, 190, 215, 221, 225, 227, 228, 229, 230, 231, 232], "hf_model_0001_0": [6, 227], "hf_model_0002_0": [6, 227], "both": [6, 38, 41, 161, 224, 227, 230, 231, 232], "adapt": [6, 155, 156, 157, 158, 159, 186, 187, 188, 225, 227, 230, 232], "merg": [6, 11, 12, 134, 143, 186, 227, 229, 232], "would": [6, 8, 10, 27, 40, 150, 152, 166, 221, 225, 226, 227, 230, 232], "addition": [6, 169, 174, 175, 214, 226, 230], "option": [6, 8, 9, 15, 19, 21, 22, 23, 31, 35, 37, 39, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 58, 62, 63, 64, 68, 72, 73, 74, 75, 79, 80, 85, 86, 87, 88, 89, 90, 95, 96, 99, 100, 101, 102, 105, 109, 110, 111, 115, 116, 117, 118, 119, 124, 127, 128, 129, 132, 134, 135, 136, 137, 138, 139, 143, 144, 149, 150, 151, 152, 153, 160, 161, 162, 163, 167, 172, 174, 177, 181, 182, 183, 186, 187, 188, 193, 194, 195, 197, 199, 205, 206, 209, 214, 215, 221, 222, 224, 226, 227], "save_adapter_weights_onli": 6, "choos": [6, 230], "primari": [6, 8, 9, 41, 42, 228], "want": [6, 8, 9, 10, 11, 37, 41, 42, 180, 181, 193, 221, 224, 225, 226, 227, 228, 229, 230], "resum": [6, 9, 154, 186, 187, 188, 232], "initi": [6, 9, 13, 38, 40, 59, 60, 61, 70, 71, 81, 82, 83, 84, 97, 98, 106, 107, 108, 121, 123, 140, 141, 142, 165, 190, 201, 202, 228, 230, 232], "frozen": [6, 165, 230, 232], "base": [6, 11, 25, 27, 39, 41, 42, 62, 63, 64, 65, 66, 67, 69, 73, 74, 75, 76, 77, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 122, 125, 126, 128, 129, 130, 133, 134, 135, 136, 137, 138, 139, 149, 154, 156, 157, 159, 160, 161, 163, 165, 166, 168, 169, 186, 191, 194, 196, 204, 206, 220, 225, 227, 228, 229, 230, 232], "well": [6, 8, 9, 222, 224, 226, 227, 229, 232], "learnt": [6, 225, 227], "someth": [6, 9, 10, 225, 227, 231], "NOT": [6, 69, 120], "refer": [6, 8, 9, 148, 149, 152, 157, 163, 165, 166, 167, 168, 169, 205, 222, 230, 231], "adapter_checkpoint": [6, 186, 187, 188], "adapter_0": [6, 227], "now": [6, 190, 192, 225, 226, 227, 228, 229, 230, 231, 232], "knowledg": 6, "creat": [6, 8, 11, 23, 27, 40, 42, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 97, 98, 101, 102, 103, 104, 106, 107, 108, 110, 111, 112, 113, 117, 119, 121, 123, 125, 126, 129, 131, 133, 136, 137, 138, 140, 141, 142, 147, 152, 154, 185, 186, 187, 188, 192, 205, 206, 208, 224, 225, 226, 227, 232], "simpl": [6, 9, 15, 152, 169, 220, 226, 228, 230, 231, 232], "forward": [6, 9, 55, 56, 57, 144, 145, 146, 148, 149, 150, 151, 152, 156, 165, 166, 167, 168, 169, 198, 215, 229, 230, 232], "modeltyp": [6, 186, 187, 188], "llama2_13b": [6, 87], "right": [6, 186, 227, 229, 230], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 160, 190, 230], "successfulli": [6, 224, 228], "vocab": [6, 11, 134, 143, 150, 229], "70": [6, 97], "x": [6, 55, 56, 57, 144, 145, 146, 148, 149, 150, 151, 152, 156, 193, 213, 230, 231, 232], "randint": 6, "1": [6, 9, 40, 51, 55, 56, 68, 79, 80, 86, 95, 96, 100, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 120, 122, 127, 128, 130, 134, 135, 136, 137, 139, 140, 141, 144, 150, 152, 154, 164, 165, 166, 167, 168, 169, 170, 174, 175, 177, 181, 182, 183, 187, 189, 193, 202, 205, 208, 209, 210, 213, 214, 224, 225, 227, 228, 230, 231, 232], "no_grad": 6, "6": [6, 40, 69, 73, 148, 152, 164, 170, 184, 210, 231, 232], "3989": 6, "9": [6, 152, 170, 227, 231, 232], "0531": 6, "3": [6, 40, 58, 95, 114, 129, 131, 132, 134, 152, 164, 170, 181, 182, 183, 184, 189, 191, 197, 199, 210, 213, 224, 225, 227, 228, 229, 231, 232], "2375": 6, "5": [6, 8, 15, 152, 154, 164, 165, 169, 170, 171, 181, 210, 227, 228, 229], "2822": 6, "4": [6, 8, 58, 144, 152, 164, 170, 180, 199, 210, 216, 222, 224, 226, 227, 229, 230, 231, 232], "4872": 6, "7469": 6, "8": [6, 45, 48, 50, 62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 87, 88, 89, 90, 91, 92, 93, 94, 101, 102, 103, 104, 110, 111, 112, 113, 117, 119, 125, 126, 129, 133, 134, 136, 137, 138, 152, 164, 170, 227, 230, 231, 232], "6737": 6, "11": [6, 152, 170, 227, 231, 232], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 152, 164, 167, 170, 179, 210], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 152], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 7, 9, 25, 39, 49, 134, 160, 177, 205, 209, 224, 225, 226, 227, 228, 229, 230, 231], "find": [6, 7, 9, 10, 165, 224, 227, 228, 230], "hope": 6, "deeper": [6, 228], "insight": [6, 227], "happi": [6, 227], "thi": [7, 8, 9, 10, 11, 20, 21, 23, 25, 32, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 69, 73, 79, 80, 86, 95, 96, 100, 105, 109, 114, 115, 116, 118, 120, 122, 127, 128, 130, 131, 132, 134, 135, 139, 144, 145, 149, 150, 151, 152, 153, 154, 155, 157, 160, 161, 163, 165, 166, 167, 169, 170, 174, 175, 177, 179, 185, 186, 187, 188, 190, 191, 193, 194, 195, 198, 202, 204, 205, 206, 208, 209, 211, 212, 214, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 221, 225, 226, 228], "start": [7, 9, 10, 177, 189, 205, 221, 222, 225, 226, 227, 228, 231], "cometlogg": 7, "checkpoint": [7, 8, 9, 153, 175, 186, 187, 188, 189, 190, 209, 212, 222, 224, 229, 230, 231, 232], "workspac": [7, 10, 205], "seen": [7, 10, 230, 232], "screenshot": [7, 10], "below": [7, 10, 15, 149, 185, 226, 229, 230, 232], "instal": [7, 8, 10, 202, 205, 208, 209, 220, 224, 226, 227, 228, 229, 230, 231, 232], "comet_ml": [7, 205], "packag": [7, 10, 205, 208, 209, 221, 226], "featur": [7, 9, 10, 221, 222, 227, 228], "via": [7, 8, 10, 41, 46, 49, 53, 156, 186, 230, 232], "pip": [7, 10, 205, 208, 209, 221, 227, 229], "login": [7, 10, 205, 209, 224, 227], "data": [7, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 31, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 78, 79, 152, 165, 168, 178, 198, 205, 206, 207, 208, 209, 226, 227, 231, 232], "command": [7, 9, 10, 191, 221, 224, 225, 226, 227, 228, 229, 230, 231, 232], "add": [7, 8, 10, 37, 40, 41, 42, 43, 53, 79, 95, 114, 134, 152, 175, 177, 188, 189, 191, 226, 227, 229, 230, 232], "built": [7, 8, 10, 52, 221, 225, 228, 232], "metric_logg": [7, 8, 9, 10], "metric_log": [7, 8, 10, 205, 206, 207, 208, 209], "project": [7, 10, 58, 62, 63, 64, 69, 73, 80, 84, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 123, 128, 129, 135, 138, 139, 144, 145, 152, 160, 161, 189, 196, 205, 209, 220, 225, 230, 232], "experiment_nam": [7, 205], "my": [7, 193, 224, 225, 226, 227, 229], "experi": [7, 8, 205, 209, 220, 222, 225, 229, 230], "automat": [7, 8, 10, 11, 46, 224, 227, 232], "grab": [7, 10, 229], "hyperparamet": [7, 169, 190, 222, 228, 230, 232], "tab": [7, 10], "actual": [7, 8, 10, 15, 19, 21, 37, 41, 42, 225, 231], "asset": 7, "artifact": [7, 10, 215], "click": [7, 10], "sampl": [7, 10, 15, 16, 17, 21, 22, 23, 24, 25, 26, 31, 33, 34, 37, 39, 40, 41, 42, 43, 49, 51, 53, 144, 149, 150, 151, 152, 168, 178, 179, 193, 225, 227], "after": [7, 9, 27, 41, 42, 72, 85, 95, 99, 124, 127, 132, 144, 147, 148, 150, 151, 160, 171, 204, 205, 206, 207, 208, 209, 225, 227, 229, 231, 232], "pars": [8, 11, 12, 176, 191, 225, 228], "effect": [8, 169, 231], "cli": [8, 10, 12, 13, 221, 227, 228], "prerequisit": [8, 225, 226, 227, 228, 229, 230, 231, 232], "Be": [8, 225, 227, 228, 229, 230, 231, 232], "familiar": [8, 225, 227, 228, 229, 230, 231, 232], "fundament": [8, 231], "There": [8, 36, 56, 181, 225, 228, 229, 230], "entri": [8, 9, 228], "point": [8, 9, 33, 34, 177, 226, 227, 228, 229, 230, 231, 232], "locat": [8, 224, 226, 229, 230, 231, 232], "thei": [8, 9, 38, 58, 150, 152, 161, 191, 196, 224, 225, 226, 230, 231], "truth": [8, 227, 229], "reproduc": [8, 205], "overridden": [8, 145, 191, 215], "quick": 8, "experiment": 8, "serv": [8, 177, 185, 226, 230], "particular": [8, 37, 38, 41, 42, 185, 226, 230, 232], "seed": [8, 9, 10, 214, 228, 231], "shuffl": [8, 40, 231], "devic": [8, 9, 160, 190, 194, 195, 198, 224, 225, 227, 228, 229, 230], "cuda": [8, 194, 195, 198, 215, 221, 227, 232], "dtype": [8, 9, 144, 147, 150, 151, 153, 195, 213, 217, 227, 231, 232], "fp32": [8, 231, 232], "enable_fsdp": 8, "mani": [8, 40, 226, 227], "object": [8, 11, 12, 16, 17, 24, 26, 48, 50, 52, 58, 134, 144, 165, 169, 185, 199, 225], "keyword": [8, 11, 37, 39, 41, 42, 43, 46, 47, 49, 53, 54, 153, 225, 226], "loss": [8, 9, 25, 27, 39, 41, 42, 45, 48, 50, 51, 165, 166, 167, 168, 169, 228, 230, 232], "subfield": 8, "dotpath": [8, 226], "wish": [8, 226], "exact": [8, 11, 227], "path": [8, 9, 10, 11, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 68, 72, 79, 85, 95, 99, 115, 124, 127, 132, 134, 143, 174, 175, 176, 186, 187, 188, 191, 215, 224, 225, 226, 227, 229, 230], "normal": [8, 37, 40, 41, 42, 79, 115, 146, 148, 150, 151, 174, 225, 226, 230, 231, 232], "python": [8, 134, 191, 197, 205, 209, 214, 218, 224, 226, 227, 231], "alpaca_dataset": [8, 44, 226], "custom": [8, 9, 27, 37, 39, 41, 42, 46, 49, 53, 212, 222, 224, 228, 229, 230], "train_on_input": [8, 19, 21, 23, 31, 33, 34, 37, 38, 39, 44, 45, 46, 48, 49, 50, 51, 52, 225, 226], "onc": [8, 27, 157, 227, 228, 229, 230, 232], "ve": [8, 147, 224, 225, 226, 227, 229, 230], "instanc": [8, 11, 37, 38, 39, 86, 100, 109, 116, 118, 128, 135, 136, 137, 140, 141, 145, 153, 158, 159, 230], "cfg": [8, 9, 12, 13, 14], "under": [8, 215, 226, 232], "preced": [8, 11, 224, 229, 230], "throw": 8, "notic": [8, 55, 56, 57, 152, 225, 226, 230], "miss": [8, 160, 161, 215, 230], "posit": [8, 11, 40, 55, 56, 57, 58, 69, 73, 116, 118, 120, 122, 128, 130, 144, 147, 149, 150, 151, 152, 229], "anoth": [8, 42, 205, 227], "handl": [8, 13, 38, 42, 79, 115, 174, 175, 225, 227, 230, 232], "def": [8, 9, 10, 13, 185, 189, 225, 226, 230, 232], "dictconfig": [8, 9, 11, 12, 13, 14, 205, 209, 215], "arg": [8, 11, 18, 28, 57, 78, 114, 146, 150, 153, 155, 172, 173, 178, 191, 207, 215, 231], "tupl": [8, 11, 27, 57, 68, 79, 95, 115, 127, 134, 147, 152, 153, 162, 163, 165, 166, 167, 168, 169, 170, 171, 173, 177, 180, 181, 182, 183, 185, 191, 200, 215, 217], "kwarg": [8, 11, 18, 28, 78, 114, 143, 146, 153, 155, 172, 173, 178, 191, 201, 205, 206, 207, 208, 209, 212, 215, 226], "str": [8, 11, 12, 15, 19, 21, 22, 23, 25, 27, 31, 33, 34, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 68, 72, 79, 85, 95, 99, 115, 124, 127, 132, 134, 143, 153, 155, 156, 158, 159, 160, 161, 164, 170, 172, 173, 174, 175, 176, 186, 187, 188, 189, 190, 191, 194, 195, 197, 198, 199, 201, 203, 205, 206, 207, 208, 209, 210, 214, 215, 216, 217, 225, 226], "mean": [8, 144, 148, 150, 151, 162, 204, 224, 225, 226, 228, 230, 231], "pass": [8, 11, 25, 27, 37, 38, 39, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 145, 153, 157, 161, 167, 175, 185, 188, 195, 196, 198, 201, 204, 205, 208, 209, 212, 215, 224, 225, 226, 230, 231, 232], "d": [8, 25, 144, 147, 150, 224, 225, 230, 231], "llama2_token": [8, 225, 227], "llama2token": [8, 85], "modeltoken": [8, 25, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 177, 225, 226], "bool": [8, 19, 21, 23, 25, 31, 33, 34, 37, 39, 40, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 58, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 79, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 125, 126, 127, 128, 129, 133, 134, 135, 136, 137, 138, 139, 150, 151, 153, 156, 160, 161, 162, 171, 173, 174, 175, 177, 181, 185, 186, 187, 188, 196, 198, 201, 202, 204, 205, 208, 212, 215, 216, 225, 232], "max_seq_len": [8, 11, 35, 37, 39, 40, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 68, 69, 72, 73, 79, 80, 85, 86, 95, 96, 99, 100, 105, 109, 115, 116, 118, 120, 122, 124, 127, 128, 130, 132, 134, 135, 139, 143, 144, 147, 149, 150, 164, 177, 225, 226, 231], "int": [8, 10, 35, 37, 39, 40, 43, 44, 45, 46, 47, 49, 53, 54, 55, 56, 57, 58, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 77, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99, 100, 101, 102, 103, 104, 105, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 122, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134, 135, 136, 137, 138, 139, 143, 144, 147, 148, 149, 150, 151, 152, 154, 156, 164, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 193, 196, 200, 204, 205, 206, 207, 208, 209, 210, 212, 214, 215, 224, 225, 226, 230, 232], "512": [8, 44, 45, 58, 226, 232], "instructdataset": [8, 44, 45, 49, 226], "alreadi": [8, 144, 189, 201, 204, 221, 224, 226, 227, 230], "overwrit": [8, 188, 221, 224], "duplic": [8, 9, 222, 224], "sometim": 8, "than": [8, 36, 144, 147, 152, 165, 182, 185, 188, 189, 216, 217, 225, 226, 227, 228, 229, 230, 232], "resolv": [8, 12, 228], "alpaca": [8, 15, 38, 44, 45, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138, 226], "disklogg": 8, "log_dir": [8, 206, 208, 209], "conveni": [8, 9, 224], "verifi": [8, 194, 195, 196, 225, 228, 230], "properli": [8, 160, 202, 224], "wa": [8, 56, 57, 58, 152, 160, 182, 225, 230, 231, 232], "cp": [8, 221, 224, 225, 227, 228, 229, 231], "7b_lora_single_devic": [8, 227, 228, 230, 232], "my_config": [8, 224], "discuss": [8, 227, 228, 229, 230], "guidelin": 8, "while": [8, 9, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138, 145, 222, 227, 231, 232], "mai": [8, 10, 152, 196, 225, 226, 228, 230], "tempt": 8, "put": [8, 9, 228, 230, 231], "much": [8, 169, 227, 229, 230, 231, 232], "give": [8, 226, 230], "maximum": [8, 35, 37, 39, 40, 43, 45, 46, 47, 49, 53, 54, 55, 56, 58, 69, 72, 73, 80, 85, 86, 95, 96, 99, 100, 105, 109, 116, 118, 120, 122, 124, 128, 130, 132, 135, 139, 144, 147, 149, 150, 164, 180, 181, 182, 183, 224], "flexibl": [8, 38, 226], "switch": 8, "encourag": [8, 79, 169, 230], "clariti": 8, "significantli": [8, 165], "easier": [8, 227, 228], "dont": 8, "slimorca_dataset": 8, "privat": 8, "expos": [8, 9, 188, 225, 228], "parent": [8, 224], "modul": [8, 11, 48, 50, 52, 55, 56, 57, 58, 118, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 189, 192, 196, 204, 211, 212, 214, 226, 228, 230, 232], "__init__": [8, 9, 230, 232], "py": [8, 11, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 134, 136, 137, 138, 144, 147, 148, 149, 154, 165, 166, 167, 168, 169, 224, 227, 229], "guarante": 8, "stabil": [8, 222, 231, 232], "underscor": 8, "_alpaca": 8, "collect": [8, 193, 228], "itself": 8, "k1": [8, 9], "v1": [8, 9, 54], "k2": [8, 9], "v2": [8, 9, 205, 226], "lora_finetune_single_devic": [8, 224, 225, 227, 228, 229, 230, 232], "home": 8, "my_model_checkpoint": 8, "file_1": 8, "file_2": 8, "my_tokenizer_path": 8, "assign": [8, 41, 42], "nest": 8, "dot": 8, "notat": [8, 144, 149, 150, 162, 163], "flag": [8, 9, 25, 39, 45, 48, 50, 51, 185, 188, 196, 224, 232], "bitsandbyt": 8, "pagedadamw8bit": 8, "delet": 8, "foreach": [8, 37], "pytorch": [8, 9, 79, 150, 153, 160, 185, 202, 208, 212, 214, 215, 220, 221, 222, 227, 229, 230, 231, 232], "llama3": [8, 37, 95, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 128, 189, 193, 196, 220, 224, 226, 227], "8b_full": [8, 224, 226], "adamw": [8, 230], "lr": [8, 154], "2e": 8, "fuse": [8, 211, 231], "nproc_per_nod": [8, 226, 229, 230, 231], "full_finetune_distribut": [8, 224, 226, 227, 228], "core": [9, 41, 42, 222, 226, 228, 232], "i": [9, 24, 25, 26, 78, 114, 144, 150, 151, 152, 153, 159, 190, 193, 226, 227, 229, 231, 232], "structur": [9, 16, 17, 23, 24, 26, 28, 31, 33, 34, 37, 41, 42, 46, 99, 132, 143, 179, 225, 226, 227, 231], "new": [9, 23, 31, 47, 48, 50, 51, 52, 121, 147, 189, 205, 206, 208, 225, 227, 228, 229, 230, 232], "user": [9, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 41, 42, 68, 79, 80, 86, 96, 100, 105, 109, 115, 116, 118, 120, 122, 127, 128, 130, 134, 135, 139, 144, 177, 182, 183, 225, 226, 228, 231], "thought": [9, 222, 228, 232], "target": [9, 169, 182, 222], "pipelin": [9, 222], "llm": [9, 220, 222, 226, 227, 229, 230], "eg": [9, 150, 186, 222], "meaning": [9, 222, 227], "fsdp": [9, 185, 190, 196, 204, 222, 228, 229], "activ": [9, 145, 198, 203, 212, 215, 222, 231, 232], "gradient": [9, 204, 211, 215, 222, 227, 229, 230, 232], "accumul": [9, 211, 215, 222], "mix": [9, 146, 224, 226, 227], "precis": [9, 146, 153, 195, 222, 228, 232], "given": [9, 11, 15, 22, 36, 45, 47, 48, 50, 51, 52, 54, 134, 156, 157, 163, 166, 172, 173, 193, 194, 195, 199, 204, 211, 216, 222, 230], "complex": 9, "becom": [9, 152, 166, 221, 226], "harder": 9, "anticip": 9, "architectur": [9, 24, 26, 78, 114, 150, 152, 189, 224, 226], "methodolog": 9, "reason": [9, 193, 227, 231], "possibl": [9, 40, 46, 180, 181, 224, 226], "trade": 9, "off": [9, 27, 79, 115, 227, 231], "memori": [9, 38, 39, 40, 43, 45, 47, 49, 53, 54, 134, 153, 160, 196, 198, 203, 204, 215, 220, 222, 227, 228, 229, 231], "vs": [9, 166, 228], "qualiti": [9, 227, 230, 231], "believ": 9, "best": [9, 181, 225], "suit": [9, 228], "b": [9, 144, 147, 149, 150, 151, 156, 162, 163, 169, 204, 209, 230, 232], "fit": [9, 37, 39, 40, 43, 45, 47, 49, 53, 54, 152, 165, 166, 181, 182, 183, 226], "solut": [9, 166], "result": [9, 58, 68, 79, 115, 127, 152, 177, 179, 215, 227, 229, 230, 231, 232], "meant": [9, 153, 190], "depend": [9, 10, 15, 186, 215, 224, 226, 227, 230, 232], "level": [9, 41, 42, 134, 178, 192, 197, 204, 222, 232], "expertis": 9, "routin": 9, "yourself": [9, 224, 229, 230], "exist": [9, 205, 221, 224, 227, 228, 229, 232], "ad": [9, 27, 41, 55, 56, 57, 115, 122, 152, 174, 188, 189, 225, 226, 230, 231, 232], "ones": 9, "modular": [9, 222], "build": [9, 46, 49, 53, 58, 69, 80, 96, 105, 120, 122, 139, 222, 229, 230], "block": [9, 40, 62, 63, 64, 69, 73, 74, 75, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 128, 129, 135, 136, 137, 138, 139, 160, 161, 222], "wandb": [9, 10, 209, 228], "log": [9, 12, 165, 166, 167, 168, 169, 197, 198, 203, 205, 206, 207, 208, 209, 227, 228, 229, 230, 232], "fulli": [9, 38], "nativ": [9, 220, 222, 230, 231, 232], "correct": [9, 20, 48, 148, 149, 150, 194, 222, 225, 226], "numer": [9, 222, 231], "pariti": [9, 222], "verif": 9, "extens": [9, 188, 222], "comparison": [9, 230, 232], "benchmark": [9, 214, 222, 227, 229, 230, 231], "limit": [9, 181, 182, 183, 190, 226, 231], "hidden": [9, 58, 145, 152], "behind": 9, "100": [9, 39, 45, 48, 50, 51, 170, 193, 210, 230, 232], "prefer": [9, 41, 52, 165, 166, 167, 168, 169, 170, 222, 224, 226], "over": [9, 42, 154, 165, 166, 191, 222, 224, 227, 230, 232], "unnecessari": 9, "abstract": [9, 16, 22, 172, 173, 222, 228, 232], "No": [9, 188, 222], "inherit": [9, 191, 222, 226], "go": [9, 24, 26, 58, 68, 78, 79, 114, 115, 127, 152, 177, 222, 226, 227, 228, 232], "upon": [9, 38, 229], "figur": [9, 230, 232], "spectrum": 9, "decid": 9, "interact": [9, 220, 228], "avail": [9, 54, 191, 194, 195, 202, 222, 224, 227, 229, 230], "paradigm": [9, 134], "consist": [9, 54, 228], "configur": [9, 39, 41, 42, 45, 46, 47, 48, 49, 50, 51, 53, 54, 73, 86, 95, 100, 109, 116, 127, 128, 135, 151, 205, 222, 225, 228, 229, 230, 231, 232], "paramet": [9, 11, 12, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 26, 27, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 192, 193, 194, 195, 196, 197, 198, 199, 201, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 220, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232], "overrid": [9, 12, 13, 224, 227, 228, 229, 232], "togeth": [9, 40, 209, 228, 230, 231], "valid": [9, 36, 160, 161, 163, 217, 221, 227, 228], "environ": [9, 194, 202, 205, 221, 224, 226, 227, 228, 231], "logic": [9, 42, 173, 189, 222, 228, 230], "api": [9, 10, 20, 29, 32, 33, 41, 42, 65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 160, 205, 224, 225, 228, 229, 232], "closer": [9, 230], "monolith": [9, 222], "trainer": [9, 165, 166, 168, 169], "A": [9, 10, 20, 29, 32, 33, 34, 38, 40, 58, 68, 79, 115, 127, 134, 143, 144, 150, 151, 152, 153, 156, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 174, 175, 177, 179, 181, 185, 189, 190, 191, 198, 199, 203, 204, 210, 219, 220, 223, 224, 225, 230, 231, 232], "wrapper": [9, 146, 174, 175, 190, 192, 224, 230], "around": [9, 37, 41, 42, 79, 115, 146, 174, 175, 198, 224, 225, 227, 230, 231, 232], "extern": [9, 226], "primarili": [9, 38, 230], "eleutherai": [9, 54, 222, 230, 231], "har": [9, 222, 230, 231], "control": [9, 39, 45, 48, 50, 51, 157, 166, 205, 214, 227], "multi": [9, 37, 144, 160, 229], "stage": [9, 152], "distil": 9, "oper": [9, 134, 152, 157, 178, 214, 231], "turn": [9, 25, 36, 37, 225], "dataload": [9, 40, 45, 48, 50], "applic": [9, 144, 186, 187, 209], "clean": [9, 10, 44], "process": [9, 10, 41, 42, 58, 134, 152, 153, 200, 201, 214, 226, 228, 231, 232], "group": [9, 144, 200, 201, 205, 206, 207, 208, 209, 224, 229, 231], "init_process_group": [9, 201], "backend": [9, 224, 231], "gloo": 9, "els": [9, 191, 209, 222, 232], "nccl": 9, "fullfinetunerecipedistribut": 9, "cleanup": 9, "stuff": 9, "carri": [9, 42], "relev": [9, 224, 227, 230], "interfac": [9, 16, 22, 27, 28, 38, 155, 178, 226], "metric": [9, 228, 231], "logger": [9, 197, 203, 205, 206, 207, 208, 209, 228], "self": [9, 10, 40, 62, 63, 64, 69, 73, 74, 75, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 122, 128, 129, 130, 135, 136, 137, 138, 139, 144, 150, 151, 155, 160, 161, 186, 189, 190, 226, 230, 232], "_devic": 9, "get_devic": 9, "_dtype": 9, "get_dtyp": 9, "ckpt_dict": 9, "wrap": [9, 185, 196, 204, 212, 225], "_model": [9, 190], "_setup_model": 9, "_token": [9, 226], "_setup_token": 9, "_optim": 9, "_setup_optim": 9, "_loss_fn": 9, "_setup_loss": 9, "_sampler": 9, "_dataload": 9, "_setup_data": 9, "backward": [9, 190, 192, 211, 215, 232], "zero_grad": 9, "curr_epoch": 9, "rang": [9, 165, 167, 169, 214, 224, 229, 231], "epochs_run": [9, 10], "total_epoch": [9, 10], "idx": [9, 40], "batch": [9, 40, 45, 48, 50, 56, 144, 147, 149, 150, 151, 152, 162, 163, 164, 165, 166, 168, 169, 170, 210, 215, 222, 226, 228, 229, 230], "enumer": 9, "_autocast": 9, "logit": [9, 193], "label": [9, 37, 39, 40, 43, 45, 46, 47, 49, 51, 53, 54, 165, 169, 170, 210], "global_step": 9, "_log_every_n_step": 9, "_metric_logg": 9, "log_dict": [9, 205, 206, 207, 208, 209], "step": [9, 40, 41, 42, 150, 154, 162, 192, 205, 206, 207, 208, 209, 211, 215, 220, 227, 230, 231, 232], "learn": [9, 38, 154, 166, 222, 225, 226, 228, 229, 230, 231, 232], "decor": [9, 13], "recipe_main": [9, 13], "fullfinetunerecip": 9, "wandblogg": [10, 230, 232], "Then": [10, 157, 228], "tip": 10, "straggler": 10, "background": 10, "crash": 10, "otherwis": [10, 56, 57, 58, 152, 202, 205, 225, 231], "exit": [10, 221, 224], "resourc": [10, 205, 206, 207, 208, 209, 231], "kill": 10, "ps": 10, "aux": 10, "grep": 10, "awk": 10, "xarg": 10, "desir": [10, 37, 41, 42, 182, 183, 213, 225], "suggest": 10, "approach": [10, 38, 226], "full_finetun": 10, "joinpath": 10, "_checkpoint": [10, 227], "_output_dir": [10, 186, 187, 188], "torchtune_model_": 10, "with_suffix": 10, "wandb_at": 10, "type": [10, 11, 13, 25, 33, 34, 35, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 105, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 156, 158, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 204, 210, 212, 213, 214, 215, 216, 217, 226, 227, 230, 231, 232], "descript": [10, 46, 224], "whatev": 10, "metadata": [10, 231], "seed_kei": 10, "epochs_kei": 10, "total_epochs_kei": 10, "max_steps_kei": 10, "max_steps_per_epoch": [10, 231], "add_fil": 10, "log_artifact": 10, "field": [11, 21, 22, 25, 33, 34, 37, 40, 41, 42, 45, 48, 50, 203, 226], "hydra": 11, "facebook": 11, "research": 11, "http": [11, 37, 39, 43, 46, 47, 49, 53, 54, 59, 60, 61, 62, 63, 64, 65, 66, 67, 70, 71, 74, 75, 76, 77, 79, 81, 82, 83, 84, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 103, 104, 110, 111, 112, 113, 121, 123, 125, 126, 129, 131, 132, 133, 134, 136, 137, 138, 140, 141, 142, 144, 148, 149, 152, 154, 160, 162, 165, 166, 167, 168, 169, 179, 185, 186, 187, 191, 197, 202, 205, 208, 209, 212, 214, 221, 226, 227, 229], "com": [11, 62, 63, 64, 74, 75, 79, 87, 88, 89, 90, 95, 101, 102, 110, 111, 129, 134, 136, 137, 138, 144, 148, 149, 154, 160, 165, 166, 167, 168, 169, 205, 221, 227, 229], "facebookresearch": [11, 148], "blob": [11, 62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 132, 134, 136, 137, 138, 144, 148, 149, 154, 165, 166, 167, 168, 169], "main": [11, 13, 79, 132, 144, 148, 149, 221, 227, 229], "_intern": 11, "_instantiate2": 11, "l148": 11, "omegaconf": 11, "num_lay": [11, 58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 150, 152], "32": [11, 152, 205, 229, 230, 231, 232], "num_head": [11, 58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 149, 150], "num_kv_head": [11, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147], "vocab_s": [11, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139], "must": [11, 27, 38, 155, 191, 205, 232], "return": [11, 13, 15, 16, 17, 22, 24, 25, 26, 27, 33, 34, 35, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 75, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 210, 213, 214, 215, 216, 225, 226, 230, 232], "nn": [11, 144, 145, 147, 150, 151, 152, 153, 155, 157, 158, 159, 185, 192, 204, 211, 212, 217, 230, 232], "parsed_yaml": 11, "embed_dim": [11, 55, 56, 57, 58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 149, 151, 152, 230], "valueerror": [11, 26, 33, 36, 37, 39, 46, 127, 135, 144, 150, 152, 186, 187, 188, 195, 198, 214, 217], "recipe_nam": 12, "rank": [12, 62, 63, 64, 73, 74, 75, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 116, 117, 118, 119, 128, 129, 135, 136, 137, 138, 156, 200, 202, 214, 228, 230, 232], "zero": [12, 147, 148, 227, 229, 231], "displai": 12, "callabl": [13, 37, 39, 41, 42, 150, 157, 185, 193, 196, 199, 204, 212], "With": [13, 227, 230, 231, 232], "my_recip": 13, "foo": 13, "bar": [13, 222, 228], "instanti": [14, 27, 59, 60, 61, 62, 63, 64, 69, 70, 71, 72, 73, 74, 75, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 116, 117, 118, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 190], "configerror": 14, "cannot": [14, 134, 188, 229], "templat": [15, 18, 20, 22, 27, 28, 29, 32, 37, 38, 39, 41, 42, 45, 46, 48, 49, 50, 51, 52, 78, 79, 114, 134], "style": [15, 40, 44, 45, 46, 51, 232], "slightli": 15, "describ": [15, 79, 95, 212, 226], "task": [15, 20, 29, 32, 38, 41, 42, 47, 225, 226, 227, 229, 230, 231, 232], "context": [15, 17, 18, 131, 157, 213, 215, 226], "respons": [15, 17, 18, 21, 25, 41, 42, 68, 79, 115, 127, 162, 163, 165, 166, 168, 169, 177, 226, 227, 228, 229], "appropri": [15, 17, 24, 25, 26, 38, 78, 154, 186, 226, 232], "Or": 15, "instruciton": 15, "classmethod": [15, 16, 17, 22, 24, 25, 26, 226], "column_map": [15, 19, 21, 22, 23, 31, 38, 39, 48, 49, 50, 51, 52, 226], "placehold": [15, 22, 39, 49, 226], "column": [15, 19, 21, 22, 23, 31, 39, 41, 42, 43, 48, 49, 50, 51, 52, 53, 144, 150, 151, 225, 226, 231], "ident": [15, 22, 23, 26, 31, 39, 40, 48, 49, 50, 51, 52, 114, 166, 227, 231], "poem": 15, "n": [15, 20, 27, 29, 32, 68, 79, 115, 127, 144, 152, 177, 181, 219, 223, 224, 225, 226, 231], "nwrite": 15, "long": [15, 40, 134, 175, 225, 226, 230], "where": [15, 27, 37, 45, 48, 50, 56, 79, 84, 115, 123, 144, 150, 152, 156, 162, 165, 166, 167, 170, 171, 174, 179, 181, 196, 204, 226], "me": 15, "tag": [16, 17, 24, 26, 27, 37, 41, 42, 78, 114, 205, 206, 207, 208, 209, 225], "system": [16, 17, 18, 23, 24, 25, 26, 27, 28, 30, 31, 33, 34, 36, 37, 41, 42, 68, 78, 79, 114, 115, 127, 134, 177, 225, 226], "assist": [16, 17, 18, 19, 21, 23, 24, 25, 27, 28, 30, 31, 33, 34, 36, 37, 41, 42, 68, 78, 79, 115, 127, 132, 134, 177, 193, 225, 226], "role": [16, 19, 23, 25, 27, 28, 31, 33, 34, 37, 41, 42, 68, 79, 115, 127, 177, 225, 226], "prepend": [16, 27, 28, 79, 95, 115, 174], "append": [16, 27, 28, 95, 115, 127, 174, 205, 221, 226], "messag": [16, 17, 18, 19, 21, 23, 24, 26, 27, 28, 31, 33, 34, 36, 37, 41, 42, 46, 48, 50, 51, 68, 72, 79, 85, 95, 99, 115, 124, 127, 132, 134, 173, 177, 221, 224, 225, 226], "accord": [16, 26, 114, 225], "openai": [17, 18, 33, 46, 167, 226], "markup": [17, 18], "languag": [17, 18, 134, 156, 165, 193, 230], "It": [17, 18, 25, 26, 27, 41, 42, 114, 152, 165, 169, 182, 205, 224, 225, 226, 232], "im_start": [17, 18], "im_end": [17, 18], "goe": [17, 18, 157], "chosen": [19, 41, 165, 166, 168, 169, 215, 226], "reject": [19, 41, 165, 166, 168, 169, 226], "q1": [19, 41], "a1": [19, 41], "a2": [19, 41], "whether": [19, 21, 23, 25, 31, 33, 34, 37, 39, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 62, 63, 64, 69, 73, 74, 75, 86, 87, 88, 89, 90, 95, 100, 101, 102, 109, 110, 111, 115, 116, 117, 118, 119, 127, 128, 129, 134, 135, 136, 137, 138, 139, 153, 156, 160, 161, 174, 175, 185, 195, 198, 205, 225, 226], "keep": [19, 21, 227, 230], "functool": [20, 29, 32, 185], "partial": [20, 29, 32, 185], "_prompt_templ": [20, 29, 32, 48, 50, 52], "prompttempl": [20, 29, 32, 41, 42, 48, 50, 51, 52], "english": 20, "ncorrect": 20, "grammar": [20, 48, 226], "user_messag": [20, 29, 32, 225], "assistant_messag": [20, 29, 32, 225], "equival": [21, 56, 166, 168, 169], "respect": [21, 24, 38, 78, 159, 181, 215, 225, 226], "alwai": [22, 48, 50, 51, 166, 191, 205], "dataclass": [23, 225], "remain": [23, 31, 33, 34, 154, 230], "unmask": [23, 31, 33, 34], "human": [24, 25, 31, 34, 78, 165, 167, 168, 225], "pre": [24, 40, 41, 42, 53, 78, 79, 152, 221, 225, 226], "taken": [24, 78, 230, 232], "inst": [24, 26, 37, 41, 42, 78, 79, 114, 225, 226], "sy": [24, 78, 79, 225, 226], "honest": [24, 78, 225, 226], "am": [24, 26, 78, 114, 225, 226, 227, 229], "pari": [24, 26, 78, 114, 226], "capit": [24, 26, 78, 114, 226], "franc": [24, 26, 78, 114, 226], "known": [24, 26, 78, 79, 114, 115, 199, 226, 231], "its": [24, 26, 40, 78, 114, 118, 144, 149, 150, 151, 166, 211, 214, 224, 225, 226, 227, 229, 230], "stun": [24, 26, 78, 114, 226], "liter": [25, 27, 30, 62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 160, 161], "ipython": [25, 27, 30, 41, 42], "union": [25, 48, 50, 51, 53, 54, 135, 139, 161, 205, 206, 207, 208, 209, 212, 214], "mask": [25, 27, 39, 40, 42, 45, 48, 50, 51, 68, 79, 95, 115, 127, 134, 144, 150, 151, 162, 167, 173, 177, 179, 225, 226], "eot": [25, 95], "repres": [25, 55, 56, 152, 170, 181, 225, 231], "individu": [25, 40, 198, 209, 212, 225, 226], "interleav": [25, 179], "tokenize_messag": [25, 37, 39, 41, 43, 45, 46, 47, 49, 52, 53, 54, 68, 79, 95, 115, 127, 134, 173, 177, 225, 226], "attach": 25, "special": [25, 37, 41, 42, 79, 95, 99, 115, 127, 132, 134, 143, 152, 172, 173, 175, 176, 177, 179, 190, 226], "writer": 25, "dictionari": [25, 27, 40, 41, 42, 164, 170, 198, 203, 205, 206, 207, 208, 209, 210, 227], "hello": [25, 68, 79, 95, 115, 127, 134, 174, 175, 225, 227, 229], "world": [25, 68, 79, 95, 115, 127, 134, 174, 175, 200, 202, 227], "calcul": [25, 27, 144, 150, 151, 152, 162, 163, 167, 181, 182, 229], "correspond": [25, 155, 158, 162, 167, 170, 195, 228, 229, 231], "consecut": [25, 36, 179], "e": [25, 37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 134, 144, 152, 153, 155, 159, 179, 181, 186, 190, 198, 215, 221, 227, 229, 230, 231, 232], "properti": [25, 151, 191, 230], "contains_media": 25, "non": [25, 161, 163], "from_dict": [25, 225], "construct": [25, 134, 179, 230], "text_cont": [25, 225], "mistral": [26, 37, 41, 42, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 189, 224, 225, 227, 228], "llama2chatformat": [26, 79, 225, 226], "achiev": [27, 211, 227, 229, 230, 231, 232], "prepend_tag": 27, "append_tag": 27, "thu": [27, 41, 42, 166, 231], "consid": [27, 38, 41, 42, 56, 57, 58, 152], "come": [27, 36, 155, 230], "question": [29, 225, 226, 227, 229], "nanswer": 29, "answer": [29, 225, 227, 229], "alia": [30, 185], "adher": [31, 33, 34], "sharegpt": [31, 34, 46], "gpt": [31, 34, 134, 144, 227], "summar": [32, 50, 225, 226], "dialogu": [32, 50, 225], "nsummari": [32, 225], "summari": [32, 38, 50, 152, 198, 226], "could": [33, 230], "eos_id": [35, 95, 175, 177], "length": [35, 36, 38, 39, 40, 43, 45, 47, 49, 53, 54, 68, 69, 72, 73, 79, 80, 85, 86, 95, 96, 99, 100, 105, 109, 115, 116, 118, 120, 122, 124, 127, 128, 130, 131, 132, 134, 135, 139, 143, 144, 147, 149, 150, 162, 163, 164, 170, 175, 177, 179, 187, 205, 210], "last": [35, 40, 53, 154, 163, 226], "replac": [35, 39, 45, 48, 50, 51, 134, 153, 230], "forth": [36, 226], "empti": [36, 224], "shorter": 36, "min": [36, 181, 230], "invalid": 36, "convert_to_messag": [37, 225], "chat_format": [37, 46, 225, 226], "chatformat": [37, 46, 226], "load_dataset_kwarg": [37, 39, 41, 42, 43, 46, 47, 49, 53, 54], "multiturn": [37, 225], "prepar": [37, 225, 231], "truncat": [37, 39, 40, 43, 47, 49, 53, 54, 68, 72, 79, 85, 95, 99, 115, 124, 127, 132, 134, 143, 171, 175, 177, 226], "local": [37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 99, 132, 143, 205, 209, 214, 221, 224, 225, 227, 228], "g": [37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 134, 144, 152, 155, 179, 181, 186, 198, 215, 229, 230, 231, 232], "csv": [37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 225, 226], "filepath": [37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54], "data_fil": [37, 39, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 54, 225, 226], "load_dataset": [37, 39, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 225, 226], "huggingfac": [37, 39, 43, 46, 47, 49, 53, 54, 123, 131, 132, 134, 140, 141, 142, 154, 165, 166, 168, 169, 186, 187, 224, 227], "co": [37, 39, 43, 46, 47, 49, 53, 54, 123, 131, 132, 140, 141, 142, 186, 187, 227], "doc": [37, 39, 42, 43, 46, 47, 49, 53, 54, 79, 95, 134, 185, 191, 197, 202, 205, 208, 209, 214, 224, 226, 227], "en": [37, 39, 43, 46, 47, 49, 53, 54, 231], "package_refer": [37, 39, 43, 46, 47, 49, 53, 54], "loading_method": [37, 39, 43, 46, 47, 49, 53, 54], "extra": [37, 41, 42, 221, 225, 230, 231, 232], "still": [37, 41, 42, 191, 230, 231, 232], "unless": 37, "concaten": [38, 68, 79, 115, 127, 134, 170, 173, 177], "sub": [38, 208], "unifi": [38, 123], "were": [38, 152, 157, 167, 225, 228, 231], "simplifi": [38, 165, 224, 230], "simultan": 38, "intern": [38, 191], "aggreg": 38, "transpar": 38, "index": [38, 40, 144, 149, 150, 151, 154, 163, 164, 170, 210, 221, 225, 227], "howev": [38, 132, 221], "constitu": 38, "might": [38, 224, 227], "larg": [38, 134, 156, 215, 224, 232], "comput": [38, 41, 42, 80, 86, 96, 100, 105, 109, 135, 139, 144, 145, 149, 150, 165, 166, 168, 169, 179, 180, 198, 214, 227, 231, 232], "cumul": 38, "maintain": [38, 232], "indic": [38, 39, 40, 58, 144, 149, 150, 151, 152, 162, 167, 171, 179, 185, 202, 225], "deleg": 38, "retriev": [38, 41, 42, 196], "lead": [38, 115, 174], "high": [38, 41, 42, 222, 230], "scale": [38, 62, 63, 64, 73, 74, 75, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 116, 117, 118, 119, 128, 129, 135, 136, 137, 138, 156, 163, 166, 169, 181, 193, 230, 231, 232], "strategi": 38, "stream": [38, 197], "demand": 38, "deriv": [38, 145, 150, 151], "dataset1": 38, "mycustomdataset": 38, "params1": 38, "dataset2": 38, "params2": 38, "concat_dataset": 38, "total": [38, 154, 163, 167, 200, 219, 223, 227, 229, 230], "data_point": 38, "1500": 38, "element": [38, 227], "accomplish": [38, 46, 49, 53], "instruct_dataset": [38, 226], "vicgal": [38, 226], "gpt4": [38, 226], "alpacainstructtempl": [38, 49, 226], "samsum": [38, 50, 226], "summarizetempl": [38, 225, 226], "focus": [38, 228], "enhanc": [38, 152, 169, 232], "divers": 38, "machin": [38, 168, 194, 224, 227], "instructtempl": [39, 226], "contribut": [39, 45, 48, 50, 51, 163, 167], "variabl": [39, 49, 134, 189, 202, 205, 226, 232], "disabl": [39, 43, 47, 49, 53, 54, 157, 214, 231], "recommend": [39, 43, 45, 47, 49, 53, 54, 114, 205, 208, 225, 227, 232], "highest": [39, 43, 45, 47, 49, 53, 54], "sequenc": [39, 40, 43, 45, 47, 49, 53, 54, 68, 69, 72, 73, 79, 80, 85, 86, 95, 96, 99, 100, 105, 109, 115, 116, 118, 120, 122, 124, 127, 128, 130, 132, 134, 135, 139, 143, 144, 147, 149, 150, 152, 163, 164, 169, 170, 171, 175, 177, 179, 210, 225], "ds": [40, 51], "padding_idx": [40, 164, 170, 210], "max_pack": 40, "split_across_pack": [40, 53], "greedi": 40, "pack": [40, 44, 45, 46, 48, 49, 50, 51, 53, 54, 144, 149, 150, 151, 231], "done": [40, 160, 195, 204, 230, 231, 232], "outsid": [40, 214, 215, 230], "sampler": [40, 228], "part": [40, 168, 225, 232], "buffer": 40, "enough": [40, 225], "attent": [40, 58, 62, 63, 64, 69, 73, 74, 75, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 122, 128, 129, 130, 131, 135, 136, 137, 138, 139, 144, 147, 149, 150, 151, 160, 161, 179, 229, 230, 232], "lower": [40, 230], "triangular": 40, "cross": [40, 179], "attend": [40, 144, 150, 151, 179], "rel": [40, 144, 149, 150, 151, 165, 198, 230], "pad": [40, 134, 152, 163, 164, 167, 170, 171, 181, 183, 210, 226], "max": [40, 68, 79, 115, 127, 134, 143, 150, 152, 154, 175, 177, 224, 230], "wise": 40, "collat": [40, 210, 226], "made": [40, 46, 49, 53, 149, 227], "smaller": [40, 166, 227, 229, 230, 231, 232], "jam": 40, "vari": 40, "s1": [40, 79, 115, 174], "s2": [40, 79, 115, 174], "s3": 40, "s4": 40, "contamin": 40, "input_po": [40, 144, 147, 149, 150, 151], "matrix": 40, "causal": [40, 144, 150, 151], "continu": [40, 152, 205, 226], "increment": 40, "move": [40, 53, 150], "entir": [40, 53, 204, 225, 232], "avoid": [40, 53, 148, 152, 153, 166, 214, 224, 231, 232], "sentenc": [40, 53, 115], "message_transform": [41, 42], "prompt_templ": [41, 42, 48, 50, 51, 52], "techniqu": [41, 222, 227, 228, 229, 230, 231], "rlhf": [41, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 226], "dpo": [41, 157, 165, 166, 168, 169, 170], "remot": [41, 42], "separ": [41, 68, 79, 115, 127, 177, 186, 225, 228, 229, 230, 232], "repons": 41, "At": [41, 42, 150], "uniqu": [41, 42, 79, 189], "extract": [41, 42, 47, 176], "becaus": [41, 42, 73, 147, 150, 152, 188, 224, 225, 231], "against": [41, 42, 169, 216, 231, 232], "unit": [41, 42, 204, 222], "row": [41, 42, 144, 150, 151, 181, 225], "final": [41, 42, 62, 63, 64, 69, 73, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 128, 129, 135, 138, 139, 145, 150, 157, 160, 161, 227, 229, 230, 232], "model_transform": [41, 42, 48, 50, 51], "sftdataset": [41, 48, 50, 51], "gear": [41, 42], "whenev": [41, 42, 230], "commun": [41, 42, 225, 226, 227], "chatmlformat": [41, 46], "ref": [41, 42, 131, 132, 209], "filter_fn": 42, "supervis": 42, "round": [42, 231], "involv": [42, 231], "incorpor": [42, 165, 226], "media": 42, "happen": 42, "ti": [42, 73, 135, 139], "agnost": [42, 226], "treat": [42, 152, 157, 191, 225], "modal": 42, "minimum": 42, "chatmltempl": 42, "filter": [42, 231], "prior": [42, 45, 46, 48, 49, 50, 51, 53, 54], "add_eo": [43, 53, 68, 79, 95, 115, 127, 134, 174, 175, 225], "freeform": [43, 53], "unstructur": [43, 53, 54], "corpu": [43, 47, 53, 54], "tabular": [43, 53], "txt": [43, 53, 134, 143, 206, 226, 228], "eo": [43, 53, 115, 127, 132, 174, 177, 225, 226], "yahma": [44, 49], "variant": [44, 48, 50], "version": [44, 73, 86, 100, 109, 116, 118, 128, 135, 144, 193, 216, 221, 225, 229, 231, 232], "page": [44, 54, 221, 222, 224, 228, 229], "tatsu": 45, "lab": 45, "codebas": [45, 48, 50, 227], "anyth": [45, 47], "subset": [45, 47, 48, 50, 51, 52, 54, 73, 86, 100, 109, 116, 118, 128, 135, 158], "10": [45, 47, 48, 50, 51, 52, 54, 152, 170, 210, 227, 229, 231, 232], "alpaca_d": 45, "batch_siz": [45, 48, 50, 144, 147, 150, 151, 164, 165, 166, 168, 171, 227, 231], "conversation_styl": [46, 226], "chatdataset": [46, 225, 226], "friendli": [46, 49, 53, 193, 225], "check": [46, 55, 56, 57, 58, 150, 151, 152, 160, 195, 202, 216, 220, 225, 227, 228, 230], "huggingfaceh4": 46, "no_robot": 46, "2096": [46, 49, 53], "packeddataset": [46, 48, 49, 50, 51, 53, 54, 226], "ccdv": 47, "cnn_dailymail": 47, "textcompletiondataset": [47, 53, 54, 226], "similar": [47, 52, 53, 54, 160, 165, 226, 227, 229, 230, 232], "cnn": 47, "dailymail": 47, "articl": [47, 54], "highlight": [47, 232], "_transform": [48, 50, 152], "liweili": 48, "c4_200m": 48, "mirror": [48, 50], "llama_recip": [48, 50], "grammarerrorcorrectiontempl": [48, 50], "grammar_d": 48, "alpaca_clean": 49, "samsung": 50, "samsum_d": 50, "open": [51, 70, 71, 226, 227], "orca": 51, "slimorca": 51, "dedup": 51, "351": 51, "82": 51, "391": 51, "221": 51, "220": 51, "193": 51, "12": [51, 152, 170, 221, 231], "471": 51, "_util": 52, "lvwerra": [52, 226], "stack": [52, 152, 215, 226], "exchang": [52, 226], "preferencedataset": [52, 226], "questionanswertempl": 52, "allenai": [53, 226, 231], "c4": [53, 226, 231], "data_dir": [53, 226], "realnewslik": [53, 226], "wikitext_document_level": 54, "wikitext": [54, 231], "103": [54, 227], "wikipedia": 54, "clip": [55, 56, 57, 58, 152, 167], "max_num_til": [55, 56, 58, 152, 180], "tile": [55, 56, 57, 58, 152, 179, 180, 184], "patch": [55, 56, 57, 58, 152, 179], "document": [55, 56, 57, 58, 144, 166, 185, 196, 204, 224, 226], "vision_transform": [55, 56, 57, 58], "visiontransform": [55, 56, 57, 58], "divid": [55, 56, 57, 58, 152, 179, 180, 184], "dimension": [55, 56, 57, 58, 152], "aspect_ratio": [55, 56, 152], "bsz": [55, 56, 152, 193], "n_img": [55, 56, 152], "n_tile": [55, 56, 152], "n_token": [55, 56, 57, 152], "aspect": [55, 56, 222], "ratio": [55, 56, 165, 166, 167], "crop": [55, 56, 57, 58, 152, 184], "tile_s": [56, 57, 58, 152, 179, 180, 184], "patch_siz": [56, 57, 58, 152, 179], "local_token_positional_embed": 56, "_position_embed": [56, 152], "tokenpositionalembed": [56, 152], "gate": [56, 189, 224, 228], "global_token_positional_embed": 56, "advanc": [56, 57, 58, 152, 226], "40": [56, 57, 58, 134, 152, 179, 232], "400": [56, 57, 58, 152, 179, 184], "10x10": [56, 57, 58, 152, 179], "grid": [56, 57, 58, 152, 179], "k": [56, 144, 230], "th": 56, "cls_output_dim": [58, 152], "out_indic": [58, 152], "output_cls_project": 58, "in_channel": [58, 152], "transformerencoderlay": 58, "cl": [58, 152, 226], "head": [58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 149, 150, 189, 229], "intermedi": [58, 69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 152, 188, 212, 229, 232], "fourth": [58, 152], "determin": [58, 161, 181], "channel": [58, 152, 231], "code_llama2": [59, 60, 61, 62, 63, 64, 65, 66, 67, 224], "transformerdecod": [59, 60, 61, 62, 63, 64, 65, 66, 67, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 121, 122, 123, 125, 126, 128, 129, 130, 131, 133, 135, 138, 139, 142, 193, 230], "w": [59, 60, 61, 70, 71, 81, 82, 83, 84, 97, 98, 106, 107, 108, 121, 123, 140, 141, 142, 152, 183, 205, 208, 209, 225, 227, 230, 232], "arxiv": [59, 60, 61, 65, 66, 67, 76, 77, 81, 82, 83, 84, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 144, 148, 149, 152, 162, 165, 166, 167, 168, 169, 179], "org": [59, 60, 61, 65, 66, 67, 76, 77, 79, 81, 82, 83, 84, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 134, 144, 148, 149, 152, 162, 165, 166, 167, 168, 169, 179, 185, 191, 197, 202, 208, 212, 214, 221], "pdf": [59, 60, 61, 162, 179], "2308": [59, 60, 61], "12950": [59, 60, 61], "lora_attn_modul": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 160, 161, 230, 232], "q_proj": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 144, 160, 161, 230, 231, 232], "k_proj": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 144, 160, 161, 230, 231, 232], "v_proj": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 144, 160, 161, 230, 231, 232], "output_proj": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 144, 160, 161, 230, 231, 232], "apply_lora_to_mlp": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 160, 161, 230], "apply_lora_to_output": [62, 63, 64, 65, 66, 67, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 138, 160, 161, 230], "lora_rank": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 230], "lora_alpha": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 230], "float": [62, 63, 64, 65, 66, 67, 69, 73, 74, 75, 76, 77, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 100, 101, 102, 103, 104, 105, 109, 110, 111, 112, 113, 116, 117, 118, 119, 120, 122, 125, 126, 128, 129, 130, 133, 135, 136, 137, 138, 139, 144, 148, 154, 156, 162, 163, 165, 166, 167, 168, 169, 193, 198, 203, 205, 206, 207, 208, 209, 230, 231, 232], "16": [62, 63, 64, 65, 66, 67, 74, 75, 76, 77, 87, 88, 89, 90, 91, 92, 93, 94, 101, 102, 103, 104, 110, 111, 112, 113, 117, 119, 125, 126, 129, 133, 136, 137, 138, 152, 170, 230, 232], "lora_dropout": [62, 63, 64, 65, 66, 67, 73, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 109, 116, 118, 128, 135, 136, 137, 138], "05": [62, 63, 64, 65, 66, 67, 80, 86, 87, 88, 89, 90, 91, 92, 93, 94, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 136, 137, 138, 139], "quantize_bas": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 156, 232], "lora": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 156, 157, 160, 161, 186, 204, 220, 222, 225, 228, 229], "code_llama2_13b": 62, "tloen": [62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138], "8bb8579e403dc78e37fe81ffbb253c413007323f": [62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138], "l41": [62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138], "l43": [62, 63, 64, 74, 75, 87, 88, 89, 90, 101, 102, 110, 111, 129, 136, 137, 138], "linear": [62, 63, 64, 65, 66, 67, 73, 74, 75, 76, 77, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 104, 109, 110, 111, 112, 113, 116, 117, 118, 119, 125, 126, 128, 129, 133, 135, 136, 137, 138, 150, 155, 156, 160, 161, 230, 231, 232], "mlp": [62, 63, 64, 69, 73, 74, 75, 80, 86, 87, 88, 89, 90, 96, 100, 101, 102, 105, 109, 110, 111, 116, 117, 118, 119, 120, 122, 128, 129, 130, 135, 136, 137, 138, 139, 150, 151, 160, 161, 229, 230], "low": [62, 63, 64, 73, 74, 75, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 116, 117, 118, 119, 128, 129, 135, 136, 137, 138, 156, 227, 230, 232], "approxim": [62, 63, 64, 73, 74, 75, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 116, 117, 118, 119, 128, 129, 135, 136, 137, 138, 156, 230], "factor": [62, 63, 64, 73, 74, 75, 86, 87, 88, 89, 90, 100, 101, 102, 109, 110, 111, 116, 117, 118, 119, 128, 129, 135, 136, 137, 138, 156, 162, 181, 227], "dropout": [62, 63, 64, 69, 73, 80, 86, 87, 88, 89, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 156, 230, 232], "probabl": [62, 63, 64, 73, 86, 87, 88, 89, 100, 109, 116, 118, 128, 135, 156, 165, 166, 167, 168, 169, 193, 227], "code_llama2_70b": 63, "code_llama2_7b": 64, "qlora": [65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 153, 220, 222, 229, 230], "per": [65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 147, 152, 153, 163, 165, 179, 180, 224, 231, 232], "paper": [65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 165, 166, 168, 169, 179, 230, 232], "ab": [65, 66, 67, 76, 77, 81, 82, 83, 84, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 144, 148, 149, 152, 165, 166, 167, 168, 169], "2305": [65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133, 144, 165, 168], "14314": [65, 66, 67, 76, 77, 91, 92, 93, 94, 103, 104, 112, 113, 125, 126, 133], "lora_code_llama2_13b": 65, "lora_code_llama2_70b": 66, "lora_code_llama2_7b": 67, "gemma": [68, 70, 71, 72, 73, 74, 75, 76, 77, 189], "sentencepiec": [68, 79, 115, 127, 174, 229], "pretrain": [68, 79, 95, 115, 127, 174, 175, 224, 225, 228, 230, 232], "spm_model": [68, 79, 115, 127, 174, 225], "tokenized_text": [68, 79, 95, 115, 127, 134, 174, 175], "add_bo": [68, 79, 95, 115, 127, 134, 174, 175, 225], "31587": [68, 79, 95, 115, 127, 174, 175], "29644": [68, 79, 95, 115, 127, 174, 175], "102": [68, 79, 95, 115, 127, 174, 175], "tokenizer_path": [68, 79, 115, 127], "concat": [68, 79, 115, 127, 177], "1788": [68, 79, 115, 127, 177], "2643": [68, 79, 115, 127, 177], "13": [68, 79, 115, 127, 152, 170, 171, 177, 232], "1792": [68, 79, 115, 127, 177], "9508": [68, 79, 115, 127, 177], "465": [68, 79, 115, 127, 177], "22137": [68, 79, 115, 127, 177], "2933": [68, 79, 115, 127, 177], "join": [68, 79, 115, 127, 177], "attribut": [68, 79, 115, 127, 157, 169, 177, 192], "head_dim": [69, 73, 144, 147, 150], "intermediate_dim": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139], "attn_dropout": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 150], "norm_ep": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139], "1e": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 148], "06": [69, 73, 148, 230], "rope_bas": [69, 73, 80, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139], "10000": [69, 73, 80, 116, 118, 120, 122, 128, 130, 149], "norm_embed": [69, 73], "gemmatransformerdecod": [69, 70, 71, 73, 74, 75, 76, 77], "transformerdecoderlay": [69, 80, 96, 105, 120, 139, 150], "rm": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139], "norm": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 150, 151], "space": [69, 80, 96, 105, 120, 134, 139, 150], "slide": [69, 120, 131], "window": [69, 120, 131, 226], "vocabulari": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 134, 135, 139, 230], "queri": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 150, 151, 229], "mha": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 150], "dimens": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144, 147, 149, 150, 152, 156, 182, 229, 230, 232], "onto": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144], "scaled_dot_product_attent": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144], "epsilon": [69, 73, 80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 167], "rotari": [69, 73, 80, 116, 118, 120, 122, 128, 130, 149, 229], "10_000": [69, 73, 116, 118, 120, 122, 130], "blog": [70, 71], "technolog": [70, 71], "develop": [70, 71, 232], "gemmatoken": 72, "gemma_2b": 74, "gemma_7b": 75, "lora_gemma_2b": 76, "lora_gemma_7b": 77, "card": [79, 95], "regist": [79, 95, 99, 127, 132, 143, 145, 153, 211, 232], "strongli": 79, "beforehand": 79, "html": [79, 134, 185, 191, 197, 202, 208, 212, 214, 220], "problem": [79, 115], "due": [79, 115, 174, 230, 232], "whitespac": [79, 115, 174], "slice": [79, 115], "gqa": [80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144], "mqa": [80, 86, 96, 100, 105, 109, 116, 118, 120, 122, 128, 130, 135, 139, 144], "kvcach": [80, 86, 96, 100, 105, 109, 128, 135, 139, 144, 150], "scale_hidden_dim_for_mlp": [80, 86, 96, 100, 105, 109, 135, 139], "2307": [81, 82, 83, 84], "09288": [81, 82, 83, 84], "classif": [84, 118, 122, 123, 189], "reward": [84, 90, 94, 119, 123, 126, 162, 163, 165, 166, 168, 169, 189], "llama2_70b": 88, "llama2_7b": [89, 230], "classifi": [90, 118, 122, 123, 226], "llama2_reward_7b": [90, 189], "lora_llama2_13b": 91, "lora_llama2_70b": 92, "lora_llama2_7b": [93, 230], "lora_llama2_reward_7b": 94, "special_token": [95, 127, 134, 175, 225], "tiktoken": [95, 175, 229], "left": [95, 127, 164, 230], "canon": [95, 99, 127, 132, 143], "tt_model": [95, 175], "token_id": [95, 115, 134, 172, 175], "truncate_at_eo": [95, 175], "skip_special_token": [95, 134, 175], "show": [95, 175, 179, 221, 224, 225, 230], "skip": [95, 144, 175], "tokenize_head": 95, "tokenize_end": 95, "header": [95, 225], "eom": 95, "wether": 95, "500000": [96, 100, 105, 109], "special_tokens_path": [99, 132, 143], "llama3token": [99, 225], "similarli": [99, 132, 143, 226, 231], "llama3_70b": 101, "llama3_8b": [102, 193, 229, 231], "lora_llama3_70b": 103, "lora_llama3_8b": 104, "llama3_1": [106, 107, 108, 109, 110, 111, 112, 113], "rtype": 106, "llama3_1_70b": 110, "llama3_1_8b": 111, "lora_llama3_1_70b": 112, "lora_llama3_1_8b": 113, "llama2chattempl": 114, "yet": [114, 225, 227], "trim_leading_whitespac": [115, 174], "unbatch": [115, 174], "bo": [115, 132, 174, 177, 225, 226], "trim": [115, 174], "num_class": [118, 122], "announc": 121, "ray2333": 123, "feedback": [123, 165], "mistraltoken": [124, 225], "lora_mistral_7b": 125, "lora_mistral_reward_7b": 126, "ignore_system_prompt": 127, "phi3_mini": [129, 189], "phi": [131, 132, 189], "128k": 131, "nor": 131, "phi3minitoken": 132, "tokenizer_config": 132, "spm": 132, "lm": [132, 167], "unk": 132, "augment": [132, 232], "endoftext": [132, 134], "phi3minisentencepiecebasetoken": 132, "lora_phi3_mini": 133, "merges_fil": [134, 143], "unk_token": 134, "bos_token": 134, "eos_token": 134, "pad_token": 134, "bpe_cache_s": 134, "151646": 134, "bpe": 134, "v4": [134, 154], "src": [134, 154], "tokenization_qwen2": 134, "word": [134, 135, 139, 166, 231], "utf": 134, "librari": [134, 165, 166, 168, 191, 195, 197, 214, 220, 222, 224, 226, 232], "stdtype": 134, "unknown": 134, "cach": [134, 144, 147, 149, 150, 151, 221, 224], "speed": [134, 175, 215, 229, 231, 232], "realli": [134, 227], "esp": 134, "chines": 134, "technic": [134, 228], "leak": 134, "appear": 134, "equal": [134, 179, 182, 184, 216], "assistant_for_gener": 134, "39": [134, 152], "385": 134, "78": 134, "675": 134, "2000": [134, 231], "remov": 134, "41": [134, 152], "tokenization_util": 134, "l541": 134, "l262": 134, "apply_chat_templ": 134, "1000000": [135, 139], "tie_word_embed": [135, 136, 137, 139, 140, 141], "tiedembeddingtransformerdecod": [135, 136, 137, 139, 140, 141], "qwen2transformerdecod": 135, "period": [135, 139], "rope": [135, 139, 144, 149], "qwen2_0_5b": 136, "qwen2_1_5b": 137, "qwen2_7b": 138, "qwen": [140, 141, 142], "qwen2token": 143, "pos_embed": [144, 230, 231], "kv_cach": 144, "introduc": [144, 148, 156, 169, 225, 226, 230, 231, 232], "13245v1": 144, "multihead": 144, "extrem": 144, "share": [144, 226, 227], "credit": 144, "lightn": 144, "lit": 144, "lit_gpt": 144, "v": [144, 150, 230], "q": [144, 230], "n_kv_head": 144, "rotarypositionalembed": [144, 230, 231], "seq_length": [144, 151, 193], "boolean": [144, 150, 151, 185], "softmax": [144, 150, 151], "j": [144, 150, 151], "seq_len": 144, "bigger": 144, "n_h": [144, 149], "num": [144, 149], "n_kv": 144, "kv": [144, 147, 150, 231], "emb": [144, 150], "h_d": [144, 149], "reset_cach": [144, 150, 151], "reset": [144, 147, 150, 151, 198], "setup_cach": [144, 150, 151], "dpython": [144, 147, 150, 151, 153, 213, 217], "gate_proj": 145, "down_proj": 145, "up_proj": 145, "silu": 145, "feed": [145, 151], "network": [145, 157, 230, 232], "fed": [145, 225], "multipli": 145, "subclass": [145, 191], "although": [145, 230, 231], "afterward": 145, "former": 145, "hook": [145, 153, 211, 232], "latter": 145, "layernorm": 146, "standalon": 147, "past": 147, "expand": 147, "k_val": 147, "v_val": 147, "assert": 147, "longer": [147, 226], "h": [147, 152, 183, 221, 224], "ep": 148, "root": [148, 208, 209], "squar": 148, "1910": 148, "07467": 148, "verfic": [148, 149], "small": [148, 227], "divis": [148, 184], "propos": 149, "2104": 149, "09864": 149, "l80": 149, "upto": 149, "init": [149, 198, 209, 232], "exceed": 149, "freq": 149, "recomput": 149, "geometr": 149, "progress": [149, 228], "rotat": 149, "angl": 149, "todo": 149, "effici": [149, 160, 196, 220, 222, 227, 228, 230, 231], "belong": [150, 192], "reduc": [150, 165, 222, 226, 230, 231, 232], "statement": 150, "improv": [150, 168, 175, 196, 229, 230], "readabl": [150, 227], "caches_are_en": 150, "arang": 150, "prompt_length": 150, "causal_mask": 150, "m_": 150, "seq": 150, "attn": [151, 230, 231, 232], "causalselfattent": [151, 230, 231], "sa_norm": 151, "mlp_norm": 151, "ff": 151, "cache_en": 151, "token_pos_embed": 152, "pre_tile_pos_emb": 152, "post_tile_pos_emb": 152, "cls_project": 152, "vit": 152, "11929": 152, "convolut": 152, "flatten": 152, "downscal": [152, 181, 182, 183], "800x400": 152, "400x400": 152, "clipimagetransform": 152, "broken": 152, "down": [152, 188, 226, 230, 232], "whole": 152, "num_til": [152, 184], "101": 152, "pool": 152, "tiledtokenpositionalembed": 152, "tilepositionalembed": 152, "tile_pos_emb": 152, "even": [152, 221, 224, 225, 226, 229, 230, 232], "8x8": 152, "14": [152, 170, 231, 232], "15": [152, 170, 196, 225, 227, 230, 232], "17": [152, 170, 230], "18": [152, 170, 229], "19": [152, 170, 232], "20": [152, 170, 171, 231], "21": 152, "22": 152, "23": [152, 154], "24": [152, 184, 228, 229], "25": [152, 227], "26": 152, "27": [152, 227], "28": [152, 227], "29": [152, 232], "30": [152, 171, 231], "31": [152, 229], "33": 152, "34": 152, "35": [152, 232], "36": 152, "37": 152, "38": [152, 227], "42": 152, "43": 152, "44": 152, "45": 152, "46": 152, "47": 152, "48": [152, 227, 232], "49": 152, "50": [152, 171, 184, 205, 227], "51": 152, "52": [152, 228], "53": 152, "54": 152, "55": [152, 228], "56": 152, "57": [152, 230, 232], "58": 152, "59": [152, 232], "60": 152, "61": [152, 227], "62": 152, "63": 152, "64": [152, 230], "num_patches_per_til": 152, "emb_dim": 152, "greater": [152, 216], "constain": 152, "anim": [152, 226], "max_n_img": 152, "n_channel": 152, "hidden_st": 152, "vision_util": 152, "tile_crop": 152, "num_channel": 152, "image_s": [152, 182, 183], "800": [152, 182, 183], "patch_grid_s": 152, "random": [152, 214, 228], "rand": [152, 181, 183, 184], "nch": 152, "tile_cropped_imag": 152, "batch_imag": 152, "unsqueez": 152, "batch_aspect_ratio": 152, "clip_vision_encod": 152, "common_util": 153, "bfloat16": [153, 213, 227, 228, 229, 230, 231], "offload_to_cpu": 153, "nf4": [153, 232], "restor": 153, "higher": [153, 166, 229, 231, 232], "offload": [153, 232], "increas": [153, 154, 165, 229, 230, 231], "peak": [153, 198, 203, 227, 229, 230, 232], "gpu": [153, 224, 227, 228, 229, 230, 231, 232], "_register_state_dict_hook": 153, "m": [153, 193, 225, 231], "mymodul": 153, "_after_": 153, "nf4tensor": [153, 232], "unquant": [153, 231, 232], "unus": 153, "num_warmup_step": 154, "num_training_step": 154, "num_cycl": [154, 215], "last_epoch": 154, "lambdalr": 154, "rate": [154, 222, 228], "schedul": [154, 215, 228], "linearli": 154, "decreas": [154, 226, 230, 231, 232], "cosin": 154, "l104": 154, "warmup": [154, 215], "phase": 154, "wave": 154, "half": 154, "lr_schedul": 154, "peft": [155, 156, 157, 158, 159, 160, 161, 186, 230, 232], "protocol": 155, "adapter_param": [155, 156, 157, 158, 159], "proj": 155, "in_dim": [155, 156, 230, 232], "out_dim": [155, 156, 230, 232], "bia": [155, 156, 230, 231, 232], "loralinear": [155, 230, 232], "alpha": [156, 230, 232], "use_bia": 156, "perturb": 156, "decomposit": [156, 230], "matric": [156, 204, 230, 232], "trainabl": [156, 159, 204, 230, 232], "mapsto": 156, "w_0x": 156, "r": [156, 230], "bax": 156, "lora_a": [156, 230, 232], "lora_b": [156, 230, 232], "temporarili": 157, "neural": [157, 230, 232], "polici": [157, 163, 165, 166, 167, 168, 169, 185, 196, 204, 212], "caller": 157, "whose": [157, 205, 211], "yield": 157, "get_adapter_param": [159, 230], "base_miss": 160, "base_unexpect": 160, "lora_miss": 160, "lora_unexpect": 160, "validate_state_dict_for_lora": [160, 230], "unlik": [160, 166], "reli": [160, 177, 227, 229], "unexpect": 160, "strict": [160, 230], "pull": [160, 224], "120600": 160, "assertionerror": [160, 161, 170], "nonempti": 160, "full_model_state_dict_kei": 161, "lora_state_dict_kei": 161, "base_model_state_dict_kei": 161, "confirm": [161, 221], "lora_modul": 161, "complement": 161, "disjoint": 161, "overlap": 161, "gamma": [162, 168, 169], "lmbda": 162, "estim": [162, 163], "1506": 162, "02438": 162, "predict": [162, 163, 167, 193], "reponse_len": [162, 163], "receiv": [162, 225], "discount": 162, "gae": 162, "lambda": 162, "particip": [162, 179], "score": 163, "logprob": [163, 166, 169], "ref_logprob": 163, "kl_coeff": 163, "valid_score_idx": 163, "kl": 163, "coeffici": [163, 167], "response_len": 163, "total_reward": 163, "combin": [163, 180], "diverg": 163, "kl_reward": 163, "beta": [165, 169], "label_smooth": [165, 169], "18290": 165, "intuit": [165, 166, 168, 169], "dispref": 165, "dynam": [165, 231], "degener": 165, "occur": 165, "naiv": 165, "trl": [165, 166, 168, 169], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 165, "dpo_train": [165, 166, 168], "l844": 165, "retain": [165, 232], "2009": 165, "01325": 165, "align": [165, 225], "regular": [165, 166, 169, 231, 232], "baselin": [165, 167, 227, 230], "rather": 165, "overhead": [165, 231], "temperatur": [165, 166, 168, 169, 193, 227], "uncertainti": [165, 169], "policy_chosen_logp": [165, 166, 168, 169], "policy_rejected_logp": [165, 166, 168, 169], "reference_chosen_logp": [165, 166, 168], "reference_rejected_logp": [165, 166, 168], "chosen_reward": [165, 166, 168, 169], "rejected_reward": [165, 166, 168, 169], "tau": 166, "optimis": 166, "ipo": 166, "2310": 166, "12036": 166, "pi": 166, "pi_ref": 166, "regress": [166, 168], "gap": 166, "likelihood": 166, "he": 166, "weaker": 166, "regularis": 166, "toward": [166, 169], "4dce042a3863db1d375358e8c8092b874b02934b": [166, 168], "l1143": 166, "reciproc": 166, "larger": [166, 169, 182, 188, 227, 229], "value_clip_rang": 167, "value_coeff": 167, "proxim": 167, "1707": 167, "06347": 167, "eqn": 167, "vwxyzjn": 167, "ccc19538e817e98a60d3253242ac15e2a562cb49": 167, "lm_human_preference_detail": 167, "train_policy_acceler": 167, "l719": 167, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 167, "ppo2": 167, "l68": 167, "l75": 167, "pi_old_logprob": 167, "pi_logprob": 167, "phi_old_valu": 167, "phi_valu": 167, "padding_mask": [167, 171], "value_padding_mask": 167, "old": 167, "participag": 167, "five": 167, "policy_loss": 167, "value_loss": 167, "clipfrac": 167, "fraction": 167, "statist": 168, "rso": 168, "hing": 168, "2309": 168, "06657": 168, "logist": 168, "slic": 168, "10425": 168, "almost": [168, 230], "vector": [168, 225], "svm": 168, "counter": 168, "l1141": 168, "simpo": 169, "free": [169, 230], "2405": 169, "14734": 169, "averag": 169, "implicit": 169, "margin": 169, "bradlei": 169, "terri": 169, "win": 169, "lose": 169, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 169, "cpo_train": 169, "l603": 169, "pretti": [169, 227], "identitc": 169, "elimin": 169, "better": [169, 222, 225, 226, 227, 231], "kind": 169, "ipoloss": 169, "ignore_idx": [170, 210], "input_id": 170, "chosen_input_id": [170, 226], "rejected_input_id": [170, 226], "chosen_label": [170, 226], "rejected_label": [170, 226], "stop_token": [171, 193], "fill_valu": 171, "stop": [171, 193], "sequence_length": 171, "pad_id": 171, "been": [171, 196, 225, 231], "stop_token_id": 171, "869": 171, "eos_mask": 171, "truncated_sequ": 171, "light": 174, "sentencepieceprocessor": 174, "prefix": 174, "bos_id": [175, 177], "lightweight": [175, 225], "break": 175, "substr": 175, "repetit": 175, "identif": 175, "regex": 175, "chunk": 175, "present": [175, 188], "absent": 175, "tokenizer_json_path": 176, "heavili": 177, "beggin": 177, "runtimeerror": [177, 190, 195, 201], "satisfi": [177, 181, 227], "loos": 178, "image_token_id": 179, "laid": 179, "fig": 179, "flamingo": 179, "2204": 179, "14198": 179, "immedi": 179, "until": 179, "img1": 179, "img2": 179, "img3": 179, "dog": 179, "cat": 179, "text_seq_len": 179, "image_seq_len": 179, "resolut": [180, 181, 182, 183], "1x1": 180, "1x2": 180, "2x1": 180, "side": [180, 181, 182, 183], "height": [180, 181, 182, 183], "width": [180, 181, 182, 183, 231], "224": [180, 181, 182, 183], "896": 180, "448": [180, 181, 182, 183], "672": [180, 181], "possible_resolut": 181, "resize_to_max_canva": 181, "canva": 181, "resiz": [181, 182, 183], "distort": [181, 183], "select": 181, "smallest": 181, "upscal": [181, 182, 183], "2x": 181, "5x": 181, "canvas": 181, "condit": [181, 193, 202, 224, 226], "pick": 181, "lowest": [181, 230], "area": [181, 227], "minim": [181, 226, 228, 230, 231, 232], "200": [181, 184, 232], "300": [181, 182, 183, 184, 227], "scale_height": 181, "1200": 181, "3600": 181, "2400": 181, "scale_width": 181, "7467": 181, "4933": 181, "scaling_factor": 181, "upscaling_opt": 181, "selected_scal": 181, "150528": 181, "100352": 181, "optimal_canva": 181, "target_s": [182, 183], "max_siz": [182, 183], "inscrib": 182, "second": [182, 227, 230, 232], "exce": [182, 183], "1194": [182, 183], "1344": [182, 183], "stai": [182, 183], "600": [182, 183, 184], "500": [182, 183, 230], "1000": [182, 183, 185, 231], "488": [182, 183], "resampl": 183, "interpolationmod": 183, "torchvis": 183, "nearest": 183, "nearest_exact": 183, "bilinear": 183, "bicub": 183, "channel_s": 184, "4x6": 184, "2x3": 184, "datatyp": [185, 232], "denot": 185, "integ": [185, 210, 214], "auto_wrap_polici": [185, 196, 212], "submodul": [185, 204], "obei": 185, "contract": 185, "get_fsdp_polici": 185, "modules_to_wrap": [185, 196, 204], "min_num_param": 185, "my_fsdp_polici": 185, "recurs": [185, 204, 208], "isinst": [185, 226], "sum": [185, 230], "p": [185, 190, 230, 231, 232], "numel": [185, 230], "stabl": [185, 202, 208, 214, 221], "safe_seri": 186, "from_pretrain": 186, "0001_of_0003": 186, "0002_of_0003": 186, "preserv": [186, 232], "weight_map": [186, 227], "convert_weight": 186, "_model_typ": [186, 189], "intermediate_checkpoint": [186, 187, 188], "adapter_onli": [186, 187, 188], "_weight_map": 186, "shard": [187, 229], "wip": 187, "qualnam": 189, "boundari": 189, "distinguish": 189, "mistral_reward_7b": 189, "my_new_model": 189, "my_custom_state_dict_map": 189, "optim_map": 190, "bare": 190, "bone": 190, "distribut": [190, 194, 201, 202, 212, 214, 222, 224, 228, 229], "optim_dict": [190, 192, 211], "cfg_optim": 190, "ckpt": 190, "optim_ckpt": 190, "placeholder_optim_dict": 190, "optiminbackwardwrapp": 190, "get_optim_kei": 190, "arbitrari": [190, 230], "optim_ckpt_map": 190, "loadabl": 190, "argpars": 191, "argumentpars": 191, "builtin": 191, "said": 191, "noth": 191, "consult": 191, "info": [191, 228], "parse_known_arg": 191, "namespac": 191, "act": 191, "precid": 191, "parse_arg": 191, "too": [191, 229], "optimizerinbackwardwrapp": 192, "top": [192, 232], "named_paramet": 192, "max_generated_token": 193, "top_k": [193, 227], "custom_generate_next_token": 193, "prune": [193, 232], "compil": [193, 227, 229, 232], "generate_next_token": 193, "hi": [193, 225], "jeremi": 193, "float32": 195, "bf16": [195, 232], "inde": [195, 227], "kernel": 195, "isn": [195, 224], "hardwar": [195, 222, 226, 227, 230], "memory_efficient_fsdp_wrap": [196, 231], "maxim": [196, 204, 220, 222], "workload": [196, 231], "alongsid": 196, "ac": 196, "fullyshardeddataparallel": [196, 204], "fsdppolicytyp": [196, 204], "handler": 197, "reset_stat": 198, "track": [198, 205], "alloc": [198, 203, 204, 229, 232], "reserv": [198, 203, 225, 232], "stat": [198, 203, 232], "int4": [199, 231], "4w": 199, "recogn": 199, "int8dynactint4weightquant": [199, 231], "8da4w": [199, 231], "int8dynactint4weightqatquant": [199, 231], "qat": [199, 220], "mode": [199, 205, 227], "aka": 200, "master": 202, "port": [202, 224], "address": 202, "hold": [202, 228], "peak_memory_act": 203, "peak_memory_alloc": 203, "peak_memory_reserv": 203, "get_memory_stat": 203, "hierarch": 204, "requires_grad": [204, 230, 232], "api_kei": 205, "experiment_kei": 205, "onlin": [205, 225], "log_cod": 205, "comet": 205, "www": 205, "site": [205, 226, 227], "ml": 205, "team": 205, "compar": [205, 208, 216, 227, 229, 230, 231, 232], "sdk": 205, "uncategor": 205, "alphanumer": 205, "charact": 205, "get_or_cr": 205, "fresh": 205, "persist": 205, "hpo": 205, "sweep": 205, "server": 205, "offlin": 205, "auto": [205, 224], "creation": 205, "experimentconfig": 205, "project_nam": 205, "my_project": [205, 209], "my_workspac": 205, "my_metr": [205, 208, 209], "importerror": [205, 209], "termin": [205, 208, 209], "comet_api_kei": 205, "flush": [205, 206, 207, 208, 209], "ndarrai": [205, 206, 207, 208, 209], "scalar": [205, 206, 207, 208, 209], "record": [205, 206, 207, 208, 209, 215], "log_config": [205, 209], "payload": [205, 206, 207, 208, 209], "filenam": 206, "log_": 206, "unixtimestamp": 206, "thread": 206, "safe": 206, "organize_log": 208, "tensorboard": 208, "subdirectori": 208, "logdir": 208, "startup": 208, "tree": [208, 226, 227, 229], "tfevent": 208, "encount": 208, "frontend": 208, "organ": [208, 224], "accordingli": [208, 231], "my_log_dir": 208, "view": [208, 228], "entiti": 209, "bias": [209, 230, 232], "sent": 209, "usernam": 209, "my_ent": 209, "my_group": 209, "account": [209, 230, 232], "link": [209, 227, 229], "capecap": 209, "6053ofw0": 209, "torchtune_config_j67sb73v": 209, "longest": 210, "token_pair": 210, "soon": 211, "readi": [211, 220, 225, 231], "grad": 211, "acwrappolicytyp": 212, "author": [212, 222, 228, 232], "fsdp_adavnced_tutori": 212, "insid": 213, "contextmanag": 213, "debug_mod": 214, "pseudo": 214, "commonli": [214, 230, 232], "numpi": 214, "determinist": 214, "global": [214, 226], "warn": 214, "nondeterminist": 214, "cudnn": 214, "set_deterministic_debug_mod": 214, "profile_memori": 215, "with_stack": 215, "record_shap": 215, "with_flop": 215, "wait_step": 215, "warmup_step": 215, "active_step": 215, "profil": 215, "layout": 215, "trace": 215, "profileract": 215, "gradient_accumul": 215, "sensibl": 215, "default_schedul": 215, "reduct": [215, 230], "iter": [215, 217, 232], "scope": 215, "flop": 215, "wait": 215, "cycl": 215, "repeat": 215, "__version__": 216, "named_param": 217, "generated_examples_python": 218, "zip": 218, "galleri": [218, 223], "sphinx": 218, "000": [219, 223, 229], "execut": [219, 223], "generated_exampl": 219, "mem": [219, 223], "mb": [219, 223], "topic": 220, "gentl": 220, "introduct": 220, "first_finetune_tutori": 220, "workflow": [220, 226, 228, 230], "requisit": 221, "proper": [221, 228], "host": [221, 224, 228], "latest": [221, 228, 232], "And": [221, 227], "ls": [221, 224, 227, 228, 229], "welcom": [221, 224], "greatest": [221, 228], "contributor": 221, "cd": [221, 227], "commit": 221, "branch": 221, "url": 221, "whl": 221, "therebi": [221, 231, 232], "forc": 221, "reinstal": 221, "opt": [221, 228], "suffix": 221, "cu121": 221, "On": [222, 230], "pointer": 222, "emphas": 222, "simplic": 222, "component": 222, "prove": 222, "democrat": 222, "box": [222, 232], "zoo": 222, "varieti": [222, 230], "integr": [222, 227, 228, 229, 230, 231, 232], "excit": 222, "checkout": 222, "quickstart": 222, "attain": 222, "chekckpoint": 222, "embodi": 222, "philosophi": 222, "usabl": 222, "composit": 222, "hard": [222, 226], "outlin": 222, "unecessari": 222, "never": 222, "thoroughli": 222, "short": 224, "subcommand": 224, "anytim": 224, "symlink": 224, "wrote": 224, "readm": [224, 227, 229], "md": 224, "lot": [224, 227], "recent": 224, "releas": [224, 229], "agre": 224, "term": 224, "perman": 224, "eat": 224, "bandwith": 224, "storag": [224, 232], "00030": 224, "ootb": 224, "full_finetune_single_devic": [224, 226, 227, 228], "7b_full_low_memori": [224, 227, 228], "8b_full_single_devic": [224, 226], "mini_full_low_memori": 224, "7b_full": [224, 227, 228], "13b_full": [224, 227, 228], "70b_full": 224, "edit": 224, "clobber": 224, "destin": 224, "lora_finetune_distribut": [224, 229, 230], "torchrun": 224, "8b_lora_single_devic": [224, 225, 229], "launch": [224, 225, 228], "nproc": 224, "node": 224, "worker": 224, "nnode": [224, 230, 231], "minimum_nod": 224, "maximum_nod": 224, "fail": 224, "rdzv": 224, "rendezv": 224, "endpoint": 224, "8b_lora": [224, 229], "bypass": 224, "vice": 224, "versa": 224, "fancy_lora": 224, "8b_fancy_lora": 224, "sai": [224, 225, 228], "know": [225, 226, 227, 230], "intend": 225, "nice": 225, "meet": 225, "overhaul": 225, "begin_of_text": 225, "start_header_id": 225, "end_header_id": 225, "eot_id": 225, "untrain": 225, "accompani": 225, "who": 225, "influenti": 225, "hip": 225, "hop": 225, "artist": 225, "2pac": 225, "rakim": 225, "c": 225, "na": 225, "flavor": [225, 226], "msg": 225, "formatted_messag": [225, 226], "nyou": [225, 226], "nwho": 225, "why": [225, 228, 230], "518": 225, "25580": 225, "29962": 225, "3532": 225, "14816": 225, "29903": 225, "6778": 225, "_spm_model": 225, "piece_to_id": 225, "place": [225, 226], "manual": [225, 232], "529": 225, "29879": 225, "29958": 225, "nhere": 225, "128000": [225, 231], "128009": 225, "pure": 225, "That": 225, "won": 225, "mess": 225, "govern": 225, "prime": 225, "strictli": 225, "ask": 225, "untouch": 225, "though": 225, "robust": 225, "forum": 225, "panda": 225, "pd": 225, "df": 225, "read_csv": 225, "your_fil": 225, "nrow": 225, "tolist": 225, "iloc": 225, "gp": 225, "satellit": 225, "thing": [225, 232], "message_convert": 225, "input_msg": 225, "output_msg": 225, "But": [225, 227, 230], "mistralchatformat": 225, "custom_dataset": 225, "2048": 225, "honor": 225, "copi": [225, 227, 228, 229, 231, 232], "custom_8b_lora_single_devic": 225, "steer": 226, "wheel": 226, "publicli": 226, "great": [226, 227], "hood": [226, 232], "text_completion_dataset": [226, 231], "padded_col": 226, "upper": 226, "constraint": [226, 230], "slow": [226, 232], "signific": [226, 231], "speedup": [226, 227, 229], "my_data": 226, "fix": [226, 231], "goal": [226, 231], "respond": 226, "plant": 226, "miner": 226, "oak": 226, "copper": 226, "ore": 226, "eleph": 226, "customtempl": 226, "importlib": 226, "import_modul": 226, "mechan": 226, "search": 226, "often": [226, 230], "interpret": 226, "runtim": 226, "pythonpath": 226, "chat_dataset": 226, "quit": [226, 232], "customchatformat": 226, "concatdataset": 226, "drive": 226, "rajpurkar": 226, "io": 226, "squad": 226, "explor": 226, "few": [226, 229, 230, 232], "adjust": [226, 231], "chosen_messag": 226, "transformed_sampl": 226, "key_chosen": 226, "rejected_messag": 226, "key_reject": 226, "c_mask": 226, "np": 226, "cross_entropy_ignore_idx": 226, "r_mask": 226, "stack_exchanged_paired_dataset": 226, "had": 226, "1024": [226, 231], "stackexchangedpairedtempl": 226, "response_j": 226, "response_k": 226, "rl": 226, "favorit": [227, 230], "seemlessli": 227, "beyond": [227, 232], "connect": [227, 231], "amount": 227, "natur": 227, "export": 227, "mobil": 227, "phone": 227, "leverag": [227, 229, 232], "plai": 227, "freez": [227, 230], "percentag": 227, "learnabl": 227, "16gb": [227, 230], "rtx": 227, "3090": 227, "4090": 227, "hour": 227, "7b_qlora_single_devic": [227, 228, 232], "473": 227, "98": [227, 232], "gb": [227, 229, 230, 231, 232], "484": 227, "01": [227, 228], "fact": [227, 229, 230], "third": 227, "eleuther_ev": [227, 229, 231], "eleuther_evalu": [227, 229, 231], "lm_eval": [227, 229], "plan": 227, "custom_eval_config": [227, 229], "truthfulqa_mc2": [227, 229, 230], "measur": [227, 229], "propens": [227, 229], "shot": [227, 229, 231], "accuraci": [227, 229, 230, 231, 232], "324": 227, "loglikelihood": 227, "195": 227, "121": 227, "197": 227, "acc": [227, 231], "388": 227, "shown": [227, 231], "489": 227, "seem": 227, "custom_generation_config": [227, 229], "kick": 227, "interest": 227, "visit": 227, "bai": 227, "92": 227, "exploratorium": 227, "san": 227, "francisco": 227, "magazin": 227, "awesom": 227, "bridg": 227, "cool": 227, "96": [227, 232], "sec": [227, 229], "83": 227, "99": [227, 230], "72": 227, "littl": 227, "torchao": [227, 229, 231, 232], "int8_weight_onli": [227, 229], "int8_dynamic_activation_int8_weight": [227, 229], "ao": [227, 229], "quant_api": [227, 229], "quantize_": [227, 229], "int4_weight_onli": [227, 229], "previous": [227, 229, 230], "benefit": 227, "doesn": 227, "fast": 227, "clone": [227, 230, 231, 232], "assumpt": 227, "new_dir": 227, "output_dict": 227, "sd_1": 227, "sd_2": 227, "dump": 227, "convert_hf_checkpoint": 227, "checkpoint_path": 227, "justin": 227, "school": 227, "math": 227, "teacher": 227, "ws": 227, "94": [227, 229], "bandwidth": [227, 229], "1391": 227, "84": 227, "thats": 227, "seamlessli": 227, "authent": [227, 228], "hopefulli": 227, "gave": 227, "grant": 228, "minut": 228, "agreement": 228, "altern": 228, "hackabl": 228, "singularli": 228, "purpos": [228, 229], "depth": 228, "principl": 228, "boilerpl": 228, "substanti": [228, 230], "custom_config": 228, "replic": 228, "lorafinetunerecipesingledevic": 228, "lora_finetune_output": 228, "log_1713194212": 228, "3697006702423096": 228, "25880": [228, 232], "83it": 228, "monitor": 228, "tqdm": 228, "interv": 228, "e2": 228, "focu": 229, "128": [229, 230], "256": [229, 231], "theta": 229, "gain": 229, "basic": 229, "observ": [229, 231], "consum": [229, 232], "vram": [229, 230, 231], "overal": 229, "8b_qlora_single_devic": 229, "least": [229, 230, 231], "coupl": [229, 230, 232], "meta_model_0": [229, 231], "did": [229, 232], "122": 229, "sarah": 229, "busi": 229, "mum": 229, "young": 229, "children": 229, "live": 229, "north": 229, "east": 229, "england": 229, "135": 229, "88": 229, "138": 229, "346": 229, "09": 229, "139": 229, "broader": 229, "teach": 230, "straight": 230, "jump": 230, "unfamiliar": 230, "oppos": [230, 232], "momentum": 230, "relat": 230, "aghajanyan": 230, "et": 230, "al": 230, "hypothes": 230, "intrins": 230, "four": 230, "eight": 230, "practic": 230, "blue": 230, "rememb": 230, "approx": 230, "15m": 230, "8192": [230, 231], "65k": 230, "frozen_out": [230, 232], "lora_out": [230, 232], "omit": 230, "base_model": 230, "lora_model": 230, "lora_llama_2_7b": [230, 232], "alon": 230, "bit": [230, 231, 232], "in_featur": [230, 231], "out_featur": [230, 231], "inplac": 230, "feel": 230, "validate_missing_and_unexpected_for_lora": 230, "peft_util": 230, "set_trainable_param": 230, "fetch": 230, "lora_param": 230, "total_param": 230, "trainable_param": 230, "2f": 230, "6742609920": 230, "4194304": 230, "7b_lora": 230, "my_model_checkpoint_path": [230, 231, 232], "tokenizer_checkpoint": [230, 231, 232], "my_tokenizer_checkpoint_path": [230, 231, 232], "factori": 230, "benefici": 230, "impact": 230, "minor": 230, "good": 230, "lora_experiment_1": 230, "smooth": [230, 232], "curv": [230, 232], "ran": 230, "footprint": [230, 231], "commod": 230, "cogniz": 230, "ax": 230, "parallel": 230, "truthfulqa": 230, "475": 230, "87": 230, "508": 230, "86": 230, "504": 230, "04": 230, "514": 230, "absolut": 230, "4gb": 230, "tradeoff": 230, "potenti": 230, "awar": 231, "incur": [231, 232], "degrad": [231, 232], "perplex": 231, "simul": 231, "ultim": 231, "ptq": 231, "fake": 231, "kept": 231, "cast": 231, "nois": 231, "henc": 231, "x_q": 231, "int8": 231, "zp": 231, "x_float": 231, "qmin": 231, "qmax": 231, "clamp": 231, "x_fq": 231, "dequant": 231, "insert": 231, "proce": 231, "prepared_model": 231, "swap": 231, "int8dynactint4weightqatlinear": 231, "int8dynactint4weightlinear": 231, "train_loop": 231, "converted_model": 231, "demonstr": 231, "recov": 231, "modif": 231, "8b_qat_ful": 231, "custom_8b_qat_ful": 231, "fake_quant_after_n_step": 231, "issu": 231, "futur": 231, "empir": 231, "led": 231, "presum": 231, "80gb": 231, "qat_distribut": 231, "op": 231, "mutat": 231, "5gb": 231, "custom_quant": 231, "groupsiz": 231, "poorli": 231, "custom_eleuther_evalu": 231, "fullmodeltorchtunecheckpoint": 231, "hellaswag": 231, "max_seq_length": 231, "my_eleuther_evalu": 231, "stderr": 231, "word_perplex": 231, "9148": 231, "byte_perplex": 231, "5357": 231, "bits_per_byt": 231, "6189": 231, "5687": 231, "0049": 231, "acc_norm": 231, "7536": 231, "0043": 231, "portion": [231, 232], "drop": 231, "74": 231, "048": 231, "190": 231, "7735": 231, "5598": 231, "6413": 231, "5481": 231, "0050": 231, "7390": 231, "0044": 231, "7251": 231, "4994": 231, "5844": 231, "5740": 231, "7610": 231, "outperform": 231, "importantli": 231, "characterist": 231, "187": 231, "958": 231, "halv": 231, "int4weightonlyquant": 231, "motiv": 231, "constrain": 231, "edg": 231, "smartphon": 231, "executorch": 231, "xnnpack": 231, "export_llama": 231, "use_sdpa_with_kv_cach": 231, "qmode": 231, "group_siz": 231, "get_bos_id": 231, "get_eos_id": 231, "128001": 231, "output_nam": 231, "llama3_8da4w": 231, "pte": 231, "881": 231, "oneplu": 231, "709": 231, "tok": 231, "815": 231, "316": 231, "364": 231, "highli": 232, "vanilla": 232, "held": 232, "therefor": 232, "bespok": 232, "normalfloat": 232, "8x": 232, "vast": 232, "major": 232, "normatfloat": 232, "doubl": 232, "themselv": 232, "deepdiv": 232, "idea": 232, "distinct": 232, "de": 232, "counterpart": 232, "set_default_devic": 232, "qlora_linear": 232, "memory_alloc": 232, "177": 232, "152": 232, "del": 232, "empty_cach": 232, "lora_linear": 232, "081": 232, "344": 232, "qlora_llama2_7b": 232, "qlora_model": 232, "essenti": 232, "reparametrize_as_dtype_state_dict_post_hook": 232, "slower": 232, "149": 232, "9157477021217346": 232, "02": 232, "08": 232, "15it": 232, "nightli": 232, "hundr": 232, "228": 232, "8158286809921265": 232, "95it": 232, "exercis": 232, "linear_nf4": 232, "to_nf4": 232, "linear_weight": 232, "autograd": 232, "incom": 232}, "objects": {"torchtune.config": [[11, 0, 1, "", "instantiate"], [12, 0, 1, "", "log_config"], [13, 0, 1, "", "parse"], [14, 0, 1, "", "validate"]], "torchtune.data": [[15, 1, 1, "", "AlpacaInstructTemplate"], [16, 1, 1, "", "ChatFormat"], [17, 1, 1, "", "ChatMLFormat"], [18, 1, 1, "", "ChatMLTemplate"], [19, 1, 1, "", "ChosenRejectedToMessages"], [20, 3, 1, "", "GrammarErrorCorrectionTemplate"], [21, 1, 1, "", "InputOutputToMessages"], [22, 1, 1, "", "InstructTemplate"], [23, 1, 1, "", "JSONToMessages"], [24, 1, 1, "", "Llama2ChatFormat"], [25, 1, 1, "", "Message"], [26, 1, 1, "", "MistralChatFormat"], [27, 1, 1, "", "PromptTemplate"], [28, 1, 1, "", "PromptTemplateInterface"], [29, 3, 1, "", "QuestionAnswerTemplate"], [30, 3, 1, "", "Role"], [31, 1, 1, "", "ShareGPTToMessages"], [32, 3, 1, "", "SummarizeTemplate"], [33, 0, 1, "", "get_openai_messages"], [34, 0, 1, "", "get_sharegpt_messages"], [35, 0, 1, "", "truncate"], [36, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[15, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[16, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[24, 2, 1, "", "format"]], "torchtune.data.Message": [[25, 4, 1, "", "contains_media"], [25, 2, 1, "", "from_dict"], [25, 4, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[26, 2, 1, "", "format"]], "torchtune.datasets": [[37, 1, 1, "", "ChatDataset"], [38, 1, 1, "", "ConcatDataset"], [39, 1, 1, "", "InstructDataset"], [40, 1, 1, "", "PackedDataset"], [41, 1, 1, "", "PreferenceDataset"], [42, 1, 1, "", "SFTDataset"], [43, 1, 1, "", "TextCompletionDataset"], [44, 0, 1, "", "alpaca_cleaned_dataset"], [45, 0, 1, "", "alpaca_dataset"], [46, 0, 1, "", "chat_dataset"], [47, 0, 1, "", "cnn_dailymail_articles_dataset"], [48, 0, 1, "", "grammar_dataset"], [49, 0, 1, "", "instruct_dataset"], [50, 0, 1, "", "samsum_dataset"], [51, 0, 1, "", "slimorca_dataset"], [52, 0, 1, "", "stack_exchange_paired_dataset"], [53, 0, 1, "", "text_completion_dataset"], [54, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[55, 1, 1, "", "TilePositionalEmbedding"], [56, 1, 1, "", "TiledTokenPositionalEmbedding"], [57, 1, 1, "", "TokenPositionalEmbedding"], [58, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[55, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[56, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[57, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[59, 0, 1, "", "code_llama2_13b"], [60, 0, 1, "", "code_llama2_70b"], [61, 0, 1, "", "code_llama2_7b"], [62, 0, 1, "", "lora_code_llama2_13b"], [63, 0, 1, "", "lora_code_llama2_70b"], [64, 0, 1, "", "lora_code_llama2_7b"], [65, 0, 1, "", "qlora_code_llama2_13b"], [66, 0, 1, "", "qlora_code_llama2_70b"], [67, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[68, 1, 1, "", "GemmaTokenizer"], [69, 0, 1, "", "gemma"], [70, 0, 1, "", "gemma_2b"], [71, 0, 1, "", "gemma_7b"], [72, 0, 1, "", "gemma_tokenizer"], [73, 0, 1, "", "lora_gemma"], [74, 0, 1, "", "lora_gemma_2b"], [75, 0, 1, "", "lora_gemma_7b"], [76, 0, 1, "", "qlora_gemma_2b"], [77, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[68, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[78, 1, 1, "", "Llama2ChatTemplate"], [79, 1, 1, "", "Llama2Tokenizer"], [80, 0, 1, "", "llama2"], [81, 0, 1, "", "llama2_13b"], [82, 0, 1, "", "llama2_70b"], [83, 0, 1, "", "llama2_7b"], [84, 0, 1, "", "llama2_reward_7b"], [85, 0, 1, "", "llama2_tokenizer"], [86, 0, 1, "", "lora_llama2"], [87, 0, 1, "", "lora_llama2_13b"], [88, 0, 1, "", "lora_llama2_70b"], [89, 0, 1, "", "lora_llama2_7b"], [90, 0, 1, "", "lora_llama2_reward_7b"], [91, 0, 1, "", "qlora_llama2_13b"], [92, 0, 1, "", "qlora_llama2_70b"], [93, 0, 1, "", "qlora_llama2_7b"], [94, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[79, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[95, 1, 1, "", "Llama3Tokenizer"], [96, 0, 1, "", "llama3"], [97, 0, 1, "", "llama3_70b"], [98, 0, 1, "", "llama3_8b"], [99, 0, 1, "", "llama3_tokenizer"], [100, 0, 1, "", "lora_llama3"], [101, 0, 1, "", "lora_llama3_70b"], [102, 0, 1, "", "lora_llama3_8b"], [103, 0, 1, "", "qlora_llama3_70b"], [104, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[95, 2, 1, "", "decode"], [95, 2, 1, "", "tokenize_message"], [95, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[105, 0, 1, "", "llama3_1"], [106, 0, 1, "", "llama3_1_405b"], [107, 0, 1, "", "llama3_1_70b"], [108, 0, 1, "", "llama3_1_8b"], [109, 0, 1, "", "lora_llama3_1"], [110, 0, 1, "", "lora_llama3_1_70b"], [111, 0, 1, "", "lora_llama3_1_8b"], [112, 0, 1, "", "qlora_llama3_1_70b"], [113, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[114, 1, 1, "", "MistralChatTemplate"], [115, 1, 1, "", "MistralTokenizer"], [116, 0, 1, "", "lora_mistral"], [117, 0, 1, "", "lora_mistral_7b"], [118, 0, 1, "", "lora_mistral_classifier"], [119, 0, 1, "", "lora_mistral_reward_7b"], [120, 0, 1, "", "mistral"], [121, 0, 1, "", "mistral_7b"], [122, 0, 1, "", "mistral_classifier"], [123, 0, 1, "", "mistral_reward_7b"], [124, 0, 1, "", "mistral_tokenizer"], [125, 0, 1, "", "qlora_mistral_7b"], [126, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[115, 2, 1, "", "decode"], [115, 2, 1, "", "encode"], [115, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[127, 1, 1, "", "Phi3MiniTokenizer"], [128, 0, 1, "", "lora_phi3"], [129, 0, 1, "", "lora_phi3_mini"], [130, 0, 1, "", "phi3"], [131, 0, 1, "", "phi3_mini"], [132, 0, 1, "", "phi3_mini_tokenizer"], [133, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[127, 2, 1, "", "decode"], [127, 2, 1, "", "tokenize_messages"]], "torchtune.models.qwen2": [[134, 1, 1, "", "Qwen2Tokenizer"], [135, 0, 1, "", "lora_qwen2"], [136, 0, 1, "", "lora_qwen2_0_5b"], [137, 0, 1, "", "lora_qwen2_1_5b"], [138, 0, 1, "", "lora_qwen2_7b"], [139, 0, 1, "", "qwen2"], [140, 0, 1, "", "qwen2_0_5b"], [141, 0, 1, "", "qwen2_1_5b"], [142, 0, 1, "", "qwen2_7b"], [143, 0, 1, "", "qwen2_tokenizer"]], "torchtune.models.qwen2.Qwen2Tokenizer": [[134, 2, 1, "", "decode"], [134, 2, 1, "", "encode"], [134, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[144, 1, 1, "", "CausalSelfAttention"], [145, 1, 1, "", "FeedForward"], [146, 1, 1, "", "Fp32LayerNorm"], [147, 1, 1, "", "KVCache"], [148, 1, 1, "", "RMSNorm"], [149, 1, 1, "", "RotaryPositionalEmbeddings"], [150, 1, 1, "", "TransformerDecoder"], [151, 1, 1, "", "TransformerDecoderLayer"], [152, 1, 1, "", "VisionTransformer"], [154, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[144, 2, 1, "", "forward"], [144, 2, 1, "", "reset_cache"], [144, 2, 1, "", "setup_cache"]], "torchtune.modules.FeedForward": [[145, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[146, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[147, 2, 1, "", "reset"], [147, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[148, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[149, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[150, 2, 1, "", "caches_are_enabled"], [150, 2, 1, "", "forward"], [150, 2, 1, "", "reset_caches"], [150, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[151, 4, 1, "", "cache_enabled"], [151, 2, 1, "", "forward"], [151, 2, 1, "", "reset_cache"], [151, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[152, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[153, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[155, 1, 1, "", "AdapterModule"], [156, 1, 1, "", "LoRALinear"], [157, 0, 1, "", "disable_adapter"], [158, 0, 1, "", "get_adapter_params"], [159, 0, 1, "", "set_trainable_params"], [160, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [161, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[155, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[156, 2, 1, "", "adapter_params"], [156, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[162, 0, 1, "", "estimate_advantages"], [163, 0, 1, "", "get_rewards_ppo"], [164, 0, 1, "", "left_padded_collate"], [170, 0, 1, "", "padded_collate_dpo"], [171, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.rlhf.loss": [[165, 1, 1, "", "DPOLoss"], [166, 1, 1, "", "IPOLoss"], [167, 1, 1, "", "PPOLoss"], [168, 1, 1, "", "RSOLoss"], [169, 1, 1, "", "SimPOLoss"]], "torchtune.modules.rlhf.loss.DPOLoss": [[165, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.IPOLoss": [[166, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.PPOLoss": [[167, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.RSOLoss": [[168, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.SimPOLoss": [[169, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[172, 1, 1, "", "BaseTokenizer"], [173, 1, 1, "", "ModelTokenizer"], [174, 1, 1, "", "SentencePieceBaseTokenizer"], [175, 1, 1, "", "TikTokenBaseTokenizer"], [176, 0, 1, "", "parse_hf_tokenizer_json"], [177, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[172, 2, 1, "", "decode"], [172, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[173, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[174, 2, 1, "", "decode"], [174, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[175, 2, 1, "", "decode"], [175, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[178, 1, 1, "", "Transform"], [179, 1, 1, "", "VisionCrossAttentionMask"], [180, 0, 1, "", "find_supported_resolutions"], [181, 0, 1, "", "get_canvas_best_fit"], [182, 0, 1, "", "get_inscribed_size"], [183, 0, 1, "", "resize_with_pad"], [184, 0, 1, "", "tile_crop"]], "torchtune.utils": [[185, 3, 1, "", "FSDPPolicyType"], [186, 1, 1, "", "FullModelHFCheckpointer"], [187, 1, 1, "", "FullModelMetaCheckpointer"], [188, 1, 1, "", "FullModelTorchTuneCheckpointer"], [189, 1, 1, "", "ModelType"], [190, 1, 1, "", "OptimizerInBackwardWrapper"], [191, 1, 1, "", "TuneRecipeArgumentParser"], [192, 0, 1, "", "create_optim_in_bwd_wrapper"], [193, 0, 1, "", "generate"], [194, 0, 1, "", "get_device"], [195, 0, 1, "", "get_dtype"], [196, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [197, 0, 1, "", "get_logger"], [198, 0, 1, "", "get_memory_stats"], [199, 0, 1, "", "get_quantizer_mode"], [200, 0, 1, "", "get_world_size_and_rank"], [201, 0, 1, "", "init_distributed"], [202, 0, 1, "", "is_distributed"], [203, 0, 1, "", "log_memory_stats"], [204, 0, 1, "", "lora_fsdp_wrap_policy"], [210, 0, 1, "", "padded_collate"], [211, 0, 1, "", "register_optim_in_bwd_hooks"], [212, 0, 1, "", "set_activation_checkpointing"], [213, 0, 1, "", "set_default_dtype"], [214, 0, 1, "", "set_seed"], [215, 0, 1, "", "setup_torch_profiler"], [216, 0, 1, "", "torch_version_ge"], [217, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[186, 2, 1, "", "load_checkpoint"], [186, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[187, 2, 1, "", "load_checkpoint"], [187, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[188, 2, 1, "", "load_checkpoint"], [188, 2, 1, "", "save_checkpoint"]], "torchtune.utils.OptimizerInBackwardWrapper": [[190, 2, 1, "", "get_optim_key"], [190, 2, 1, "", "load_state_dict"], [190, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[191, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[205, 1, 1, "", "CometLogger"], [206, 1, 1, "", "DiskLogger"], [207, 1, 1, "", "StdoutLogger"], [208, 1, 1, "", "TensorBoardLogger"], [209, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.CometLogger": [[205, 2, 1, "", "close"], [205, 2, 1, "", "log"], [205, 2, 1, "", "log_config"], [205, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.DiskLogger": [[206, 2, 1, "", "close"], [206, 2, 1, "", "log"], [206, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[207, 2, 1, "", "close"], [207, 2, 1, "", "log"], [207, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[208, 2, 1, "", "close"], [208, 2, 1, "", "log"], [208, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[209, 2, 1, "", "close"], [209, 2, 1, "", "log"], [209, 2, 1, "", "log_config"], [209, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 20, 29, 30, 32, 185, 220, 222, 224, 227, 229, 230, 231, 232], "config": [0, 8, 9, 224, 228], "data": [1, 5, 20, 29, 30, 32, 225], "text": [1, 226, 229], "templat": [1, 225, 226], "type": 1, "convert": 1, "messag": [1, 25], "transform": [1, 4, 178], "helper": 1, "function": 1, "dataset": [2, 225, 226], "exampl": 2, "gener": [2, 193, 227, 229], "builder": 2, "class": [2, 9], "model": [3, 4, 10, 224, 227, 228, 229, 230, 231], "llama3": [3, 96, 225, 229, 231], "1": 3, "llama2": [3, 80, 225, 227, 230, 232], "code": 3, "llama": 3, "qwen": 3, "2": 3, "phi": 3, "3": 3, "mistral": [3, 120], "gemma": [3, 69], "clip": 3, "modul": 4, "compon": [4, 8], "build": [4, 221, 232], "block": 4, "base": 4, "token": [4, 225], "util": [4, 5, 185], "peft": 4, "vision": 4, "reinforc": 4, "learn": 4, "from": [4, 225, 232], "human": 4, "feedback": 4, "rlhf": 4, "loss": 4, "checkpoint": [5, 6, 10, 227], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 226, 230, 232], "manag": 5, "perform": [5, 230], "profil": 5, "metric": [5, 7, 10], "log": [5, 7, 10], "miscellan": 5, "overview": [6, 222, 227], "format": [6, 226], "handl": 6, "differ": 6, "hfcheckpoint": 6, "metacheckpoint": 6, "torchtunecheckpoint": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 227, 230, 232], "put": [6, 232], "thi": 6, "all": [6, 8, 232], "togeth": [6, 232], "comet": 7, "logger": [7, 10], "about": 8, "where": 8, "do": 8, "paramet": 8, "live": 8, "write": 8, "configur": [8, 226], "us": [8, 9, 225, 227, 232], "instanti": [8, 11], "referenc": 8, "other": [8, 227], "field": 8, "interpol": 8, "valid": [8, 14, 224], "your": [8, 9, 227, 228], "best": 8, "practic": 8, "airtight": 8, "public": 8, "api": 8, "onli": 8, "command": 8, "line": 8, "overrid": 8, "remov": 8, "what": [9, 222, 230, 231, 232], "ar": 9, "recip": [9, 224, 228, 230, 231], "script": 9, "run": [9, 224, 227], "cli": [9, 224], "pars": [9, 13], "weight": 10, "bias": 10, "w": 10, "b": 10, "log_config": 12, "alpacainstructtempl": 15, "chatformat": 16, "chatmlformat": 17, "chatmltempl": 18, "chosenrejectedtomessag": 19, "grammarerrorcorrectiontempl": 20, "inputoutputtomessag": 21, "instructtempl": 22, "jsontomessag": 23, "llama2chatformat": 24, "mistralchatformat": 26, "prompttempl": 27, "prompttemplateinterfac": 28, "questionanswertempl": 29, "role": 30, "sharegpttomessag": 31, "summarizetempl": 32, "get_openai_messag": 33, "get_sharegpt_messag": 34, "truncat": 35, "validate_messag": 36, "chatdataset": 37, "concatdataset": 38, "instructdataset": 39, "packeddataset": 40, "preferencedataset": 41, "sftdataset": 42, "textcompletiondataset": 43, "alpaca_cleaned_dataset": 44, "alpaca_dataset": 45, "chat_dataset": 46, "cnn_dailymail_articles_dataset": 47, "grammar_dataset": 48, "instruct_dataset": 49, "samsum_dataset": 50, "slimorca_dataset": 51, "stack_exchange_paired_dataset": 52, "text_completion_dataset": 53, "wikitext_dataset": 54, "tilepositionalembed": 55, "tiledtokenpositionalembed": 56, "tokenpositionalembed": 57, "clip_vision_encod": 58, "code_llama2_13b": 59, "code_llama2_70b": 60, "code_llama2_7b": 61, "lora_code_llama2_13b": 62, "lora_code_llama2_70b": 63, "lora_code_llama2_7b": 64, "qlora_code_llama2_13b": 65, "qlora_code_llama2_70b": 66, "qlora_code_llama2_7b": 67, "gemmatoken": 68, "gemma_2b": 70, "gemma_7b": 71, "gemma_token": 72, "lora_gemma": 73, "lora_gemma_2b": 74, "lora_gemma_7b": 75, "qlora_gemma_2b": 76, "qlora_gemma_7b": 77, "llama2chattempl": 78, "llama2token": 79, "llama2_13b": 81, "llama2_70b": 82, "llama2_7b": 83, "llama2_reward_7b": 84, "llama2_token": 85, "lora_llama2": 86, "lora_llama2_13b": 87, "lora_llama2_70b": 88, "lora_llama2_7b": 89, "lora_llama2_reward_7b": 90, "qlora_llama2_13b": 91, "qlora_llama2_70b": 92, "qlora_llama2_7b": 93, "qlora_llama2_reward_7b": 94, "llama3token": 95, "llama3_70b": 97, "llama3_8b": 98, "llama3_token": 99, "lora_llama3": 100, "lora_llama3_70b": 101, "lora_llama3_8b": 102, "qlora_llama3_70b": 103, "qlora_llama3_8b": 104, "llama3_1": 105, "llama3_1_405b": 106, "llama3_1_70b": 107, "llama3_1_8b": 108, "lora_llama3_1": 109, "lora_llama3_1_70b": 110, "lora_llama3_1_8b": 111, "qlora_llama3_1_70b": 112, "qlora_llama3_1_8b": 113, "mistralchattempl": 114, "mistraltoken": 115, "lora_mistr": 116, "lora_mistral_7b": 117, "lora_mistral_classifi": 118, "lora_mistral_reward_7b": 119, "mistral_7b": 121, "mistral_classifi": 122, "mistral_reward_7b": 123, "mistral_token": 124, "qlora_mistral_7b": 125, "qlora_mistral_reward_7b": 126, "phi3minitoken": 127, "lora_phi3": 128, "lora_phi3_mini": 129, "phi3": 130, "phi3_mini": 131, "phi3_mini_token": 132, "qlora_phi3_mini": 133, "qwen2token": 134, "lora_qwen2": 135, "lora_qwen2_0_5b": 136, "lora_qwen2_1_5b": 137, "lora_qwen2_7b": 138, "qwen2": 139, "qwen2_0_5b": 140, "qwen2_1_5b": 141, "qwen2_7b": 142, "qwen2_token": 143, "causalselfattent": 144, "todo": [144, 151], "feedforward": 145, "fp32layernorm": 146, "kvcach": 147, "rmsnorm": 148, "rotarypositionalembed": 149, "transformerdecod": 150, "transformerdecoderlay": 151, "visiontransform": 152, "reparametrize_as_dtype_state_dict_post_hook": 153, "get_cosine_schedule_with_warmup": 154, "adaptermodul": 155, "loralinear": 156, "disable_adapt": 157, "get_adapter_param": 158, "set_trainable_param": 159, "validate_missing_and_unexpected_for_lora": 160, "validate_state_dict_for_lora": 161, "estimate_advantag": 162, "get_rewards_ppo": 163, "left_padded_col": 164, "dpoloss": 165, "ipoloss": 166, "ppoloss": 167, "rsoloss": 168, "simpoloss": 169, "padded_collate_dpo": 170, "truncate_sequence_at_first_stop_token": 171, "basetoken": 172, "modeltoken": 173, "sentencepiecebasetoken": 174, "tiktokenbasetoken": 175, "parse_hf_tokenizer_json": 176, "tokenize_messages_no_special_token": 177, "visioncrossattentionmask": 179, "find_supported_resolut": 180, "get_canvas_best_fit": 181, "get_inscribed_s": 182, "resize_with_pad": 183, "tile_crop": 184, "fsdppolicytyp": 185, "fullmodelhfcheckpoint": 186, "fullmodelmetacheckpoint": 187, "fullmodeltorchtunecheckpoint": 188, "modeltyp": 189, "optimizerinbackwardwrapp": 190, "tunerecipeargumentpars": 191, "create_optim_in_bwd_wrapp": 192, "get_devic": 194, "get_dtyp": 195, "get_full_finetune_fsdp_wrap_polici": 196, "get_logg": 197, "get_memory_stat": 198, "get_quantizer_mod": 199, "get_world_size_and_rank": 200, "init_distribut": 201, "is_distribut": 202, "log_memory_stat": 203, "lora_fsdp_wrap_polici": 204, "cometlogg": 205, "disklogg": 206, "stdoutlogg": 207, "tensorboardlogg": 208, "wandblogg": 209, "padded_col": 210, "register_optim_in_bwd_hook": 211, "set_activation_checkpoint": 212, "set_default_dtyp": 213, "set_se": 214, "setup_torch_profil": 215, "torch_version_g": 216, "validate_expected_param_dtyp": 217, "comput": [219, 223], "time": [219, 223], "welcom": 220, "document": 220, "get": [220, 224, 229], "start": [220, 224], "tutori": 220, "instal": 221, "instruct": [221, 226, 229], "via": [221, 229], "pypi": 221, "git": 221, "clone": 221, "nightli": 221, "kei": 222, "concept": 222, "design": 222, "principl": 222, "download": [224, 227, 228], "list": 224, "built": [224, 226], "copi": 224, "fine": [225, 226, 228, 229], "tune": [225, 226, 228, 229], "chat": [225, 226], "chang": 225, "prompt": 225, "special": 225, "when": 225, "should": 225, "i": 225, "custom": [225, 226], "hug": [226, 227], "face": [226, 227], "set": 226, "max": 226, "sequenc": 226, "length": 226, "sampl": 226, "pack": 226, "unstructur": 226, "corpu": 226, "multipl": 226, "local": 226, "remot": 226, "fulli": 226, "end": 227, "workflow": 227, "7b": 227, "finetun": [227, 230, 231, 232], "evalu": [227, 229, 231], "eleutherai": [227, 229], "s": [227, 229], "eval": [227, 229], "har": [227, 229], "speed": 227, "up": 227, "quantiz": [227, 229, 231], "librari": 227, "upload": 227, "hub": 227, "first": 228, "llm": 228, "select": 228, "modifi": 228, "train": 228, "next": 228, "step": 228, "meta": 229, "8b": 229, "access": 229, "our": 229, "faster": 229, "how": 230, "doe": 230, "work": 230, "appli": [230, 231], "trade": 230, "off": 230, "qat": 231, "lower": 231, "devic": 231, "option": 231, "qlora": 232, "save": 232, "deep": 232, "dive": 232}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})