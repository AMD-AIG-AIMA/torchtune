Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.Role", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.loss.IPOLoss", "generated/torchtune.modules.loss.RSOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.loss.IPOLoss.rst", "generated/torchtune.modules.loss.RSOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "torchtune.data.Role", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_classifier_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "IPOLoss", "RSOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 21, 23, 31, 32, 33, 36, 40, 82, 100, 107, 108, 109, 176, 180, 181, 184, 186, 187, 188], "prompt": [1, 14, 17, 18, 19, 21, 23, 24, 25, 26, 29, 31, 33, 36, 37, 39, 40, 41, 42, 59, 69, 82, 92, 104, 117, 135, 149, 182, 183, 185], "chat": [1, 2, 15, 16, 19, 25, 26, 29, 37, 42, 69, 109], "includ": [1, 6, 7, 8, 15, 18, 49, 60, 69, 70, 83, 97, 109, 126, 138, 142, 143, 147, 178, 180, 181, 182, 183, 184, 185, 186, 188], "some": [1, 6, 7, 16, 95, 128, 129, 176, 178, 180, 181, 182, 183, 184, 186, 187, 188], "specif": [1, 4, 7, 8, 10, 152, 181, 182, 183, 187, 188], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29, 31, 33, 36, 37, 40, 42, 69, 82, 139, 142, 143, 144, 145, 180, 181, 183, 184, 185, 186], "differ": [1, 7, 9, 29, 30, 31, 33, 46, 47, 48, 92, 119, 122, 132, 145, 166, 173, 178, 180, 181, 183, 185, 186, 187, 188], "dataset": [1, 5, 7, 14, 17, 18, 20, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 122, 123, 178, 184, 185, 187], "model": [1, 2, 6, 7, 8, 10, 16, 21, 29, 30, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 134, 135, 142, 143, 144, 145, 148, 149, 152, 154, 160, 167, 168, 176, 178, 181, 182, 188], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 23, 24, 26, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 61, 62, 71, 72, 73, 86, 92, 98, 100, 109, 112, 117, 118, 119, 121, 122, 123, 124, 125, 128, 131, 132, 134, 136, 138, 139, 142, 143, 144, 146, 147, 148, 149, 163, 164, 167, 175, 177, 179, 180, 182, 183, 184, 185, 186, 187], "common": [1, 2, 4, 7, 135, 180, 181, 182, 185, 186, 187], "json": [1, 6, 25, 26, 86, 109, 134, 142, 180, 182, 183, 187], "messag": [1, 15, 16, 19, 21, 25, 26, 28, 29, 37, 59, 69, 82, 92, 104, 135, 177, 180, 181, 182], "miscellan": 1, "function": [1, 4, 7, 8, 10, 12, 29, 47, 48, 49, 111, 112, 119, 120, 122, 127, 130, 131, 141, 142, 149, 150, 156, 160, 166, 170, 178, 181, 182, 188], "us": [1, 2, 3, 4, 6, 9, 10, 12, 16, 19, 20, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 69, 70, 75, 83, 86, 87, 105, 109, 111, 112, 115, 116, 117, 118, 119, 120, 122, 127, 130, 132, 133, 136, 137, 139, 141, 142, 143, 145, 146, 147, 149, 150, 151, 152, 154, 160, 161, 162, 163, 164, 170, 176, 177, 178, 180, 182, 184, 185, 186, 187], "modifi": [1, 7, 8, 9, 120, 178, 183, 185, 186, 187, 188], "For": [2, 5, 6, 7, 8, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 44, 45, 46, 47, 48, 49, 60, 64, 69, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 117, 119, 137, 138, 142, 147, 148, 155, 164, 168, 170, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188], "detail": [2, 6, 35, 37, 42, 46, 47, 48, 49, 69, 99, 114, 119, 141, 152, 160, 170, 180, 183, 184, 185, 186, 187, 188], "usag": [2, 120, 145, 146, 171, 177, 180, 182, 183, 184, 185, 187, 188], "guid": [2, 7, 9, 178, 181, 182, 184, 186], "pleas": [2, 5, 46, 47, 48, 49, 56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 119, 141, 152, 160, 168, 177, 188], "see": [2, 5, 6, 9, 19, 21, 35, 37, 42, 45, 56, 57, 58, 67, 68, 69, 79, 80, 81, 90, 91, 99, 102, 103, 110, 114, 119, 125, 141, 145, 147, 152, 153, 160, 164, 168, 170, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "our": [2, 6, 8, 178, 181, 182, 183, 184, 186, 187, 188], "tutori": [2, 6, 69, 168, 178, 181, 182, 183, 184, 185, 186, 187, 188], "support": [2, 3, 6, 8, 9, 10, 20, 21, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 64, 75, 87, 93, 95, 105, 108, 109, 111, 113, 119, 124, 126, 137, 139, 143, 144, 146, 151, 154, 155, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "sever": 2, "wide": 2, "help": [2, 6, 19, 117, 119, 142, 147, 176, 177, 178, 180, 181, 182, 183, 184, 185, 187, 188], "quickli": [2, 7, 34, 181, 182], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 23, 24, 29, 34, 47, 48, 49, 69, 119, 163, 164, 176, 177, 178, 180, 181, 182, 185, 186, 187, 188], "fine": [2, 6, 8, 9, 20, 32, 69, 176, 178, 183, 186, 187], "tune": [2, 3, 6, 7, 8, 9, 12, 20, 32, 69, 176, 177, 178, 180, 183, 186, 187, 188], "also": [2, 6, 7, 8, 9, 10, 30, 37, 40, 44, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 109, 111, 117, 126, 150, 152, 154, 160, 164, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188], "like": [2, 6, 7, 8, 9, 29, 109, 119, 144, 177, 180, 181, 182, 183, 184, 186, 187], "These": [2, 4, 6, 7, 8, 10, 32, 119, 136, 147, 181, 182, 183, 184, 185, 186, 187, 188], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 17, 18, 19, 21, 23, 24, 28, 31, 32, 33, 36, 37, 39, 40, 41, 47, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 69, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 117, 119, 126, 127, 130, 131, 136, 138, 141, 142, 143, 145, 146, 148, 149, 151, 154, 158, 160, 166, 171, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "especi": [2, 178, 180, 183], "specifi": [2, 6, 7, 8, 10, 37, 70, 75, 83, 87, 111, 117, 118, 141, 149, 152, 155, 160, 164, 168, 171, 180, 181, 182, 183, 184, 185, 187, 188], "yaml": [2, 7, 8, 10, 11, 12, 30, 37, 40, 44, 147, 164, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "config": [2, 6, 9, 10, 11, 12, 13, 30, 37, 40, 44, 111, 130, 142, 146, 147, 164, 171, 178, 181, 182, 183, 185, 186, 187, 188], "represent": [2, 186, 187, 188], "abov": [2, 6, 120, 138, 158, 177, 183, 185, 186, 187, 188], "all": [3, 4, 8, 13, 29, 30, 32, 37, 49, 86, 109, 111, 112, 117, 119, 120, 127, 137, 138, 142, 146, 147, 148, 158, 167, 173, 174, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187], "famili": [3, 8, 36, 38, 42, 43, 45, 145, 178, 180, 185], "request": [3, 14, 151, 182, 183], "access": [3, 6, 7, 8, 30, 142, 148, 180, 183, 184], "hug": [3, 6, 16, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 86, 109, 121, 134, 178, 180, 184, 185], "face": [3, 6, 16, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 86, 109, 121, 134, 178, 180, 184, 185], "To": [3, 6, 7, 8, 9, 32, 119, 142, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "download": [3, 6, 174, 177, 181, 182, 185, 186, 187, 188], "8b": [3, 85, 89, 91, 106, 180, 181, 187], "meta": [3, 6, 19, 69, 82, 116, 142, 143, 180, 181, 183, 184], "hf": [3, 6, 104, 122, 123, 124, 142, 180, 181, 183, 184, 185], "token": [3, 6, 7, 8, 20, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 60, 63, 64, 69, 70, 74, 75, 82, 83, 86, 87, 92, 93, 95, 97, 99, 101, 104, 105, 107, 109, 111, 116, 117, 118, 119, 132, 133, 134, 135, 136, 149, 152, 165, 180, 182, 183, 184, 185, 186, 187, 188], "hf_token": 3, "70b": [3, 51, 54, 57, 72, 77, 80, 84, 88, 90, 185], "ignor": [3, 6, 104, 111, 112, 180], "pattern": [3, 133, 180], "origin": [3, 6, 35, 36, 120, 126, 181, 183, 185, 186, 187, 188], "consolid": [3, 6, 185], "7b": [3, 6, 31, 33, 34, 36, 38, 40, 44, 45, 52, 55, 58, 62, 66, 73, 78, 81, 94, 96, 98, 100, 142, 143, 181, 184, 185, 186, 188], "2": [3, 6, 9, 28, 32, 42, 46, 47, 59, 69, 82, 92, 104, 111, 119, 123, 132, 133, 135, 137, 138, 139, 142, 143, 155, 165, 166, 169, 170, 171, 172, 181, 183, 184, 185, 186, 187], "13b": [3, 6, 50, 53, 56, 71, 76, 79], "codellama": 3, "mini": [3, 104, 106, 107, 108, 109, 110], "4k": [3, 107, 108, 109], "microsoft": [3, 108, 109], "ai": [3, 98, 111, 164, 181, 185], "v0": 3, "1": [3, 6, 8, 32, 42, 46, 47, 59, 69, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 111, 117, 119, 121, 122, 123, 124, 132, 133, 135, 138, 139, 143, 145, 149, 158, 163, 164, 165, 166, 169, 170, 180, 181, 183, 184, 185, 186, 187, 188], "mistralai": [3, 180], "size": [3, 6, 8, 10, 36, 39, 41, 47, 48, 49, 111, 114, 115, 116, 117, 119, 136, 137, 139, 140, 156, 158, 178, 180, 182, 183, 184, 185, 186, 187], "2b": [3, 61, 65], "googl": [3, 61, 62], "gguf": 3, "vision": [3, 49], "compon": [3, 6, 8, 13, 166, 178, 182, 184, 186, 188], "multimod": [3, 20], "encod": [3, 4, 49, 59, 69, 82, 92, 104, 122, 132, 133, 135, 136, 181], "perform": [4, 6, 32, 69, 112, 119, 127, 149, 178, 181, 183, 185, 187, 188], "direct": [4, 8, 122, 166, 177], "text": [4, 20, 29, 32, 34, 37, 38, 44, 45, 69, 82, 92, 104, 132, 133, 136, 181, 183, 187], "id": [4, 6, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 69, 82, 92, 104, 111, 116, 117, 118, 132, 133, 134, 136, 142, 144, 149, 165, 166, 181, 182, 183], "decod": [4, 60, 64, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 117, 132, 133, 149, 181], "typic": [4, 7, 32, 34, 44, 109, 122, 182, 187, 188], "byte": [4, 133, 188], "pair": [4, 7, 14, 43, 123, 133, 165, 166, 182], "underli": [4, 92, 132, 188], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 120, 125, 128, 130, 139, 146, 147, 155, 177, 178, 182, 183, 185, 186, 188], "can": [4, 6, 7, 8, 9, 10, 13, 20, 29, 30, 31, 33, 34, 36, 37, 38, 40, 44, 45, 46, 47, 49, 69, 92, 115, 116, 119, 127, 132, 133, 137, 139, 141, 142, 145, 147, 152, 160, 163, 164, 168, 171, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 23, 24, 25, 26, 27, 29, 31, 33, 34, 37, 38, 40, 44, 45, 69, 92, 113, 120, 128, 129, 130, 131, 132, 135, 142, 143, 144, 146, 149, 157, 160, 170, 173, 180, 181, 182, 183, 184, 185, 186, 187], "preprocess": [4, 32, 119], "imag": [4, 20, 46, 47, 48, 49, 119, 136, 137, 138, 139, 140, 186], "offer": 5, "allow": [5, 30, 130, 138, 163, 180, 187, 188], "seamless": 5, "transit": 5, "between": [5, 6, 123, 142, 145, 182, 183, 185, 186, 187, 188], "train": [5, 6, 8, 9, 19, 29, 30, 31, 32, 36, 37, 39, 40, 41, 42, 44, 69, 111, 113, 116, 117, 118, 120, 121, 122, 142, 143, 144, 151, 154, 160, 171, 176, 178, 180, 181, 182, 183, 185, 186, 187, 188], "interoper": [5, 6, 8, 178, 183, 188], "rest": [5, 181, 188], "ecosystem": [5, 6, 8, 178, 183, 185, 188], "comprehens": 5, "overview": [5, 7, 9, 176, 184, 186, 188], "deep": [5, 6, 7, 8, 9, 178, 184, 185], "dive": [5, 6, 7, 8, 9, 178, 184, 185], "enabl": [5, 7, 8, 9, 30, 53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 88, 89, 90, 91, 94, 96, 102, 103, 106, 110, 126, 170, 171, 185, 186, 188], "work": [5, 6, 8, 147, 178, 180, 183, 185, 188], "set": [5, 6, 7, 8, 9, 20, 31, 32, 33, 34, 36, 38, 39, 40, 41, 42, 44, 45, 70, 75, 82, 83, 87, 93, 95, 97, 99, 104, 105, 107, 111, 116, 117, 127, 129, 139, 141, 150, 152, 158, 160, 168, 169, 170, 171, 178, 180, 181, 183, 184, 185, 186, 187], "consumpt": [5, 30], "dure": [5, 6, 30, 31, 32, 36, 39, 41, 111, 114, 116, 117, 118, 119, 120, 154, 181, 183, 185, 186, 187, 188], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 27, 29, 30, 31, 32, 33, 42, 49, 117, 119, 122, 127, 144, 147, 150, 152, 164, 171, 178, 180, 181, 182, 183, 184, 185], "debug": [5, 6, 7, 8, 180], "finetun": [5, 6, 7, 8, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 176, 178, 184, 185], "job": [5, 9, 170, 184], "variou": [5, 18], "walk": [6, 8, 163, 178, 181, 182, 183, 184, 187, 188], "you": [6, 7, 8, 9, 10, 18, 19, 20, 24, 29, 31, 33, 34, 36, 38, 40, 44, 45, 119, 138, 145, 147, 149, 163, 164, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "through": [6, 7, 8, 9, 49, 112, 119, 127, 178, 180, 181, 182, 183, 184, 187, 188], "design": [6, 8], "behavior": [6, 160, 181, 182], "associ": [6, 7, 8, 49, 60, 70, 83, 97, 149, 183, 186], "util": [6, 7, 8, 9, 10, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 178, 183, 184, 185, 187, 188], "what": [6, 7, 9, 19, 21, 23, 34, 39, 41, 119, 176, 181, 182, 183, 184, 185], "cover": [6, 7, 8, 9, 181, 183, 188], "how": [6, 7, 8, 9, 24, 119, 141, 168, 176, 180, 181, 182, 183, 184, 185, 187, 188], "we": [6, 7, 8, 9, 31, 32, 33, 34, 36, 38, 40, 44, 45, 69, 92, 111, 114, 116, 117, 119, 122, 126, 137, 138, 142, 143, 144, 149, 151, 155, 160, 167, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "them": [6, 7, 29, 30, 31, 33, 40, 59, 69, 92, 104, 112, 119, 120, 135, 180, 181, 182, 183, 186, 187, 188], "scenario": [6, 30], "full": [6, 7, 8, 37, 40, 56, 57, 58, 59, 67, 68, 69, 79, 80, 81, 90, 91, 92, 102, 103, 104, 110, 130, 131, 135, 178, 180, 182, 185, 186, 187], "compos": [6, 119], "which": [6, 7, 8, 30, 31, 32, 34, 36, 39, 41, 53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 92, 93, 94, 95, 96, 105, 106, 111, 116, 117, 118, 119, 121, 130, 131, 132, 138, 142, 143, 144, 146, 151, 161, 164, 168, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 112, 130, 142, 143, 144, 178, 181, 182, 183, 185, 188], "evalu": [6, 8, 176, 178, 184, 186, 188], "gener": [6, 8, 14, 17, 23, 24, 29, 31, 32, 33, 38, 42, 69, 92, 127, 169, 170, 171, 174, 176, 181, 182, 186, 187, 188], "each": [6, 8, 15, 18, 30, 32, 46, 47, 48, 49, 53, 54, 55, 59, 64, 65, 66, 69, 75, 76, 77, 78, 87, 88, 89, 92, 93, 94, 95, 96, 104, 105, 106, 111, 116, 117, 118, 119, 122, 123, 124, 130, 131, 135, 136, 138, 140, 166, 170, 171, 178, 180, 182, 183, 184, 185, 186, 187], "make": [6, 7, 8, 9, 111, 118, 119, 178, 180, 181, 183, 184, 185, 186, 187, 188], "easi": [6, 8, 178, 182, 186], "understand": [6, 7, 8, 176, 178, 181, 182, 186, 188], "extend": [6, 8, 178], "befor": [6, 28, 31, 32, 33, 46, 47, 49, 60, 64, 111, 117, 118, 119, 126, 133, 142, 180, 183, 187], "let": [6, 7, 9, 180, 181, 182, 183, 184, 185, 186, 188], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 53, 54, 55, 59, 69, 75, 76, 77, 78, 82, 87, 88, 89, 92, 93, 94, 95, 96, 104, 105, 106, 109, 111, 114, 116, 117, 118, 119, 120, 122, 123, 124, 125, 128, 130, 131, 138, 141, 142, 143, 146, 150, 152, 154, 160, 163, 168, 169, 178, 180, 181, 182, 184, 186, 187, 188], "defin": [6, 7, 8, 112, 125, 126, 128, 182, 184, 186], "concept": [6, 183, 184], "In": [6, 7, 8, 29, 47, 48, 49, 116, 119, 123, 126, 138, 141, 160, 163, 164, 181, 183, 185, 186, 187, 188], "ll": [6, 7, 8, 149, 155, 178, 181, 182, 183, 184, 185, 187, 188], "talk": 6, "about": [6, 8, 119, 122, 164, 178, 180, 181, 183, 184, 185, 186, 187, 188], "take": [6, 7, 8, 10, 112, 114, 119, 120, 142, 144, 147, 150, 166, 181, 182, 183, 184, 185, 186, 188], "close": [6, 8, 161, 162, 163, 164, 186], "look": [6, 7, 8, 148, 163, 177, 181, 182, 183, 184, 185, 186, 187], "veri": [6, 30, 117, 180, 183], "simpli": [6, 7, 32, 122, 123, 180, 181, 182, 183, 185, 188], "dictat": 6, "state_dict": [6, 120, 130, 142, 143, 144, 145, 146, 186, 188], "store": [6, 30, 161, 164, 186, 188], "file": [6, 7, 8, 9, 10, 11, 12, 59, 69, 82, 86, 92, 104, 109, 132, 133, 134, 142, 143, 144, 147, 161, 164, 171, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188], "disk": [6, 34, 161], "weight": [6, 8, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 111, 120, 122, 125, 126, 130, 132, 142, 143, 144, 145, 155, 160, 164, 176, 180, 181, 183, 184, 185, 186, 187, 188], "string": [6, 20, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 69, 82, 92, 104, 125, 132, 133, 135, 150, 151, 155, 180, 182], "kei": [6, 7, 9, 14, 17, 23, 24, 25, 29, 31, 33, 40, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 117, 118, 129, 130, 131, 142, 144, 146, 166, 171, 180, 183, 184, 186, 188], "identifi": 6, "state": [6, 8, 119, 120, 122, 128, 129, 130, 131, 142, 143, 144, 146, 148, 183, 185, 186, 188], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 37, 38, 40, 44, 45, 82, 104, 120, 128, 129, 130, 131, 133, 134, 142, 143, 144, 146, 148, 154, 157, 159, 165, 166, 167, 182], "If": [6, 7, 13, 14, 17, 18, 20, 21, 23, 24, 25, 27, 28, 29, 31, 33, 36, 39, 40, 41, 42, 49, 70, 75, 82, 83, 87, 104, 111, 116, 117, 118, 119, 120, 126, 131, 138, 139, 142, 143, 144, 145, 146, 149, 150, 151, 152, 154, 155, 157, 163, 164, 170, 171, 173, 177, 180, 181, 182, 183, 184, 185, 186, 187], "don": [6, 7, 8, 164, 170, 180, 181, 182, 183, 184, 185, 188], "t": [6, 7, 8, 123, 151, 164, 170, 180, 181, 182, 183, 184, 185, 188], "match": [6, 29, 31, 33, 40, 104, 131, 138, 177, 180, 182, 183, 185, 186], "up": [6, 8, 9, 31, 32, 33, 34, 36, 38, 40, 44, 45, 133, 136, 137, 139, 148, 171, 180, 181, 182, 184, 185, 186, 188], "exactli": [6, 131, 187], "those": [6, 145, 186], "definit": [6, 186], "either": [6, 131, 142, 149, 168, 180, 186, 187, 188], "run": [6, 7, 9, 12, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 112, 114, 117, 120, 142, 143, 144, 146, 148, 158, 163, 164, 167, 177, 178, 181, 182, 184, 185, 186, 187, 188], "explicit": 6, "error": [6, 7, 28, 142, 170, 180], "load": [6, 8, 29, 30, 31, 32, 33, 34, 130, 142, 143, 144, 146, 147, 163, 181, 182, 183, 185, 186], "rais": [6, 10, 13, 21, 25, 28, 37, 42, 104, 111, 114, 117, 119, 130, 131, 142, 143, 144, 146, 151, 154, 157, 164, 166, 170, 173], "an": [6, 7, 8, 9, 10, 14, 28, 30, 34, 39, 41, 44, 45, 46, 47, 48, 75, 87, 93, 95, 99, 105, 111, 117, 119, 122, 125, 127, 128, 129, 136, 137, 138, 139, 141, 142, 143, 144, 146, 150, 152, 164, 171, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "except": [6, 20, 21, 135, 182], "wors": 6, "silent": [6, 112], "succe": 6, "infer": [6, 19, 29, 60, 69, 97, 111, 114, 116, 117, 118, 150, 176, 181, 183, 184, 185, 187, 188], "expect": [6, 7, 10, 14, 17, 18, 23, 24, 29, 31, 33, 37, 40, 116, 131, 146, 164, 173, 181, 182, 186, 187], "addit": [6, 7, 8, 10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 69, 122, 130, 141, 142, 143, 144, 151, 152, 157, 160, 161, 163, 164, 168, 178, 181, 184, 186], "line": [6, 8, 14, 147, 180, 182, 184, 185], "need": [6, 7, 8, 9, 18, 29, 32, 42, 111, 112, 117, 119, 160, 163, 164, 167, 177, 180, 181, 182, 183, 184, 185, 186, 188], "shape": [6, 46, 47, 48, 49, 111, 114, 116, 117, 118, 119, 122, 123, 124, 126, 136, 138, 140, 149, 171], "valu": [6, 7, 26, 42, 50, 51, 52, 60, 61, 62, 64, 70, 71, 72, 73, 75, 83, 84, 85, 87, 93, 95, 97, 98, 99, 100, 105, 107, 111, 114, 115, 117, 118, 121, 130, 142, 145, 146, 147, 149, 161, 162, 163, 164, 166, 170, 180, 182, 184, 185, 186, 187], "two": [6, 7, 28, 47, 119, 136, 138, 178, 183, 184, 185, 186, 187, 188], "popular": [6, 178, 182, 183], "llama2": [6, 7, 8, 10, 19, 29, 31, 33, 34, 36, 38, 40, 42, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 112, 117, 118, 145, 176, 178, 180, 184, 185, 187], "offici": [6, 19, 181, 184, 185], "implement": [6, 8, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 69, 92, 104, 112, 115, 116, 119, 121, 122, 123, 124, 125, 126, 142, 155, 163, 178, 182, 186, 187, 188], "when": [6, 7, 8, 12, 30, 32, 34, 69, 111, 116, 117, 118, 119, 120, 121, 130, 137, 139, 149, 152, 163, 167, 180, 183, 185, 186, 187, 188], "llama": [6, 19, 29, 69, 82, 115, 116, 142, 143, 180, 181, 183, 184, 185, 186], "websit": 6, "get": [6, 7, 8, 9, 29, 69, 92, 151, 153, 154, 156, 177, 178, 181, 182, 183, 184, 186, 187], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 30, 32, 34, 47, 48, 49, 111, 119, 130, 142, 143, 144, 146, 148, 180, 181, 182, 183, 184, 185, 186, 188], "pth": [6, 183], "inspect": [6, 183, 186, 188], "content": [6, 15, 20, 25, 26, 29, 59, 69, 92, 104, 135, 181, 182], "easili": [6, 7, 178, 182, 186, 187, 188], "torch": [6, 7, 46, 47, 48, 113, 114, 117, 119, 120, 121, 122, 123, 124, 138, 139, 140, 144, 146, 148, 149, 150, 151, 154, 155, 157, 158, 165, 166, 167, 168, 169, 170, 171, 172, 173, 183, 184, 185, 186, 188], "import": [6, 7, 10, 37, 40, 44, 119, 122, 163, 164, 181, 182, 183, 184, 186, 187, 188], "00": [6, 175, 179, 184], "mmap": [6, 183], "true": [6, 7, 20, 30, 31, 32, 34, 35, 36, 37, 39, 40, 41, 44, 49, 56, 57, 58, 59, 60, 64, 67, 68, 69, 79, 80, 81, 82, 90, 91, 92, 102, 103, 104, 110, 111, 117, 118, 120, 127, 132, 133, 135, 136, 138, 141, 142, 143, 144, 152, 154, 157, 158, 160, 163, 171, 172, 180, 181, 182, 183, 185, 186, 187, 188], "weights_onli": [6, 144], "map_loc": [6, 183], "cpu": [6, 8, 120, 151, 171, 177, 180, 183, 188], "tensor": [6, 46, 47, 48, 49, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 126, 138, 139, 140, 142, 149, 161, 162, 163, 164, 165, 166, 169, 186, 188], "item": 6, "print": [6, 9, 30, 36, 39, 41, 42, 59, 69, 82, 92, 104, 119, 132, 133, 135, 149, 172, 181, 182, 184, 186, 187, 188], "f": [6, 9, 36, 39, 41, 181, 183, 186, 188], "tok_embed": [6, 117], "32000": [6, 10, 186], "4096": [6, 10, 31, 33, 34, 36, 38, 40, 44, 45, 111, 116, 182, 186, 187], "len": [6, 30, 36, 39, 41, 117, 119], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 29, 30, 31, 32, 33, 39, 41, 42, 43, 46, 47, 48, 49, 53, 54, 55, 59, 64, 65, 66, 69, 75, 76, 77, 78, 82, 87, 88, 89, 92, 93, 95, 104, 105, 106, 113, 115, 116, 119, 120, 121, 122, 123, 124, 127, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 144, 147, 150, 151, 153, 155, 164, 169, 171, 172, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "contain": [6, 20, 25, 30, 32, 34, 44, 59, 69, 82, 86, 92, 104, 109, 111, 114, 116, 117, 118, 125, 128, 129, 130, 133, 135, 137, 142, 143, 144, 146, 147, 148, 154, 159, 163, 165, 166, 171, 181, 183, 185, 186], "input": [6, 14, 17, 18, 23, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 92, 104, 111, 112, 113, 115, 116, 117, 118, 119, 126, 132, 133, 136, 139, 140, 142, 144, 165, 166, 170, 173, 181, 182, 186, 188], "embed": [6, 46, 47, 48, 49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 115, 116, 117, 119, 152, 181, 185, 187], "tabl": [6, 181, 188], "call": [6, 10, 20, 112, 119, 120, 130, 147, 161, 162, 163, 164, 167, 171, 181, 182, 186, 188], "layer": [6, 8, 49, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 70, 75, 76, 77, 78, 79, 80, 81, 83, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 102, 103, 105, 106, 107, 110, 111, 117, 118, 119, 126, 130, 131, 141, 152, 178, 185, 186, 187, 188], "have": [6, 7, 10, 47, 48, 49, 111, 114, 119, 125, 131, 136, 138, 144, 146, 147, 152, 160, 163, 173, 177, 181, 182, 183, 184, 185, 186, 187, 188], "dim": [6, 111, 112, 115, 116, 117], "most": [6, 7, 137, 181, 184, 186, 188], "within": [6, 7, 10, 29, 32, 42, 46, 64, 75, 87, 93, 95, 105, 112, 119, 149, 163, 170, 171, 180, 182, 183, 185, 186, 188], "hub": [6, 180, 182, 184], "default": [6, 7, 16, 20, 25, 26, 27, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 73, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 104, 105, 106, 107, 109, 111, 112, 115, 116, 117, 118, 120, 121, 122, 126, 130, 132, 133, 135, 142, 143, 144, 147, 149, 151, 156, 160, 161, 164, 165, 166, 169, 170, 171, 177, 180, 181, 182, 183, 185, 186, 187, 188], "everi": [6, 8, 46, 47, 48, 112, 119, 163, 171, 177, 180, 188], "repo": [6, 142, 143, 145, 180, 183], "first": [6, 7, 10, 28, 32, 49, 114, 117, 119, 142, 147, 176, 178, 181, 182, 183, 185, 186, 187, 188], "big": [6, 183], "split": [6, 30, 32, 133, 181, 182, 183, 187], "across": [6, 8, 30, 142, 163, 170, 183, 185, 187], "bin": [6, 180, 183], "correctli": [6, 8, 13, 130, 142, 177, 181, 184, 188], "piec": 6, "one": [6, 8, 28, 59, 69, 92, 104, 112, 119, 135, 138, 144, 181, 182, 183, 184, 185, 188], "pytorch_model": [6, 183], "00001": [6, 180], "00002": [6, 180], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 32, 38, 49, 64, 75, 87, 93, 95, 105, 119, 126, 128, 130, 132, 143, 144, 146, 147, 149, 151, 152, 154, 155, 160, 180, 182, 183, 184, 185, 186, 187, 188], "doe": [6, 21, 25, 29, 32, 60, 69, 97, 108, 111, 117, 118, 125, 135, 142, 144, 146, 147, 180, 181, 183, 187], "fewer": [6, 111], "sinc": [6, 7, 10, 112, 138, 139, 142, 144, 181, 183, 185, 187], "instead": [6, 8, 32, 37, 40, 44, 49, 112, 114, 119, 126, 180, 183, 185, 186, 187], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 42, 44, 45, 125, 129, 131, 133, 142, 143, 144, 145, 146, 147, 148, 149, 150, 161, 162, 163, 164, 173, 180, 181, 183, 185, 187], "caus": [6, 92, 132, 139], "try": [6, 7, 181, 183, 184, 185, 188], "same": [6, 7, 46, 47, 53, 54, 55, 59, 65, 66, 69, 76, 77, 78, 88, 89, 92, 104, 106, 114, 118, 119, 135, 146, 147, 152, 164, 180, 181, 183, 185, 186, 187, 188], "As": [6, 7, 8, 9, 126, 178, 183, 185, 188], "re": [6, 7, 144, 178, 181, 183, 184, 185, 186], "care": [6, 112, 142, 144, 183, 185, 186], "end": [6, 8, 20, 30, 34, 44, 82, 92, 133, 176, 178, 181, 185, 186, 187], "number": [6, 8, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 117, 119, 121, 136, 137, 142, 143, 144, 149, 156, 170, 171, 180, 184, 186], "just": [6, 14, 178, 180, 181, 182, 184, 185, 186, 187], "save": [6, 8, 9, 120, 142, 143, 144, 146, 152, 160, 164, 176, 180, 181, 182, 183, 185, 186, 187], "less": [6, 42, 183, 184, 185, 188], "prone": 6, "manag": [6, 30, 127, 169, 181], "invari": 6, "accept": [6, 7, 42, 141, 182, 184, 188], "multipl": [6, 7, 8, 20, 29, 30, 111, 117, 118, 119, 126, 137, 138, 161, 162, 163, 164, 166, 171, 184, 185], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 181, 182, 183, 187], "worri": [6, 181, 184], "explicitli": [6, 125, 178, 186], "convert": [6, 25, 26, 29, 142, 165, 181, 183, 187, 188], "time": [6, 59, 60, 69, 92, 97, 104, 135, 161, 163, 171, 180, 181, 182, 183, 185, 188], "produc": [6, 146, 187, 188], "back": [6, 28, 127, 142, 182, 186, 188], "form": [6, 7, 8, 28, 180], "One": [6, 183, 187], "advantag": [6, 186], "being": [6, 142, 143, 144, 148, 150, 187, 188], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 25, 26, 32, 37, 40, 44, 53, 54, 55, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 111, 112, 119, 125, 130, 131, 140, 141, 147, 159, 161, 162, 163, 164, 177, 178, 182, 183, 184, 185, 186, 187, 188], "abl": [6, 8, 183, 184, 185, 187], "post": [6, 119, 167, 171, 187, 188], "tool": [6, 20, 182, 183, 184], "quantiz": [6, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 126, 144, 155, 176, 184, 188], "eval": [6, 176, 178, 187], "without": [6, 7, 9, 14, 130, 138, 139, 177, 178, 181, 183, 186, 187], "code": [6, 8, 50, 51, 52, 53, 54, 55, 56, 57, 58, 117, 174, 178, 182, 184], "chang": [6, 7, 9, 14, 144, 177, 180, 183, 184, 185, 186, 187, 188], "OR": [6, 25], "convers": [6, 15, 16, 19, 21, 25, 26, 28, 29, 37, 42, 142, 144, 145, 178, 181, 182, 183, 185, 186, 188], "script": [6, 9, 180, 183, 184, 185], "wai": [6, 7, 29, 130, 180, 181, 182, 183, 184, 185], "surround": [6, 8, 178], "load_checkpoint": [6, 8, 142, 143, 144, 145], "save_checkpoint": [6, 8, 9, 142, 143, 144], "convertor": 6, "avail": [6, 8, 45, 147, 150, 151, 158, 178, 180, 183, 185, 186], "here": [6, 7, 9, 14, 16, 17, 23, 24, 39, 115, 116, 180, 181, 182, 183, 184, 185, 186, 187, 188], "three": [6, 8, 122, 123, 124, 184], "hfcheckpoint": 6, "read": [6, 142, 143, 144, 178], "write": [6, 8, 14, 142, 143, 144, 161, 181, 182, 184], "compat": [6, 142, 144, 187], "transform": [6, 8, 29, 31, 33, 49, 53, 54, 55, 60, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 117, 118, 119, 121, 136, 137, 138, 139, 140, 168, 186, 187], "framework": [6, 8, 178], "mention": [6, 183, 188], "assum": [6, 14, 17, 18, 23, 24, 31, 33, 40, 111, 116, 117, 118, 121, 128, 133, 146, 148, 151, 160, 181, 183, 186], "checkpoint_dir": [6, 7, 142, 143, 144, 183, 185, 187], "necessari": [6, 42, 161, 162, 163, 164, 181, 186], "easiest": [6, 183, 184], "sure": [6, 7, 181, 183, 184, 185, 186, 187, 188], "everyth": [6, 8, 147, 178, 184], "follow": [6, 8, 20, 25, 26, 29, 32, 111, 121, 136, 137, 144, 145, 146, 158, 164, 171, 176, 177, 180, 182, 183, 184, 185, 186, 187, 188], "flow": [6, 29, 31, 32, 33, 187, 188], "By": [6, 180, 185, 186, 187, 188], "safetensor": [6, 142, 180], "output": [6, 18, 30, 36, 39, 42, 49, 53, 54, 55, 60, 64, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 105, 106, 111, 112, 113, 115, 116, 117, 118, 119, 126, 129, 130, 131, 136, 139, 144, 149, 152, 162, 171, 177, 180, 181, 182, 183, 184, 185, 186, 188], "dir": [6, 164, 177, 180, 183, 184, 185, 187], "output_dir": [6, 7, 142, 143, 144, 171, 183, 185, 186, 187, 188], "argument": [6, 7, 10, 18, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 111, 141, 147, 152, 157, 161, 163, 164, 168, 180, 181, 182, 185, 186, 187], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 117, 171, 180, 182, 183, 186, 188], "_component_": [6, 7, 9, 10, 30, 37, 40, 44, 171, 181, 182, 183, 185, 186, 187], "fullmodelhfcheckpoint": [6, 183], "directori": [6, 7, 142, 143, 144, 161, 163, 164, 171, 180, 181, 183, 184, 185], "sort": [6, 142, 144], "so": [6, 7, 32, 119, 138, 142, 147, 177, 178, 181, 183, 184, 185, 186, 187, 188], "order": [6, 8, 142, 144, 163, 164, 184], "matter": [6, 142, 144, 180, 186], "checkpoint_fil": [6, 7, 9, 142, 143, 144, 183, 185, 186, 187, 188], "restart": [6, 180], "previou": [6, 32, 142, 143, 144], "more": [6, 7, 8, 35, 37, 42, 69, 114, 116, 119, 130, 141, 144, 147, 164, 168, 170, 178, 180, 182, 183, 184, 185, 186, 187, 188], "next": [6, 32, 49, 119, 136, 149, 185, 188], "section": [6, 8, 154, 176, 183, 185, 188], "recipe_checkpoint": [6, 142, 143, 144, 187], "null": [6, 7, 187], "usual": [6, 116, 142, 164, 180, 183, 186], "model_typ": [6, 142, 143, 144, 183, 185, 187], "resume_from_checkpoint": [6, 142, 143, 144], "fals": [6, 7, 20, 25, 26, 29, 30, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 49, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 102, 103, 104, 105, 106, 110, 111, 117, 118, 126, 127, 130, 132, 138, 142, 143, 144, 158, 171, 180, 181, 182, 183, 185, 186, 187, 188], "requir": [6, 7, 30, 34, 42, 44, 69, 142, 144, 146, 155, 157, 158, 160, 163, 164, 166, 170, 171, 177, 180, 181, 182, 184, 187, 188], "param": [6, 8, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 126, 128, 129, 131, 142, 160, 186, 187, 188], "directli": [6, 7, 8, 10, 37, 40, 44, 122, 141, 142, 180, 183, 184, 185, 186, 187, 188], "ensur": [6, 7, 13, 28, 42, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 142, 144, 151, 178, 182, 184], "out": [6, 7, 8, 29, 31, 36, 37, 39, 41, 136, 142, 143, 176, 178, 180, 181, 183, 184, 185, 186, 188], "case": [6, 8, 9, 20, 47, 48, 49, 119, 142, 146, 151, 155, 160, 161, 168, 178, 180, 181, 182, 183, 185, 186, 188], "discrep": [6, 142], "along": [6, 185, 186], "found": [6, 7, 9, 115, 116, 180, 186, 188], "metacheckpoint": 6, "github": [6, 10, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 111, 115, 116, 121, 122, 123, 124, 130, 177, 182, 184], "repositori": [6, 19, 183, 184], "fullmodelmetacheckpoint": [6, 185, 187], "torchtunecheckpoint": 6, "current": [6, 32, 60, 64, 75, 87, 93, 95, 97, 105, 108, 111, 114, 116, 117, 118, 143, 144, 152, 155, 156, 161, 163, 167, 170, 183, 184, 185, 187], "test": [6, 7, 8, 178, 181], "complet": [6, 8, 14, 32, 38, 109, 123, 181, 182, 183, 184, 185], "written": [6, 7, 8, 142, 143, 161, 162, 163, 164, 178], "begin": [6, 32, 69, 92, 119, 133, 181, 185, 188], "partit": [6, 142, 188], "ha": [6, 69, 92, 119, 125, 127, 128, 131, 144, 146, 173, 181, 182, 183, 184, 185, 186, 188], "standard": [6, 17, 25, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 162, 178, 181, 183, 185], "key_1": [6, 144], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 183], "inform": [6, 164, 168, 178, 180, 183, 184, 185], "subsequ": [6, 8, 119, 136], "recipe_st": [6, 142, 143, 144], "pt": [6, 9, 142, 143, 144, 183, 185, 187], "epoch": [6, 8, 9, 121, 142, 143, 144, 180, 181, 183, 184, 185, 187], "optim": [6, 7, 8, 30, 60, 69, 97, 108, 121, 122, 124, 144, 146, 148, 154, 166, 167, 171, 181, 183, 184, 185, 186, 188], "etc": [6, 8, 142, 154, 184], "prevent": [6, 32, 122, 180], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 64, 117, 125, 146, 167, 170, 181, 182, 183, 186, 187, 188], "updat": [6, 7, 8, 114, 122, 146, 171, 177, 181, 183, 184, 185, 186, 187, 188], "hf_model_0001_0": [6, 183], "hf_model_0002_0": [6, 183], "both": [6, 30, 131, 180, 183, 186, 187, 188], "adapt": [6, 125, 126, 127, 128, 129, 142, 143, 144, 181, 183, 186, 188], "merg": [6, 10, 11, 142, 183, 185, 188], "would": [6, 7, 9, 32, 117, 119, 123, 177, 181, 182, 183, 186, 188], "primari": [6, 7, 8, 184], "want": [6, 7, 8, 9, 10, 29, 137, 138, 149, 177, 180, 181, 182, 183, 184, 185, 186], "resum": [6, 8, 121, 142, 143, 144, 188], "initi": [6, 8, 12, 30, 32, 50, 51, 52, 61, 62, 71, 72, 73, 84, 85, 98, 100, 122, 146, 157, 158, 184, 186, 188], "frozen": [6, 122, 186, 188], "base": [6, 10, 20, 31, 33, 42, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 102, 103, 105, 106, 107, 110, 116, 121, 122, 123, 124, 126, 127, 129, 130, 131, 142, 147, 150, 152, 160, 161, 176, 181, 183, 184, 185, 186, 188], "well": [6, 7, 8, 178, 180, 182, 183, 185, 188], "learnt": [6, 181, 183], "someth": [6, 8, 9, 181, 183, 187], "NOT": [6, 60, 97], "refer": [6, 7, 8, 115, 116, 119, 122, 123, 124, 127, 178, 186, 187], "adapter_checkpoint": [6, 142, 143, 144], "adapter_0": [6, 183], "now": [6, 146, 148, 181, 182, 183, 184, 185, 186, 187, 188], "knowledg": 6, "creat": [6, 7, 10, 32, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 71, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 88, 89, 90, 91, 94, 96, 98, 100, 102, 103, 106, 108, 110, 114, 119, 121, 141, 142, 143, 144, 148, 161, 163, 180, 181, 182, 183, 185, 188], "simpl": [6, 8, 14, 17, 23, 24, 119, 176, 182, 184, 186, 187, 188], "forward": [6, 8, 46, 47, 48, 111, 112, 113, 115, 116, 117, 118, 119, 122, 123, 124, 126, 154, 171, 185, 186, 188], "modeltyp": [6, 142, 143, 144], "llama2_13b": [6, 76], "right": [6, 142, 183, 185, 186], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 130, 146, 186], "successfulli": [6, 180, 184], "vocab": [6, 10, 117, 185], "70": [6, 84], "x": [6, 46, 47, 48, 111, 112, 113, 115, 116, 117, 118, 119, 126, 149, 169, 186, 187, 188], "randint": 6, "0": [6, 8, 32, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 69, 70, 75, 76, 77, 78, 79, 80, 81, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 111, 117, 119, 121, 122, 123, 124, 126, 135, 138, 149, 155, 163, 164, 165, 166, 170, 172, 175, 179, 181, 182, 183, 184, 185, 186, 187, 188], "no_grad": 6, "6": [6, 32, 60, 64, 115, 119, 140, 165, 166, 183, 187, 188], "3989": 6, "9": [6, 119, 166, 183, 187, 188], "0531": 6, "3": [6, 32, 49, 82, 106, 108, 109, 119, 138, 139, 140, 145, 147, 153, 155, 165, 166, 169, 180, 181, 183, 184, 185, 187, 188], "2375": 6, "5": [6, 7, 14, 119, 121, 122, 138, 165, 166, 183, 184, 185], "2822": 6, "4": [6, 7, 42, 49, 111, 119, 137, 155, 165, 166, 172, 178, 180, 182, 183, 185, 186, 187, 188], "4872": 6, "7469": 6, "8": [6, 36, 39, 41, 53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 88, 89, 90, 91, 94, 96, 102, 103, 106, 110, 119, 166, 183, 186, 187, 188], "6737": 6, "11": [6, 119, 166, 183, 185, 187, 188], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 119, 136, 165, 166], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 119], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 20, 29, 31, 33, 40, 42, 130, 135, 164, 180, 181, 182, 183, 184, 185, 186, 187], "find": [6, 8, 9, 122, 180, 183, 184, 186], "list": [6, 7, 15, 16, 19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 49, 53, 54, 55, 56, 57, 58, 59, 64, 65, 66, 67, 68, 69, 75, 76, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 102, 103, 104, 105, 106, 110, 119, 125, 126, 130, 131, 132, 133, 135, 136, 137, 138, 142, 143, 144, 147, 149, 153, 165, 166, 181, 182, 184, 185, 187], "builder": [6, 35, 38, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 71, 72, 73, 76, 77, 78, 79, 80, 81, 84, 85, 88, 89, 90, 91, 94, 96, 98, 100, 102, 103, 106, 108, 110, 181, 182, 188], "hope": 6, "deeper": [6, 184], "insight": [6, 183], "happi": [6, 183], "thi": [7, 8, 9, 10, 17, 20, 24, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 42, 44, 45, 47, 48, 49, 60, 64, 69, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 108, 109, 111, 112, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 130, 131, 132, 133, 135, 136, 141, 142, 143, 144, 146, 147, 149, 150, 151, 154, 158, 160, 161, 163, 164, 166, 167, 168, 170, 176, 177, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "pars": [7, 10, 11, 134, 147, 181, 184], "effect": [7, 187], "cli": [7, 9, 11, 12, 177, 183, 184], "prerequisit": [7, 181, 182, 183, 184, 185, 186, 187, 188], "Be": [7, 181, 183, 184, 185, 186, 187, 188], "familiar": [7, 181, 183, 184, 185, 186, 187, 188], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 177, 181, 182, 184], "instal": [7, 9, 158, 163, 164, 176, 180, 183, 184, 185, 186, 187, 188], "fundament": [7, 187], "There": [7, 28, 47, 138, 181, 183, 184, 185, 186], "entri": [7, 8, 184], "point": [7, 8, 25, 26, 135, 182, 183, 184, 185, 186, 187, 188], "locat": [7, 180, 185, 186, 187, 188], "thei": [7, 8, 30, 49, 117, 119, 131, 147, 152, 180, 181, 182, 186, 187], "truth": [7, 183, 185], "reproduc": 7, "overridden": [7, 112, 147, 171], "quick": [7, 30], "experiment": 7, "serv": [7, 135, 141, 182, 186], "particular": [7, 29, 30, 42, 141, 182, 186, 188], "seed": [7, 8, 9, 170, 184, 187], "shuffl": [7, 32, 187], "devic": [7, 8, 130, 146, 150, 151, 154, 180, 181, 183, 184, 185, 186], "cuda": [7, 150, 151, 154, 171, 177, 183, 188], "dtype": [7, 8, 114, 117, 120, 151, 169, 173, 183, 187, 188], "fp32": [7, 187, 188], "enable_fsdp": 7, "mani": [7, 32, 182, 183], "object": [7, 10, 11, 15, 16, 19, 21, 49, 111, 122, 141, 155, 181], "keyword": [7, 10, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 120, 181, 182], "loss": [7, 8, 20, 31, 36, 39, 41, 122, 123, 124, 184, 186, 188], "exampl": [7, 8, 9, 10, 12, 14, 17, 23, 24, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 44, 45, 49, 59, 69, 82, 92, 104, 111, 119, 122, 123, 124, 125, 127, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 145, 146, 149, 155, 163, 164, 165, 166, 169, 172, 174, 175, 177, 179, 180, 181, 182, 183, 185, 186, 187, 188], "subfield": 7, "dotpath": 7, "wish": [7, 182], "exact": [7, 10, 183], "path": [7, 8, 9, 10, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 63, 69, 74, 82, 86, 92, 101, 104, 109, 132, 133, 134, 142, 143, 144, 147, 171, 180, 181, 182, 183, 185, 186], "normal": [7, 29, 32, 69, 92, 113, 115, 117, 118, 132, 181, 182, 186, 187, 188], "python": [7, 147, 153, 164, 170, 174, 180, 183, 187], "alpaca_dataset": [7, 35, 182], "custom": [7, 8, 29, 31, 33, 37, 40, 44, 168, 178, 180, 183, 184, 185, 186], "train_on_input": [7, 25, 26, 29, 30, 31, 35, 36, 37, 39, 40, 41, 42, 181, 182], "onc": [7, 127, 183, 184, 185, 186, 188], "ve": [7, 114, 180, 181, 182, 183, 185, 186], "instanc": [7, 10, 30, 75, 87, 93, 95, 105, 112, 120, 128, 129, 186], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 37, 180, 183, 188], "under": [7, 171, 182, 183, 185, 188], "preced": [7, 10, 180, 185, 186], "actual": [7, 9, 14, 17, 23, 24, 29, 181, 187], "throw": 7, "notic": [7, 46, 47, 48, 119, 181, 182, 186], "miss": [7, 130, 131, 171, 186], "posit": [7, 10, 32, 46, 47, 48, 49, 60, 64, 93, 95, 97, 99, 105, 107, 111, 114, 116, 117, 118, 119, 185], "anoth": [7, 183], "handl": [7, 12, 30, 69, 92, 132, 133, 181, 183, 186, 188], "def": [7, 8, 9, 12, 141, 145, 181, 182, 186, 188], "dictconfig": [7, 8, 10, 11, 12, 13, 164, 171], "arg": [7, 10, 48, 113, 117, 120, 125, 147, 162, 171, 187], "tupl": [7, 10, 30, 42, 59, 69, 82, 92, 104, 114, 119, 120, 122, 123, 124, 135, 137, 138, 139, 141, 147, 156, 165, 166, 171, 173], "kwarg": [7, 10, 113, 120, 125, 147, 157, 161, 162, 163, 164, 168, 171, 182], "str": [7, 10, 11, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 63, 69, 74, 82, 86, 92, 101, 104, 109, 120, 125, 126, 128, 129, 130, 131, 132, 133, 134, 142, 143, 144, 146, 147, 150, 151, 153, 154, 155, 157, 159, 161, 162, 163, 164, 165, 166, 170, 171, 172, 173, 181, 182], "mean": [7, 111, 115, 117, 118, 160, 180, 181, 182, 184, 186, 187], "pass": [7, 10, 20, 29, 30, 31, 33, 34, 37, 38, 40, 44, 45, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 112, 120, 127, 131, 133, 141, 144, 151, 152, 154, 157, 160, 163, 164, 168, 171, 180, 181, 182, 186, 187, 188], "add": [7, 9, 29, 32, 34, 44, 69, 119, 135, 144, 145, 147, 182, 183, 185, 186, 188], "d": [7, 20, 111, 114, 117, 180, 181, 186, 187], "llama2_token": [7, 181, 183], "tmp": [7, 146, 181, 184, 185], "llama2token": [7, 74], "option": [7, 8, 14, 17, 18, 23, 24, 27, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 53, 54, 55, 59, 64, 65, 66, 69, 70, 75, 76, 77, 78, 82, 83, 86, 87, 88, 89, 92, 93, 94, 95, 96, 104, 105, 106, 109, 111, 116, 117, 118, 119, 120, 130, 131, 132, 135, 138, 139, 142, 143, 144, 149, 150, 151, 153, 155, 161, 164, 170, 171, 177, 178, 180, 182, 183], "modeltoken": [7, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 135, 181, 182], "bool": [7, 20, 25, 26, 29, 31, 32, 34, 35, 36, 37, 39, 40, 41, 42, 44, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 75, 76, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 102, 103, 104, 105, 106, 110, 120, 126, 130, 131, 132, 133, 135, 138, 141, 142, 143, 144, 152, 154, 157, 158, 160, 163, 168, 171, 172, 181, 188], "max_seq_len": [7, 10, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 60, 64, 69, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 111, 114, 116, 117, 135, 181, 182, 187], "int": [7, 9, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 57, 58, 59, 60, 64, 65, 66, 67, 68, 69, 70, 75, 76, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 102, 103, 104, 105, 106, 107, 110, 111, 114, 115, 116, 117, 119, 121, 126, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 149, 152, 156, 160, 161, 162, 163, 164, 165, 166, 168, 170, 171, 180, 181, 182, 186, 188], "512": [7, 35, 36, 49, 182, 188], "instructdataset": [7, 35, 36, 39, 40, 41, 182], "alreadi": [7, 145, 157, 160, 177, 180, 182, 183, 186], "overwrit": [7, 144, 177, 180], "duplic": [7, 8, 178, 180], "sometim": 7, "than": [7, 28, 42, 111, 114, 119, 122, 141, 144, 145, 172, 173, 181, 182, 183, 184, 185, 186, 188], "resolv": [7, 11, 184], "alpaca": [7, 14, 30, 35, 36, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 182], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 161, 162, 163, 164], "disklogg": 7, "log_dir": [7, 161, 163, 164], "conveni": [7, 8, 180], "verifi": [7, 150, 151, 152, 181, 184, 186], "properli": [7, 130, 158, 180], "experi": [7, 164, 176, 178, 181, 185, 186], "wa": [7, 47, 48, 49, 119, 130, 181, 183, 185, 186, 187, 188], "cp": [7, 177, 180, 181, 183, 184, 185, 187], "7b_lora_single_devic": [7, 183, 184, 186, 188], "my_config": [7, 180], "discuss": [7, 184, 186], "guidelin": 7, "while": [7, 8, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 112, 178, 183, 187, 188], "mai": [7, 9, 119, 152, 181, 182, 184, 186], "tempt": 7, "put": [7, 8, 184, 186, 187], "much": [7, 183, 185, 186, 187, 188], "give": [7, 182, 186], "maximum": [7, 27, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 60, 64, 70, 75, 82, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 116, 117, 137, 138, 139, 180], "flexibl": [7, 30, 182], "switch": 7, "encourag": [7, 69, 186], "clariti": 7, "significantli": [7, 122], "easier": [7, 183, 184], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 144, 181, 184], "parent": [7, 180], "modul": [7, 10, 46, 47, 48, 49, 95, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 145, 148, 152, 160, 167, 168, 170, 184, 186, 188], "__init__": [7, 8, 186, 188], "py": [7, 10, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 111, 114, 115, 116, 121, 122, 123, 124, 180, 183, 185], "guarante": 7, "stabil": [7, 178, 187, 188], "underscor": 7, "_alpaca": 7, "collect": [7, 149, 184], "itself": 7, "via": [7, 9, 37, 40, 44, 126, 142, 186, 188], "k1": [7, 8], "v1": [7, 8, 45], "k2": [7, 8], "v2": [7, 8, 182], "lora_finetune_single_devic": [7, 180, 181, 183, 184, 185, 186, 188], "checkpoint": [7, 8, 120, 133, 142, 143, 144, 145, 146, 164, 168, 178, 180, 185, 186, 187, 188], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 29, 30, 31, 32, 33, 34, 37, 40, 46, 47, 48, 59, 69, 82, 92, 99, 104, 111, 112, 113, 114, 115, 116, 117, 118, 119, 122, 123, 124, 125, 126, 128, 129, 132, 133, 136, 142, 143, 144, 145, 146, 147, 161, 162, 163, 164, 181, 182, 184, 186, 188], "assign": [7, 34], "nest": 7, "dot": 7, "notat": [7, 111, 116, 117], "certain": [7, 171, 181], "flag": [7, 8, 20, 31, 36, 39, 41, 141, 144, 152, 180, 188], "built": [7, 9, 43, 177, 181, 184, 188], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 29], "pytorch": [7, 8, 69, 117, 120, 130, 141, 158, 163, 168, 170, 171, 176, 177, 178, 185, 186, 187, 188], "llama3": [7, 29, 42, 82, 84, 85, 86, 87, 88, 89, 90, 91, 105, 145, 149, 152, 176, 180, 182], "8b_full": [7, 180, 182], "adamw": [7, 186], "lr": [7, 121], "2e": 7, "fuse": [7, 167, 187], "nproc_per_nod": [7, 182, 185, 186, 187], "full_finetune_distribut": [7, 180, 182, 183, 184], "core": [8, 178, 182, 184, 188], "i": [8, 19, 20, 21, 111, 117, 118, 119, 120, 129, 146, 149, 182, 183, 185, 187, 188], "structur": [8, 15, 16, 19, 21, 25, 26, 29, 37, 86, 109, 136, 181, 182, 183, 187], "new": [8, 38, 98, 114, 145, 161, 163, 181, 183, 184, 185, 186, 188], "user": [8, 15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 69, 70, 75, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 111, 135, 139, 181, 182, 184, 187], "thought": [8, 178, 184, 188], "target": [8, 178], "pipelin": [8, 178], "llm": [8, 176, 178, 182, 183, 186], "eg": [8, 117, 142, 178], "meaning": [8, 178, 183], "featur": [8, 9, 177, 178, 183, 184], "fsdp": [8, 141, 146, 152, 160, 178, 184, 185], "activ": [8, 112, 154, 159, 168, 171, 178, 187, 188], "gradient": [8, 160, 167, 171, 178, 183, 185, 186, 188], "accumul": [8, 167, 171, 178], "mix": [8, 113, 180, 182, 183], "precis": [8, 113, 120, 151, 178, 184, 188], "appli": [8, 29, 31, 33, 53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 69, 70, 75, 76, 77, 78, 79, 80, 81, 83, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 102, 103, 105, 106, 110, 111, 115, 116, 117, 118, 130, 131, 168, 178, 188], "given": [8, 10, 14, 17, 18, 23, 24, 28, 123, 126, 127, 149, 150, 151, 155, 160, 167, 172, 178, 186], "complex": 8, "becom": [8, 119, 123, 177, 182], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 117, 119, 145, 180, 182], "methodolog": 8, "reason": [8, 149, 183, 187], "possibl": [8, 29, 32, 37, 137, 138, 180, 182], "trade": 8, "off": [8, 69, 92, 183, 187], "memori": [8, 30, 31, 32, 33, 34, 36, 38, 40, 44, 45, 120, 130, 152, 154, 159, 160, 171, 176, 178, 183, 184, 185, 187], "vs": [8, 123, 184], "qualiti": [8, 183, 186, 187], "believ": 8, "best": [8, 138, 181], "suit": [8, 184], "b": [8, 111, 114, 116, 117, 118, 126, 160, 164, 186, 188], "fit": [8, 29, 31, 32, 33, 34, 36, 38, 40, 44, 45, 119, 122, 123, 138, 139, 182], "solut": [8, 123], "result": [8, 49, 59, 69, 92, 104, 119, 135, 136, 171, 183, 185, 186, 187, 188], "meant": [8, 120, 146], "depend": [8, 9, 14, 142, 171, 180, 182, 183, 186, 188], "level": [8, 148, 153, 160, 178, 188], "expertis": 8, "routin": 8, "yourself": [8, 180, 185, 186], "exist": [8, 177, 180, 183, 184, 185, 188], "ad": [8, 46, 47, 48, 92, 99, 119, 132, 144, 145, 181, 186, 187, 188], "ones": 8, "modular": [8, 178], "build": [8, 37, 40, 44, 49, 60, 70, 83, 97, 99, 178, 185, 186], "block": [8, 32, 53, 54, 55, 60, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 105, 106, 130, 131, 178], "wandb": [8, 9, 164, 184], "log": [8, 11, 122, 123, 124, 153, 154, 159, 161, 162, 163, 164, 183, 184, 185, 188], "fulli": [8, 30], "nativ": [8, 176, 178, 186, 187, 188], "correct": [8, 17, 39, 115, 116, 117, 150, 178, 181, 182], "numer": [8, 178, 187], "pariti": [8, 178], "verif": 8, "extens": [8, 144, 178], "comparison": [8, 186, 188], "benchmark": [8, 170, 178, 183, 185, 186, 187], "limit": [8, 138, 139, 146, 182, 187], "hidden": [8, 49, 112, 119], "behind": 8, "100": [8, 31, 36, 39, 41, 42, 149, 165, 166, 186, 188], "prefer": [8, 23, 43, 122, 123, 124, 166, 178, 180, 182], "over": [8, 121, 122, 123, 147, 178, 180, 183, 185, 186, 188], "unnecessari": 8, "abstract": [8, 15, 18, 178, 184, 188], "No": [8, 144, 178], "inherit": [8, 147, 178, 182], "go": [8, 19, 21, 49, 59, 69, 92, 104, 119, 135, 178, 182, 183, 184, 188], "upon": [8, 30, 185], "figur": [8, 186, 188], "spectrum": 8, "decid": 8, "interact": [8, 176, 184], "start": [8, 9, 30, 135, 145, 177, 178, 181, 182, 183, 184, 187], "paradigm": 8, "consist": [8, 45, 184], "configur": [8, 31, 33, 36, 37, 38, 39, 40, 41, 42, 44, 45, 64, 75, 82, 87, 93, 104, 105, 118, 178, 181, 184, 185, 186, 187, 188], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 148, 149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 176, 178, 180, 181, 182, 183, 184, 185, 186, 187, 188], "command": [8, 9, 147, 177, 180, 181, 182, 183, 184, 185, 186, 187, 188], "overrid": [8, 11, 12, 180, 183, 184, 185, 188], "togeth": [8, 32, 164, 184, 186, 187], "valid": [8, 28, 130, 131, 173, 177, 183, 184], "environ": [8, 150, 158, 177, 180, 183, 184, 187], "logic": [8, 145, 178, 184, 186], "api": [8, 9, 25, 56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 130, 180, 181, 183, 184, 185, 188], "closer": [8, 186], "monolith": [8, 178], "trainer": [8, 122, 123, 124], "A": [8, 9, 25, 26, 30, 32, 49, 59, 69, 92, 104, 111, 117, 118, 119, 120, 122, 123, 124, 126, 130, 132, 133, 135, 136, 138, 141, 146, 147, 154, 155, 159, 160, 165, 166, 175, 176, 179, 180, 181, 183, 186, 187, 188], "wrapper": [8, 113, 132, 133, 146, 148, 180, 186], "around": [8, 29, 69, 92, 113, 132, 133, 154, 180, 181, 183, 186, 187, 188], "extern": [8, 182], "primarili": [8, 30, 186], "eleutherai": [8, 178, 186, 187], "har": [8, 178, 186, 187], "control": [8, 31, 36, 39, 41, 123, 127, 170, 183], "multi": [8, 29, 111, 130, 185], "stage": [8, 119], "distil": 8, "oper": [8, 30, 119, 127, 170, 187], "turn": [8, 20, 28, 29, 181], "dataload": [8, 32, 36, 39, 41], "applic": [8, 111, 142, 143, 164], "clean": [8, 9, 35], "after": [8, 104, 111, 114, 115, 117, 118, 130, 160, 161, 162, 163, 164, 181, 187, 188], "process": [8, 9, 49, 119, 120, 156, 157, 170, 182, 184, 187, 188], "group": [8, 111, 156, 157, 161, 162, 163, 164, 180, 185, 187], "init_process_group": [8, 157], "backend": [8, 180, 187], "gloo": 8, "els": [8, 147, 164, 178, 188], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 30, 123, 144, 147, 152, 171, 182, 184, 185, 186, 187], "stuff": 8, "carri": 8, "relev": [8, 180, 183, 186], "interfac": [8, 15, 18, 30, 125, 182], "metric": [8, 184, 187], "logger": [8, 153, 159, 161, 162, 163, 164, 184], "self": [8, 9, 32, 53, 54, 55, 60, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 111, 117, 118, 125, 130, 131, 142, 145, 146, 182, 186, 188], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 141, 152, 160, 168, 181], "_model": [8, 146], "_setup_model": 8, "_token": [8, 182], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 146, 148, 167, 171, 188], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 122, 170, 180, 185, 187], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 32], "batch": [8, 32, 36, 39, 41, 47, 111, 114, 116, 117, 119, 122, 123, 124, 165, 166, 171, 178, 182, 184, 185, 186], "enumer": 8, "_autocast": 8, "logit": [8, 149], "label": [8, 29, 31, 32, 33, 34, 36, 37, 38, 40, 42, 43, 44, 45, 122, 165, 166], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 161, 162, 163, 164], "step": [8, 32, 117, 121, 148, 161, 162, 163, 164, 167, 171, 176, 183, 186, 187, 188], "learn": [8, 30, 121, 123, 178, 181, 182, 184, 185, 186, 187, 188], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 23, 24, 27, 28, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 59, 69, 70, 75, 82, 83, 86, 87, 92, 104, 109, 111, 114, 116, 117, 118, 119, 127, 129, 130, 131, 132, 135, 139, 142, 143, 144, 145, 149, 150, 151, 153, 155, 159, 161, 162, 163, 164, 167, 168, 169, 170, 171, 173, 181, 182, 183, 187], "fullfinetunerecip": 8, "wandblogg": [9, 186, 188], "workspac": 9, "seen": [9, 186, 188], "screenshot": 9, "below": [9, 14, 116, 141, 182, 185, 186, 188], "packag": [9, 163, 164, 177], "pip": [9, 163, 164, 177, 183, 185], "Then": [9, 127, 184], "login": [9, 164, 180, 183], "project": [9, 49, 53, 54, 55, 60, 64, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 105, 106, 111, 112, 119, 130, 131, 152, 164, 176, 181, 186, 188], "grab": [9, 185], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 47, 48, 49, 119, 158, 181, 187], "exit": [9, 177, 180], "resourc": [9, 161, 162, 163, 164, 187], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 29, 31, 32, 33, 34, 40, 42, 44, 111, 116, 117, 118, 119, 124, 136, 149, 181, 183], "desir": [9, 29, 139, 169, 181], "suggest": 9, "approach": [9, 30, 182], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 183], "_output_dir": [9, 142, 143, 144], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 171], "type": [9, 10, 12, 20, 25, 26, 27, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 123, 124, 126, 128, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 165, 166, 168, 169, 170, 171, 172, 182, 183, 186, 187, 188], "descript": [9, 37, 42, 180], "whatev": 9, "metadata": [9, 187], "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": [9, 187], "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 25, 26, 29, 32, 36, 39, 41, 159, 182], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 61, 62, 65, 66, 67, 68, 69, 71, 72, 73, 76, 77, 78, 79, 80, 81, 82, 88, 89, 90, 91, 98, 100, 102, 103, 106, 108, 109, 110, 111, 115, 116, 119, 121, 122, 123, 124, 130, 136, 141, 142, 143, 147, 153, 158, 163, 164, 168, 170, 177, 182, 183], "com": [10, 53, 54, 55, 65, 66, 69, 76, 77, 78, 82, 88, 89, 106, 111, 115, 116, 121, 122, 123, 124, 130, 177], "facebookresearch": [10, 115], "blob": [10, 53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106, 109, 111, 115, 116, 121, 122, 123, 124], "main": [10, 12, 69, 109, 111, 115, 116, 177, 183, 185], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 117, 119], "32": [10, 119, 185, 186, 187, 188], "num_head": [10, 49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 116, 117], "num_kv_head": [10, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114], "vocab_s": [10, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "must": [10, 30, 125, 147, 188], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 111, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 165, 166, 169, 170, 171, 172, 181, 182, 186, 188], "nn": [10, 111, 112, 114, 117, 118, 119, 120, 125, 127, 128, 129, 141, 148, 160, 167, 168, 173, 186, 188], "parsed_yaml": 10, "embed_dim": [10, 46, 47, 48, 49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 116, 118, 119, 186], "valueerror": [10, 21, 25, 28, 37, 42, 104, 111, 114, 117, 119, 142, 143, 144, 151, 154, 170, 173], "recipe_nam": 11, "rank": [11, 53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 93, 94, 95, 96, 105, 106, 126, 156, 158, 170, 184, 186, 188], "zero": [11, 114, 115, 183, 185, 187], "displai": 11, "callabl": [12, 29, 31, 33, 117, 127, 141, 149, 152, 155, 160, 168], "With": [12, 183, 186, 187, 188], "my_recip": 12, "foo": 12, "bar": [12, 178, 184], "instanti": [13, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 105, 106, 107, 108, 109, 146], "configerror": 13, "cannot": [13, 144, 185], "data": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 34, 36, 37, 39, 40, 41, 42, 43, 44, 45, 69, 119, 122, 124, 154, 161, 162, 163, 164, 182, 183, 187, 188], "templat": [14, 17, 18, 23, 24, 29, 30, 31, 33, 36, 37, 39, 40, 41, 42, 69], "style": [14, 32, 35, 36, 37, 42, 188], "slightli": 14, "describ": [14, 69, 82, 168, 182], "task": [14, 24, 30, 38, 181, 182, 183, 185, 186, 187, 188], "further": [14, 119, 180, 182, 186, 187, 188], "context": [14, 16, 108, 127, 169, 171, 182], "respons": [14, 16, 59, 69, 92, 104, 122, 123, 124, 135, 182, 183, 184, 185], "appropri": [14, 16, 19, 20, 21, 30, 121, 142, 182, 188], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 182], "map": [14, 17, 18, 23, 24, 25, 26, 29, 30, 31, 32, 33, 40, 82, 104, 129, 133, 134, 142, 146, 148, 161, 162, 163, 164, 167, 171, 181, 182, 183, 186], "column_map": [14, 17, 18, 23, 24, 29, 30, 31, 33, 40, 182], "placehold": [14, 17, 18, 23, 24, 29, 31, 33, 40, 182], "column": [14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 44, 111, 117, 118, 181, 182, 187], "ident": [14, 17, 18, 21, 23, 24, 31, 32, 33, 40, 123, 183, 187], "poem": 14, "n": [14, 23, 59, 69, 92, 104, 111, 119, 135, 138, 175, 179, 180, 181, 182, 187], "nwrite": 14, "long": [14, 32, 133, 181, 186], "where": [14, 17, 23, 24, 29, 30, 36, 39, 41, 47, 69, 92, 111, 117, 119, 122, 123, 126, 132, 136, 138, 152, 160, 166, 182], "me": 14, "tag": [15, 16, 19, 21, 29, 161, 162, 163, 164, 181], "system": [15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 69, 92, 104, 135, 181, 182], "assist": [15, 16, 19, 20, 22, 25, 26, 28, 29, 59, 69, 92, 104, 109, 135, 149, 181, 182], "role": [15, 20, 25, 26, 29, 59, 69, 92, 104, 135, 181, 182], "prepend": [15, 69, 82, 92, 132], "append": [15, 82, 92, 104, 132, 177], "accord": [15, 21, 181], "openai": [16, 25, 37, 182], "markup": 16, "languag": [16, 122, 126, 149, 186], "It": [16, 20, 21, 119, 122, 180, 181, 182, 188], "im_start": 16, "im_end": 16, "goe": [16, 127], "grammar": [17, 39, 182], "english": 17, "sentenc": [17, 32, 92], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 186], "lazi": 17, "dog": [17, 136], "alwai": [18, 123, 147], "human": [19, 26, 122, 124, 181], "pre": [19, 32, 44, 69, 119, 177, 181, 182], "taken": [19, 186, 188], "inst": [19, 21, 29, 69, 181, 182], "sy": [19, 69, 181, 182], "respect": [19, 30, 129, 138, 171, 181, 182], "honest": [19, 181, 182], "am": [19, 21, 181, 182, 183, 185], "pari": [19, 21, 24, 182], "capit": [19, 21, 23, 24, 182], "franc": [19, 21, 23, 24, 182], "known": [19, 21, 69, 92, 155, 182, 187], "its": [19, 21, 32, 95, 111, 116, 117, 118, 123, 167, 170, 180, 181, 182, 183, 185, 186], "stun": [19, 21, 182], "liter": [20, 22, 53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 130, 131], "ipython": [20, 22], "union": [20, 131, 161, 162, 163, 164, 168, 170], "mask": [20, 31, 32, 36, 39, 41, 59, 69, 82, 92, 104, 111, 117, 118, 135, 136, 181, 182], "eot": [20, 82], "repres": [20, 46, 47, 119, 138, 166, 181, 187], "individu": [20, 32, 154, 164, 168, 181, 182], "interleav": [20, 136], "tokenize_messag": [20, 29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 69, 82, 92, 104, 135, 181, 182], "attach": 20, "special": [20, 29, 69, 82, 86, 92, 104, 109, 119, 133, 134, 135, 136, 146, 182], "variabl": [20, 29, 30, 31, 33, 40, 158, 188], "writer": 20, "dictionari": [20, 32, 154, 159, 161, 162, 163, 164, 166, 183], "hello": [20, 24, 59, 69, 82, 92, 104, 132, 133, 181, 183, 185], "world": [20, 59, 69, 82, 92, 104, 132, 133, 156, 158, 183], "whether": [20, 25, 26, 29, 31, 34, 36, 37, 39, 40, 41, 42, 44, 53, 54, 55, 60, 64, 65, 66, 75, 76, 77, 78, 82, 87, 88, 89, 92, 93, 94, 95, 96, 104, 105, 106, 120, 126, 130, 131, 132, 133, 141, 151, 154, 181, 182], "calcul": [20, 111, 117, 119, 138, 185], "correspond": [20, 125, 128, 151, 166, 184, 185, 187], "consecut": [20, 28, 136], "e": [20, 46, 47, 48, 49, 111, 119, 120, 125, 129, 136, 138, 142, 146, 154, 171, 177, 183, 185, 186, 187, 188], "properti": [20, 147, 186], "contains_media": 20, "non": [20, 131], "from_dict": [20, 181], "construct": [20, 136, 186], "text_cont": [20, 181], "mistral": [21, 29, 42, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 145, 180, 181, 183, 184], "llama2chatformat": [21, 69, 181, 182], "alia": [22, 141], "similar": [23, 38, 43, 44, 45, 122, 130, 182, 183, 185, 186, 188], "stackexchangedpair": 23, "question": [23, 181, 182, 183, 185], "answer": [23, 181, 183, 185], "nanswer": 23, "summar": [24, 41, 181, 182], "dialogu": [24, 41, 181], "summari": [24, 30, 41, 119, 154, 182], "dialog": 24, "did": [24, 183, 185, 188], "know": [24, 181, 182, 183, 185, 186], "adher": [25, 26], "could": [25, 186], "remain": [25, 26, 121, 186], "unmask": [25, 26], "sharegpt": [26, 37], "gpt": [26, 111, 183], "eos_id": [27, 133, 135], "length": [27, 28, 30, 31, 32, 33, 34, 36, 38, 40, 42, 44, 45, 59, 60, 64, 69, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 108, 111, 114, 116, 117, 133, 135, 136, 143, 165, 166], "last": [27, 32, 121, 182], "replac": [27, 31, 36, 39, 41, 120, 186], "forth": [28, 182], "come": [28, 125, 186], "empti": [28, 180], "shorter": 28, "min": [28, 138, 186], "invalid": 28, "convert_to_messag": [29, 181], "chat_format": [29, 37, 42, 181, 182], "chatformat": [29, 37, 182], "load_dataset_kwarg": [29, 31, 33, 34, 37, 38, 40, 44, 45], "multiturn": [29, 181], "prepar": [29, 181, 187], "truncat": [29, 31, 32, 33, 34, 38, 40, 42, 44, 45, 59, 69, 82, 92, 104, 133, 135, 182], "anyth": [29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "load_dataset": [29, 31, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 181, 182], "huggingfac": [29, 31, 33, 34, 37, 38, 40, 44, 45, 100, 108, 109, 121, 122, 123, 124, 142, 143, 180, 183], "co": [29, 31, 33, 34, 37, 38, 40, 44, 45, 100, 108, 109, 142, 143, 183], "doc": [29, 31, 33, 34, 37, 38, 40, 44, 45, 69, 82, 141, 147, 153, 158, 163, 164, 170, 180, 183], "en": [29, 31, 33, 34, 37, 38, 40, 44, 45, 187], "package_refer": [29, 31, 33, 34, 37, 38, 40, 44, 45], "loading_method": [29, 31, 33, 34, 37, 38, 40, 44, 45], "extra": [29, 177, 181, 186, 187, 188], "still": [29, 147, 186, 187, 188], "unless": 29, "check": [29, 37, 46, 47, 48, 49, 117, 119, 130, 151, 158, 172, 176, 181, 183, 184, 186], "concaten": [30, 59, 69, 92, 104, 135, 166], "sub": [30, 163], "unifi": [30, 100], "were": [30, 119, 127, 181, 184, 187], "simplifi": [30, 122, 180, 186], "simultan": 30, "intern": [30, 147], "aggreg": 30, "transpar": 30, "index": [30, 32, 111, 116, 117, 118, 121, 165, 166, 177, 181, 183], "howev": [30, 109, 177], "constitu": 30, "might": [30, 180, 183], "larg": [30, 126, 171, 180, 188], "comput": [30, 70, 75, 83, 87, 111, 112, 116, 117, 122, 123, 124, 136, 137, 154, 170, 183, 187, 188], "cumul": 30, "maintain": [30, 188], "indic": [30, 32, 49, 111, 116, 117, 118, 119, 136, 141, 158, 181], "deleg": 30, "retriev": [30, 152], "lead": [30, 92, 132], "high": [30, 178, 186], "scale": [30, 53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 93, 94, 95, 96, 105, 106, 123, 126, 138, 149, 186, 187, 188], "consid": [30, 47, 48, 49, 119], "strategi": 30, "stream": [30, 153], "demand": 30, "deriv": [30, 112, 117, 118], "_dataset": 30, "_len": 30, "total": [30, 121, 156, 175, 179, 183, 185, 186], "combin": [30, 137], "_index": 30, "lookup": 30, "dataset1": 30, "mycustomdataset": 30, "params1": 30, "dataset2": 30, "params2": 30, "concat_dataset": 30, "data_point": 30, "1500": 30, "element": [30, 183], "accomplish": [30, 37, 40, 44], "instruct_dataset": [30, 182], "vicgal": [30, 182], "gpt4": [30, 182], "alpacainstructtempl": [30, 40, 182], "samsum": [30, 41, 182], "summarizetempl": [30, 181, 182], "focus": [30, 184], "enhanc": [30, 119, 188], "divers": 30, "machin": [30, 124, 150, 180, 183], "instructtempl": [31, 33, 182], "contribut": [31, 36, 39, 41], "disabl": [31, 33, 34, 38, 40, 44, 45, 127, 170, 187], "recommend": [31, 33, 34, 36, 38, 40, 44, 45, 163, 181, 183, 188], "highest": [31, 33, 34, 36, 38, 40, 44, 45], "sequenc": [31, 32, 33, 34, 36, 38, 40, 42, 44, 45, 59, 60, 64, 69, 70, 75, 82, 83, 87, 92, 93, 95, 97, 99, 104, 105, 107, 111, 114, 116, 117, 119, 133, 135, 136, 165, 166, 181], "ds": [32, 42], "padding_idx": [32, 165, 166], "max_pack": 32, "split_across_pack": 32, "greedi": 32, "pack": [32, 35, 36, 37, 39, 40, 41, 42, 44, 111, 116, 117, 118, 187], "done": [32, 130, 151, 160, 186, 187, 188], "outsid": [32, 170, 171, 183, 185, 186], "sampler": [32, 184], "part": [32, 124, 181, 188], "buffer": 32, "enough": [32, 181], "attent": [32, 49, 53, 54, 55, 60, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 108, 111, 114, 116, 117, 118, 130, 131, 136, 185, 186, 188], "lower": [32, 186], "triangular": 32, "cross": [32, 136], "attend": [32, 111, 117, 118, 136], "rel": [32, 111, 116, 117, 118, 122, 154, 186], "pad": [32, 119, 138, 139, 149, 165, 166, 182], "max": [32, 42, 59, 69, 92, 104, 117, 119, 121, 133, 135, 180, 186], "wise": 32, "collat": [32, 165, 182], "made": [32, 37, 40, 44, 116, 183], "smaller": [32, 123, 183, 185, 186, 187, 188], "jam": 32, "vari": 32, "s1": [32, 69, 92, 132], "s2": [32, 69, 92, 132], "s3": 32, "s4": 32, "contamin": 32, "input_po": [32, 111, 114, 116, 117, 118], "matrix": 32, "causal": [32, 111, 117, 118], "continu": [32, 119, 182], "increment": 32, "move": [32, 117], "entir": [32, 160, 181, 188], "avoid": [32, 115, 119, 120, 123, 170, 180, 187, 188], "add_eo": [34, 44, 59, 69, 82, 92, 104, 132, 133, 181], "freeform": [34, 44], "unstructur": [34, 44, 45], "corpu": [34, 38, 44, 45], "local": [34, 44, 86, 109, 164, 170, 177, 180, 181, 183, 184], "tabular": [34, 44], "eo": [34, 44, 92, 104, 109, 132, 135, 181, 182], "yahma": [35, 40], "variant": [35, 39, 41], "version": [35, 64, 75, 87, 93, 95, 105, 111, 149, 172, 177, 181, 185, 187, 188], "page": [35, 45, 177, 178, 180, 184, 185], "tatsu": 36, "lab": 36, "codebas": [36, 39, 41, 183], "prior": [36, 37, 39, 40, 41, 42, 44], "alpaca_d": 36, "batch_siz": [36, 39, 41, 111, 114, 117, 118, 122, 123, 124, 183, 187], "conversation_styl": [37, 182], "chatdataset": [37, 42, 181, 182], "friendli": [37, 40, 44, 149, 181], "huggingfaceh4": 37, "no_robot": 37, "chatmlformat": 37, "2096": [37, 40, 44], "packeddataset": [37, 40, 44, 182], "ccdv": 38, "cnn_dailymail": 38, "textcompletiondataset": [38, 44, 45, 182], "cnn": 38, "dailymail": 38, "articl": [38, 45], "extract": [38, 134], "highlight": [38, 188], "liweili": 39, "c4_200m": 39, "mirror": [39, 41], "llama_recip": [39, 41], "grammar_d": 39, "alpaca_clean": 40, "samsum_d": 41, "open": [42, 61, 62, 182, 183], "orca": 42, "slimorca": 42, "dedup": 42, "1024": [42, 43, 182, 187], "prescrib": 42, "least": [42, 185, 186, 187], "though": [42, 181], "10": [42, 119, 165, 166, 183, 185, 187, 188], "351": 42, "82": [42, 183], "391": 42, "221": 42, "220": 42, "193": 42, "12": [42, 119, 166, 177, 187], "471": 42, "lvwerra": [43, 182], "stack": [43, 119, 171, 182], "exchang": [43, 182], "preferencedataset": [43, 182], "stackexchangepair": 43, "omit": [44, 186], "allenai": [44, 182, 187], "c4": [44, 182, 187], "data_dir": [44, 182], "realnewslik": [44, 182], "wikitext": [45, 187], "subset": [45, 64, 75, 87, 93, 95, 105, 128], "103": [45, 183], "raw": 45, "wikipedia": 45, "clip": [46, 47, 48, 49, 119], "max_num_til": [46, 47, 49, 119, 137], "tile": [46, 47, 48, 49, 119, 136, 137, 140], "patch": [46, 47, 48, 49, 119, 136], "document": [46, 47, 48, 49, 111, 123, 141, 152, 160, 180, 182], "vision_transform": [46, 47, 48, 49], "visiontransform": [46, 47, 48, 49], "divid": [46, 47, 48, 49, 119, 136, 137, 140], "dimension": [46, 47, 48, 49, 119], "aspect_ratio": [46, 47, 119], "bsz": [46, 47, 119, 149], "n_img": [46, 47, 119], "n_tile": [46, 47, 119], "n_token": [46, 47, 48, 119], "aspect": [46, 47, 178], "ratio": [46, 47, 122, 123], "crop": [46, 47, 48, 49, 119, 140], "g": [46, 47, 48, 49, 111, 119, 125, 136, 138, 142, 154, 171, 185, 186, 187, 188], "tile_s": [47, 48, 49, 119, 136, 137, 140], "patch_siz": [47, 48, 49, 119, 136], "local_token_positional_embed": 47, "equival": [47, 123, 124], "_position_embed": [47, 119], "tokenpositionalembed": [47, 119], "gate": [47, 145, 180, 184], "global_token_positional_embed": 47, "advanc": [47, 48, 49, 119, 182], "40": [47, 48, 49, 119, 136, 188], "400": [47, 48, 49, 119, 136, 140], "10x10": [47, 48, 49, 119, 136], "grid": [47, 48, 49, 119, 136], "k": [47, 111, 186], "th": 47, "cls_output_dim": [49, 119], "out_indic": [49, 119], "output_cls_project": 49, "in_channel": [49, 119], "transformerencoderlay": 49, "cl": [49, 119, 182], "head": [49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 116, 117, 145, 185], "intermedi": [49, 60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 119, 144, 168, 185, 188], "fourth": [49, 119], "determin": [49, 131, 138], "channel": [49, 119, 187], "code_llama2": [50, 51, 52, 53, 54, 55, 56, 57, 58, 180], "transformerdecod": [50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 73, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 105, 106, 107, 108, 110, 149, 186], "w": [50, 51, 52, 61, 62, 71, 72, 73, 84, 85, 98, 100, 119, 139, 163, 164, 181, 183, 186, 188], "arxiv": [50, 51, 52, 56, 57, 58, 67, 68, 71, 72, 73, 79, 80, 81, 90, 91, 102, 103, 110, 111, 115, 116, 119, 122, 123, 124, 136], "org": [50, 51, 52, 56, 57, 58, 67, 68, 69, 71, 72, 73, 79, 80, 81, 90, 91, 102, 103, 110, 111, 115, 116, 119, 122, 123, 124, 136, 141, 147, 153, 158, 163, 168, 170, 177], "pdf": [50, 51, 52, 136], "2308": [50, 51, 52], "12950": [50, 51, 52], "lora_attn_modul": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 130, 131, 186, 188], "q_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 111, 130, 131, 186, 187, 188], "k_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 111, 130, 131, 186, 187, 188], "v_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 111, 130, 131, 186, 187, 188], "output_proj": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 111, 130, 131, 186, 187, 188], "apply_lora_to_mlp": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 130, 131, 186], "apply_lora_to_output": [53, 54, 55, 56, 57, 58, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 130, 131, 186], "lora_rank": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 186], "lora_alpha": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 186], "float": [53, 54, 55, 56, 57, 58, 60, 64, 65, 66, 67, 68, 70, 75, 76, 77, 78, 79, 80, 81, 83, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 102, 103, 105, 106, 107, 110, 111, 115, 121, 122, 123, 124, 126, 149, 154, 159, 161, 162, 163, 164, 186, 187, 188], "16": [53, 54, 55, 56, 57, 58, 65, 66, 67, 68, 76, 77, 78, 79, 80, 81, 88, 89, 90, 91, 94, 96, 102, 103, 106, 110, 119, 166, 186, 188], "lora_dropout": [53, 54, 55, 56, 57, 58, 64, 75, 76, 77, 78, 79, 80, 81, 87, 93, 95, 105], "05": [53, 54, 55, 56, 57, 58, 70, 75, 76, 77, 78, 79, 80, 81, 83, 87, 93, 95, 97, 99, 105, 107], "quantize_bas": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 126, 188], "lora": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 126, 127, 130, 131, 142, 160, 176, 178, 181, 184, 185], "code_llama2_13b": 53, "tloen": [53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106], "8bb8579e403dc78e37fe81ffbb253c413007323f": [53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106], "l41": [53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106], "l43": [53, 54, 55, 65, 66, 76, 77, 78, 88, 89, 106], "linear": [53, 54, 55, 56, 57, 58, 64, 65, 66, 67, 68, 75, 76, 77, 78, 79, 80, 81, 87, 88, 89, 90, 91, 93, 94, 95, 96, 102, 103, 105, 106, 110, 117, 125, 126, 130, 131, 186, 187, 188], "mlp": [53, 54, 55, 60, 64, 65, 66, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 117, 118, 130, 131, 185, 186], "final": [53, 54, 55, 60, 64, 70, 75, 76, 77, 78, 83, 87, 88, 89, 93, 94, 95, 96, 97, 105, 106, 112, 117, 127, 130, 131, 183, 185, 186, 188], "low": [53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 93, 94, 95, 96, 105, 106, 126, 183, 186, 188], "approxim": [53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 93, 94, 95, 96, 105, 106, 126, 186], "factor": [53, 54, 55, 64, 65, 66, 75, 76, 77, 78, 87, 88, 89, 93, 94, 95, 96, 105, 106, 126, 138, 183], "dropout": [53, 54, 55, 60, 64, 70, 75, 76, 77, 78, 83, 87, 93, 95, 97, 99, 105, 107, 111, 126, 186, 188], "probabl": [53, 54, 55, 64, 75, 76, 77, 78, 87, 93, 95, 105, 122, 123, 124, 126, 149, 183], "code_llama2_70b": 54, "code_llama2_7b": 55, "qlora": [56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 120, 176, 178, 185, 186], "per": [56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 114, 119, 120, 122, 136, 137, 180, 185, 187, 188], "paper": [56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 122, 123, 124, 136, 186, 188], "ab": [56, 57, 58, 67, 68, 71, 72, 73, 79, 80, 81, 90, 91, 102, 103, 110, 111, 115, 116, 119, 122, 123, 124], "2305": [56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110, 111, 122, 124], "14314": [56, 57, 58, 67, 68, 79, 80, 81, 90, 91, 102, 103, 110], "lora_code_llama2_13b": 56, "lora_code_llama2_70b": 57, "lora_code_llama2_7b": 58, "gemma": [59, 61, 62, 63, 64, 65, 66, 67, 68, 145], "sentencepiec": [59, 69, 92, 104, 132, 185], "pretrain": [59, 69, 82, 92, 104, 132, 133, 180, 181, 184, 186, 188], "spm_model": [59, 69, 92, 104, 132, 181], "tokenized_text": [59, 69, 82, 92, 104, 132, 133], "add_bo": [59, 69, 82, 92, 104, 132, 133, 181], "31587": [59, 69, 82, 92, 104, 132, 133], "29644": [59, 69, 82, 92, 104, 132, 133], "102": [59, 69, 82, 92, 104, 132, 133], "tokenizer_path": [59, 69, 92, 104], "separ": [59, 69, 92, 104, 135, 142, 181, 184, 185, 186, 188], "concat": [59, 69, 92, 104, 135], "1788": [59, 69, 92, 104, 135], "2643": [59, 69, 92, 104, 135], "13": [59, 69, 92, 104, 119, 135, 166, 183, 185, 188], "1792": [59, 69, 92, 104, 135], "9508": [59, 69, 92, 104, 135], "465": [59, 69, 92, 104, 135], "22137": [59, 69, 92, 104, 135], "2933": [59, 69, 92, 104, 135], "join": [59, 69, 92, 104, 135], "attribut": [59, 69, 92, 104, 127, 135, 148], "head_dim": [60, 64, 111, 114, 117], "intermediate_dim": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "attn_dropout": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 117], "norm_ep": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "1e": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 115], "06": [60, 64, 115, 186], "rope_bas": [60, 64, 83, 87, 93, 95, 97, 99, 105, 107], "10000": [60, 64, 93, 95, 97, 99, 105, 107, 116], "norm_embed": [60, 64], "gemmatransformerdecod": [60, 61, 62, 64, 65, 66, 67, 68], "transformerdecoderlay": [60, 70, 83, 97, 117], "rm": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "norm": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 117, 118], "space": [60, 70, 83, 97, 117], "slide": [60, 97, 108], "window": [60, 97, 108, 182], "vocabulari": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "queri": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 117, 118, 185], "mha": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 117], "dimens": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111, 114, 116, 117, 119, 126, 185, 186, 188], "onto": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111], "scaled_dot_product_attent": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111], "epsilon": [60, 64, 70, 75, 83, 87, 93, 95, 97, 99, 105, 107], "rotari": [60, 64, 93, 95, 97, 99, 105, 107, 116, 185], "10_000": [60, 64, 93, 95, 97, 99, 107], "blog": [61, 62], "technolog": [61, 62], "develop": [61, 62, 188], "gemmatoken": 63, "becaus": [64, 114, 117, 119, 144, 180, 181, 183, 185, 187], "ti": 64, "gemma_2b": 65, "gemma_7b": 66, "lora_gemma_2b": 67, "lora_gemma_7b": 68, "card": [69, 82], "regist": [69, 82, 86, 104, 109, 112, 120, 167, 188], "uniqu": [69, 145], "strongli": 69, "beforehand": 69, "html": [69, 141, 147, 153, 158, 163, 168, 170, 176], "problem": [69, 92], "due": [69, 92, 132, 186, 188], "whitespac": [69, 92, 132], "slice": [69, 92], "gqa": [70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111], "mqa": [70, 75, 83, 87, 93, 95, 97, 99, 105, 107, 111], "kvcach": [70, 75, 83, 87, 105, 111, 117], "scale_hidden_dim_for_mlp": [70, 75, 83, 87], "2307": [71, 72, 73], "09288": [71, 72, 73], "llama2_70b": 77, "llama2_7b": [78, 186], "lora_llama2_13b": 79, "lora_llama2_70b": 80, "lora_llama2_7b": [81, 186], "special_token": [82, 104, 133, 181], "tiktoken": [82, 133, 185], "left": [82, 104, 186], "canon": [82, 86, 104, 109], "tt_model": [82, 133], "token_id": [82, 92, 133], "truncate_at_eo": [82, 133], "tokenize_head": 82, "tokenize_end": 82, "header": [82, 181], "eom": 82, "500000": [83, 87], "special_tokens_path": [86, 109], "llama3token": [86, 181], "similarli": [86, 109, 182, 187], "llama3_70b": 88, "llama3_8b": [89, 149, 185, 187], "lora_llama3_70b": 90, "lora_llama3_8b": 91, "trim_leading_whitespac": [92, 132], "unbatch": [92, 132], "bo": [92, 109, 132, 135, 181, 182], "trim": [92, 132], "num_class": [95, 99], "classifi": [95, 96, 99, 100, 103, 182], "announc": 98, "classif": [99, 145], "ray2333": 100, "reward": [100, 122, 123, 124], "feedback": [100, 122], "mistraltoken": [101, 181], "lora_mistral_7b": 102, "lora_mistral_classifier_7b": 103, "phi3": [104, 105, 106, 108, 109, 110, 145, 180], "ignore_system_prompt": 104, "phi3_mini": [106, 145], "ref": [108, 109, 164], "phi": [108, 109, 145], "128k": 108, "nor": 108, "phi3minitoken": 109, "tokenizer_config": 109, "spm": 109, "lm": 109, "unk": 109, "augment": [109, 188], "endoftext": 109, "phi3minisentencepiecebasetoken": 109, "lora_phi3_mini": 110, "pos_embed": [111, 186, 187], "kv_cach": 111, "introduc": [111, 115, 126, 181, 182, 186, 187, 188], "13245v1": 111, "multihead": 111, "extrem": 111, "share": [111, 182, 183], "credit": 111, "lightn": 111, "lit": 111, "lit_gpt": 111, "v": [111, 117, 186], "q": [111, 186], "n_kv_head": 111, "rotarypositionalembed": [111, 186, 187], "cach": [111, 114, 116, 117, 177, 180], "rope": [111, 116], "seq_length": [111, 118, 149], "boolean": [111, 117, 118, 141], "softmax": [111, 117, 118], "row": [111, 117, 118, 138, 181], "j": [111, 117, 118], "seq_len": 111, "bigger": 111, "n_h": [111, 116], "num": [111, 116], "n_kv": 111, "kv": [111, 114, 117, 187], "emb": [111, 117], "h_d": [111, 116], "gate_proj": 112, "down_proj": 112, "up_proj": 112, "silu": 112, "feed": [112, 118], "network": [112, 127, 186, 188], "fed": [112, 181], "multipli": 112, "subclass": [112, 147], "although": [112, 186, 187], "afterward": 112, "former": 112, "hook": [112, 120, 167, 188], "latter": 112, "layernorm": 113, "standalon": 114, "past": 114, "expand": 114, "dpython": [114, 117, 120], "reset": [114, 117, 154], "k_val": 114, "v_val": 114, "h": [114, 119, 139, 177, 180], "longer": [114, 182], "ep": 115, "root": [115, 163, 164], "squar": 115, "1910": 115, "07467": 115, "verfic": [115, 116], "small": [115, 183], "divis": [115, 140], "propos": 116, "2104": 116, "09864": 116, "l80": 116, "upto": 116, "init": [116, 154, 164, 188], "exceed": 116, "freq": 116, "recomput": 116, "geometr": 116, "progress": [116, 184], "rotat": 116, "angl": 116, "todo": 116, "effici": [116, 130, 152, 176, 178, 183, 184, 186, 187], "belong": [117, 148], "reduc": [117, 122, 178, 182, 186, 187, 188], "statement": 117, "improv": [117, 124, 133, 152, 183, 185, 186], "readabl": [117, 183], "At": 117, "arang": 117, "prompt_length": 117, "causal_mask": 117, "m_": 117, "seq": 117, "reset_cach": 117, "setup_cach": 117, "attn": [118, 186, 187, 188], "causalselfattent": [118, 186, 187], "sa_norm": 118, "mlp_norm": 118, "ff": 118, "token_pos_embed": 119, "pre_tile_pos_emb": 119, "post_tile_pos_emb": 119, "cls_project": 119, "vit": 119, "11929": 119, "convolut": 119, "flatten": 119, "treat": [119, 127, 147, 181], "downscal": [119, 138, 139], "800x400": 119, "400x400": 119, "_transform": 119, "clipimagetransform": 119, "broken": 119, "down": [119, 144, 182, 186, 188], "whole": 119, "num_til": [119, 140], "101": 119, "pool": 119, "tiledtokenpositionalembed": 119, "tilepositionalembed": 119, "tile_pos_emb": 119, "even": [119, 177, 180, 181, 182, 185, 186, 188], "8x8": 119, "14": [119, 166, 187, 188], "15": [119, 152, 166, 181, 183, 186, 188], "17": [119, 166, 183, 186], "18": [119, 166, 185], "19": [119, 166, 183, 185, 188], "20": [119, 166, 187], "21": 119, "22": 119, "23": [119, 121, 185], "24": [119, 140, 184, 185], "25": [119, 183], "26": 119, "27": [119, 183], "28": [119, 183], "29": [119, 188], "30": [119, 187], "31": [119, 185], "33": 119, "34": 119, "35": [119, 188], "36": 119, "37": 119, "38": [119, 183], "39": 119, "41": 119, "42": 119, "43": 119, "44": 119, "45": 119, "46": 119, "47": 119, "48": [119, 183, 188], "49": 119, "50": [119, 140, 183], "51": 119, "52": [119, 184], "53": 119, "54": 119, "55": [119, 184], "56": 119, "57": [119, 185, 186, 188], "58": 119, "59": [119, 188], "60": 119, "61": [119, 183], "62": [119, 183, 185], "63": 119, "64": [119, 186], "num_patches_per_til": 119, "emb_dim": 119, "greater": [119, 172], "constain": 119, "anim": [119, 182], "max_n_img": 119, "n_channel": 119, "hidden_st": 119, "vision_util": 119, "tile_crop": 119, "num_channel": 119, "image_s": [119, 139], "800": [119, 139], "patch_grid_s": 119, "random": [119, 170, 184], "rand": [119, 138, 139, 140], "nch": 119, "tile_cropped_imag": 119, "batch_imag": 119, "unsqueez": 119, "batch_aspect_ratio": 119, "clip_vision_encod": 119, "common_util": 120, "bfloat16": [120, 169, 183, 184, 185, 186, 187], "offload_to_cpu": 120, "nf4": [120, 188], "restor": 120, "higher": [120, 123, 185, 187, 188], "offload": [120, 188], "increas": [120, 121, 122, 185, 186, 187], "peak": [120, 154, 159, 183, 185, 186, 188], "gpu": [120, 180, 183, 184, 185, 186, 187, 188], "_register_state_dict_hook": 120, "m": [120, 149, 181, 187], "mymodul": 120, "_after_": 120, "nf4tensor": [120, 188], "unquant": [120, 183, 187, 188], "unus": 120, "num_warmup_step": 121, "num_training_step": 121, "num_cycl": [121, 171], "last_epoch": 121, "lambdalr": 121, "rate": [121, 178, 184], "schedul": [121, 171, 184], "linearli": 121, "decreas": [121, 182, 186, 187, 188], "cosin": 121, "v4": 121, "src": 121, "l104": 121, "warmup": [121, 171], "phase": 121, "wave": 121, "half": 121, "lr_schedul": 121, "beta": 122, "label_smooth": 122, "dpo": [122, 123, 124, 127, 166], "18290": 122, "intuit": [122, 123, 124], "dispref": 122, "incorpor": [122, 182], "dynam": [122, 187], "degener": 122, "occur": 122, "naiv": 122, "trl": [122, 123, 124], "librari": [122, 123, 124, 147, 151, 153, 170, 176, 178, 180, 182, 188], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 122, "dpo_train": [122, 123, 124], "l844": 122, "retain": [122, 188], "ppo": 122, "2009": 122, "01325": 122, "polici": [122, 123, 124, 127, 141, 152, 160, 168], "align": [122, 181], "regular": [122, 123, 187, 188], "baselin": [122, 183, 186], "rather": 122, "overhead": [122, 187], "temperatur": [122, 123, 124, 149, 183], "uncertainti": 122, "policy_chosen_logp": [122, 123, 124], "policy_rejected_logp": [122, 123, 124], "reference_chosen_logp": [122, 123, 124], "reference_rejected_logp": [122, 123, 124], "chosen": [122, 123, 124, 171, 182], "reject": [122, 123, 124, 182], "chosen_reward": [122, 123, 124], "rejected_reward": [122, 123, 124], "tau": 123, "optimis": 123, "ipo": 123, "2310": 123, "12036": 123, "pi": 123, "pi_ref": 123, "regress": [123, 124], "gap": 123, "likelihood": 123, "he": 123, "weaker": 123, "regularis": 123, "logprob": 123, "word": [123, 187], "unlik": [123, 130, 183, 185], "toward": 123, "thu": [123, 187], "4dce042a3863db1d375358e8c8092b874b02934b": [123, 124], "l1143": 123, "reciproc": 123, "larger": [123, 144, 183, 185], "gamma": 124, "statist": 124, "rso": 124, "hing": 124, "2309": 124, "06657": 124, "logist": 124, "slic": 124, "10425": 124, "almost": [124, 183, 185, 186], "vector": [124, 181], "svm": 124, "counter": 124, "l1141": 124, "peft": [125, 126, 127, 128, 129, 130, 131, 142, 186, 188], "protocol": 125, "adapter_param": [125, 126, 127, 128, 129], "proj": 125, "in_dim": [125, 126, 186, 188], "out_dim": [125, 126, 186, 188], "bia": [125, 126, 186, 187, 188], "loralinear": [125, 186, 188], "alpha": [126, 186, 188], "use_bia": 126, "perturb": 126, "decomposit": [126, 186], "matric": [126, 160, 186, 188], "trainabl": [126, 129, 160, 186, 188], "mapsto": 126, "w_0x": 126, "r": [126, 186], "bax": 126, "lora_a": [126, 186, 188], "lora_b": [126, 186, 188], "temporarili": 127, "neural": [127, 186, 188], "caller": 127, "whose": [127, 167], "yield": 127, "get_adapter_param": [129, 186], "base_miss": 130, "base_unexpect": 130, "lora_miss": 130, "lora_unexpect": 130, "validate_state_dict_for_lora": [130, 186], "reli": [130, 135], "unexpect": 130, "strict": [130, 186], "pull": [130, 180], "120600": 130, "assertionerror": [130, 131, 166], "nonempti": 130, "full_model_state_dict_kei": 131, "lora_state_dict_kei": 131, "base_model_state_dict_kei": 131, "confirm": [131, 177], "lora_modul": 131, "complement": 131, "disjoint": 131, "overlap": 131, "light": 132, "sentencepieceprocessor": 132, "addition": [132, 133, 170, 182, 186], "prefix": 132, "bos_id": [133, 135], "lightweight": [133, 181], "break": 133, "substr": 133, "repetit": 133, "speed": [133, 171, 185, 187, 188], "identif": 133, "regex": 133, "chunk": 133, "present": [133, 144], "absent": 133, "tokenizer_json_path": 134, "heavili": 135, "image_token_id": 136, "particip": 136, "show": [136, 177, 180, 181, 186], "laid": 136, "fig": 136, "flamingo": 136, "2204": 136, "14198": 136, "immedi": 136, "until": 136, "img1": 136, "img2": 136, "img3": 136, "cat": 136, "text_seq_len": 136, "image_seq_len": 136, "equal": [136, 140, 172], "resolut": [137, 138, 139], "1x1": 137, "1x2": 137, "2x1": 137, "side": [137, 138, 139], "height": [137, 138, 139], "width": [137, 138, 139, 187], "224": [137, 138, 139], "896": 137, "448": [137, 138, 139], "672": [137, 138], "possible_resolut": 138, "resize_to_max_canva": 138, "canva": 138, "resiz": [138, 139], "distort": [138, 139], "select": 138, "smallest": 138, "upscal": [138, 139], "2x": 138, "5x": 138, "canvas": 138, "satisfi": [138, 183], "condit": [138, 149, 158, 180, 182], "pick": 138, "lowest": [138, 186], "area": [138, 183], "minim": [138, 182, 184, 186, 187, 188], "200": [138, 140, 188], "300": [138, 139, 140, 183], "scale_height": 138, "1200": 138, "3600": 138, "2400": 138, "scale_width": 138, "7467": 138, "4933": 138, "scaling_factor": 138, "upscaling_opt": 138, "selected_scal": 138, "150528": 138, "100352": 138, "optimal_canva": 138, "target_s": 139, "resampl": 139, "interpolationmod": 139, "max_upscaling_s": 139, "exce": 139, "torchvis": 139, "nearest": 139, "nearest_exact": 139, "bilinear": 139, "bicub": 139, "1194": 139, "1344": 139, "stai": 139, "600": [139, 140], "500": [139, 186], "1000": [139, 141, 187], "488": 139, "channel_s": 140, "4x6": 140, "2x3": 140, "datatyp": [141, 188], "denot": 141, "integ": [141, 165, 170], "auto_wrap_polici": [141, 152, 168], "submodul": [141, 160], "obei": 141, "contract": 141, "get_fsdp_polici": 141, "modules_to_wrap": [141, 152, 160], "min_num_param": 141, "my_fsdp_polici": 141, "recurs": [141, 160, 163], "isinst": [141, 182], "sum": [141, 186], "p": [141, 146, 186, 187, 188], "numel": [141, 186], "functool": 141, "partial": 141, "stabl": [141, 158, 163, 170, 177], "safe_seri": 142, "from_pretrain": 142, "0001_of_0003": 142, "0002_of_0003": 142, "preserv": [142, 188], "weight_map": [142, 183], "convert_weight": 142, "_model_typ": [142, 145], "intermediate_checkpoint": [142, 143, 144], "_weight_map": 142, "shard": [143, 185], "wip": 143, "qualnam": 145, "boundari": 145, "distinguish": 145, "my_new_model": 145, "my_custom_state_dict_map": 145, "mistral_reward": 145, "mistral_classifi": 145, "optim_map": 146, "bare": 146, "bone": 146, "distribut": [146, 150, 157, 158, 168, 170, 178, 180, 184, 185], "optim_dict": [146, 148, 167], "cfg_optim": 146, "ckpt": 146, "optim_ckpt": 146, "placeholder_optim_dict": 146, "optiminbackwardwrapp": 146, "get_optim_kei": 146, "arbitrari": [146, 186], "hyperparamet": [146, 178, 184, 186, 188], "optim_ckpt_map": 146, "runtimeerror": [146, 151, 157], "loadabl": 146, "argpars": 147, "argumentpars": 147, "builtin": 147, "said": 147, "noth": 147, "consult": 147, "info": [147, 184], "parse_known_arg": 147, "namespac": 147, "act": 147, "precid": 147, "parse_arg": 147, "too": [147, 185], "optimizerinbackwardwrapp": 148, "top": [148, 183, 188], "named_paramet": 148, "max_generated_token": 149, "pad_id": 149, "top_k": [149, 183], "stop_token": 149, "custom_generate_next_token": 149, "predict": 149, "prune": [149, 188], "stop": 149, "compil": [149, 183, 185, 188], "generate_next_token": 149, "llama3_token": [149, 181, 185], "hi": [149, 181], "my": [149, 180, 181, 182, 183, 185], "jeremi": 149, "float32": 151, "bf16": [151, 188], "inde": [151, 183], "kernel": 151, "isn": [151, 180], "hardwar": [151, 178, 182, 183, 186], "memory_efficient_fsdp_wrap": [152, 187], "maxim": [152, 160, 176, 178], "been": [152, 181, 185, 187], "workload": [152, 187], "alongsid": 152, "ac": 152, "fullyshardeddataparallel": [152, 160], "fsdppolicytyp": [152, 160], "handler": 153, "reset_stat": 154, "track": 154, "alloc": [154, 159, 160, 185, 188], "reserv": [154, 159, 181, 188], "stat": [154, 159, 188], "int4": [155, 187], "4w": [155, 183, 185], "recogn": 155, "int4weightonlyquant": [155, 183, 185, 187], "int8weightonlyquant": 155, "8w": 155, "int4weightonlygptqquant": 155, "gptq": 155, "int8dynactint4weightquant": [155, 187], "8da4w": [155, 187], "int8dynactint4weightqatquant": [155, 187], "qat": [155, 176], "mode": [155, 183], "aka": 156, "master": 158, "port": [158, 180], "address": 158, "hold": [158, 184], "peak_memory_act": 159, "peak_memory_alloc": 159, "peak_memory_reserv": 159, "get_memory_stat": 159, "own": [160, 170, 180, 181, 182, 183, 186], "unit": [160, 178], "hierarch": 160, "requires_grad": [160, 186, 188], "filenam": 161, "log_": 161, "unixtimestamp": 161, "txt": [161, 182, 184], "thread": 161, "safe": 161, "flush": [161, 162, 163, 164], "ndarrai": [161, 162, 163, 164], "scalar": [161, 162, 163, 164], "record": [161, 162, 163, 164, 171], "payload": [161, 162, 163, 164], "organize_log": 163, "tensorboard": 163, "subdirectori": 163, "compar": [163, 172, 183, 186, 187, 188], "logdir": 163, "startup": 163, "tree": [163, 182, 183], "tfevent": 163, "encount": 163, "frontend": 163, "organ": [163, 180], "accordingli": [163, 187], "my_log_dir": 163, "view": [163, 183, 184], "my_metr": [163, 164], "termin": [163, 164], "entiti": 164, "bias": 164, "sent": 164, "usernam": 164, "my_project": 164, "my_ent": 164, "my_group": 164, "importerror": 164, "account": [164, 186, 188], "log_config": 164, "link": [164, 183], "capecap": 164, "6053ofw0": 164, "torchtune_config_j67sb73v": 164, "ignore_idx": [165, 166], "longest": 165, "token_pair": 165, "input_id": 166, "chosen_input_id": [166, 182], "chosen_label": [166, 182], "rejected_input_id": [166, 182], "rejected_label": [166, 182], "soon": 167, "readi": [167, 176, 181, 187], "grad": 167, "achiev": [167, 183, 185, 186, 187, 188], "acwrappolicytyp": 168, "author": [168, 178, 184, 188], "fsdp_adavnced_tutori": 168, "insid": 169, "contextmanag": 169, "debug_mod": 170, "pseudo": 170, "commonli": [170, 183, 186, 188], "numpi": 170, "determinist": 170, "global": [170, 182], "warn": 170, "nondeterminist": 170, "cudnn": 170, "set_deterministic_debug_mod": 170, "algorithm": 170, "profile_memori": 171, "with_stack": 171, "record_shap": 171, "with_flop": 171, "wait_step": 171, "warmup_step": 171, "active_step": 171, "profil": 171, "layout": 171, "trace": 171, "profileract": 171, "gradient_accumul": 171, "sensibl": 171, "default_schedul": 171, "reduct": [171, 186], "iter": [171, 173, 188], "scope": 171, "flop": 171, "wait": 171, "cycl": 171, "repeat": 171, "against": [172, 187, 188], "__version__": 172, "named_param": 173, "generated_examples_python": 174, "zip": 174, "galleri": [174, 179], "sphinx": 174, "000": [175, 179, 185], "execut": [175, 179], "generated_exampl": 175, "mem": [175, 179], "mb": [175, 179], "topic": 176, "gentl": 176, "introduct": 176, "first_finetune_tutori": 176, "workflow": [176, 182, 184, 186], "requisit": 177, "proper": [177, 184], "host": [177, 180, 184], "latest": [177, 184, 188], "And": [177, 183, 185], "ls": [177, 180, 183, 184, 185], "welcom": [177, 180], "greatest": [177, 184], "contributor": 177, "cd": [177, 183], "commit": 177, "branch": 177, "url": 177, "whl": 177, "therebi": [177, 187, 188], "forc": 177, "reinstal": 177, "opt": [177, 184], "suffix": 177, "cu121": 177, "On": [178, 186], "pointer": 178, "emphas": 178, "simplic": 178, "component": 178, "reus": 178, "prove": 178, "democrat": 178, "box": [178, 188], "zoo": 178, "varieti": [178, 186], "techniqu": [178, 183, 184, 186, 187], "integr": [178, 183, 184, 185, 186, 187, 188], "excit": 178, "checkout": 178, "quickstart": 178, "attain": 178, "better": [178, 181, 182, 183, 187], "chekckpoint": 178, "embodi": 178, "philosophi": 178, "usabl": 178, "composit": 178, "hard": [178, 182], "outlin": 178, "unecessari": 178, "never": 178, "thoroughli": 178, "short": 180, "subcommand": 180, "anytim": 180, "symlink": 180, "auto": 180, "wrote": 180, "readm": 180, "md": 180, "lot": [180, 183], "recent": 180, "releas": [180, 185], "agre": 180, "term": 180, "perman": 180, "eat": 180, "bandwith": 180, "storag": [180, 188], "00030": 180, "ootb": 180, "full_finetune_single_devic": [180, 182, 183, 184], "7b_full_low_memori": [180, 183, 184], "8b_full_single_devic": [180, 182], "mini_full_low_memori": 180, "7b_full": [180, 183, 184], "13b_full": [180, 183, 184], "70b_full": 180, "edit": 180, "clobber": 180, "destin": 180, "lora_finetune_distribut": [180, 185, 186], "torchrun": 180, "8b_lora_single_devic": [180, 181, 185], "launch": [180, 181, 184], "nproc": 180, "node": 180, "worker": 180, "nnode": [180, 186, 187], "minimum_nod": 180, "maximum_nod": 180, "fail": 180, "rdzv": 180, "rendezv": 180, "endpoint": 180, "8b_lora": [180, 185], "bypass": 180, "vice": 180, "versa": 180, "fancy_lora": 180, "8b_fancy_lora": 180, "sai": [180, 181, 184], "intend": 181, "nice": 181, "meet": 181, "overhaul": 181, "begin_of_text": 181, "start_header_id": 181, "end_header_id": 181, "eot_id": 181, "yet": [181, 183], "untrain": 181, "accompani": 181, "who": 181, "influenti": 181, "hip": 181, "hop": 181, "artist": [181, 185], "2pac": 181, "rakim": 181, "c": 181, "na": 181, "flavor": [181, 182], "msg": 181, "formatted_messag": [181, 182], "nyou": [181, 182], "nwho": 181, "why": [181, 184, 186], "user_messag": 181, "518": 181, "25580": 181, "29962": 181, "3532": 181, "14816": 181, "29903": 181, "6778": 181, "_spm_model": 181, "piece_to_id": 181, "place": 181, "manual": [181, 188], "529": 181, "29879": 181, "29958": 181, "nhere": 181, "128000": [181, 187], "128009": 181, "pure": 181, "That": 181, "won": [181, 183, 185], "mess": 181, "govern": 181, "prime": 181, "strictli": 181, "ask": 181, "untouch": 181, "nsummari": 181, "robust": 181, "csv": [181, 182], "onlin": 181, "forum": 181, "panda": 181, "pd": 181, "df": 181, "read_csv": 181, "your_fil": 181, "nrow": 181, "tolist": 181, "iloc": 181, "gp": 181, "receiv": 181, "commun": [181, 182, 183], "satellit": 181, "thing": [181, 188], "dataclass": 181, "message_convert": 181, "input_msg": 181, "output_msg": 181, "assistant_messag": 181, "But": [181, 183, 185, 186], "mistralchatformat": 181, "custom_dataset": 181, "2048": 181, "data_fil": [181, 182], "honor": 181, "copi": [181, 183, 184, 185, 187, 188], "folder": 181, "custom_8b_lora_single_devic": 181, "steer": 182, "wheel": 182, "publicli": 182, "great": [182, 183], "hood": [182, 183, 188], "text_completion_dataset": [182, 187], "padded_col": 182, "upper": 182, "constraint": [182, 186], "slow": [182, 188], "signific": [182, 187], "speedup": [182, 185], "my_data": 182, "fix": [182, 187], "goal": [182, 187], "agnost": 182, "respond": 182, "plant": 182, "miner": 182, "oak": 182, "copper": 182, "ore": 182, "eleph": 182, "customtempl": 182, "chat_dataset": 182, "quit": [182, 188], "customchatformat": 182, "concatdataset": 182, "drive": 182, "rajpurkar": 182, "io": 182, "squad": 182, "explor": 182, "rlhf": 182, "few": [182, 185, 186, 188], "adjust": [182, 187], "chosen_messag": 182, "transformed_sampl": 182, "key_chosen": 182, "rejected_messag": 182, "key_reject": 182, "c_mask": 182, "np": 182, "cross_entropy_ignore_idx": 182, "r_mask": 182, "stack_exchanged_paired_dataset": 182, "had": 182, "stackexchangedpairedtempl": 182, "response_j": 182, "response_k": 182, "rl": 182, "favorit": [183, 185, 186], "seemlessli": 183, "beyond": [183, 188], "connect": [183, 187], "amount": 183, "natur": 183, "export": 183, "mobil": 183, "phone": 183, "leverag": [183, 185, 188], "plai": 183, "freez": [183, 186], "percentag": 183, "learnabl": 183, "keep": [183, 186], "16gb": [183, 186], "rtx": 183, "3090": 183, "4090": 183, "hour": 183, "7b_qlora_single_devic": [183, 184, 188], "473": 183, "98": [183, 188], "gb": [183, 185, 186, 187, 188], "484": 183, "01": [183, 184], "fact": [183, 185, 186], "third": 183, "realli": 183, "eleuther_ev": [183, 185, 187], "eleuther_evalu": [183, 185, 187], "lm_eval": [183, 185], "plan": 183, "custom_eval_config": [183, 185], "truthfulqa_mc2": [183, 185, 186], "measur": [183, 185], "propens": [183, 185], "shot": [183, 185, 187], "accuraci": [183, 185, 186, 187, 188], "324": 183, "loglikelihood": 183, "195": 183, "121": 183, "second": [183, 185, 186, 188], "197": 183, "acc": [183, 187], "388": 183, "shown": [183, 187], "489": 183, "seem": 183, "custom_generation_config": [183, 185], "kick": 183, "interest": 183, "site": 183, "visit": 183, "bai": 183, "92": [183, 185], "exploratorium": 183, "san": 183, "francisco": 183, "magazin": 183, "awesom": 183, "bridg": 183, "pretti": 183, "cool": 183, "96": [183, 188], "sec": [183, 185], "83": 183, "99": [183, 186], "72": 183, "littl": 183, "saw": 183, "took": [183, 185], "torchao": [183, 185, 187, 188], "bit": [183, 185, 186, 187, 188], "custom_quantization_config": [183, 185], "68": 183, "76": 183, "69": 183, "95": [183, 185], "67": 183, "engin": [183, 185], "fullmodeltorchtunecheckpoint": [183, 185, 187], "groupsiz": [183, 185, 187], "256": [183, 185, 187], "park": 183, "sit": 183, "hill": 183, "beauti": 183, "85": 183, "sped": 183, "3x": [183, 185], "benefit": 183, "doesn": 183, "fast": 183, "clone": [183, 186, 187, 188], "assumpt": 183, "new_dir": 183, "output_dict": 183, "sd_1": 183, "sd_2": 183, "dump": 183, "convert_hf_checkpoint": 183, "checkpoint_path": 183, "justin": 183, "school": 183, "math": 183, "teacher": 183, "ws": 183, "94": [183, 185], "bandwidth": [183, 185], "1391": 183, "84": 183, "thats": 183, "seamlessli": 183, "authent": [183, 184], "hopefulli": 183, "gave": 183, "grant": 184, "minut": 184, "agreement": 184, "altern": 184, "hackabl": 184, "singularli": 184, "technic": 184, "purpos": [184, 185], "depth": 184, "principl": 184, "boilerpl": 184, "substanti": [184, 186], "custom_config": 184, "replic": 184, "lorafinetunerecipesingledevic": 184, "lora_finetune_output": 184, "log_1713194212": 184, "3697006702423096": 184, "25880": [184, 188], "83it": 184, "monitor": 184, "tqdm": 184, "interv": 184, "e2": 184, "focu": 185, "128": [185, 186], "theta": 185, "gain": 185, "illustr": 185, "basic": 185, "observ": [185, 187], "consum": [185, 188], "vram": [185, 186, 187], "overal": 185, "8b_qlora_single_devic": 185, "coupl": [185, 186, 188], "meta_model_0": [185, 187], "122": 185, "sarah": 185, "busi": 185, "mum": 185, "young": 185, "children": 185, "live": 185, "north": 185, "east": 185, "england": 185, "135": 185, "88": 185, "138": 185, "346": 185, "09": 185, "139": 185, "far": 185, "drill": 185, "90": 185, "93": 185, "91": 185, "104": 185, "four": [185, 186], "again": 185, "jake": 185, "disciplin": 185, "passion": 185, "draw": 185, "paint": 185, "broader": 185, "teach": 186, "straight": 186, "unfamiliar": 186, "oppos": [186, 188], "momentum": 186, "relat": 186, "aghajanyan": 186, "et": 186, "al": 186, "hypothes": 186, "intrins": 186, "often": 186, "eight": 186, "practic": 186, "blue": 186, "rememb": 186, "approx": 186, "15m": 186, "8192": [186, 187], "65k": 186, "frozen_out": [186, 188], "lora_out": [186, 188], "base_model": 186, "choos": 186, "lora_model": 186, "lora_llama_2_7b": [186, 188], "alon": 186, "in_featur": [186, 187], "out_featur": [186, 187], "inplac": 186, "feel": 186, "free": 186, "whenev": 186, "peft_util": 186, "set_trainable_param": 186, "fetch": 186, "lora_param": 186, "total_param": 186, "trainable_param": 186, "2f": 186, "6742609920": 186, "4194304": 186, "7b_lora": 186, "my_model_checkpoint_path": [186, 187, 188], "tokenizer_checkpoint": [186, 187, 188], "my_tokenizer_checkpoint_path": [186, 187, 188], "factori": 186, "benefici": 186, "impact": 186, "minor": 186, "good": 186, "lora_experiment_1": 186, "smooth": [186, 188], "curv": [186, 188], "ran": 186, "footprint": [186, 187], "commod": 186, "cogniz": 186, "ax": 186, "parallel": 186, "truthfulqa": 186, "previous": 186, "475": 186, "87": 186, "508": 186, "86": 186, "504": 186, "04": 186, "514": 186, "absolut": 186, "4gb": 186, "tradeoff": 186, "potenti": 186, "awar": 187, "incur": [187, 188], "degrad": [187, 188], "perplex": 187, "simul": 187, "ultim": 187, "ptq": 187, "fake": 187, "kept": 187, "cast": 187, "nois": 187, "henc": 187, "x_q": 187, "int8": 187, "zp": 187, "x_float": 187, "qmin": 187, "qmax": 187, "round": 187, "clamp": 187, "x_fq": 187, "dequant": 187, "involv": 187, "insert": 187, "proce": 187, "prepared_model": 187, "swap": 187, "int8dynactint4weightqatlinear": 187, "int8dynactint4weightlinear": 187, "train_loop": 187, "converted_model": 187, "demonstr": 187, "recov": 187, "modif": 187, "8b_qat_ful": 187, "custom_8b_qat_ful": 187, "2000": 187, "fake_quant_after_n_step": 187, "issu": 187, "futur": 187, "empir": 187, "led": 187, "presum": 187, "80gb": 187, "qat_distribut": 187, "op": 187, "mutat": 187, "5gb": 187, "custom_quant": 187, "poorli": 187, "custom_eleuther_evalu": 187, "hellaswag": 187, "max_seq_length": 187, "my_eleuther_evalu": 187, "filter": 187, "stderr": 187, "word_perplex": 187, "9148": 187, "byte_perplex": 187, "5357": 187, "bits_per_byt": 187, "6189": 187, "5687": 187, "0049": 187, "acc_norm": 187, "7536": 187, "0043": 187, "portion": [187, 188], "drop": 187, "74": 187, "048": 187, "190": 187, "7735": 187, "5598": 187, "6413": 187, "5481": 187, "0050": 187, "7390": 187, "0044": 187, "7251": 187, "4994": 187, "5844": 187, "5740": 187, "7610": 187, "outperform": 187, "importantli": 187, "characterist": 187, "187": 187, "958": 187, "halv": 187, "motiv": 187, "constrain": 187, "edg": 187, "smartphon": 187, "executorch": 187, "xnnpack": 187, "export_llama": 187, "use_sdpa_with_kv_cach": 187, "qmode": 187, "group_siz": 187, "get_bos_id": 187, "get_eos_id": 187, "128001": 187, "output_nam": 187, "llama3_8da4w": 187, "pte": 187, "881": 187, "oneplu": 187, "709": 187, "tok": 187, "815": 187, "316": 187, "364": 187, "highli": 188, "vanilla": 188, "held": 188, "therefor": 188, "bespok": 188, "normalfloat": 188, "8x": 188, "vast": 188, "major": 188, "normatfloat": 188, "doubl": 188, "themselv": 188, "deepdiv": 188, "idea": 188, "distinct": 188, "de": 188, "counterpart": 188, "set_default_devic": 188, "qlora_linear": 188, "memory_alloc": 188, "177": 188, "152": 188, "del": 188, "empty_cach": 188, "lora_linear": 188, "081": 188, "344": 188, "qlora_llama2_7b": 188, "qlora_model": 188, "essenti": 188, "reparametrize_as_dtype_state_dict_post_hook": 188, "slower": 188, "149": 188, "9157477021217346": 188, "02": 188, "08": 188, "15it": 188, "nightli": 188, "hundr": 188, "228": 188, "8158286809921265": 188, "95it": 188, "exercis": 188, "linear_nf4": 188, "to_nf4": 188, "linear_weight": 188, "autograd": 188, "incom": 188}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 4, 1, "", "Role"], [23, 1, 1, "", "StackExchangedPairedTemplate"], [24, 1, 1, "", "SummarizeTemplate"], [25, 0, 1, "", "get_openai_messages"], [26, 0, 1, "", "get_sharegpt_messages"], [27, 0, 1, "", "truncate"], [28, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 3, 1, "", "contains_media"], [20, 2, 1, "", "from_dict"], [20, 3, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[23, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[24, 2, 1, "", "format"]], "torchtune.datasets": [[29, 1, 1, "", "ChatDataset"], [30, 1, 1, "", "ConcatDataset"], [31, 1, 1, "", "InstructDataset"], [32, 1, 1, "", "PackedDataset"], [33, 1, 1, "", "PreferenceDataset"], [34, 1, 1, "", "TextCompletionDataset"], [35, 0, 1, "", "alpaca_cleaned_dataset"], [36, 0, 1, "", "alpaca_dataset"], [37, 0, 1, "", "chat_dataset"], [38, 0, 1, "", "cnn_dailymail_articles_dataset"], [39, 0, 1, "", "grammar_dataset"], [40, 0, 1, "", "instruct_dataset"], [41, 0, 1, "", "samsum_dataset"], [42, 0, 1, "", "slimorca_dataset"], [43, 0, 1, "", "stack_exchanged_paired_dataset"], [44, 0, 1, "", "text_completion_dataset"], [45, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[46, 1, 1, "", "TilePositionalEmbedding"], [47, 1, 1, "", "TiledTokenPositionalEmbedding"], [48, 1, 1, "", "TokenPositionalEmbedding"], [49, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[46, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[47, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[48, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[50, 0, 1, "", "code_llama2_13b"], [51, 0, 1, "", "code_llama2_70b"], [52, 0, 1, "", "code_llama2_7b"], [53, 0, 1, "", "lora_code_llama2_13b"], [54, 0, 1, "", "lora_code_llama2_70b"], [55, 0, 1, "", "lora_code_llama2_7b"], [56, 0, 1, "", "qlora_code_llama2_13b"], [57, 0, 1, "", "qlora_code_llama2_70b"], [58, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[59, 1, 1, "", "GemmaTokenizer"], [60, 0, 1, "", "gemma"], [61, 0, 1, "", "gemma_2b"], [62, 0, 1, "", "gemma_7b"], [63, 0, 1, "", "gemma_tokenizer"], [64, 0, 1, "", "lora_gemma"], [65, 0, 1, "", "lora_gemma_2b"], [66, 0, 1, "", "lora_gemma_7b"], [67, 0, 1, "", "qlora_gemma_2b"], [68, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[59, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[69, 1, 1, "", "Llama2Tokenizer"], [70, 0, 1, "", "llama2"], [71, 0, 1, "", "llama2_13b"], [72, 0, 1, "", "llama2_70b"], [73, 0, 1, "", "llama2_7b"], [74, 0, 1, "", "llama2_tokenizer"], [75, 0, 1, "", "lora_llama2"], [76, 0, 1, "", "lora_llama2_13b"], [77, 0, 1, "", "lora_llama2_70b"], [78, 0, 1, "", "lora_llama2_7b"], [79, 0, 1, "", "qlora_llama2_13b"], [80, 0, 1, "", "qlora_llama2_70b"], [81, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[69, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[82, 1, 1, "", "Llama3Tokenizer"], [83, 0, 1, "", "llama3"], [84, 0, 1, "", "llama3_70b"], [85, 0, 1, "", "llama3_8b"], [86, 0, 1, "", "llama3_tokenizer"], [87, 0, 1, "", "lora_llama3"], [88, 0, 1, "", "lora_llama3_70b"], [89, 0, 1, "", "lora_llama3_8b"], [90, 0, 1, "", "qlora_llama3_70b"], [91, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[82, 2, 1, "", "decode"], [82, 2, 1, "", "tokenize_message"], [82, 2, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[92, 1, 1, "", "MistralTokenizer"], [93, 0, 1, "", "lora_mistral"], [94, 0, 1, "", "lora_mistral_7b"], [95, 0, 1, "", "lora_mistral_classifier"], [96, 0, 1, "", "lora_mistral_classifier_7b"], [97, 0, 1, "", "mistral"], [98, 0, 1, "", "mistral_7b"], [99, 0, 1, "", "mistral_classifier"], [100, 0, 1, "", "mistral_classifier_7b"], [101, 0, 1, "", "mistral_tokenizer"], [102, 0, 1, "", "qlora_mistral_7b"], [103, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.mistral.MistralTokenizer": [[92, 2, 1, "", "decode"], [92, 2, 1, "", "encode"], [92, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[104, 1, 1, "", "Phi3MiniTokenizer"], [105, 0, 1, "", "lora_phi3"], [106, 0, 1, "", "lora_phi3_mini"], [107, 0, 1, "", "phi3"], [108, 0, 1, "", "phi3_mini"], [109, 0, 1, "", "phi3_mini_tokenizer"], [110, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[104, 2, 1, "", "decode"], [104, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[111, 1, 1, "", "CausalSelfAttention"], [112, 1, 1, "", "FeedForward"], [113, 1, 1, "", "Fp32LayerNorm"], [114, 1, 1, "", "KVCache"], [115, 1, 1, "", "RMSNorm"], [116, 1, 1, "", "RotaryPositionalEmbeddings"], [117, 1, 1, "", "TransformerDecoder"], [118, 1, 1, "", "TransformerDecoderLayer"], [119, 1, 1, "", "VisionTransformer"], [121, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[111, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[112, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[113, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[114, 2, 1, "", "reset"], [114, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[115, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[116, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[117, 2, 1, "", "forward"], [117, 2, 1, "", "reset_caches"], [117, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[118, 2, 1, "", "forward"]], "torchtune.modules.VisionTransformer": [[119, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[120, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[122, 1, 1, "", "DPOLoss"], [123, 1, 1, "", "IPOLoss"], [124, 1, 1, "", "RSOLoss"]], "torchtune.modules.loss.DPOLoss": [[122, 2, 1, "", "forward"]], "torchtune.modules.loss.IPOLoss": [[123, 2, 1, "", "forward"]], "torchtune.modules.loss.RSOLoss": [[124, 2, 1, "", "forward"]], "torchtune.modules.peft": [[125, 1, 1, "", "AdapterModule"], [126, 1, 1, "", "LoRALinear"], [127, 0, 1, "", "disable_adapter"], [128, 0, 1, "", "get_adapter_params"], [129, 0, 1, "", "set_trainable_params"], [130, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [131, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[125, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[126, 2, 1, "", "adapter_params"], [126, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[132, 1, 1, "", "SentencePieceBaseTokenizer"], [133, 1, 1, "", "TikTokenBaseTokenizer"], [134, 0, 1, "", "parse_hf_tokenizer_json"], [135, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[132, 2, 1, "", "decode"], [132, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[133, 2, 1, "", "decode"], [133, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[136, 1, 1, "", "VisionCrossAttentionMask"], [137, 0, 1, "", "find_supported_resolutions"], [138, 0, 1, "", "get_canvas_best_fit"], [139, 0, 1, "", "resize_with_pad"], [140, 0, 1, "", "tile_crop"]], "torchtune.utils": [[141, 4, 1, "", "FSDPPolicyType"], [142, 1, 1, "", "FullModelHFCheckpointer"], [143, 1, 1, "", "FullModelMetaCheckpointer"], [144, 1, 1, "", "FullModelTorchTuneCheckpointer"], [145, 1, 1, "", "ModelType"], [146, 1, 1, "", "OptimizerInBackwardWrapper"], [147, 1, 1, "", "TuneRecipeArgumentParser"], [148, 0, 1, "", "create_optim_in_bwd_wrapper"], [149, 0, 1, "", "generate"], [150, 0, 1, "", "get_device"], [151, 0, 1, "", "get_dtype"], [152, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [153, 0, 1, "", "get_logger"], [154, 0, 1, "", "get_memory_stats"], [155, 0, 1, "", "get_quantizer_mode"], [156, 0, 1, "", "get_world_size_and_rank"], [157, 0, 1, "", "init_distributed"], [158, 0, 1, "", "is_distributed"], [159, 0, 1, "", "log_memory_stats"], [160, 0, 1, "", "lora_fsdp_wrap_policy"], [165, 0, 1, "", "padded_collate"], [166, 0, 1, "", "padded_collate_dpo"], [167, 0, 1, "", "register_optim_in_bwd_hooks"], [168, 0, 1, "", "set_activation_checkpointing"], [169, 0, 1, "", "set_default_dtype"], [170, 0, 1, "", "set_seed"], [171, 0, 1, "", "setup_torch_profiler"], [172, 0, 1, "", "torch_version_ge"], [173, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[142, 2, 1, "", "load_checkpoint"], [142, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[143, 2, 1, "", "load_checkpoint"], [143, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[144, 2, 1, "", "load_checkpoint"], [144, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[145, 5, 1, "", "GEMMA"], [145, 5, 1, "", "LLAMA2"], [145, 5, 1, "", "LLAMA3"], [145, 5, 1, "", "MISTRAL"], [145, 5, 1, "", "MISTRAL_REWARD"], [145, 5, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[146, 2, 1, "", "get_optim_key"], [146, 2, 1, "", "load_state_dict"], [146, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[147, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[161, 1, 1, "", "DiskLogger"], [162, 1, 1, "", "StdoutLogger"], [163, 1, 1, "", "TensorBoardLogger"], [164, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[161, 2, 1, "", "close"], [161, 2, 1, "", "log"], [161, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[162, 2, 1, "", "close"], [162, 2, 1, "", "log"], [162, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[163, 2, 1, "", "close"], [163, 2, 1, "", "log"], [163, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[164, 2, 1, "", "close"], [164, 2, 1, "", "log"], [164, 2, 1, "", "log_config"], [164, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:property", "4": "py:data", "5": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"], "4": ["py", "data", "Python data"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 22, 141, 176, 178, 180, 183, 185, 186, 187, 188], "config": [0, 7, 8, 180, 184], "data": [1, 5, 22, 181], "text": [1, 182, 185], "templat": [1, 181, 182], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 181, 182], "exampl": 2, "gener": [2, 149, 183, 185], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 180, 183, 184, 185, 186, 187], "llama3": [3, 83, 181, 185, 187], "llama2": [3, 70, 181, 183, 186, 188], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": [3, 97], "gemma": [3, 60], "clip": 3, "modul": 4, "compon": [4, 7], "build": [4, 177, 188], "block": 4, "base": 4, "token": [4, 181], "util": [4, 5, 141], "peft": 4, "loss": 4, "vision": 4, "transform": 4, "checkpoint": [5, 6, 9, 183], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 182, 186, 188], "manag": 5, "perform": [5, 186], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 178, 183], "format": [6, 182], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 183, 186, 188], "put": [6, 188], "thi": 6, "all": [6, 7, 188], "togeth": [6, 188], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 182], "us": [7, 8, 181, 183, 188], "instanti": [7, 10], "referenc": 7, "other": [7, 183], "field": 7, "interpol": 7, "valid": [7, 13, 180], "your": [7, 8, 183, 184], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 178, 186, 187, 188], "ar": 8, "recip": [8, 180, 184, 186, 187], "script": 8, "run": [8, 180, 183], "cli": [8, 180], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "role": 22, "stackexchangedpairedtempl": 23, "summarizetempl": 24, "get_openai_messag": 25, "get_sharegpt_messag": 26, "truncat": 27, "validate_messag": 28, "chatdataset": 29, "concatdataset": 30, "instructdataset": 31, "packeddataset": 32, "preferencedataset": 33, "textcompletiondataset": 34, "alpaca_cleaned_dataset": 35, "alpaca_dataset": 36, "chat_dataset": 37, "cnn_dailymail_articles_dataset": 38, "grammar_dataset": 39, "instruct_dataset": 40, "samsum_dataset": 41, "slimorca_dataset": 42, "stack_exchanged_paired_dataset": 43, "text_completion_dataset": 44, "wikitext_dataset": 45, "tilepositionalembed": 46, "tiledtokenpositionalembed": 47, "tokenpositionalembed": 48, "clip_vision_encod": 49, "code_llama2_13b": 50, "code_llama2_70b": 51, "code_llama2_7b": 52, "lora_code_llama2_13b": 53, "lora_code_llama2_70b": 54, "lora_code_llama2_7b": 55, "qlora_code_llama2_13b": 56, "qlora_code_llama2_70b": 57, "qlora_code_llama2_7b": 58, "gemmatoken": 59, "gemma_2b": 61, "gemma_7b": 62, "gemma_token": 63, "lora_gemma": 64, "lora_gemma_2b": 65, "lora_gemma_7b": 66, "qlora_gemma_2b": 67, "qlora_gemma_7b": 68, "llama2token": 69, "llama2_13b": 71, "llama2_70b": 72, "llama2_7b": 73, "llama2_token": 74, "lora_llama2": 75, "lora_llama2_13b": 76, "lora_llama2_70b": 77, "lora_llama2_7b": 78, "qlora_llama2_13b": 79, "qlora_llama2_70b": 80, "qlora_llama2_7b": 81, "llama3token": 82, "llama3_70b": 84, "llama3_8b": 85, "llama3_token": 86, "lora_llama3": 87, "lora_llama3_70b": 88, "lora_llama3_8b": 89, "qlora_llama3_70b": 90, "qlora_llama3_8b": 91, "mistraltoken": 92, "lora_mistr": 93, "lora_mistral_7b": 94, "lora_mistral_classifi": 95, "lora_mistral_classifier_7b": 96, "mistral_7b": 98, "mistral_classifi": 99, "mistral_classifier_7b": 100, "mistral_token": 101, "qlora_mistral_7b": 102, "qlora_mistral_classifier_7b": 103, "phi3minitoken": 104, "lora_phi3": 105, "lora_phi3_mini": 106, "phi3": 107, "phi3_mini": 108, "phi3_mini_token": 109, "qlora_phi3_mini": 110, "causalselfattent": 111, "todo": [111, 118], "feedforward": 112, "fp32layernorm": 113, "kvcach": 114, "rmsnorm": 115, "rotarypositionalembed": 116, "transformerdecod": 117, "transformerdecoderlay": 118, "visiontransform": 119, "reparametrize_as_dtype_state_dict_post_hook": 120, "get_cosine_schedule_with_warmup": 121, "dpoloss": 122, "ipoloss": 123, "rsoloss": 124, "adaptermodul": 125, "loralinear": 126, "disable_adapt": 127, "get_adapter_param": 128, "set_trainable_param": 129, "validate_missing_and_unexpected_for_lora": 130, "validate_state_dict_for_lora": 131, "sentencepiecebasetoken": 132, "tiktokenbasetoken": 133, "parse_hf_tokenizer_json": 134, "tokenize_messages_no_special_token": 135, "visioncrossattentionmask": 136, "find_supported_resolut": 137, "get_canvas_best_fit": 138, "resize_with_pad": 139, "tile_crop": 140, "fsdppolicytyp": 141, "fullmodelhfcheckpoint": 142, "fullmodelmetacheckpoint": 143, "fullmodeltorchtunecheckpoint": 144, "modeltyp": 145, "optimizerinbackwardwrapp": 146, "tunerecipeargumentpars": 147, "create_optim_in_bwd_wrapp": 148, "get_devic": 150, "get_dtyp": 151, "get_full_finetune_fsdp_wrap_polici": 152, "get_logg": 153, "get_memory_stat": 154, "get_quantizer_mod": 155, "get_world_size_and_rank": 156, "init_distribut": 157, "is_distribut": 158, "log_memory_stat": 159, "lora_fsdp_wrap_polici": 160, "disklogg": 161, "stdoutlogg": 162, "tensorboardlogg": 163, "wandblogg": 164, "padded_col": 165, "padded_collate_dpo": 166, "register_optim_in_bwd_hook": 167, "set_activation_checkpoint": 168, "set_default_dtyp": 169, "set_se": 170, "setup_torch_profil": 171, "torch_version_g": 172, "validate_expected_param_dtyp": 173, "comput": [175, 179], "time": [175, 179], "welcom": 176, "document": 176, "get": [176, 180, 185], "start": [176, 180], "tutori": 176, "instal": 177, "instruct": [177, 182, 185], "via": [177, 185], "pypi": 177, "git": 177, "clone": 177, "nightli": 177, "kei": 178, "concept": 178, "design": 178, "principl": 178, "download": [180, 183, 184], "list": 180, "built": [180, 182], "copi": 180, "fine": [181, 182, 184, 185], "tune": [181, 182, 184, 185], "chat": [181, 182], "chang": 181, "from": [181, 188], "prompt": 181, "special": 181, "when": 181, "should": 181, "i": 181, "custom": [181, 182], "hug": [182, 183], "face": [182, 183], "set": 182, "max": 182, "sequenc": 182, "length": 182, "sampl": 182, "pack": 182, "unstructur": 182, "corpu": 182, "multipl": 182, "local": 182, "remot": 182, "fulli": 182, "end": 183, "workflow": 183, "7b": 183, "finetun": [183, 186, 187, 188], "evalu": [183, 185, 187], "eleutherai": [183, 185], "s": [183, 185], "eval": [183, 185], "har": [183, 185], "speed": 183, "up": 183, "quantiz": [183, 185, 187], "librari": 183, "upload": 183, "hub": 183, "first": 184, "llm": 184, "select": 184, "modifi": 184, "train": 184, "next": 184, "step": 184, "meta": 185, "8b": 185, "access": 185, "our": 185, "faster": 185, "how": 186, "doe": 186, "work": 186, "appli": [186, 187], "trade": 186, "off": 186, "qat": 187, "lower": 187, "devic": 187, "option": 187, "qlora": 188, "save": 188, "deep": 188, "dive": 188}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})