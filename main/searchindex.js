Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"support": [2, 6, 8, 9, 10, 20, 24, 26, 27, 28, 29, 30, 31, 32, 33, 51, 52, 62, 69, 72, 77, 90, 92, 93, 94, 95, 96, 97, 98], "sever": [2, 93], "wide": [2, 93], "us": [2, 4, 6, 9, 10, 11, 15, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 52, 53, 55, 56, 57, 58, 59, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 85, 88, 89, 90, 93, 95, 96, 97], "help": [2, 6, 18, 57, 68, 70, 88, 89, 90, 92, 93, 94, 95, 96, 98], "quickli": [2, 7, 92, 93], "bootstrap": [2, 93], "your": [2, 5, 9, 10, 24, 80, 81, 88, 89, 90, 92, 93, 96, 97, 98], "fine": [2, 6, 8, 9, 88, 90, 94, 97], "tune": [2, 3, 6, 7, 8, 9, 11, 88, 89, 90, 94, 97, 98], "also": [2, 6, 7, 8, 9, 10, 29, 52, 57, 62, 71, 73, 81, 89, 92, 93, 94, 95, 96, 97, 98], "common": [2, 4, 7, 92, 93, 96, 97], "format": [2, 5, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24, 26, 27, 28, 29, 31, 33, 68, 69, 92, 94, 95, 96, 97], "like": [2, 6, 7, 8, 9, 24, 89, 92, 93, 94, 95, 97], "chat": [2, 14, 15, 18, 19, 22, 24, 29, 33], "model": [2, 6, 7, 8, 10, 15, 20, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 68, 69, 73, 83, 84, 88, 90, 92, 93, 98], "instruct": [2, 3, 13, 15, 17, 19, 20, 26, 27, 28, 31, 51, 88, 92, 95, 97, 98], "These": [2, 4, 6, 7, 8, 10, 70, 92, 93, 94, 95, 96, 97, 98], "ar": [2, 4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29, 30, 31, 32, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 57, 62, 67, 68, 69, 72, 89, 90, 92, 93, 94, 95, 96, 97, 98], "especi": [2, 90, 94], "specifi": [2, 6, 7, 8, 10, 29, 52, 67, 73, 81, 84, 92, 93, 94, 95, 96, 98], "from": [2, 3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 49, 53, 57, 58, 60, 61, 63, 65, 68, 69, 70, 80, 81, 87, 89, 91, 93, 94, 95, 96, 97], "yaml": [2, 7, 8, 10, 11, 29, 31, 70, 81, 90, 92, 94, 95, 96, 97, 98], "config": [2, 6, 9, 10, 11, 12, 29, 31, 52, 68, 70, 81, 90, 92, 93, 94, 96, 97, 98], "represent": [2, 97, 98], "abov": [2, 6, 59, 89, 94, 96, 97, 98], "all": [3, 4, 8, 12, 24, 25, 29, 52, 53, 57, 59, 66, 68, 70, 86, 88, 90, 91, 92, 94, 95, 96, 97], "famili": [3, 8, 27, 28, 33, 90, 96], "download": [3, 6, 86, 89, 92, 96, 97, 98], "meta": [3, 6, 18, 68, 69, 92, 94, 95], "llama": [3, 6, 18, 24, 29, 55, 56, 68, 69, 92, 94, 95, 96, 97], "8b": [3, 44, 45, 46, 92], "hf": [3, 6, 68, 92, 94, 95, 96], "token": [3, 6, 7, 8, 19, 24, 26, 27, 28, 29, 30, 31, 32, 33, 52, 56, 57, 58, 65, 66, 73, 93, 94, 95, 96, 97, 98], "access_token": 3, "pre": [3, 18, 89, 92], "train": [3, 5, 6, 8, 9, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 52, 59, 60, 68, 69, 72, 83, 88, 90, 92, 93, 94, 96, 97, 98], "can": [3, 4, 6, 7, 8, 9, 10, 12, 19, 24, 25, 26, 27, 28, 29, 31, 55, 56, 65, 67, 68, 70, 73, 80, 81, 84, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98], "hug": [3, 6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 60, 90, 93, 95, 96], "face": [3, 6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 60, 90, 93, 95, 96], "hub": [3, 6, 95], "follow": [3, 6, 8, 22, 24, 52, 60, 81, 88, 89, 93, 94, 95, 96, 97, 98], "command": [3, 8, 9, 70, 89, 92, 94, 95, 96, 97, 98], "2": [3, 6, 9, 23, 33, 52, 65, 68, 69, 82, 85, 92, 94, 95, 96, 97], "7b": [3, 6, 26, 27, 28, 31, 37, 40, 42, 48, 49, 68, 69, 92, 95, 96, 97, 98], "mini": [3, 51], "microsoft": [3, 51], "4k": [3, 51], "hf_token": 3, "ignor": [3, 6, 52, 53], "pattern": [3, 66], "ai": [3, 49, 52, 81, 92, 96], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 33, 52, 57, 60, 65, 66, 69, 80, 81, 82, 85, 92, 94, 95, 96, 97, 98], "googl": [3, 34], "2b": [3, 34], "offer": 5, "allow": [5, 25, 80, 98], "seamless": 5, "transit": 5, "between": [5, 6, 68, 94, 96, 97, 98], "interoper": [5, 6, 8, 90, 94, 98], "rest": [5, 92, 98], "ecosystem": [5, 6, 8, 90, 94, 96, 98], "For": [5, 6, 7, 8, 24, 25, 26, 27, 28, 29, 31, 52, 57, 70, 81, 84, 85, 89, 92, 93, 94, 95, 96, 97, 98], "comprehens": 5, "overview": [5, 7, 9, 95, 97, 98], "pleas": [5, 41, 42, 47, 50, 67, 73, 84, 89, 98], "see": [5, 6, 9, 18, 20, 29, 33, 41, 42, 47, 50, 54, 61, 67, 70, 73, 74, 81, 83, 84, 85, 89, 90, 92, 93, 94, 95, 96, 97, 98], "deep": [5, 6, 7, 8, 9, 90, 95, 96], "dive": [5, 6, 7, 8, 9, 90, 95, 96], "enabl": [5, 7, 8, 9, 25, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 62, 83, 85, 96, 97, 98], "work": [5, 6, 8, 70, 90, 94, 96, 98], "set": [5, 6, 7, 8, 9, 26, 27, 28, 30, 31, 32, 33, 56, 57, 64, 67, 71, 73, 84, 85, 90, 92, 93, 94, 95, 96, 97], "consumpt": [5, 25], "dure": [5, 6, 25, 26, 27, 28, 30, 32, 52, 54, 56, 57, 58, 59, 92, 94, 96, 97, 98], "provid": [5, 6, 7, 8, 10, 15, 20, 24, 25, 26, 33, 57, 70, 73, 81, 90, 92, 93, 94, 95, 96], "debug": [5, 6, 7, 8], "finetun": [5, 6, 7, 8, 38, 39, 40, 45, 46, 77, 88, 90, 95, 96], "job": [5, 9, 85, 95], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 90, 95, 96], "walk": [6, 8, 80, 90, 92, 93, 94, 95, 98], "you": [6, 7, 8, 9, 10, 17, 18, 24, 26, 27, 28, 31, 70, 80, 81, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98], "through": [6, 7, 8, 9, 53, 90, 92, 93, 94, 95, 98], "design": [6, 8], "behavior": [6, 92, 93], "associ": [6, 7, 8, 94, 97], "util": [6, 7, 8, 9, 10, 25, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 90, 94, 95, 96, 98], "what": [6, 7, 9, 18, 20, 30, 32, 88, 92, 93, 94, 95, 96], "cover": [6, 7, 8, 9, 92, 94, 98], "how": [6, 7, 8, 9, 67, 84, 88, 92, 93, 94, 95, 96, 98], "we": [6, 7, 8, 9, 26, 27, 28, 31, 52, 54, 56, 57, 62, 65, 68, 69, 72, 90, 92, 93, 94, 95, 96, 97, 98], "them": [6, 7, 24, 25, 26, 31, 33, 53, 59, 65, 92, 93, 94, 97, 98], "scenario": [6, 25], "full": [6, 7, 8, 41, 42, 47, 50, 65, 90, 96, 97], "compos": 6, "compon": [6, 8, 12, 83, 90, 93, 95, 97, 98], "which": [6, 8, 25, 26, 27, 28, 30, 32, 38, 39, 40, 45, 46, 48, 52, 56, 57, 58, 60, 65, 68, 69, 72, 78, 81, 84, 90, 92, 93, 94, 95, 96, 97, 98], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 21, 22, 24, 26, 29, 31, 59, 63, 64, 65, 68, 69, 85, 92, 93, 94, 95, 96, 97], "recip": [6, 7, 9, 10, 11, 53, 68, 69, 90, 92, 93, 94, 96, 98], "evalu": [6, 8, 88, 90, 95, 97, 98], "gener": [6, 8, 13, 16, 21, 24, 26, 33, 65, 85, 86, 88, 92, 93, 97, 98], "each": [6, 8, 14, 17, 25, 38, 39, 40, 45, 46, 48, 52, 56, 57, 65, 66, 85, 90, 93, 94, 95, 96, 97], "make": [6, 7, 8, 9, 52, 58, 90, 94, 95, 96, 97, 98], "easi": [6, 8, 90, 97], "understand": [6, 7, 8, 88, 90, 92, 93, 97, 98], "extend": [6, 8, 90], "befor": [6, 23, 26, 57, 58, 62, 68, 94], "let": [6, 7, 9, 92, 93, 94, 95, 96, 97, 98], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 20, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 45, 46, 48, 52, 56, 57, 58, 59, 61, 63, 66, 67, 68, 69, 71, 73, 80, 83, 84, 90, 92, 93, 95, 97, 98], "defin": [6, 7, 8, 53, 61, 62, 63, 95, 97], "some": [6, 7, 15, 63, 64, 88, 90, 92, 94, 95, 97, 98], "concept": [6, 94, 95], "In": [6, 7, 8, 24, 56, 62, 67, 80, 81, 92, 94, 96, 97, 98], "ll": [6, 7, 8, 66, 90, 92, 93, 94, 95, 96, 98], "talk": 6, "about": [6, 8, 68, 81, 90, 92, 94, 95, 96, 97, 98], "take": [6, 7, 8, 10, 53, 54, 59, 68, 70, 71, 92, 93, 94, 95, 96, 97, 98], "close": [6, 8, 78, 79, 80, 81, 97], "look": [6, 7, 8, 80, 89, 92, 93, 94, 95, 96, 97], "veri": [6, 25, 57, 94], "simpli": [6, 7, 92, 93, 94, 96, 98], "dictat": 6, "state_dict": [6, 59, 68, 69, 97, 98], "store": [6, 25, 78, 81, 97, 98], "file": [6, 7, 8, 9, 10, 11, 65, 66, 68, 69, 70, 78, 81, 83, 87, 90, 91, 92, 93, 94, 95, 96, 97, 98], "disk": [6, 78], "weight": [6, 8, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 59, 61, 62, 68, 69, 81, 88, 92, 94, 95, 96, 97, 98], "string": [6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 61, 65, 66, 71, 72], "kei": [6, 7, 9, 24, 26, 31, 33, 52, 54, 57, 64, 68, 94, 95, 97, 98], "identifi": 6, "state": [6, 8, 59, 63, 64, 68, 69, 94, 96, 97, 98], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 19, 21, 22, 24, 26, 29, 31, 59, 63, 64, 68, 69, 76], "If": [6, 7, 12, 13, 16, 17, 20, 21, 23, 24, 26, 27, 28, 30, 31, 32, 33, 52, 59, 62, 68, 69, 71, 72, 73, 76, 80, 81, 85, 89, 92, 93, 94, 95, 96, 97], "don": [6, 7, 8, 81, 85, 92, 94, 95, 96, 98], "t": [6, 7, 8, 33, 66, 72, 81, 85, 92, 94, 95, 96, 98], "match": [6, 24, 26, 31, 33, 89, 94, 96, 97], "up": [6, 8, 9, 26, 27, 28, 31, 92, 93, 95, 96, 97, 98], "exactli": 6, "those": [6, 97], "definit": [6, 97], "either": [6, 68, 84, 97, 98], "run": [6, 7, 9, 11, 53, 54, 57, 59, 68, 69, 80, 81, 89, 90, 92, 95, 96, 97, 98], "explicit": 6, "error": [6, 7, 23, 68, 85], "load": [6, 8, 24, 25, 26, 68, 69, 70, 80, 92, 94, 96, 97], "rais": [6, 10, 12, 20, 23, 29, 33, 52, 57, 68, 69, 72, 76, 81, 85], "an": [6, 7, 8, 9, 10, 13, 19, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 52, 57, 61, 63, 64, 67, 68, 69, 73, 81, 90, 92, 93, 94, 95, 96, 97, 98], "except": [6, 19, 20, 93], "wors": 6, "silent": [6, 53], "succe": 6, "infer": [6, 18, 24, 52, 54, 56, 57, 58, 88, 92, 94, 95, 96, 98], "expect": [6, 7, 10, 13, 16, 17, 21, 24, 26, 29, 31, 56, 81, 92, 93, 97], "addit": [6, 7, 8, 10, 24, 26, 29, 31, 67, 68, 69, 72, 73, 76, 78, 80, 81, 84, 90, 92, 95, 97], "line": [6, 8, 70, 95, 96], "need": [6, 7, 8, 9, 17, 24, 33, 52, 53, 57, 80, 81, 89, 92, 93, 94, 95, 96, 97, 98], "shape": [6, 52, 54, 56, 57, 58, 62], "valu": [6, 7, 22, 33, 34, 35, 36, 37, 43, 44, 49, 52, 54, 55, 57, 60, 68, 70, 78, 79, 80, 81, 85, 95, 96, 97], "two": [6, 7, 23, 90, 94, 95, 96, 97, 98], "popular": [6, 90, 93, 94], "llama2": [6, 7, 8, 10, 18, 22, 24, 26, 27, 28, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 53, 57, 58, 65, 88, 90, 95, 96], "offici": [6, 18, 92, 95, 96], "implement": [6, 8, 24, 26, 27, 28, 29, 30, 31, 32, 33, 53, 55, 56, 60, 61, 62, 68, 80, 90, 97, 98], "when": [6, 7, 8, 11, 19, 25, 57, 59, 60, 73, 80, 94, 96, 97, 98], "websit": 6, "get": [6, 7, 8, 9, 24, 65, 72, 74, 75, 89, 90, 92, 93, 94, 95, 97], "access": [6, 7, 8, 25, 68, 94, 95], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 52, 68, 69, 92, 93, 94, 95, 96, 97, 98], "pth": [6, 94, 96], "inspect": [6, 94, 97, 98], "content": [6, 19, 22, 24, 65, 92, 93], "easili": [6, 7, 90, 93, 97, 98], "torch": [6, 25, 54, 57, 59, 60, 71, 72, 76, 83, 84, 85, 94, 95, 96, 97, 98], "import": [6, 7, 10, 29, 80, 81, 92, 93, 94, 95, 97, 98], "consolid": [6, 96], "00": [6, 87, 91, 95, 96], "mmap": [6, 94], "true": [6, 7, 19, 26, 27, 28, 29, 30, 32, 41, 42, 47, 50, 59, 65, 66, 67, 68, 69, 73, 76, 80, 92, 93, 94, 96, 97, 98], "weights_onli": 6, "map_loc": [6, 94], "cpu": [6, 8, 59, 72, 89, 94, 98], "tensor": [6, 52, 53, 54, 55, 56, 57, 58, 59, 62, 68, 78, 79, 80, 81, 82, 97, 98], "item": 6, "print": [6, 9, 25, 27, 28, 30, 32, 33, 65, 92, 93, 95, 97, 98], "f": [6, 9, 27, 28, 30, 32, 92, 94, 97, 98], "tok_embed": [6, 57], "size": [6, 8, 10, 27, 28, 30, 32, 52, 54, 55, 56, 57, 58, 75, 90, 93, 94, 95, 96, 97], "32000": [6, 10, 97], "4096": [6, 10, 26, 27, 28, 31, 52, 56, 97], "len": [6, 25, 27, 28, 30, 32, 57], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 23, 24, 25, 26, 30, 32, 33, 38, 39, 40, 45, 46, 55, 56, 59, 60, 65, 66, 67, 68, 70, 71, 72, 74, 81, 83, 85, 89, 90, 92, 93, 94, 95, 96, 97, 98], "contain": [6, 19, 25, 52, 54, 56, 57, 58, 61, 63, 64, 65, 66, 68, 69, 70, 80, 82, 92, 94, 96, 97], "includ": [6, 7, 8, 14, 17, 62, 68, 69, 70, 90, 92, 94, 95, 96, 97, 98], "input": [6, 13, 14, 17, 24, 26, 27, 28, 29, 30, 31, 33, 52, 53, 55, 56, 57, 58, 62, 65, 68, 82, 85, 92, 93, 97, 98], "embed": [6, 52, 54, 55, 56, 57, 73, 92, 96], "tabl": [6, 92, 98], "call": [6, 10, 19, 53, 59, 70, 78, 79, 80, 81, 92, 97, 98], "layer": [6, 8, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 57, 58, 62, 67, 73, 90, 96, 97, 98], "have": [6, 7, 10, 52, 54, 61, 70, 73, 80, 83, 89, 92, 93, 94, 95, 96, 97, 98], "dim": [6, 52, 53, 55, 56, 57, 58], "most": [6, 7, 66, 92, 95, 97, 98], "within": [6, 7, 10, 24, 33, 53, 80, 85, 94, 96, 97, 98], "default": [6, 7, 15, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 52, 53, 55, 56, 57, 58, 59, 60, 62, 65, 66, 68, 69, 70, 72, 78, 81, 82, 83, 85, 89, 94, 96, 97, 98], "everi": [6, 8, 53, 80, 89, 98], "repo": [6, 68, 69, 94], "first": [6, 7, 10, 23, 57, 66, 68, 70, 88, 90, 92, 94, 96, 97, 98], "big": [6, 94], "split": [6, 92, 93, 94], "across": [6, 8, 25, 68, 80, 85, 94, 96], "bin": [6, 94], "To": [6, 7, 8, 9, 68, 89, 90, 92, 93, 94, 95, 96, 97, 98], "correctli": [6, 8, 12, 68, 89, 92, 95, 98], "piec": 6, "one": [6, 8, 23, 53, 65, 92, 93, 94, 95, 96, 98], "pytorch_model": [6, 94], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 19, 52, 56, 57, 58, 62, 63, 65, 69, 70, 72, 73, 94, 95, 96, 97, 98], "doe": [6, 20, 24, 51, 61, 68, 70, 92, 94], "fewer": [6, 52], "sinc": [6, 7, 10, 53, 68, 92, 94, 96], "instead": [6, 8, 29, 31, 53, 54, 62, 94, 96, 97], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 21, 24, 26, 29, 31, 33, 61, 64, 66, 68, 69, 70, 71, 78, 79, 80, 81, 92, 94, 96], "caus": [6, 65], "try": [6, 7, 92, 94, 95, 96, 98], "same": [6, 7, 38, 39, 40, 45, 46, 52, 54, 58, 65, 70, 73, 81, 92, 94, 96, 97, 98], "As": [6, 7, 8, 9, 62, 90, 94, 96, 98], "re": [6, 7, 66, 90, 92, 94, 95, 96, 97], "care": [6, 53, 68, 94, 96, 97], "end": [6, 8, 19, 25, 66, 88, 90, 92, 96, 97], "number": [6, 8, 24, 26, 27, 28, 29, 31, 33, 52, 54, 57, 60, 68, 69, 75, 85, 95, 97], "just": [6, 13, 90, 92, 93, 95, 96, 97], "save": [6, 8, 9, 59, 68, 69, 73, 81, 88, 92, 94, 96, 97], "less": [6, 33, 94, 95, 96, 98], "prone": 6, "manag": [6, 25, 83, 92], "invari": 6, "accept": [6, 7, 33, 65, 67, 95, 98], "multipl": [6, 7, 8, 19, 24, 25, 62, 78, 79, 80, 81, 93, 95, 96], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 92, 93, 94], "worri": [6, 92, 95], "explicitli": [6, 61, 90, 97], "convert": [6, 22, 24, 68, 82, 92, 94, 98], "time": [6, 65, 78, 80, 92, 94, 96, 98], "produc": [6, 98], "back": [6, 23, 68, 97, 98], "origin": [6, 27, 28, 59, 62, 92, 93, 94, 96, 97, 98], "form": [6, 7, 8, 23], "One": [6, 94], "advantag": [6, 97], "being": [6, 68, 69, 71, 98], "should": [6, 7, 8, 14, 17, 18, 19, 20, 22, 29, 31, 38, 39, 40, 45, 46, 48, 52, 53, 61, 67, 70, 78, 79, 80, 81, 89, 90, 93, 94, 95, 96, 97, 98], "abl": [6, 8, 93, 94, 95, 96], "post": [6, 98], "tool": [6, 94, 95], "quantiz": [6, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 62, 88, 95, 98], "eval": [6, 88, 90], "without": [6, 7, 9, 89, 90, 92, 93, 94, 97], "code": [6, 8, 57, 86, 90, 93, 95], "chang": [6, 7, 9, 13, 89, 93, 94, 95, 96, 97, 98], "OR": 6, "convers": [6, 14, 15, 18, 20, 22, 23, 24, 29, 33, 68, 90, 92, 93, 94, 96, 97, 98], "script": [6, 9, 94, 95, 96], "wai": [6, 7, 24, 92, 93, 94, 95, 96], "surround": [6, 8, 90], "load_checkpoint": [6, 8, 68, 69], "save_checkpoint": [6, 8, 9, 68, 69], "method": [6, 7, 8, 9, 11, 24, 26, 27, 28, 29, 30, 31, 32, 33, 59, 61, 63, 70, 89, 90, 94, 96, 97, 98], "convertor": 6, "avail": [6, 8, 70, 71, 72, 90, 94, 96, 97], "here": [6, 7, 9, 15, 30, 55, 56, 92, 93, 94, 95, 96, 97, 98], "three": [6, 8, 95], "hfcheckpoint": 6, "read": [6, 68, 69, 90], "write": [6, 8, 68, 69, 78, 92, 93, 95], "compat": [6, 68], "transform": [6, 8, 24, 26, 38, 39, 40, 45, 46, 48, 57, 58, 60, 84, 97], "framework": [6, 8, 90], "mention": [6, 94, 98], "assum": [6, 13, 16, 17, 21, 26, 31, 60, 63, 66, 72, 94, 97], "checkpoint_dir": [6, 7, 68, 69, 94, 96], "necessari": [6, 33, 78, 79, 80, 81, 92, 97], "json": [6, 68, 83, 94], "easiest": [6, 94, 95], "sure": [6, 7, 94, 95, 96, 97, 98], "everyth": [6, 8, 70, 90, 95], "flow": [6, 24, 26, 98], "By": [6, 96, 97, 98], "safetensor": 6, "output": [6, 17, 27, 28, 30, 33, 38, 39, 40, 45, 46, 48, 52, 53, 55, 56, 57, 58, 62, 64, 73, 79, 83, 89, 92, 93, 94, 95, 96, 97, 98], "dir": [6, 81, 89, 94, 95, 96], "output_dir": [6, 7, 68, 69, 83, 94, 96, 97, 98], "argument": [6, 7, 10, 17, 24, 26, 29, 31, 33, 41, 42, 47, 50, 52, 67, 70, 73, 76, 78, 80, 81, 84, 92, 96, 97], "snippet": [6, 93], "explain": 6, "setup": [6, 7, 8, 57, 84, 94, 97, 98], "_component_": [6, 7, 9, 10, 29, 92, 93, 94, 96, 97], "fullmodelhfcheckpoint": [6, 94], "directori": [6, 7, 68, 69, 78, 80, 81, 94, 95, 96], "sort": [6, 68], "id": [6, 24, 26, 27, 28, 29, 31, 33, 65, 66, 68, 82, 92, 94], "so": [6, 7, 68, 70, 89, 90, 92, 94, 95, 96, 97, 98], "order": [6, 8, 68, 80, 81, 95], "matter": [6, 68, 97], "checkpoint_fil": [6, 7, 9, 68, 69, 94, 96, 97, 98], "restart": 6, "previou": [6, 68, 69], "more": [6, 7, 8, 29, 33, 54, 56, 67, 70, 81, 83, 84, 85, 90, 93, 94, 95, 96, 97, 98], "next": [6, 96, 98], "section": [6, 8, 88, 94, 96, 98], "recipe_checkpoint": [6, 68, 69], "null": [6, 7], "usual": [6, 56, 68, 81, 94, 97], "model_typ": [6, 68, 69, 94, 96], "resume_from_checkpoint": [6, 68, 69], "fals": [6, 7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 62, 65, 66, 68, 69, 83, 92, 93, 94, 96, 97, 98], "requir": [6, 7, 25, 33, 68, 80, 81, 85, 89, 92, 93, 95, 98], "param": [6, 8, 38, 39, 40, 45, 46, 62, 63, 64, 68, 97, 98], "directli": [6, 7, 8, 10, 29, 31, 67, 68, 93, 94, 95, 96, 97, 98], "ensur": [6, 7, 12, 23, 33, 52, 68, 72, 90, 95], "out": [6, 7, 8, 24, 26, 27, 28, 29, 30, 32, 68, 69, 88, 90, 92, 94, 95, 96, 97, 98], "case": [6, 8, 9, 19, 52, 68, 72, 78, 84, 90, 92, 93, 94, 96, 97, 98], "discrep": [6, 68], "along": [6, 96, 97], "detail": [6, 29, 33, 54, 67, 73, 83, 85, 94, 95, 96, 97, 98], "found": [6, 7, 9, 55, 56, 97, 98], "metacheckpoint": 6, "github": [6, 10, 38, 39, 40, 45, 46, 52, 55, 56, 60, 89, 95], "repositori": [6, 18, 94, 95], "fullmodelmetacheckpoint": [6, 96], "torchtunecheckpoint": 6, "perform": [6, 53, 90, 92, 94, 96, 98], "current": [6, 51, 52, 56, 57, 58, 69, 73, 75, 78, 80, 85, 94, 95, 96], "test": [6, 7, 8, 90, 92], "complet": [6, 8, 92, 93, 94, 95, 96], "written": [6, 7, 8, 68, 69, 78, 79, 80, 81, 90], "begin": [6, 65, 66, 92, 96, 98], "partit": [6, 98], "ha": [6, 61, 63, 65, 93, 94, 95, 96, 97, 98], "standard": [6, 79, 90, 92, 94, 96], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 94], "inform": [6, 19, 81, 84, 90, 94, 95, 96], "subsequ": [6, 8], "recipe_st": [6, 68, 69], "pt": [6, 9, 68, 69, 94, 96], "epoch": [6, 8, 9, 60, 68, 69, 92, 94, 95, 96], "optim": [6, 7, 8, 25, 51, 60, 92, 94, 95, 96, 97, 98], "etc": [6, 8, 68, 95], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 19, 57, 61, 65, 68, 83, 85, 92, 93, 94, 97, 98], "updat": [6, 7, 8, 89, 92, 94, 95, 96, 97, 98], "hf_model_0001_0": [6, 94], "hf_model_0002_0": [6, 94], "both": [6, 25, 94, 97, 98], "adapt": [6, 61, 62, 63, 64, 68, 69, 92, 94, 97, 98], "merg": [6, 10, 68, 94, 96, 98], "would": [6, 7, 9, 57, 89, 92, 93, 94, 97, 98], "our": [6, 8, 90, 92, 93, 94, 95, 97, 98], "tutori": [6, 84, 90, 92, 93, 94, 95, 96, 97, 98], "primari": [6, 7, 8, 95], "want": [6, 7, 8, 9, 10, 24, 89, 92, 94, 95, 96, 97], "resum": [6, 8, 60, 68, 69, 98], "initi": [6, 8, 11, 25, 34, 35, 36, 37, 43, 44, 49, 76, 95, 97, 98], "frozen": [6, 97, 98], "base": [6, 10, 26, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 56, 60, 62, 64, 68, 70, 73, 78, 88, 92, 94, 95, 96, 97, 98], "well": [6, 7, 8, 90, 93, 94, 96, 98], "learnt": [6, 92, 94], "someth": [6, 8, 9, 92, 94], "NOT": 6, "refer": [6, 7, 8, 55, 56, 90, 97], "adapter_checkpoint": [6, 68, 69], "adapter_0": [6, 94], "now": [6, 65, 92, 93, 94, 95, 96, 97, 98], "knowledg": 6, "creat": [6, 7, 10, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 54, 60, 67, 68, 69, 78, 80, 92, 93, 94, 96, 98], "simpl": [6, 8, 88, 95, 97, 98], "forward": [6, 8, 52, 53, 55, 56, 57, 58, 62, 96, 97, 98], "13b": [6, 35, 38, 41], "modeltyp": [6, 68, 69], "llama2_13b": [6, 38], "right": [6, 68, 94, 96, 97], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 97], "successfulli": [6, 95], "vocab": [6, 10, 57, 96], "70": [6, 43], "x": [6, 52, 53, 55, 56, 57, 58, 62, 97, 98], "randint": 6, "0": [6, 8, 38, 39, 40, 41, 42, 52, 57, 60, 62, 65, 80, 81, 82, 85, 87, 91, 92, 94, 95, 96, 97, 98], "no_grad": 6, "6": [6, 55, 82, 94, 98], "3989": 6, "9": [6, 94, 98], "0531": 6, "3": [6, 51, 66, 70, 74, 82, 92, 94, 95, 96, 98], "2375": 6, "5": [6, 60, 82, 83, 94, 95, 96], "2822": 6, "4": [6, 33, 52, 82, 90, 94, 96, 97, 98], "4872": 6, "7469": 6, "8": [6, 27, 28, 30, 32, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 94, 97, 98], "6737": 6, "11": [6, 94, 96, 98], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 82], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 24, 26, 31, 33, 81, 92, 93, 94, 95, 96, 97], "find": [6, 8, 9, 94, 95, 97], "list": [6, 7, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 61, 62, 65, 66, 68, 69, 70, 74, 77, 82, 92, 93, 95, 96], "builder": [6, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 92, 93, 98], "hope": 6, "deeper": [6, 95], "insight": [6, 94], "happi": [6, 94], "thi": [7, 8, 9, 10, 19, 24, 25, 26, 27, 28, 29, 31, 33, 51, 52, 53, 56, 57, 58, 59, 60, 61, 65, 67, 68, 69, 70, 71, 72, 78, 80, 81, 83, 84, 85, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98], "guid": [7, 9, 90, 92, 93, 95, 97], "pars": [7, 10, 66, 70, 92, 95], "effect": 7, "cli": [7, 9, 11, 89, 94, 95], "prerequisit": [7, 92, 93, 94, 95, 96, 97, 98], "Be": [7, 92, 94, 95, 96, 97, 98], "familiar": [7, 92, 94, 95, 96, 97, 98], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 92, 93, 95], "instal": [7, 9, 80, 81, 88, 94, 95, 96, 97, 98], "fundament": 7, "There": [7, 14, 23, 92, 94, 95, 96, 97], "entri": [7, 8, 95], "point": [7, 8, 22, 93, 94, 95, 96, 97, 98], "locat": [7, 96, 97, 98], "thei": [7, 8, 19, 25, 57, 70, 92, 93, 97], "truth": [7, 94, 96], "reproduc": 7, "overridden": [7, 53, 70], "quick": [7, 25], "experiment": 7, "modifi": [7, 8, 9, 59, 90, 94, 96, 97, 98], "serv": [7, 67, 93, 97], "particular": [7, 24, 25, 33, 67, 93, 97, 98], "seed": [7, 8, 9, 85, 95], "shuffl": 7, "devic": [7, 8, 71, 72, 92, 94, 95, 96, 97], "cuda": [7, 71, 72, 89, 94, 98], "dtype": [7, 8, 54, 59, 72, 77, 94, 98], "fp32": [7, 98], "enable_fsdp": 7, "mani": [7, 93, 94], "object": [7, 10, 14, 15, 18, 20, 52, 67, 92, 93], "keyword": [7, 10, 24, 26, 29, 31, 33, 59, 92], "loss": [7, 8, 26, 27, 28, 30, 32, 95, 97, 98], "function": [7, 8, 10, 11, 24, 52, 53, 59, 67, 71, 75, 85, 90, 92, 93, 98], "exampl": [7, 8, 9, 10, 11, 15, 18, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 52, 61, 65, 67, 68, 69, 80, 81, 82, 86, 87, 89, 91, 92, 93, 94, 96, 97, 98], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 94], "path": [7, 8, 9, 10, 24, 26, 27, 28, 29, 30, 31, 32, 33, 65, 66, 68, 69, 70, 83, 92, 94, 96, 97], "normal": [7, 24, 55, 57, 58, 65, 92, 93, 97, 98], "python": [7, 66, 70, 74, 81, 85, 86, 94], "alpaca_dataset": [7, 27, 93], "custom": [7, 8, 24, 26, 29, 31, 84, 90, 94, 95, 96, 97], "train_on_input": [7, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 93], "onc": [7, 94, 95, 96, 97, 98], "ve": [7, 54, 66, 92, 93, 94, 96, 97], "instanc": [7, 10, 25, 53, 59, 63, 64, 97], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 29, 94, 98], "under": [7, 94, 96, 98], "preced": [7, 10, 93, 96, 97], "actual": [7, 9, 24, 92], "throw": 7, "notic": [7, 92, 93, 97], "miss": [7, 97], "posit": [7, 10, 52, 56, 57, 58, 96], "anoth": [7, 94], "handl": [7, 11, 19, 25, 65, 92, 94, 97, 98], "def": [7, 8, 9, 11, 67, 92, 93, 97, 98], "dictconfig": [7, 8, 10, 11, 12, 81], "arg": [7, 10, 57, 59, 61, 70, 79], "tupl": [7, 10, 25, 33, 59, 65, 66, 67, 70, 75, 82], "kwarg": [7, 10, 59, 61, 70, 76, 78, 79, 80, 81, 84], "str": [7, 10, 13, 16, 17, 19, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 59, 61, 62, 63, 64, 65, 66, 68, 69, 70, 71, 72, 74, 77, 78, 79, 80, 81, 83, 85, 92], "mean": [7, 55, 92, 95, 97], "pass": [7, 10, 24, 25, 26, 29, 31, 52, 53, 59, 67, 72, 73, 76, 80, 81, 84, 92, 97, 98], "add": [7, 9, 24, 66, 70, 93, 94, 96, 97, 98], "d": [7, 19, 52, 57, 58, 66, 92, 93, 97], "llama2_token": [7, 94], "tmp": [7, 92, 95, 96], "option": [7, 8, 13, 16, 17, 21, 24, 26, 29, 31, 33, 38, 39, 40, 45, 46, 48, 52, 56, 57, 58, 59, 65, 66, 68, 69, 71, 72, 74, 78, 81, 83, 85, 89, 90, 94], "bool": [7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 59, 62, 65, 66, 67, 68, 69, 73, 76, 80, 83, 84, 98], "max_seq_len": [7, 10, 24, 26, 27, 28, 29, 31, 33, 52, 54, 56, 57, 65, 66, 92, 93], "int": [7, 9, 24, 25, 26, 27, 28, 29, 31, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 54, 55, 56, 57, 60, 62, 65, 66, 67, 68, 69, 73, 75, 78, 79, 80, 81, 82, 84, 85, 92, 93, 97, 98], "512": [7, 27, 28, 93, 98], "instructdataset": [7, 27, 28, 30, 31, 32, 93], "alreadi": [7, 76, 89, 94, 97], "overwrit": [7, 89], "duplic": [7, 8, 90], "sometim": 7, "than": [7, 23, 33, 52, 67, 92, 94, 95, 96, 97, 98], "resolv": [7, 95], "alpaca": [7, 13, 27, 28, 38, 39, 40, 45, 46, 93], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 78, 79, 80, 81], "disklogg": 7, "log_dir": [7, 78, 80, 81], "conveni": [7, 8], "verifi": [7, 71, 72, 73, 92, 95, 97], "properli": 7, "experi": [7, 81, 88, 90, 92, 96, 97], "wa": [7, 92, 94, 96, 97, 98], "cp": [7, 89, 92, 94, 95, 96], "7b_lora_single_devic": [7, 94, 95, 97, 98], "my_config": 7, "discuss": [7, 95, 97], "guidelin": 7, "while": [7, 8, 38, 39, 40, 45, 46, 53, 90, 94, 98], "mai": [7, 9, 73, 83, 92, 93, 95, 97], "tempt": 7, "put": [7, 8, 95, 97], "much": [7, 94, 96, 97, 98], "give": [7, 97], "maximum": [7, 24, 26, 27, 28, 29, 31, 33, 52, 54, 56, 57, 66], "flexibl": [7, 25, 93], "switch": 7, "encourag": [7, 97], "clariti": 7, "significantli": 7, "easier": [7, 94, 95], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 98], "expos": [7, 8, 92, 93, 95], "parent": 7, "modul": [7, 10, 33, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 73, 84, 85, 92, 95, 97, 98], "__init__": [7, 8, 97, 98], "py": [7, 10, 38, 39, 40, 45, 46, 52, 54, 55, 56, 60, 94, 96], "guarante": 7, "stabil": [7, 90, 98], "underscor": 7, "_alpaca": 7, "collect": [7, 95], "differ": [7, 9, 24, 25, 26, 65, 90, 92, 94, 96, 97, 98], "itself": 7, "via": [7, 9, 29, 62, 97, 98], "pair": [7, 82, 93], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 92, 94, 95, 96, 97, 98], "checkpoint": [7, 8, 59, 66, 68, 69, 81, 84, 90, 96, 97, 98], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 29, 31, 33, 52, 53, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 68, 69, 70, 78, 79, 80, 81, 92, 93, 95, 97, 98], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 52, 56, 57, 58], "core": [8, 90, 93, 95, 98], "i": [8, 18, 20, 59, 64, 66, 93, 94, 96, 98], "structur": [8, 14, 15, 18, 20, 24, 92, 93, 94], "new": [8, 49, 78, 80, 92, 94, 95, 96, 97, 98], "user": [8, 14, 15, 18, 19, 20, 22, 23, 24, 52, 65, 92, 93, 95], "thought": [8, 90, 95, 98], "target": [8, 90], "pipelin": [8, 90], "llm": [8, 88, 90, 93, 94, 97], "eg": [8, 57, 68, 90], "meaning": [8, 90, 94], "featur": [8, 9, 89, 90, 94, 95], "fsdp": [8, 67, 73, 90, 95, 96], "activ": [8, 53, 84, 90, 98], "gradient": [8, 90, 94, 96, 97, 98], "accumul": [8, 90], "mix": [8, 94], "precis": [8, 59, 72, 90, 95, 98], "appli": [8, 24, 26, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 55, 56, 57, 58, 84, 90, 98], "given": [8, 10, 17, 23, 62, 71, 72, 90, 97], "complex": 8, "becom": [8, 89, 93], "harder": 8, "anticip": 8, "architectur": [8, 18, 20, 57, 93], "methodolog": 8, "reason": [8, 94], "possibl": [8, 24, 29, 93], "trade": 8, "off": [8, 65, 94], "memori": [8, 25, 26, 27, 28, 31, 59, 73, 88, 90, 93, 94, 95, 96], "vs": [8, 95], "qualiti": [8, 94, 97], "believ": 8, "best": [8, 92], "suit": [8, 95], "specif": [8, 10, 73, 92, 93, 94, 98], "b": [8, 52, 56, 57, 58, 62, 81, 97, 98], "fit": [8, 24, 26, 27, 28, 31], "solut": 8, "result": [8, 65, 94, 96, 97, 98], "meant": [8, 59], "depend": [8, 9, 13, 94, 97, 98], "level": [8, 74, 90, 98], "expertis": 8, "routin": 8, "yourself": [8, 96, 97], "exist": [8, 89, 93, 94, 95, 96, 98], "ad": [8, 65, 92, 97, 98], "ones": 8, "modular": [8, 90], "build": [8, 29, 31, 90, 96, 97], "block": [8, 38, 39, 40, 45, 46, 48, 90], "wandb": [8, 9, 81, 95], "log": [8, 74, 78, 79, 80, 81, 94, 95, 96, 98], "fulli": [8, 25], "nativ": [8, 88, 90, 97, 98], "pytorch": [8, 57, 59, 67, 80, 83, 84, 85, 88, 89, 90, 96, 97, 98], "correct": [8, 16, 30, 55, 56, 57, 71, 90, 92, 93], "numer": [8, 90], "pariti": [8, 90], "verif": 8, "extens": [8, 90], "comparison": [8, 97, 98], "benchmark": [8, 85, 90, 94, 96, 97], "limit": 8, "hidden": [8, 53], "behind": 8, "100": [8, 26, 27, 28, 30, 32, 33, 82, 83, 97, 98], "flag": [8, 26, 27, 28, 30, 32, 67, 73, 98], "prefer": [8, 90, 93], "over": [8, 60, 70, 90, 93, 94, 96, 97, 98], "unnecessari": 8, "abstract": [8, 14, 17, 90, 95, 98], "No": [8, 90], "inherit": [8, 70, 90], "go": [8, 18, 20, 65, 90, 93, 94, 95, 98], "upon": [8, 25, 96], "figur": [8, 97, 98], "spectrum": 8, "decid": 8, "interact": [8, 88, 95], "start": [8, 9, 25, 66, 89, 90, 92, 93, 94, 95], "paradigm": 8, "consist": [8, 95], "configur": [8, 26, 27, 28, 29, 30, 31, 32, 33, 58, 90, 92, 95, 96, 97, 98], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 76, 78, 79, 80, 81, 82, 83, 84, 85, 88, 90, 92, 93, 94, 95, 96, 97, 98], "overrid": [8, 11, 94, 95, 96, 98], "togeth": [8, 81, 95, 97], "valid": [8, 23, 89, 94, 95], "environ": [8, 89, 94, 95], "logic": [8, 68, 90, 95, 97], "api": [8, 9, 41, 42, 47, 50, 92, 94, 95, 96, 98], "closer": [8, 97], "monolith": [8, 90], "trainer": [8, 75], "A": [8, 9, 22, 25, 59, 62, 65, 66, 67, 68, 70, 82, 87, 88, 91, 92, 94, 97, 98], "wrapper": [8, 65, 66, 97], "around": [8, 24, 65, 66, 83, 92, 94, 97, 98], "extern": 8, "primarili": [8, 25, 97], "eleutherai": [8, 90, 97], "har": [8, 90, 97], "control": [8, 26, 27, 28, 30, 32, 85, 94], "multi": [8, 24, 52, 96], "stage": 8, "distil": 8, "oper": [8, 25, 83, 85], "turn": [8, 19, 23, 24, 66, 92], "dataload": [8, 27, 28, 30, 32], "applic": [8, 52, 68, 69, 81], "clean": [8, 9, 27], "after": [8, 54, 55, 78, 79, 80, 81, 92, 98], "process": [8, 9, 59, 85, 95, 98], "group": [8, 52, 78, 79, 80, 81, 96], "init_process_group": [8, 76], "backend": 8, "gloo": 8, "els": [8, 70, 81, 90, 98], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 25, 70, 73, 93, 95, 96, 97], "stuff": 8, "carri": 8, "relev": [8, 19, 94, 97], "interfac": [8, 14, 17, 25, 61], "metric": [8, 95], "logger": [8, 74, 78, 79, 80, 81, 95], "self": [8, 9, 38, 39, 40, 45, 46, 48, 52, 57, 58, 61, 93, 97, 98], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 67, 73, 83, 84, 92], "_model": 8, "_setup_model": 8, "_token": [8, 93], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 98], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 85, 96], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 27, 28, 30, 32, 52, 54, 56, 57, 58, 65, 82, 90, 93, 95, 96, 97], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 24, 26, 27, 28, 29, 31, 33, 82], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 78, 79, 80, 81], "step": [8, 57, 60, 66, 78, 79, 80, 81, 83, 88, 94, 97, 98], "learn": [8, 25, 60, 90, 92, 93, 95, 96, 97, 98], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 20, 21, 23, 24, 26, 29, 31, 52, 56, 57, 58, 64, 65, 66, 68, 69, 71, 72, 74, 78, 79, 80, 81, 84, 85, 92, 94], "fullfinetunerecip": 8, "direct": [8, 89], "wandblogg": [9, 97, 98], "workspac": 9, "seen": [9, 97, 98], "screenshot": 9, "below": [9, 56, 67, 93, 96, 97, 98], "packag": [9, 80, 81, 89], "pip": [9, 80, 81, 89, 94, 96], "Then": [9, 95], "login": [9, 81, 94], "built": [9, 89, 92, 93, 95, 98], "project": [9, 38, 39, 40, 45, 46, 48, 52, 53, 73, 81, 88, 97, 98], "grab": [9, 96], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 92], "exit": [9, 89], "resourc": [9, 78, 79, 80, 81], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 31, 33, 92, 93, 94], "desir": [9, 24, 92], "suggest": 9, "approach": [9, 25, 93], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 94], "_output_dir": [9, 68, 69], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 19, 22, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 62, 63, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 83, 84, 94, 97, 98], "descript": [9, 29, 33], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 19, 22, 24, 27, 28, 30, 32], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 24, 26, 29, 31, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 50, 51, 52, 55, 56, 60, 67, 68, 69, 70, 74, 80, 81, 83, 84, 85, 89, 94], "com": [10, 38, 39, 40, 45, 46, 52, 55, 56, 60, 89], "facebookresearch": [10, 55, 56], "blob": [10, 38, 39, 40, 45, 46, 52, 55, 56, 60], "main": [10, 11, 52, 55, 56, 89, 94, 96], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 57], "32": [10, 96, 97, 98], "num_head": [10, 52, 54, 56, 57], "num_kv_head": [10, 52, 54], "vocab_s": 10, "must": [10, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 61, 66, 70, 98], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 77, 82, 83, 85, 92, 93, 97, 98], "nn": [10, 52, 53, 54, 57, 58, 59, 61, 63, 64, 67, 84, 97, 98], "parsed_yaml": 10, "embed_dim": [10, 52, 56, 58, 97], "valueerror": [10, 20, 23, 29, 33, 52, 57, 68, 69, 72, 85], "callabl": [11, 24, 26, 57, 67, 73, 84], "With": [11, 94, 97, 98], "my_recip": 11, "foo": 11, "bar": [11, 90, 95], "instanti": [12, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51], "configerror": 12, "cannot": [12, 96], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 78, 79, 80, 81, 93, 94, 98], "prompt": [13, 14, 16, 17, 18, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 57, 65, 93, 94, 96], "templat": [13, 14, 16, 17, 21, 24, 26, 27, 28, 30, 31, 32, 33], "style": [13, 27, 28, 29, 33, 98], "slightli": 13, "classmethod": [13, 14, 15, 16, 17, 18, 19, 20, 21], "map": [13, 16, 17, 21, 22, 24, 25, 26, 31, 33, 64, 68, 78, 79, 80, 81, 92, 94, 97], "column_map": [13, 16, 17, 21, 24, 26, 31, 33, 93], "placehold": [13, 14, 16, 17, 21, 24, 26, 31, 33], "column": [13, 16, 17, 21, 24, 26, 31, 33, 92], "ident": [13, 16, 17, 20, 21, 26, 31, 94], "role": [14, 19, 22, 24, 65, 92, 93], "system": [14, 15, 18, 19, 20, 22, 23, 24, 65, 92, 93], "assist": [14, 15, 18, 19, 22, 23, 24, 65, 92, 93], "messag": [14, 15, 18, 20, 22, 23, 24, 29, 65, 66, 89, 92, 93], "accord": [14, 20, 92], "openai": 15, "markup": 15, "languag": [15, 62, 97], "It": [15, 20, 92, 93, 98], "huggingfac": [15, 24, 26, 29, 31, 51, 60, 68, 69, 93, 94], "im_start": 15, "context": [15, 51, 83, 93], "im_end": 15, "goe": 15, "respons": [15, 65, 93, 94, 95, 96], "appropri": [15, 18, 20, 25, 60, 93, 98], "tag": [15, 18, 20, 24, 66, 78, 79, 80, 81, 92], "grammar": [16, 30, 93], "sentenc": 16, "alwai": [17, 70], "human": [18, 22, 92], "taken": [18, 97, 98], "inst": [18, 20, 24, 92, 93], "sy": [18, 92, 93], "respect": [18, 25, 64, 92, 93], "honest": [18, 92, 93], "am": [18, 20, 92, 93, 94, 96], "pari": [18, 20, 93], "capit": [18, 20, 93], "franc": [18, 20, 93], "known": [18, 20, 65, 93], "its": [18, 20, 85, 93, 94, 96, 97], "stun": [18, 20, 93], "liter": [19, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50], "mask": [19, 26, 27, 28, 30, 32, 52, 58, 65, 66, 92, 93], "ipython": 19, "eot": 19, "dataclass": [19, 92], "repres": [19, 92], "individu": [19, 81, 84, 92, 93], "tiktoken": [19, 66, 96], "special": [19, 24, 66], "variabl": [19, 24, 25, 26, 31, 33, 98], "writer": 19, "whether": [19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 45, 46, 48, 59, 62, 65, 66, 67, 72, 92], "correspond": [19, 61, 63, 72, 95, 96], "consecut": [19, 23], "from_dict": [19, 92], "construct": [19, 97], "dictionari": [19, 78, 79, 80, 81, 94], "mistral": [20, 24, 48, 49, 50, 92, 94, 95], "llama2chatformat": [20, 33, 92, 93], "summar": [21, 32, 92, 93], "task": [21, 25, 92, 93, 94, 96, 97, 98], "dialogu": [21, 32, 92], "dialog": 21, "adher": [22, 33], "sharegpt": [22, 29, 93], "gpt": [22, 52, 94], "remain": [22, 60, 97], "unmask": 22, "forth": 23, "come": [23, 61, 97], "empti": 23, "shorter": 23, "length": [23, 25, 26, 27, 28, 31, 33, 51, 52, 54, 56, 57, 58, 65, 66, 69, 82], "min": [23, 97], "invalid": 23, "convert_to_messag": [24, 92], "chat_format": [24, 29, 33, 92, 93], "chatformat": [24, 29, 33, 93], "load_dataset_kwarg": [24, 26, 29, 31], "multiturn": [24, 92], "foreach": 24, "prepar": [24, 92], "truncat": [24, 26, 31, 33, 65, 66], "encod": [24, 26, 27, 28, 29, 30, 31, 32, 33, 65, 66, 92], "decod": [24, 26, 27, 28, 29, 30, 31, 32, 33, 57, 65, 66, 92], "anyth": [24, 26, 27, 28, 29, 30, 31, 32, 33], "load_dataset": [24, 26, 27, 28, 29, 30, 31, 32, 33, 92], "co": [24, 26, 29, 31, 51, 68, 69, 94], "doc": [24, 26, 29, 31, 67, 70, 74, 80, 81, 83, 85, 94], "en": [24, 26, 29, 31], "package_refer": [24, 26, 29, 31], "loading_method": [24, 26, 29, 31], "text": [24, 65, 66, 92, 93, 94], "extra": [24, 89, 97, 98], "still": [24, 70, 97, 98], "llama3": [24, 43, 44, 45, 46, 47, 73, 88], "where": [24, 25, 27, 28, 30, 32, 52, 57, 62, 65, 93], "unless": 24, "check": [24, 29, 57, 72, 88, 92, 94, 95, 97], "concaten": [25, 65, 93], "sub": [25, 80], "unifi": 25, "were": [25, 92, 95], "simplifi": [25, 97], "simultan": 25, "intern": [25, 70], "aggreg": 25, "transpar": 25, "index": [25, 60, 82, 89, 92, 94], "howev": [25, 89], "constitu": 25, "might": [25, 94], "larg": [25, 62, 98], "comput": [25, 52, 53, 56, 57, 85, 94, 98], "cumul": 25, "maintain": [25, 98], "indic": [25, 67, 92, 93], "deleg": 25, "retriev": [25, 73], "lead": [25, 65], "high": [25, 90, 97], "scale": [25, 38, 39, 40, 45, 46, 48, 62, 97, 98], "consid": 25, "strategi": 25, "stream": [25, 74], "demand": 25, "deriv": [25, 53, 57, 58], "_dataset": 25, "_len": 25, "total": [25, 60, 75, 87, 91, 94, 96, 97], "combin": [25, 93], "_index": 25, "lookup": 25, "dataset1": 25, "mycustomdataset": 25, "params1": 25, "dataset2": 25, "params2": 25, "concat_dataset": 25, "data_point": 25, "1500": 25, "element": [25, 66, 94], "focus": [25, 95], "enhanc": [25, 98], "divers": 25, "machin": [25, 71, 94], "instructtempl": [26, 93], "contribut": [26, 27, 28, 30, 32], "replac": [26, 27, 28, 30, 32, 59, 97], "disabl": [26, 31, 85], "recommend": [26, 27, 28, 31, 80, 92, 94, 98], "highest": [26, 27, 28, 31], "sequenc": [26, 27, 28, 31, 33, 52, 54, 56, 57, 58, 65, 66, 82, 92], "yahma": 27, "codebas": [27, 28, 30, 32, 94], "alpaca_d": [27, 28], "batch_siz": [27, 28, 30, 32, 52, 58, 94], "tatsu": [28, 93], "lab": [28, 93], "conversation_styl": [29, 93], "chatdataset": [29, 33, 92], "made": [29, 31, 56, 94], "friendli": [29, 31, 92], "huggingfaceh4": 29, "no_robot": 29, "chatmlformat": 29, "2096": 29, "accomplish": 29, "liweili": 30, "c4_200m": 30, "variant": [30, 32], "mirror": [30, 32], "llama_recip": [30, 32], "grammar_d": 30, "samsum": [32, 93], "summari": [32, 93], "samsum_d": 32, "_util": 33, "open": [33, 34, 93, 94], "orca": [33, 93], "slimorca": [33, 93], "dedup": [33, 93], "_chat_format": 33, "1024": [33, 93], "doesn": [33, 94], "prescrib": 33, "least": [33, 96, 97], "though": [33, 92], "max": [33, 57, 60, 65, 97], "ds": 33, "10": [33, 82, 94, 96, 98], "351": 33, "82": [33, 94], "391": 33, "221": 33, "220": 33, "193": 33, "12": [33, 89], "471": 33, "gemma": 34, "gemmatransformerdecod": 34, "w": [34, 35, 36, 37, 43, 44, 49, 80, 81, 92, 94, 97, 98], "blog": 34, "technolog": 34, "develop": [34, 98], "transformerdecod": [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 97], "arxiv": [35, 36, 37, 41, 42, 47, 50, 52, 55, 56], "org": [35, 36, 37, 41, 42, 47, 50, 52, 55, 56, 67, 70, 74, 80, 83, 84, 85, 89], "ab": [35, 36, 37, 41, 42, 47, 50, 56], "2307": [35, 36, 37], "09288": [35, 36, 37], "70b": [36, 39, 43, 45, 96], "lora_attn_modul": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97, 98], "q_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 97, 98], "k_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 97, 98], "v_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 97, 98], "output_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 97, 98], "apply_lora_to_mlp": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97], "apply_lora_to_output": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97], "lora_rank": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97], "lora_alpha": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97], "float": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 52, 55, 60, 62, 78, 79, 80, 81, 97, 98], "16": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 97, 98], "lora_dropout": [38, 39, 40, 41, 42], "05": [38, 39, 40, 41, 42], "quantize_bas": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 62, 98], "lora": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 62, 88, 90, 92, 95, 96], "tloen": [38, 39, 40, 45, 46], "8bb8579e403dc78e37fe81ffbb253c413007323f": [38, 39, 40, 45, 46], "l41": [38, 39, 40, 45, 46], "l43": [38, 39, 40, 45, 46], "linear": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 57, 61, 62, 97, 98], "attent": [38, 39, 40, 45, 46, 48, 51, 52, 54, 56, 57, 58, 96, 97, 98], "mlp": [38, 39, 40, 45, 46, 48, 57, 58, 96, 97], "final": [38, 39, 40, 45, 46, 48, 53, 57, 66, 94, 96, 97, 98], "rank": [38, 39, 40, 45, 46, 48, 62, 75, 85, 95, 97, 98], "low": [38, 39, 40, 45, 46, 48, 62, 94, 97, 98], "approxim": [38, 39, 40, 45, 46, 48, 62, 97], "factor": [38, 39, 40, 45, 46, 48, 62, 94], "llama2_70b": 39, "llama2_7b": [40, 97], "qlora": [41, 42, 47, 50, 59, 88, 90, 96, 97], "per": [41, 42, 47, 50, 54, 59, 96, 98], "paper": [41, 42, 47, 50, 97, 98], "2305": [41, 42, 47, 50, 52], "14314": [41, 42, 47, 50], "lora_llama2_13b": 41, "lora_llama2_7b": [42, 97], "llama3_70b": 45, "llama3_8b": [46, 96], "lora_llama3_8b": 47, "announc": 49, "lora_mistral_7b": 50, "phi3": 51, "ref": [51, 81], "phi": 51, "128k": 51, "nor": 51, "slide": 51, "window": 51, "head_dim": [52, 54, 56, 57], "pos_embed": [52, 97], "kv_cach": 52, "kvcach": [52, 57], "attn_dropout": [52, 57], "head": [52, 54, 56, 57, 96], "queri": [52, 54, 57, 96], "gqa": 52, "introduc": [52, 55, 62, 92, 97, 98], "pdf": [52, 55], "13245v1": 52, "version": [52, 89, 96, 98], "multihead": 52, "mha": [52, 57], "n": [52, 65, 66, 87, 91, 92, 93], "extrem": 52, "share": [52, 93, 94], "mqa": 52, "credit": 52, "document": [52, 67, 73], "lightn": 52, "lit": 52, "lit_gpt": 52, "v": [52, 57, 97], "k": [52, 97], "q": [52, 97], "n_kv_head": 52, "dimens": [52, 54, 56, 57, 62, 96, 97, 98], "calcul": [52, 96], "e": [52, 59, 61, 64, 89, 94, 96, 97, 98], "g": [52, 61, 96, 97, 98], "rotarypositionalembed": [52, 97], "cach": [52, 54, 56, 89], "rope": [52, 56], "dropout": [52, 62, 97, 98], "onto": 52, "scaled_dot_product_attent": 52, "input_po": [52, 56, 57, 58], "seq_length": [52, 58], "seq_len": [52, 56], "bigger": 52, "n_h": [52, 56], "num": [52, 56], "n_kv": 52, "kv": [52, 54, 57], "emb": [52, 57, 58], "h_d": [52, 56], "gate_proj": 53, "down_proj": 53, "up_proj": 53, "silu": 53, "feed": [53, 58], "network": [53, 97, 98], "fed": [53, 92], "multipli": 53, "subclass": [53, 70], "although": [53, 97], "afterward": 53, "former": 53, "regist": [53, 59, 98], "hook": [53, 59, 98], "latter": 53, "max_batch_s": 54, "standalon": 54, "past": 54, "becaus": [54, 57, 92, 94, 96], "expand": 54, "dpython": [54, 59], "ep": 55, "1e": 55, "06": [55, 97], "root": [55, 80, 81], "squar": 55, "1910": 55, "07467": 55, "verfic": [55, 56], "small": [55, 94], "avoid": [55, 59, 85, 98], "divis": 55, "zero": [55, 94, 96], "10000": 56, "rotari": [56, 96], "propos": 56, "2104": 56, "09864": 56, "l450": 56, "upto": 56, "init": [56, 81, 98], "exceed": 56, "freq": 56, "recomput": 56, "geometr": 56, "progress": [56, 95], "rotat": 56, "angl": 56, "bsz": 56, "todo": 56, "effici": [56, 73, 88, 90, 94, 95, 97], "transformerdecoderlay": 57, "norm": [57, 58], "move": 57, "space": 57, "belong": 57, "reduc": [57, 90, 93, 97, 98], "statement": 57, "improv": [57, 73, 94, 96, 97], "readabl": [57, 94], "At": 57, "arang": 57, "prompt_length": 57, "causal_mask": 57, "m_": 57, "seq": 57, "attn": [58, 97, 98], "causalselfattent": [58, 97], "sa_norm": 58, "mlp_norm": 58, "ff": 58, "common_util": 59, "bfloat16": [59, 94, 95, 96, 97], "offload_to_cpu": 59, "nf4": [59, 98], "restor": 59, "higher": [59, 96, 98], "offload": [59, 98], "increas": [59, 60, 96, 97], "peak": [59, 94, 96, 97, 98], "gpu": [59, 94, 95, 96, 97, 98], "usag": [59, 89, 94, 95, 96, 98], "_register_state_dict_hook": 59, "m": [59, 66, 92], "mymodul": 59, "_after_": 59, "nf4tensor": [59, 98], "unquant": [59, 94, 98], "unus": 59, "num_warmup_step": 60, "num_training_step": 60, "num_cycl": 60, "last_epoch": 60, "lambdalr": 60, "rate": [60, 90, 95], "schedul": [60, 83, 95], "linearli": 60, "lr": 60, "decreas": [60, 97, 98], "cosin": 60, "v4": 60, "23": [60, 96], "src": 60, "l104": 60, "warmup": [60, 83], "phase": 60, "wave": 60, "half": 60, "last": 60, "lr_schedul": 60, "peft": [61, 62, 63, 64, 97, 98], "protocol": 61, "adapter_param": [61, 62, 63, 64], "proj": 61, "in_dim": [61, 62, 97, 98], "out_dim": [61, 62, 97, 98], "bia": [61, 62, 97, 98], "loralinear": [61, 97, 98], "alpha": [62, 97, 98], "use_bia": 62, "perturb": 62, "decomposit": [62, 97], "matric": [62, 97, 98], "trainabl": [62, 64, 97, 98], "mapsto": 62, "w_0x": 62, "r": [62, 66, 97], "bax": 62, "probabl": [62, 94], "lora_a": [62, 97, 98], "lora_b": [62, 97, 98], "subset": 63, "get_adapter_param": [64, 97], "sentencepieceprocessor": 65, "pretrain": [65, 66, 92, 95, 97, 98], "non": 65, "spm_model": [65, 92], "tokenized_text": 65, "hello": [65, 92, 94, 96], "world": [65, 75, 94], "add_bo": [65, 66, 92], "add_eo": [65, 66, 92], "31587": 65, "29644": 65, "102": 65, "trim_leading_whitespac": 65, "prefix": 65, "unbatch": 65, "prepend": [65, 66], "bo": [65, 66, 92], "append": [65, 89], "eo": [65, 66, 92], "trim": 65, "whitespac": 65, "underli": [65, 98], "sentencepiec": [65, 96], "s1": 65, "s2": 65, "due": [65, 97, 98], "tokenize_messag": [65, 66, 92, 93], "problem": 65, "slice": 65, "tokenizer_path": 65, "separ": [65, 68, 92, 95, 96, 97, 98], "concat": 65, "1788": 65, "2643": 65, "13": [65, 94, 96, 98], "1792": 65, "9508": 65, "465": 65, "22137": 65, "2933": 65, "join": 65, "attribut": 65, "llama3_tiktoken": 66, "p": [66, 67, 97, 98], "l": 66, "all_special_token": 66, "bos_token": 66, "begin_of_text": [66, 92], "eos_token": 66, "end_of_text": 66, "start_header_id": [66, 92], "end_header_id": [66, 92], "step_id": 66, "eom_id": 66, "eot_id": [66, 92], "python_tag": 66, "identif": 66, "regex": 66, "second": [66, 94, 96, 97, 98], "uniqu": 66, "256": [66, 93, 94, 96], "header": [66, 92], "token_id": 66, "truncate_at_eo": 66, "tokenize_head": 66, "datatyp": [67, 98], "polici": [67, 73, 84], "denot": 67, "boolean": 67, "integ": [67, 82, 85], "auto_wrap_polici": [67, 73, 84], "submodul": 67, "obei": 67, "contract": 67, "get_fsdp_polici": 67, "modules_to_wrap": [67, 73], "min_num_param": 67, "my_fsdp_polici": 67, "recurs": [67, 80], "isinst": 67, "sum": [67, 97], "numel": [67, 97], "1000": 67, "functool": 67, "partial": 67, "stabl": [67, 80, 83, 85, 89], "html": [67, 70, 74, 80, 83, 84, 85], "alia": 67, "few": [68, 93, 96, 97, 98], "0001_of_0003": 68, "0002_of_0003": 68, "preserv": [68, 98], "weight_map": [68, 94], "intermediate_checkpoint": [68, 69], "parit": 68, "_weight_map": 68, "shard": [69, 96], "wip": 69, "argpars": 70, "argumentpars": 70, "builtin": 70, "said": 70, "noth": 70, "treat": [70, 92], "consult": 70, "info": [70, 95], "librari": [70, 74, 85, 88, 90, 98], "parse_known_arg": 70, "namespac": 70, "act": 70, "precid": 70, "parse_arg": 70, "properti": [70, 97], "too": [70, 96], "availab": 71, "distribut": [71, 76, 84, 85, 90, 95, 96], "bf16": [72, 98], "request": [72, 93, 94], "inde": [72, 94], "kernel": 72, "runtimeerror": [72, 76], "float32": 72, "done": [72, 97, 98], "isn": 72, "hardwar": [72, 90, 94, 97], "memory_efficient_fsdp_wrap": 73, "maxim": [73, 88, 90], "been": [73, 96], "workload": 73, "15": [73, 92, 94, 97, 98], "alongsid": 73, "ac": 73, "fullyshardeddataparallel": 73, "const": 73, "fsdppolicytyp": 73, "handler": 74, "aka": 75, "filenam": 78, "log_": 78, "unixtimestamp": 78, "txt": [78, 95], "thread": 78, "safe": 78, "flush": [78, 79, 80, 81], "union": [78, 79, 80, 81, 84, 85], "ndarrai": [78, 79, 80, 81], "scalar": [78, 79, 80, 81], "record": [78, 79, 80, 81], "payload": [78, 79, 80, 81], "organize_log": 80, "tensorboard": 80, "subdirectori": 80, "compar": [80, 94, 97, 98], "logdir": 80, "startup": 80, "tree": [80, 93, 94], "tfevent": 80, "encount": 80, "frontend": 80, "organ": 80, "accordingli": 80, "my_log_dir": 80, "view": [80, 94, 95], "my_metr": [80, 81], "termin": [80, 81], "entiti": 81, "bias": 81, "sent": 81, "usernam": 81, "my_project": 81, "my_ent": 81, "my_group": 81, "importerror": 81, "account": [81, 97, 98], "log_config": 81, "local": [81, 85, 89, 92, 94, 95], "link": [81, 94], "capecap": 81, "6053ofw0": 81, "torchtune_config_j67sb73v": 81, "padding_idx": 82, "ignore_idx": 82, "pad": 82, "longest": 82, "tokenpair": 82, "collat": 82, "token_pair": 82, "torchtune_perf_trac": 83, "contextmanag": 83, "wait": 83, "trace": 83, "speed": [83, 96, 98], "reduct": [83, 97], "acwrappolicytyp": 84, "describ": [84, 93], "author": [84, 90, 95, 98], "intermedi": [84, 96, 98], "fsdp_adavnced_tutori": 84, "debug_mod": 85, "pseudo": 85, "random": [85, 95], "commonli": [85, 94, 97, 98], "numpi": 85, "own": [85, 92, 93, 94, 97], "determinist": 85, "global": 85, "warn": 85, "nondeterminist": 85, "addition": [85, 97], "cudnn": 85, "set_deterministic_debug_mod": 85, "algorithm": 85, "outsid": [85, 94, 96, 97], "generated_examples_python": 86, "zip": 86, "galleri": [86, 91], "sphinx": 86, "000": [87, 91, 96], "execut": [87, 91], "generated_exampl": 87, "mem": [87, 91], "mb": [87, 91], "topic": 88, "gentl": 88, "introduct": 88, "readi": [88, 92], "workflow": [88, 93, 95, 97], "requisit": 89, "proper": [89, 95], "host": [89, 95], "page": [89, 90, 95, 96], "latest": [89, 95, 98], "confirm": 89, "And": [89, 94, 96], "h": 89, "ls": [89, 94, 95, 96], "welcom": 89, "show": [89, 92, 97], "greatest": [89, 95], "contributor": 89, "cd": [89, 94], "even": [89, 92, 96, 97, 98], "commit": 89, "branch": 89, "url": 89, "whl": 89, "therebi": [89, 98], "forc": 89, "reinstal": 89, "opt": [89, 95], "suffix": 89, "cu121": 89, "On": [90, 97], "pointer": 90, "emphas": 90, "aspect": 90, "simplic": 90, "component": 90, "reus": 90, "prove": 90, "democrat": 90, "box": [90, 98], "zoo": 90, "varieti": [90, 97], "techniqu": [90, 94, 95, 97], "integr": [90, 94, 95, 96, 97, 98], "excit": 90, "checkout": 90, "quickstart": 90, "attain": 90, "better": [90, 92, 93, 94], "chekckpoint": 90, "hyperparamet": [90, 95, 97, 98], "embodi": 90, "philosophi": 90, "usabl": 90, "composit": 90, "hard": 90, "outlin": 90, "unecessari": 90, "never": 90, "thoroughli": 90, "unit": 90, "know": [92, 93, 94, 96, 97], "align": 92, "intend": 92, "hi": 92, "nice": 92, "meet": 92, "overhaul": 92, "entir": [92, 98], "sai": [92, 93, 95], "accompani": 92, "who": 92, "influenti": 92, "hip": 92, "hop": 92, "artist": [92, 96], "2pac": 92, "rakim": 92, "c": 92, "na": 92, "flavor": [92, 93], "certain": 92, "msg": 92, "formatted_messag": [92, 93], "nyou": [92, 93], "nwho": 92, "sentencepiecetoken": 92, "why": [92, 95, 97], "user_messag": 92, "518": 92, "25580": 92, "29962": 92, "3532": 92, "14816": 92, "29903": 92, "6778": 92, "piece_to_id": 92, "reserv": [92, 98], "vector": 92, "place": 92, "manual": [92, 98], "529": 92, "29879": 92, "29958": 92, "tiktokentoken": 92, "nhere": 92, "_encode_special_token": 92, "128000": 92, "128009": 92, "part": [92, 98], "pure": 92, "That": 92, "won": [92, 94, 96], "mess": 92, "govern": 92, "prime": 92, "strictli": 92, "summarizetempl": [92, 93], "lightweight": 92, "ask": 92, "untouch": 92, "nsummari": 92, "long": [92, 97], "robust": 92, "enough": 92, "csv": 92, "question": [92, 93, 94, 96], "answer": [92, 94, 96], "onlin": 92, "forum": 92, "panda": 92, "pd": 92, "df": 92, "read_csv": 92, "your_fil": 92, "nrow": 92, "tolist": 92, "row": 92, "iloc": 92, "gp": 92, "receiv": 92, "commun": [92, 94], "satellit": 92, "thing": [92, 98], "message_convert": 92, "input_msg": 92, "output_msg": 92, "assistant_messag": 92, "But": [92, 94, 96, 97], "mistralchatformat": 92, "custom_dataset": 92, "2048": 92, "data_fil": 92, "honor": 92, "copi": [92, 94, 95, 96, 98], "8b_lora_single_devic": [92, 96], "my": [92, 94, 96], "launch": [92, 95], "custom_8b_lora_single_devic": 92, "steer": 93, "wheel": 93, "publicli": 93, "great": [93, 94], "iter": [93, 98], "knob": 93, "tweak": 93, "footprint": [93, 97], "could": [93, 97], "achiev": [93, 94, 96, 97, 98], "concatdataset": 93, "instruct_dataset": 93, "vicgal": 93, "gpt4": 93, "alpacainstructtempl": 93, "demonstr": 93, "fix": 93, "goal": 93, "agnost": 93, "condit": 93, "respond": 93, "further": [93, 97, 98], "classifi": 93, "anim": 93, "plant": 93, "miner": 93, "oak": 93, "copper": 93, "ore": 93, "eleph": 93, "mydataset": 93, "onthehub": 93, "customtempl": 93, "similar": [93, 94, 96, 97, 98], "quit": [93, 98], "similarli": 93, "chat_dataset": 93, "incorpor": 93, "advanc": 93, "preferencedataset": 93, "rlhf": 93, "adjust": 93, "chosen": 93, "reject": 93, "chosen_messag": 93, "transformed_sampl": 93, "key_chosen": 93, "rejected_messag": 93, "key_reject": 93, "chosen_input_id": 93, "c_mask": 93, "chosen_label": 93, "np": 93, "cross_entropy_ignore_idx": 93, "rejected_input_id": 93, "r_mask": 93, "rejected_label": 93, "purpos": [93, 95, 96], "stack_exchanged_paired_dataset": 93, "had": 93, "lvwerra": 93, "stack": 93, "exchang": 93, "stackexchangedpairedtempl": 93, "response_j": 93, "response_k": 93, "data_dir": 93, "rl": 93, "favorit": [94, 96, 97], "seemlessli": 94, "beyond": [94, 98], "connect": 94, "larger": [94, 96], "amount": 94, "natur": 94, "export": 94, "mobil": 94, "phone": 94, "leverag": [94, 96, 98], "mode": 94, "lot": 94, "plai": 94, "freez": [94, 97], "percentag": 94, "learnabl": 94, "keep": [94, 97], "16gb": [94, 97], "rtx": 94, "3090": 94, "4090": 94, "hour": 94, "full_finetune_single_devic": [94, 95], "7b_full_low_memori": [94, 95], "full_finetune_distribut": [94, 95], "7b_full": [94, 95], "13b_full": [94, 95], "7b_qlora_single_devic": [94, 95, 98], "473": 94, "98": [94, 98], "gb": [94, 96, 97, 98], "50": 94, "484": 94, "01": [94, 95], "fact": [94, 96, 97], "third": 94, "smaller": [94, 96, 97, 98], "realli": 94, "eleuther_ev": [94, 96], "eleuther_evalu": [94, 96], "lm_eval": [94, 96], "plan": 94, "custom_eval_config": [94, 96], "truthfulqa_mc2": [94, 96, 97], "measur": [94, 96], "propens": [94, 96], "shot": [94, 96], "accuraci": [94, 96, 97, 98], "baselin": [94, 97], "324": 94, "loglikelihood": 94, "195": 94, "121": 94, "27": 94, "197": 94, "acc": 94, "388": 94, "38": 94, "shown": 94, "489": 94, "48": [94, 98], "seem": 94, "custom_generation_config": [94, 96], "kick": 94, "top_k": 94, "300": 94, "temperatur": 94, "interest": 94, "site": 94, "visit": 94, "bai": 94, "area": 94, "92": [94, 96], "exploratorium": 94, "san": 94, "francisco": 94, "magazin": 94, "awesom": 94, "bridg": 94, "pretti": 94, "cool": 94, "96": [94, 98], "61": 94, "sec": [94, 96], "25": 94, "83": 94, "99": [94, 97], "72": 94, "littl": 94, "saw": 94, "took": [94, 96], "torchao": [94, 96, 98], "bit": [94, 96, 97, 98], "custom_quantization_config": [94, 96], "68": 94, "19": [94, 96, 98], "76": 94, "69": 94, "95": [94, 96], "67": 94, "4w": [94, 96], "unlik": [94, 96], "engin": [94, 96], "fullmodeltorchtunecheckpoint": [94, 96], "int4weightonlyquant": [94, 96], "groupsiz": [94, 96], "did": [94, 96, 98], "park": 94, "sit": 94, "top": [94, 98], "hill": 94, "beauti": 94, "62": [94, 96], "17": [94, 97], "85": 94, "compil": [94, 96, 98], "hood": [94, 98], "sped": 94, "almost": [94, 96, 97], "3x": [94, 96], "benefit": 94, "yet": 94, "fast": 94, "clone": [94, 97, 98], "assumpt": 94, "satisfi": 94, "new_dir": 94, "output_dict": 94, "sd_1": 94, "sd_2": 94, "dump": 94, "convert_hf_checkpoint": 94, "checkpoint_path": 94, "justin": 94, "school": 94, "math": 94, "teacher": 94, "ws": 94, "94": [94, 96], "103": 94, "28": 94, "bandwidth": [94, 96], "1391": 94, "84": 94, "thats": 94, "seamlessli": 94, "authent": [94, 95], "hopefulli": 94, "gave": 94, "gate": 95, "grant": 95, "minut": 95, "agreement": 95, "altern": 95, "hackabl": 95, "singularli": 95, "technic": 95, "depth": 95, "principl": 95, "minim": [95, 97, 98], "boilerpl": 95, "hold": 95, "substanti": [95, 97], "custom_config": 95, "replic": 95, "lorafinetunerecipesingledevic": 95, "lora_finetune_output": 95, "log_1713194212": 95, "sampler": 95, "52": 95, "3697006702423096": 95, "25880": [95, 98], "24": [95, 96], "55": 95, "83it": 95, "monitor": 95, "tqdm": 95, "interv": 95, "e2": 95, "releas": 96, "focu": 96, "128": [96, 97], "theta": 96, "gain": 96, "illustr": 96, "basic": 96, "observ": 96, "18": 96, "consum": [96, 98], "vram": [96, 97], "overal": 96, "nproc_per_nod": [96, 97], "lora_finetune_distribut": [96, 97], "8b_lora": 96, "8b_qlora_single_devic": 96, "alloc": [96, 98], "coupl": [96, 97, 98], "llama3_token": 96, "122": 96, "sarah": 96, "busi": 96, "mum": 96, "young": 96, "children": 96, "live": 96, "north": 96, "east": 96, "england": 96, "135": 96, "88": 96, "138": 96, "346": 96, "09": 96, "139": 96, "31": 96, "far": 96, "drill": 96, "90": 96, "93": 96, "91": 96, "104": 96, "four": [96, 97], "again": 96, "jake": 96, "disciplin": 96, "passion": 96, "draw": 96, "paint": 96, "57": [96, 97, 98], "speedup": 96, "broader": 96, "teach": 97, "straight": 97, "jump": 97, "neural": [97, 98], "unfamiliar": 97, "oppos": [97, 98], "momentum": 97, "adamw": 97, "arbitrari": 97, "relat": 97, "aghajanyan": 97, "et": 97, "al": 97, "hypothes": 97, "intrins": 97, "lower": 97, "down": [97, 98], "often": 97, "eight": 97, "practic": 97, "imag": 97, "left": 97, "blue": 97, "rememb": 97, "approx": 97, "15m": 97, "8192": 97, "65k": 97, "requires_grad": [97, 98], "frozen_out": [97, 98], "lora_out": [97, 98], "omit": 97, "base_model": 97, "choos": 97, "lora_model": 97, "lora_llama_2_7b": [97, 98], "alon": 97, "in_featur": 97, "out_featur": 97, "inplac": 97, "feel": 97, "free": 97, "strict": 97, "whenev": 97, "validate_state_dict_for_lora": 97, "peft_util": 97, "set_trainable_param": 97, "fetch": 97, "lora_param": 97, "total_param": 97, "trainable_param": 97, "2f": 97, "6742609920": 97, "4194304": 97, "nnode": 97, "7b_lora": 97, "my_model_checkpoint_path": [97, 98], "tokenizer_checkpoint": [97, 98], "my_tokenizer_checkpoint_path": [97, 98], "constraint": 97, "factori": 97, "benefici": 97, "impact": 97, "rel": 97, "minor": 97, "good": 97, "64": 97, "lora_experiment_1": 97, "smooth": [97, 98], "curv": [97, 98], "500": 97, "ran": 97, "commod": 97, "cogniz": 97, "ax": 97, "parallel": 97, "truthfulqa": 97, "previous": 97, "475": 97, "87": 97, "508": 97, "86": 97, "504": 97, "04": 97, "514": 97, "lowest": 97, "absolut": 97, "4gb": 97, "tradeoff": 97, "potenti": 97, "highli": 98, "vanilla": 98, "held": 98, "therefor": 98, "bespok": 98, "normalfloat": 98, "8x": 98, "retain": 98, "vast": 98, "major": 98, "highlight": 98, "degrad": 98, "normatfloat": 98, "doubl": 98, "themselv": 98, "prune": 98, "deepdiv": 98, "idea": 98, "distinct": 98, "storag": 98, "de": 98, "incur": 98, "counterpart": 98, "set_default_devic": 98, "qlora_linear": 98, "memory_alloc": 98, "177": 98, "152": 98, "byte": 98, "del": 98, "empty_cach": 98, "lora_linear": 98, "081": 98, "344": 98, "qlora_llama2_7b": 98, "qlora_model": 98, "essenti": 98, "reparametrize_as_dtype_state_dict_post_hook": 98, "stat": 98, "against": 98, "35": 98, "40": 98, "29": 98, "slow": 98, "slower": 98, "149": 98, "9157477021217346": 98, "02": 98, "08": 98, "14": 98, "15it": 98, "nightli": 98, "200": 98, "hundr": 98, "228": 98, "8158286809921265": 98, "59": 98, "95it": 98, "exercis": 98, "portion": 98, "augment": 98, "linear_nf4": 98, "to_nf4": 98, "linear_weight": 98, "autograd": 98, "regular": 98, "incom": 98}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "Message"], [20, 1, 1, "", "MistralChatFormat"], [21, 1, 1, "", "SummarizeTemplate"], [22, 0, 1, "", "sharegpt_to_llama2_messages"], [23, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.Message": [[19, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[20, 2, 1, "", "format"], [20, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[21, 2, 1, "", "format"]], "torchtune.datasets": [[24, 1, 1, "", "ChatDataset"], [25, 1, 1, "", "ConcatDataset"], [26, 1, 1, "", "InstructDataset"], [27, 0, 1, "", "alpaca_cleaned_dataset"], [28, 0, 1, "", "alpaca_dataset"], [29, 0, 1, "", "chat_dataset"], [30, 0, 1, "", "grammar_dataset"], [31, 0, 1, "", "instruct_dataset"], [32, 0, 1, "", "samsum_dataset"], [33, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[34, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[35, 0, 1, "", "llama2_13b"], [36, 0, 1, "", "llama2_70b"], [37, 0, 1, "", "llama2_7b"], [38, 0, 1, "", "lora_llama2_13b"], [39, 0, 1, "", "lora_llama2_70b"], [40, 0, 1, "", "lora_llama2_7b"], [41, 0, 1, "", "qlora_llama2_13b"], [42, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[43, 0, 1, "", "llama3_70b"], [44, 0, 1, "", "llama3_8b"], [45, 0, 1, "", "lora_llama3_70b"], [46, 0, 1, "", "lora_llama3_8b"], [47, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[48, 0, 1, "", "lora_mistral_7b"], [49, 0, 1, "", "mistral_7b"], [50, 0, 1, "", "qlora_mistral_7b"]], "torchtune.models.phi3": [[51, 0, 1, "", "phi3_mini"]], "torchtune.modules": [[52, 1, 1, "", "CausalSelfAttention"], [53, 1, 1, "", "FeedForward"], [54, 1, 1, "", "KVCache"], [55, 1, 1, "", "RMSNorm"], [56, 1, 1, "", "RotaryPositionalEmbeddings"], [57, 1, 1, "", "TransformerDecoder"], [58, 1, 1, "", "TransformerDecoderLayer"], [60, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[52, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[53, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[55, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[56, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[57, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[58, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[59, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[61, 1, 1, "", "AdapterModule"], [62, 1, 1, "", "LoRALinear"], [63, 0, 1, "", "get_adapter_params"], [64, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[61, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[62, 2, 1, "", "adapter_params"], [62, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[65, 1, 1, "", "SentencePieceTokenizer"], [66, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[65, 2, 1, "", "decode"], [65, 2, 1, "", "encode"], [65, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[66, 2, 1, "", "decode"], [66, 2, 1, "", "encode"], [66, 2, 1, "", "tokenize_message"], [66, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[67, 4, 1, "", "FSDPPolicyType"], [68, 1, 1, "", "FullModelHFCheckpointer"], [69, 1, 1, "", "FullModelMetaCheckpointer"], [70, 1, 1, "", "TuneRecipeArgumentParser"], [71, 0, 1, "", "get_device"], [72, 0, 1, "", "get_dtype"], [73, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [74, 0, 1, "", "get_logger"], [75, 0, 1, "", "get_world_size_and_rank"], [76, 0, 1, "", "init_distributed"], [77, 0, 1, "", "list_dtypes"], [82, 0, 1, "", "padded_collate"], [83, 0, 1, "", "profiler"], [84, 0, 1, "", "set_activation_checkpointing"], [85, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[68, 2, 1, "", "load_checkpoint"], [68, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[69, 2, 1, "", "load_checkpoint"], [69, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[70, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[78, 1, 1, "", "DiskLogger"], [79, 1, 1, "", "StdoutLogger"], [80, 1, 1, "", "TensorBoardLogger"], [81, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[78, 2, 1, "", "close"], [78, 2, 1, "", "log"], [78, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[79, 2, 1, "", "close"], [79, 2, 1, "", "log"], [79, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[80, 2, 1, "", "close"], [80, 2, 1, "", "log"], [80, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[81, 2, 1, "", "close"], [81, 2, 1, "", "log"], [81, 2, 1, "", "log_config"], [81, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 67, 88, 90, 94, 96, 97, 98], "config": [0, 7, 8, 95], "data": [1, 5, 92], "instruct": [1, 89, 93, 96], "templat": [1, 92, 93], "chat": [1, 92, 93], "format": [1, 6, 93], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 92, 93], "exampl": 2, "gener": [2, 94, 96], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 94, 95, 96, 97], "llama3": [3, 92, 96], "llama2": [3, 92, 94, 97, 98], "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 89, 98], "block": 4, "token": [4, 92], "peft": 4, "util": [4, 5, 67], "checkpoint": [5, 6, 9, 94], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 97, 98], "manag": 5, "perform": [5, 97], "profil": [5, 83], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 90, 94], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 94, 97, 98], "put": [6, 98], "thi": 6, "all": [6, 7, 98], "togeth": [6, 98], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 93], "us": [7, 8, 92, 94, 98], "instanti": [7, 10], "referenc": 7, "other": [7, 94], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 94, 95], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 90, 97, 98], "ar": 8, "recip": [8, 95, 97], "script": 8, "run": [8, 94], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "messag": 19, "mistralchatformat": 20, "summarizetempl": 21, "sharegpt_to_llama2_messag": 22, "validate_messag": 23, "chatdataset": 24, "concatdataset": 25, "instructdataset": 26, "alpaca_cleaned_dataset": 27, "alpaca_dataset": 28, "chat_dataset": 29, "grammar_dataset": 30, "instruct_dataset": 31, "samsum_dataset": 32, "slimorca_dataset": 33, "gemma_2b": 34, "llama2_13b": 35, "llama2_70b": 36, "llama2_7b": 37, "lora_llama2_13b": 38, "lora_llama2_70b": 39, "lora_llama2_7b": 40, "qlora_llama2_13b": 41, "qlora_llama2_7b": 42, "llama3_70b": 43, "llama3_8b": 44, "lora_llama3_70b": 45, "lora_llama3_8b": 46, "qlora_llama3_8b": 47, "lora_mistral_7b": 48, "mistral_7b": 49, "qlora_mistral_7b": 50, "phi3_mini": 51, "causalselfattent": 52, "todo": [52, 58], "feedforward": 53, "kvcach": 54, "rmsnorm": 55, "rotarypositionalembed": 56, "transformerdecod": 57, "transformerdecoderlay": 58, "reparametrize_as_dtype_state_dict_post_hook": 59, "get_cosine_schedule_with_warmup": 60, "adaptermodul": 61, "loralinear": 62, "get_adapter_param": 63, "set_trainable_param": 64, "sentencepiecetoken": 65, "tiktokentoken": 66, "fsdppolicytyp": 67, "fullmodelhfcheckpoint": 68, "fullmodelmetacheckpoint": 69, "tunerecipeargumentpars": 70, "get_devic": 71, "get_dtyp": 72, "get_full_finetune_fsdp_wrap_polici": 73, "get_logg": 74, "get_world_size_and_rank": 75, "init_distribut": 76, "list_dtyp": 77, "disklogg": 78, "stdoutlogg": 79, "tensorboardlogg": 80, "wandblogg": 81, "padded_col": 82, "set_activation_checkpoint": 84, "set_se": 85, "comput": [87, 91], "time": [87, 91], "welcom": 88, "document": 88, "get": [88, 96], "start": 88, "tutori": 88, "instal": 89, "via": [89, 96], "pypi": 89, "git": 89, "clone": 89, "nightli": 89, "kei": 90, "concept": 90, "design": 90, "principl": 90, "fine": [92, 93, 95, 96], "tune": [92, 93, 95, 96], "chang": 92, "from": [92, 98], "prompt": 92, "special": 92, "when": 92, "should": 92, "i": 92, "custom": [92, 93], "fulli": 93, "end": 94, "workflow": 94, "download": [94, 95], "7b": 94, "finetun": [94, 97, 98], "evalu": [94, 96], "eleutherai": [94, 96], "s": [94, 96], "eval": [94, 96], "har": [94, 96], "speed": 94, "up": 94, "quantiz": [94, 96], "librari": 94, "upload": 94, "hug": 94, "face": 94, "hub": 94, "first": 95, "llm": 95, "select": 95, "modifi": 95, "train": 95, "next": 95, "step": 95, "meta": 96, "8b": 96, "access": 96, "text": 96, "our": 96, "faster": 96, "how": 97, "doe": 97, "work": 97, "appli": 97, "trade": 97, "off": 97, "qlora": 98, "save": 98, "deep": 98, "dive": 98}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})