Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.modules.tokenizers.Tokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.profiler", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.modules.tokenizers.Tokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "lora_mistral_7b", "lora_mistral_classifier_7b", "mistral_7b", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "lora_phi3_mini", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceTokenizer", "TikTokenTokenizer", "Tokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "profiler", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"For": [2, 5, 6, 7, 8, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 41, 42, 87, 92, 108, 113, 114, 121, 130, 135, 137, 143, 146, 147, 148, 149, 150, 151, 152, 153], "detail": [2, 6, 34, 39, 85, 89, 107, 118, 126, 133, 137, 146, 149, 150, 151, 152, 153], "usag": [2, 94, 111, 112, 143, 146, 148, 149, 150, 151, 153], "guid": [2, 7, 9, 144, 147, 148, 150, 152], "pleas": [2, 5, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 107, 118, 126, 135, 143, 153], "see": [2, 5, 6, 9, 19, 21, 34, 39, 42, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 85, 86, 89, 97, 107, 111, 113, 118, 119, 126, 130, 133, 135, 137, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "our": [2, 6, 8, 144, 147, 148, 149, 150, 152, 153], "tutori": [2, 6, 135, 144, 147, 148, 149, 150, 151, 152, 153], "support": [2, 6, 8, 9, 10, 21, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 84, 85, 87, 98, 109, 110, 112, 117, 120, 144, 146, 147, 148, 149, 150, 151, 152, 153], "sever": 2, "wide": 2, "us": [2, 4, 6, 9, 10, 12, 16, 19, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 87, 88, 90, 91, 92, 93, 94, 96, 99, 102, 104, 105, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 120, 126, 127, 128, 129, 130, 137, 142, 143, 144, 146, 148, 150, 151, 152], "help": [2, 6, 19, 92, 108, 113, 142, 143, 144, 146, 147, 148, 149, 150, 151, 153], "quickli": [2, 7, 31, 147, 148], "bootstrap": 2, "your": [2, 5, 9, 10, 26, 31, 129, 130, 142, 143, 144, 146, 147, 148, 151, 152, 153], "fine": [2, 6, 8, 9, 29, 142, 144, 149, 152], "tune": [2, 3, 6, 7, 8, 9, 12, 29, 142, 143, 144, 146, 149, 152, 153], "also": [2, 6, 7, 8, 9, 10, 34, 37, 41, 85, 87, 92, 98, 116, 118, 120, 126, 130, 143, 146, 147, 148, 149, 150, 151, 152, 153], "common": [2, 4, 7, 146, 147, 148, 151, 152], "format": [2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 26, 28, 30, 32, 33, 34, 37, 39, 106, 108, 109, 110, 111, 146, 147, 149, 150, 151, 152], "like": [2, 6, 7, 8, 9, 26, 85, 110, 143, 146, 147, 148, 149, 150, 152], "chat": [2, 15, 16, 19, 20, 23, 26, 34, 39, 85], "model": [2, 6, 7, 8, 10, 16, 21, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 114, 115, 118, 120, 126, 133, 134, 135, 142, 144, 147, 148, 153], "instruct": [2, 3, 14, 16, 18, 20, 21, 28, 29, 30, 32, 33, 37, 41, 79, 84, 85, 142, 146, 147, 150, 152, 153], "These": [2, 4, 6, 7, 8, 10, 29, 113, 147, 148, 149, 150, 151, 152, 153], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 25, 28, 29, 30, 32, 33, 34, 36, 37, 38, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 92, 98, 99, 102, 103, 107, 108, 109, 111, 112, 114, 115, 117, 120, 124, 126, 132, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "especi": [2, 144, 146, 149], "specifi": [2, 6, 7, 8, 10, 34, 87, 92, 93, 96, 107, 115, 118, 121, 126, 130, 135, 146, 147, 148, 149, 150, 151, 153], "from": [2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 40, 41, 42, 43, 44, 45, 52, 53, 59, 60, 61, 78, 79, 88, 92, 93, 95, 97, 100, 103, 104, 108, 109, 110, 112, 113, 114, 115, 129, 130, 134, 141, 143, 145, 146, 148, 149, 150, 151, 152], "yaml": [2, 7, 8, 10, 11, 12, 34, 37, 41, 113, 130, 144, 146, 147, 148, 149, 150, 151, 152, 153], "config": [2, 6, 9, 10, 11, 12, 13, 34, 37, 41, 87, 102, 108, 112, 113, 130, 144, 147, 148, 149, 151, 152, 153], "represent": [2, 152, 153], "abov": [2, 6, 94, 124, 143, 149, 151, 152, 153], "all": [3, 4, 8, 13, 26, 27, 29, 34, 87, 88, 92, 94, 99, 105, 108, 112, 113, 114, 124, 134, 139, 140, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152], "famili": [3, 8, 32, 33, 35, 39, 40, 42, 111, 144, 146, 151], "download": [3, 6, 140, 143, 147, 148, 151, 152, 153], "meta": [3, 6, 19, 108, 109, 146, 147, 149, 150], "8b": [3, 70, 73, 75, 83, 146, 147], "hf": [3, 6, 96, 108, 146, 147, 149, 150, 151], "token": [3, 6, 7, 8, 20, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 54, 62, 71, 80, 85, 87, 91, 92, 93, 104, 105, 115, 118, 131, 146, 148, 149, 150, 151, 152, 153], "access_token": 3, "pre": [3, 19, 29, 143, 147, 148], "train": [3, 5, 6, 8, 9, 19, 26, 27, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 87, 91, 92, 93, 94, 95, 108, 109, 110, 117, 120, 126, 133, 142, 144, 146, 147, 148, 149, 151, 152, 153], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 41, 42, 90, 91, 99, 104, 107, 108, 111, 113, 118, 126, 129, 130, 135, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "hug": [3, 6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 95, 144, 146, 150, 151], "face": [3, 6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 95, 144, 146, 150, 151], "hub": [3, 6, 146, 148, 150], "follow": [3, 6, 8, 23, 26, 29, 87, 95, 110, 111, 112, 124, 130, 142, 143, 146, 148, 149, 150, 151, 152, 153], "command": [3, 8, 9, 113, 143, 146, 147, 148, 149, 150, 151, 152, 153], "2": [3, 6, 9, 25, 29, 39, 87, 104, 108, 109, 131, 132, 136, 137, 138, 147, 149, 150, 151, 152], "7b": [3, 6, 28, 30, 31, 32, 33, 35, 37, 41, 42, 45, 48, 51, 53, 56, 61, 65, 68, 76, 77, 78, 79, 108, 109, 147, 150, 151, 152, 153], "codellama": 3, "mini": [3, 83, 84, 85, 86], "microsoft": [3, 84, 85], "4k": [3, 84, 85], "hf_token": 3, "ignor": [3, 6, 85, 87, 88, 146], "pattern": [3, 105, 146], "ai": [3, 78, 87, 130, 147, 151], "mistralai": [3, 146], "v0": 3, "1": [3, 6, 8, 29, 39, 87, 92, 95, 96, 104, 105, 109, 111, 115, 124, 129, 130, 131, 132, 136, 137, 146, 147, 149, 150, 151, 152, 153], "size": [3, 6, 8, 10, 32, 33, 36, 38, 87, 89, 90, 91, 92, 122, 124, 144, 146, 148, 149, 150, 151, 152], "2b": [3, 52, 55], "googl": [3, 52, 53], "offer": 5, "allow": [5, 27, 102, 129, 146, 153], "seamless": 5, "transit": 5, "between": [5, 6, 108, 111, 148, 149, 151, 152, 153], "interoper": [5, 6, 8, 144, 149, 153], "rest": [5, 147, 153], "ecosystem": [5, 6, 8, 144, 149, 151, 153], "comprehens": 5, "overview": [5, 7, 9, 142, 150, 152, 153], "deep": [5, 6, 7, 8, 9, 144, 150, 151], "dive": [5, 6, 7, 8, 9, 144, 150, 151], "enabl": [5, 7, 8, 9, 27, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 98, 133, 137, 151, 152, 153], "work": [5, 6, 8, 113, 144, 146, 149, 151, 153], "set": [5, 6, 7, 8, 9, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 41, 42, 91, 92, 99, 101, 107, 116, 118, 124, 126, 135, 136, 137, 144, 146, 147, 149, 150, 151, 152], "consumpt": [5, 27], "dure": [5, 6, 27, 28, 29, 32, 33, 36, 38, 87, 89, 91, 92, 93, 94, 120, 147, 149, 151, 152, 153], "provid": [5, 6, 7, 8, 10, 16, 21, 24, 26, 27, 28, 29, 30, 39, 92, 99, 110, 113, 116, 118, 130, 144, 146, 147, 148, 149, 150, 151], "debug": [5, 6, 7, 8, 146], "finetun": [5, 6, 7, 8, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 142, 144, 150, 151], "job": [5, 9, 137, 150], "variou": [5, 18], "dataset": [5, 7, 14, 17, 18, 20, 22, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 144, 150, 151], "walk": [6, 8, 129, 144, 147, 148, 149, 150, 153], "you": [6, 7, 8, 9, 10, 18, 19, 26, 28, 30, 31, 32, 33, 35, 37, 41, 42, 111, 113, 115, 129, 130, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "through": [6, 7, 8, 9, 88, 99, 144, 146, 147, 148, 149, 150, 153], "design": [6, 8], "behavior": [6, 126, 147, 148], "associ": [6, 7, 8, 115, 149, 152], "util": [6, 7, 8, 9, 10, 27, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 144, 149, 150, 151, 153], "what": [6, 7, 9, 19, 21, 31, 36, 38, 142, 147, 148, 149, 150, 151], "cover": [6, 7, 8, 9, 147, 149, 153], "how": [6, 7, 8, 9, 107, 135, 142, 146, 147, 148, 149, 150, 151, 153], "we": [6, 7, 8, 9, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 87, 89, 91, 92, 98, 104, 108, 109, 110, 115, 117, 121, 126, 134, 144, 146, 147, 148, 149, 150, 151, 152, 153], "them": [6, 7, 26, 27, 28, 30, 37, 88, 94, 104, 146, 147, 148, 149, 152, 153], "scenario": [6, 27], "full": [6, 7, 8, 34, 37, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 102, 103, 104, 144, 146, 148, 151, 152], "compos": 6, "compon": [6, 8, 13, 132, 133, 144, 148, 150, 152, 153], "which": [6, 7, 8, 27, 28, 29, 31, 32, 33, 36, 38, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 87, 91, 92, 93, 95, 102, 103, 104, 108, 109, 110, 112, 117, 127, 130, 135, 144, 146, 147, 148, 149, 150, 151, 152, 153], "plug": 6, "ani": [6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 26, 28, 30, 31, 34, 35, 37, 41, 42, 94, 100, 101, 102, 103, 104, 108, 109, 110, 112, 115, 123, 126, 137, 139, 146, 147, 148, 149, 150, 151, 152], "recip": [6, 7, 9, 10, 11, 12, 88, 102, 108, 109, 110, 144, 147, 148, 149, 151, 153], "evalu": [6, 8, 142, 144, 150, 152, 153], "gener": [6, 8, 14, 17, 22, 26, 28, 29, 30, 35, 39, 99, 104, 136, 137, 140, 142, 147, 148, 152, 153], "each": [6, 8, 15, 18, 27, 29, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 87, 91, 92, 93, 96, 102, 103, 104, 105, 132, 137, 144, 146, 148, 149, 150, 151, 152], "make": [6, 7, 8, 9, 87, 93, 144, 146, 149, 150, 151, 152, 153], "easi": [6, 8, 144, 148, 152], "understand": [6, 7, 8, 142, 144, 147, 148, 152, 153], "extend": [6, 8, 144], "befor": [6, 25, 28, 29, 30, 87, 92, 93, 98, 108, 146, 149], "let": [6, 7, 9, 146, 147, 148, 149, 150, 151, 152, 153], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 46, 47, 48, 63, 64, 65, 72, 73, 76, 77, 83, 85, 87, 89, 91, 92, 93, 94, 96, 97, 100, 102, 103, 105, 107, 108, 109, 112, 116, 118, 120, 126, 129, 133, 135, 136, 144, 146, 147, 148, 150, 152, 153], "defin": [6, 7, 8, 88, 97, 98, 100, 148, 150, 152], "some": [6, 7, 16, 100, 101, 142, 144, 146, 147, 148, 149, 150, 152, 153], "concept": [6, 149, 150], "In": [6, 7, 8, 26, 91, 98, 107, 126, 129, 130, 147, 149, 151, 152, 153], "ll": [6, 7, 8, 105, 115, 121, 144, 147, 148, 149, 150, 151, 153], "talk": 6, "about": [6, 8, 96, 130, 144, 146, 147, 149, 150, 151, 152, 153], "take": [6, 7, 8, 10, 88, 89, 94, 108, 110, 113, 116, 132, 147, 148, 149, 150, 151, 152, 153], "close": [6, 8, 127, 128, 129, 130, 152], "look": [6, 7, 8, 114, 129, 143, 147, 148, 149, 150, 151, 152], "veri": [6, 27, 92, 146, 149], "simpli": [6, 7, 29, 146, 147, 148, 149, 151, 153], "dictat": 6, "state_dict": [6, 94, 102, 108, 109, 110, 111, 112, 152, 153], "store": [6, 27, 127, 130, 152, 153], "file": [6, 7, 8, 9, 10, 11, 12, 104, 105, 108, 109, 110, 113, 127, 130, 133, 141, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], "disk": [6, 31, 127], "weight": [6, 8, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 94, 97, 98, 102, 108, 109, 110, 111, 121, 126, 130, 142, 146, 147, 149, 150, 151, 152, 153], "string": [6, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 97, 104, 105, 106, 116, 117, 121, 146, 148], "kei": [6, 7, 9, 26, 28, 30, 37, 87, 89, 92, 93, 101, 102, 103, 108, 110, 112, 132, 146, 149, 150, 152, 153], "identifi": 6, "state": [6, 8, 94, 100, 101, 102, 103, 108, 109, 110, 112, 114, 149, 151, 152, 153], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 26, 28, 30, 31, 34, 35, 37, 41, 42, 94, 100, 101, 102, 103, 108, 109, 110, 112, 114, 120, 123, 125, 131, 132, 134, 148], "If": [6, 7, 13, 14, 17, 18, 21, 22, 24, 25, 26, 28, 30, 32, 33, 36, 37, 38, 39, 87, 91, 92, 93, 94, 96, 98, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 120, 121, 123, 129, 130, 137, 139, 143, 146, 147, 148, 149, 150, 151, 152], "don": [6, 7, 8, 130, 137, 146, 147, 148, 149, 150, 151, 153], "t": [6, 7, 8, 105, 117, 130, 137, 146, 147, 148, 149, 150, 151, 153], "match": [6, 26, 28, 30, 37, 103, 143, 146, 148, 149, 151, 152], "up": [6, 8, 9, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 114, 146, 147, 148, 150, 151, 152, 153], "exactli": [6, 103], "those": [6, 111, 152], "definit": [6, 152], "either": [6, 103, 108, 115, 135, 146, 152, 153], "run": [6, 7, 9, 12, 88, 89, 92, 94, 108, 109, 110, 112, 114, 124, 129, 130, 134, 143, 144, 147, 148, 150, 151, 152, 153], "explicit": 6, "error": [6, 7, 25, 108, 137, 146], "load": [6, 8, 26, 27, 28, 29, 30, 31, 102, 108, 109, 110, 112, 113, 129, 147, 148, 149, 151, 152], "rais": [6, 10, 13, 21, 25, 34, 39, 87, 89, 92, 96, 102, 103, 108, 109, 110, 112, 117, 120, 123, 130, 132, 137, 139], "an": [6, 7, 8, 9, 10, 14, 20, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 87, 92, 96, 97, 99, 100, 101, 107, 108, 109, 110, 112, 116, 118, 130, 144, 146, 147, 148, 149, 150, 151, 152, 153], "except": [6, 20, 21, 148], "wors": 6, "silent": [6, 88], "succe": 6, "infer": [6, 19, 26, 87, 89, 91, 92, 93, 116, 142, 147, 149, 150, 151, 153], "expect": [6, 7, 10, 14, 17, 18, 22, 26, 28, 30, 34, 37, 91, 103, 112, 130, 139, 147, 148, 152], "addit": [6, 7, 8, 10, 26, 28, 30, 31, 34, 35, 37, 41, 42, 102, 107, 108, 109, 110, 117, 118, 123, 126, 127, 129, 130, 135, 144, 147, 150, 152], "line": [6, 8, 113, 146, 148, 150, 151], "need": [6, 7, 8, 9, 18, 26, 29, 39, 87, 88, 92, 126, 129, 130, 134, 143, 146, 147, 148, 149, 150, 151, 152, 153], "shape": [6, 87, 89, 91, 92, 93, 96, 98, 115], "valu": [6, 7, 23, 39, 43, 44, 45, 52, 53, 59, 60, 61, 69, 70, 78, 79, 87, 89, 90, 92, 93, 95, 102, 108, 111, 112, 113, 115, 127, 128, 129, 130, 132, 137, 146, 148, 150, 151, 152], "two": [6, 7, 25, 144, 149, 150, 151, 152, 153], "popular": [6, 144, 148, 149], "llama2": [6, 7, 8, 10, 19, 23, 26, 28, 30, 31, 32, 33, 35, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 88, 92, 93, 104, 111, 142, 144, 146, 150, 151], "offici": [6, 19, 147, 150, 151], "implement": [6, 8, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 88, 90, 91, 95, 96, 97, 98, 108, 121, 129, 144, 148, 152, 153], "when": [6, 7, 8, 12, 20, 27, 29, 31, 87, 91, 92, 93, 94, 95, 102, 115, 118, 129, 134, 146, 149, 151, 152, 153], "llama": [6, 19, 26, 34, 90, 91, 108, 109, 146, 147, 149, 150, 151, 152], "websit": 6, "get": [6, 7, 8, 9, 26, 104, 117, 119, 120, 122, 143, 144, 147, 148, 149, 150, 152], "access": [6, 7, 8, 27, 108, 114, 146, 149, 150], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 29, 31, 87, 102, 108, 109, 110, 112, 114, 146, 147, 148, 149, 150, 151, 152, 153], "pth": [6, 149], "inspect": [6, 149, 152, 153], "content": [6, 20, 23, 26, 104, 147, 148], "easili": [6, 7, 144, 148, 152, 153], "torch": [6, 7, 27, 89, 92, 94, 95, 96, 110, 112, 114, 115, 116, 117, 120, 123, 124, 132, 133, 134, 135, 136, 137, 138, 139, 149, 150, 151, 152, 153], "import": [6, 7, 10, 34, 37, 41, 129, 130, 147, 148, 149, 150, 152, 153], "consolid": [6, 151], "00": [6, 141, 145, 150], "mmap": [6, 149], "true": [6, 7, 20, 28, 29, 32, 33, 34, 36, 37, 38, 41, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 87, 92, 93, 94, 99, 104, 105, 107, 108, 109, 110, 118, 120, 123, 124, 126, 129, 138, 146, 147, 148, 149, 151, 152, 153], "weights_onli": [6, 110], "map_loc": [6, 149], "cpu": [6, 8, 94, 117, 143, 146, 149, 153], "tensor": [6, 87, 88, 89, 90, 91, 92, 93, 94, 96, 98, 108, 115, 127, 128, 129, 130, 131, 132, 136, 152, 153], "item": 6, "print": [6, 9, 27, 32, 33, 36, 38, 39, 104, 115, 138, 147, 148, 150, 152, 153], "f": [6, 9, 32, 33, 36, 38, 147, 149, 152, 153], "tok_embed": [6, 92], "32000": [6, 10, 152], "4096": [6, 10, 28, 30, 31, 32, 33, 35, 37, 41, 42, 87, 91, 148, 152], "len": [6, 27, 32, 33, 36, 38, 92], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 25, 26, 27, 28, 29, 30, 36, 38, 39, 40, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 90, 91, 94, 95, 96, 99, 104, 105, 107, 108, 110, 113, 116, 117, 119, 121, 130, 133, 136, 138, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "contain": [6, 20, 27, 29, 31, 41, 87, 89, 91, 92, 93, 97, 100, 101, 102, 104, 105, 108, 109, 110, 112, 113, 114, 120, 125, 129, 131, 132, 147, 149, 151, 152], "includ": [6, 7, 8, 15, 18, 85, 98, 108, 109, 113, 144, 146, 147, 148, 149, 150, 151, 152, 153], "input": [6, 14, 15, 18, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 87, 88, 90, 91, 92, 93, 98, 104, 108, 110, 131, 132, 137, 139, 147, 148, 152, 153], "embed": [6, 87, 89, 90, 91, 92, 118, 147, 151], "tabl": [6, 147, 153], "call": [6, 10, 20, 88, 94, 102, 113, 127, 128, 129, 130, 134, 147, 148, 152, 153], "layer": [6, 8, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 92, 93, 98, 102, 103, 107, 118, 144, 151, 152, 153], "have": [6, 7, 10, 87, 89, 97, 103, 110, 112, 113, 118, 126, 129, 133, 139, 143, 147, 148, 149, 150, 151, 152, 153], "dim": [6, 87, 88, 90, 91, 92], "most": [6, 7, 105, 147, 150, 152, 153], "within": [6, 7, 10, 26, 29, 39, 88, 115, 129, 137, 146, 148, 149, 151, 152, 153], "default": [6, 7, 16, 20, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 55, 56, 59, 60, 61, 63, 64, 65, 69, 70, 72, 73, 76, 77, 78, 79, 83, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 102, 104, 105, 108, 109, 110, 113, 115, 117, 122, 126, 127, 130, 131, 132, 133, 136, 137, 143, 146, 147, 148, 149, 151, 152, 153], "everi": [6, 8, 88, 129, 143, 146, 153], "repo": [6, 108, 109, 111, 146, 149], "first": [6, 7, 10, 25, 29, 89, 92, 105, 108, 113, 142, 144, 147, 148, 149, 151, 152, 153], "big": [6, 149], "split": [6, 29, 147, 148, 149], "across": [6, 8, 27, 108, 129, 137, 149, 151], "bin": [6, 146, 149], "To": [6, 7, 8, 9, 29, 108, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "correctli": [6, 8, 13, 102, 108, 143, 147, 150, 153], "piec": 6, "one": [6, 8, 25, 88, 96, 104, 110, 147, 148, 149, 150, 151, 153], "pytorch_model": [6, 149], "00001": [6, 146], "00002": [6, 146], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 29, 35, 98, 100, 102, 104, 109, 110, 112, 113, 115, 117, 118, 120, 121, 126, 146, 148, 149, 150, 151, 152, 153], "doe": [6, 21, 26, 29, 84, 87, 92, 93, 97, 108, 110, 112, 113, 146, 147, 149], "fewer": [6, 87], "sinc": [6, 7, 10, 88, 108, 110, 147, 149, 151], "instead": [6, 8, 29, 34, 37, 41, 88, 89, 98, 146, 149, 151, 152], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 26, 28, 30, 31, 37, 39, 41, 42, 97, 101, 103, 105, 108, 109, 110, 111, 112, 113, 114, 115, 116, 127, 128, 129, 130, 139, 146, 147, 149, 151], "caus": [6, 104], "try": [6, 7, 147, 149, 150, 151, 153], "same": [6, 7, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 87, 89, 93, 104, 112, 113, 118, 130, 146, 147, 149, 151, 152, 153], "As": [6, 7, 8, 9, 98, 144, 149, 151, 153], "re": [6, 7, 105, 110, 144, 147, 149, 150, 151, 152], "care": [6, 88, 108, 110, 149, 151, 152], "end": [6, 8, 20, 27, 104, 105, 142, 144, 147, 151, 152], "number": [6, 8, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 87, 89, 92, 95, 108, 109, 110, 115, 122, 137, 146, 150, 152], "just": [6, 14, 144, 146, 147, 148, 150, 151, 152], "save": [6, 8, 9, 94, 108, 109, 110, 112, 118, 126, 130, 142, 146, 147, 148, 149, 151, 152], "less": [6, 39, 149, 150, 151, 153], "prone": 6, "manag": [6, 27, 99, 133, 136, 147], "invari": 6, "accept": [6, 7, 39, 104, 107, 148, 150, 153], "multipl": [6, 7, 8, 20, 26, 27, 87, 92, 93, 98, 127, 128, 129, 130, 132, 150, 151], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 147, 148, 149], "worri": [6, 147, 150], "explicitli": [6, 97, 144, 152], "convert": [6, 23, 26, 108, 131, 147, 149, 153], "time": [6, 104, 127, 129, 146, 147, 148, 149, 151, 153], "produc": [6, 112, 153], "back": [6, 25, 99, 108, 148, 152, 153], "origin": [6, 32, 33, 94, 98, 147, 149, 151, 152, 153], "form": [6, 7, 8, 25, 146], "One": [6, 149], "advantag": [6, 152], "being": [6, 108, 109, 110, 114, 116, 153], "should": [6, 7, 8, 15, 18, 19, 20, 21, 23, 29, 34, 37, 41, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 87, 88, 96, 97, 102, 103, 107, 113, 125, 127, 128, 129, 130, 143, 144, 148, 149, 150, 151, 152, 153], "abl": [6, 8, 149, 150, 151], "post": [6, 134, 153], "tool": [6, 148, 149, 150], "quantiz": [6, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 98, 110, 121, 142, 150, 153], "eval": [6, 142, 144], "without": [6, 7, 9, 102, 143, 144, 147, 149, 152], "code": [6, 8, 43, 44, 45, 46, 47, 48, 49, 50, 51, 92, 140, 144, 148, 150], "chang": [6, 7, 9, 14, 110, 143, 149, 150, 151, 152, 153], "OR": 6, "convers": [6, 15, 16, 19, 21, 23, 25, 26, 34, 39, 108, 110, 111, 144, 147, 148, 149, 151, 152, 153], "script": [6, 9, 146, 149, 150, 151], "wai": [6, 7, 26, 102, 146, 147, 148, 149, 150, 151], "surround": [6, 8, 144], "load_checkpoint": [6, 8, 108, 109, 110, 111], "save_checkpoint": [6, 8, 9, 108, 109, 110], "method": [6, 7, 8, 9, 12, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 94, 97, 100, 102, 112, 113, 121, 143, 144, 148, 149, 151, 152, 153], "convertor": 6, "avail": [6, 8, 42, 113, 116, 117, 124, 144, 146, 149, 151, 152], "here": [6, 7, 9, 16, 36, 90, 91, 146, 147, 148, 149, 150, 151, 152, 153], "three": [6, 8, 96, 150], "hfcheckpoint": 6, "read": [6, 108, 109, 110, 144], "write": [6, 8, 108, 109, 110, 127, 147, 148, 150], "compat": [6, 108, 110], "transform": [6, 8, 26, 28, 30, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 92, 93, 95, 135, 152], "framework": [6, 8, 144], "mention": [6, 149, 153], "assum": [6, 14, 17, 18, 22, 28, 30, 37, 87, 91, 92, 93, 95, 100, 105, 112, 114, 117, 126, 149, 152], "checkpoint_dir": [6, 7, 108, 109, 110, 149, 151], "necessari": [6, 39, 127, 128, 129, 130, 147, 152], "json": [6, 85, 108, 133, 146, 148, 149], "easiest": [6, 149, 150], "sure": [6, 7, 149, 150, 151, 152, 153], "everyth": [6, 8, 113, 144, 150], "flow": [6, 26, 28, 29, 30, 153], "By": [6, 146, 151, 152, 153], "safetensor": [6, 146], "output": [6, 18, 32, 33, 36, 39, 46, 47, 48, 63, 64, 65, 72, 73, 76, 77, 83, 87, 88, 90, 91, 92, 93, 98, 101, 102, 103, 110, 115, 118, 128, 133, 143, 146, 147, 148, 149, 150, 151, 152, 153], "dir": [6, 130, 143, 146, 149, 150, 151], "output_dir": [6, 7, 108, 109, 110, 133, 149, 151, 152, 153], "argument": [6, 7, 10, 18, 26, 28, 30, 31, 34, 35, 37, 39, 41, 42, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 87, 107, 113, 118, 123, 127, 129, 130, 135, 146, 147, 148, 151, 152], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 92, 146, 148, 149, 152, 153], "_component_": [6, 7, 9, 10, 34, 37, 41, 147, 148, 149, 151, 152], "fullmodelhfcheckpoint": [6, 149], "directori": [6, 7, 108, 109, 110, 127, 129, 130, 146, 149, 150, 151], "sort": [6, 108, 110], "id": [6, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 87, 91, 92, 93, 104, 105, 106, 108, 110, 115, 131, 132, 147, 148, 149], "so": [6, 7, 29, 108, 113, 143, 144, 147, 149, 150, 151, 152, 153], "order": [6, 8, 108, 110, 129, 130, 150], "matter": [6, 108, 110, 146, 152], "checkpoint_fil": [6, 7, 9, 108, 109, 110, 149, 151, 152, 153], "restart": [6, 146], "previou": [6, 29, 108, 109, 110], "more": [6, 7, 8, 34, 39, 85, 89, 91, 102, 107, 110, 113, 130, 133, 135, 137, 144, 146, 148, 149, 150, 151, 152, 153], "next": [6, 29, 115, 151, 153], "section": [6, 8, 120, 142, 149, 151, 153], "recipe_checkpoint": [6, 108, 109, 110], "null": [6, 7], "usual": [6, 91, 108, 130, 146, 149, 152], "model_typ": [6, 108, 109, 110, 149, 151], "resume_from_checkpoint": [6, 108, 109, 110], "fals": [6, 7, 20, 23, 26, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 92, 93, 98, 99, 102, 104, 105, 108, 109, 110, 124, 133, 146, 147, 148, 149, 151, 152, 153], "requir": [6, 7, 27, 31, 39, 41, 108, 110, 112, 123, 124, 126, 129, 130, 132, 137, 143, 146, 147, 148, 150, 153], "param": [6, 8, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 98, 100, 101, 103, 108, 126, 152, 153], "directli": [6, 7, 8, 10, 34, 37, 41, 107, 108, 146, 149, 150, 151, 152, 153], "ensur": [6, 7, 13, 25, 39, 87, 108, 110, 117, 144, 148, 150], "out": [6, 7, 8, 26, 28, 32, 33, 34, 36, 38, 108, 109, 142, 144, 146, 147, 149, 150, 151, 152, 153], "case": [6, 8, 9, 20, 87, 108, 112, 117, 121, 126, 127, 135, 144, 146, 147, 148, 149, 151, 152, 153], "discrep": [6, 108], "along": [6, 151, 152], "found": [6, 7, 9, 90, 91, 146, 152, 153], "metacheckpoint": 6, "github": [6, 10, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 87, 90, 91, 95, 96, 102, 143, 148, 150], "repositori": [6, 19, 149, 150], "fullmodelmetacheckpoint": [6, 151], "torchtunecheckpoint": 6, "perform": [6, 29, 85, 88, 99, 115, 144, 147, 149, 151, 153], "current": [6, 29, 84, 85, 87, 89, 91, 92, 93, 109, 110, 118, 122, 127, 129, 134, 137, 149, 150, 151], "test": [6, 7, 8, 144, 147], "complet": [6, 8, 29, 35, 85, 147, 148, 149, 150, 151], "written": [6, 7, 8, 108, 109, 127, 128, 129, 130, 144], "begin": [6, 29, 104, 105, 147, 151, 153], "partit": [6, 108, 153], "ha": [6, 97, 99, 100, 103, 104, 110, 112, 139, 148, 149, 150, 151, 152, 153], "standard": [6, 128, 144, 147, 149, 151], "key_1": [6, 110], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 149], "inform": [6, 20, 130, 135, 144, 146, 149, 150, 151], "subsequ": [6, 8], "recipe_st": [6, 108, 109, 110], "pt": [6, 9, 108, 109, 110, 149, 151], "epoch": [6, 8, 9, 95, 108, 109, 110, 146, 147, 149, 150, 151], "optim": [6, 7, 8, 27, 84, 95, 96, 110, 112, 114, 120, 132, 134, 147, 149, 150, 151, 152, 153], "etc": [6, 8, 108, 120, 150], "prevent": [6, 29, 146], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 92, 97, 104, 112, 133, 134, 137, 147, 148, 149, 152, 153], "updat": [6, 7, 8, 89, 112, 143, 147, 149, 150, 151, 152, 153], "hf_model_0001_0": [6, 149], "hf_model_0002_0": [6, 149], "both": [6, 27, 103, 146, 149, 152, 153], "adapt": [6, 97, 98, 99, 100, 101, 108, 109, 110, 147, 149, 152, 153], "merg": [6, 10, 11, 108, 149, 151, 153], "would": [6, 7, 9, 29, 92, 143, 147, 148, 149, 152, 153], "primari": [6, 7, 8, 150], "want": [6, 7, 8, 9, 10, 26, 115, 143, 146, 147, 148, 149, 150, 151, 152], "resum": [6, 8, 95, 108, 109, 110, 153], "initi": [6, 8, 12, 27, 29, 43, 44, 45, 52, 53, 59, 60, 61, 69, 70, 78, 79, 112, 123, 124, 150, 152, 153], "frozen": [6, 152, 153], "base": [6, 10, 28, 30, 39, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 91, 95, 96, 98, 99, 101, 102, 103, 108, 113, 116, 118, 126, 127, 142, 147, 149, 150, 151, 152, 153], "well": [6, 7, 8, 144, 146, 148, 149, 151, 153], "learnt": [6, 147, 149], "someth": [6, 8, 9, 147, 149], "NOT": 6, "refer": [6, 7, 8, 90, 91, 96, 99, 144, 152], "adapter_checkpoint": [6, 108, 109, 110], "adapter_0": [6, 149], "now": [6, 104, 112, 114, 147, 148, 149, 150, 151, 152, 153], "knowledg": 6, "creat": [6, 7, 10, 29, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 86, 89, 95, 107, 108, 109, 110, 114, 127, 129, 146, 147, 148, 149, 151, 153], "simpl": [6, 8, 142, 148, 150, 152, 153], "forward": [6, 8, 87, 88, 90, 91, 92, 93, 96, 98, 120, 151, 152, 153], "13b": [6, 43, 46, 49, 59, 63, 66], "modeltyp": [6, 108, 109, 110], "llama2_13b": [6, 63], "right": [6, 108, 149, 151, 152], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 102, 112, 152], "successfulli": [6, 146, 150], "vocab": [6, 10, 92, 151], "70": [6, 69], "x": [6, 87, 88, 90, 91, 92, 93, 98, 115, 136, 152, 153], "randint": 6, "0": [6, 8, 29, 46, 47, 48, 49, 50, 51, 63, 64, 65, 66, 67, 68, 87, 92, 95, 96, 98, 104, 115, 129, 130, 131, 132, 137, 138, 141, 145, 147, 148, 149, 150, 151, 152, 153], "no_grad": 6, "6": [6, 29, 90, 131, 132, 149, 153], "3989": 6, "9": [6, 132, 149, 153], "0531": 6, "3": [6, 29, 83, 84, 85, 105, 111, 113, 119, 131, 132, 136, 146, 147, 149, 150, 151, 153], "2375": 6, "5": [6, 7, 95, 96, 131, 132, 133, 149, 150, 151], "2822": 6, "4": [6, 7, 39, 87, 131, 132, 138, 144, 146, 148, 149, 151, 152, 153], "4872": 6, "7469": 6, "8": [6, 32, 33, 36, 38, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 132, 149, 152, 153], "6737": 6, "11": [6, 132, 149, 151, 153], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 131, 132], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 26, 28, 30, 37, 39, 102, 130, 146, 147, 148, 149, 150, 151, 152], "find": [6, 8, 9, 146, 149, 150, 152], "list": [6, 7, 15, 16, 19, 21, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 97, 98, 102, 103, 104, 105, 106, 108, 109, 110, 113, 115, 119, 131, 132, 147, 148, 150, 151], "builder": [6, 35, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 86, 147, 148, 153], "hope": 6, "deeper": [6, 150], "insight": [6, 149], "happi": [6, 149], "thi": [7, 8, 9, 10, 20, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 41, 42, 84, 85, 87, 88, 91, 92, 93, 94, 95, 97, 99, 102, 103, 104, 107, 108, 109, 110, 112, 113, 115, 116, 117, 120, 124, 126, 127, 129, 130, 132, 133, 134, 135, 137, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153], "pars": [7, 10, 11, 105, 113, 147, 150], "effect": 7, "cli": [7, 9, 11, 12, 143, 149, 150], "prerequisit": [7, 147, 148, 149, 150, 151, 152, 153], "Be": [7, 147, 149, 150, 151, 152, 153], "familiar": [7, 147, 149, 150, 151, 152, 153], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 143, 147, 148, 150], "instal": [7, 9, 124, 129, 130, 142, 146, 149, 150, 151, 152, 153], "fundament": 7, "There": [7, 15, 25, 147, 149, 150, 151, 152], "entri": [7, 8, 150], "point": [7, 8, 23, 148, 149, 150, 151, 152, 153], "locat": [7, 146, 151, 152, 153], "thei": [7, 8, 20, 27, 92, 103, 113, 118, 146, 147, 148, 152], "truth": [7, 149, 151], "reproduc": 7, "overridden": [7, 88, 113], "quick": [7, 27], "experiment": 7, "modifi": [7, 8, 9, 94, 144, 149, 151, 152, 153], "serv": [7, 107, 148, 152], "particular": [7, 26, 27, 39, 107, 148, 152, 153], "seed": [7, 8, 9, 137, 150], "shuffl": [7, 29], "devic": [7, 8, 102, 112, 116, 117, 120, 146, 147, 149, 150, 151, 152], "cuda": [7, 116, 117, 120, 143, 149, 153], "dtype": [7, 8, 89, 92, 94, 117, 136, 139, 149, 153], "fp32": [7, 153], "enable_fsdp": 7, "mani": [7, 29, 148, 149], "object": [7, 10, 11, 15, 16, 19, 21, 87, 107, 121, 147], "keyword": [7, 10, 26, 28, 30, 31, 34, 35, 37, 39, 41, 42, 94, 147, 148], "loss": [7, 8, 28, 32, 33, 36, 38, 96, 150, 152, 153], "function": [7, 8, 10, 12, 26, 87, 88, 94, 96, 99, 102, 103, 107, 108, 115, 116, 122, 126, 132, 137, 144, 147, 148, 153], "exampl": [7, 8, 9, 10, 12, 16, 19, 21, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 87, 96, 97, 99, 104, 107, 108, 109, 111, 112, 115, 121, 129, 130, 131, 132, 136, 138, 140, 141, 143, 145, 146, 147, 148, 149, 151, 152, 153], "subfield": 7, "dotpath": 7, "wish": [7, 148], "exact": [7, 10, 149], "path": [7, 8, 9, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 54, 62, 71, 80, 85, 104, 105, 108, 109, 110, 113, 133, 146, 147, 148, 149, 151, 152], "normal": [7, 26, 29, 90, 92, 93, 104, 147, 148, 152, 153], "python": [7, 105, 113, 119, 130, 137, 140, 146, 149], "alpaca_dataset": [7, 32, 148], "custom": [7, 8, 26, 28, 30, 34, 37, 41, 135, 144, 146, 149, 150, 151, 152], "train_on_input": [7, 23, 26, 28, 32, 33, 34, 36, 37, 38, 39, 147, 148], "onc": [7, 99, 149, 150, 151, 152, 153], "ve": [7, 89, 105, 147, 148, 149, 151, 152], "instanc": [7, 10, 27, 88, 94, 100, 101, 152], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 34, 146, 149, 153], "under": [7, 148, 149, 151, 153], "preced": [7, 10, 146, 151, 152], "actual": [7, 9, 26, 147], "throw": 7, "notic": [7, 147, 148, 152], "miss": [7, 102, 103, 152], "posit": [7, 10, 29, 87, 89, 91, 92, 93, 151], "anoth": [7, 149], "handl": [7, 12, 20, 27, 104, 147, 149, 152, 153], "def": [7, 8, 9, 12, 107, 111, 147, 148, 152, 153], "dictconfig": [7, 8, 10, 11, 12, 13, 130], "arg": [7, 10, 92, 94, 97, 106, 113, 128], "tupl": [7, 10, 27, 39, 89, 94, 96, 104, 105, 107, 113, 122, 131, 132, 139], "kwarg": [7, 10, 94, 97, 106, 113, 123, 127, 128, 129, 130, 135, 148], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 54, 62, 71, 80, 85, 94, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 112, 113, 116, 117, 119, 120, 121, 123, 125, 127, 128, 129, 130, 131, 132, 133, 137, 138, 139, 147, 148], "mean": [7, 87, 90, 92, 93, 126, 146, 147, 148, 150, 152], "pass": [7, 10, 26, 27, 28, 30, 31, 34, 35, 37, 41, 42, 87, 88, 94, 99, 103, 107, 110, 117, 118, 120, 123, 126, 129, 130, 135, 146, 147, 148, 152, 153], "add": [7, 9, 26, 29, 105, 110, 111, 113, 148, 149, 151, 152, 153], "d": [7, 20, 87, 89, 92, 105, 146, 147, 152], "llama2_token": [7, 149], "tmp": [7, 112, 147, 150, 151], "option": [7, 8, 14, 17, 18, 22, 24, 26, 28, 29, 30, 31, 34, 35, 37, 39, 41, 42, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 87, 91, 92, 93, 94, 102, 103, 104, 105, 108, 109, 110, 115, 116, 117, 119, 121, 127, 130, 133, 137, 143, 144, 146, 148, 149], "bool": [7, 20, 23, 26, 28, 29, 32, 33, 34, 36, 37, 38, 39, 41, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 94, 98, 102, 103, 104, 105, 106, 107, 108, 109, 110, 118, 120, 123, 124, 126, 129, 133, 135, 138, 147, 153], "max_seq_len": [7, 10, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 87, 89, 91, 92, 104, 105, 147, 148], "int": [7, 9, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 89, 90, 91, 92, 95, 98, 104, 105, 106, 107, 108, 109, 110, 115, 118, 122, 126, 127, 128, 129, 130, 131, 132, 135, 137, 146, 147, 148, 152, 153], "512": [7, 32, 33, 148, 153], "instructdataset": [7, 32, 33, 36, 37, 38, 148], "alreadi": [7, 111, 123, 126, 143, 146, 148, 149, 152], "overwrit": [7, 110, 143], "duplic": [7, 8, 144, 146], "sometim": 7, "than": [7, 25, 39, 87, 89, 107, 110, 111, 138, 139, 147, 148, 149, 150, 151, 152, 153], "resolv": [7, 11, 150], "alpaca": [7, 14, 32, 33, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 148], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 127, 128, 129, 130], "disklogg": 7, "log_dir": [7, 127, 129, 130], "conveni": [7, 8, 146], "verifi": [7, 116, 117, 118, 147, 150, 152], "properli": [7, 102, 124, 146], "experi": [7, 130, 142, 144, 147, 151, 152], "wa": [7, 102, 147, 149, 151, 152, 153], "cp": [7, 143, 146, 147, 149, 150, 151], "7b_lora_single_devic": [7, 149, 150, 152, 153], "my_config": 7, "discuss": [7, 85, 150, 152], "guidelin": 7, "while": [7, 8, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 88, 144, 149, 153], "mai": [7, 9, 118, 133, 147, 148, 150, 152], "tempt": 7, "put": [7, 8, 150, 152], "much": [7, 149, 151, 152, 153], "give": [7, 148, 152], "maximum": [7, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 87, 89, 91, 92, 105, 146], "flexibl": [7, 27, 148], "switch": 7, "encourag": [7, 152], "clariti": 7, "significantli": 7, "easier": [7, 149, 150], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 29, 31, 41, 85, 96, 148, 153], "expos": [7, 8, 110, 147, 150], "parent": [7, 146], "modul": [7, 10, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 111, 114, 118, 126, 134, 135, 137, 147, 150, 152, 153], "__init__": [7, 8, 152, 153], "py": [7, 10, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 87, 89, 90, 91, 95, 96, 146, 149, 151], "guarante": 7, "stabil": [7, 144, 153], "underscor": 7, "_alpaca": 7, "collect": [7, 115, 150], "differ": [7, 9, 26, 27, 28, 30, 104, 111, 132, 139, 144, 146, 147, 149, 151, 152, 153], "itself": 7, "via": [7, 9, 34, 37, 41, 98, 108, 152, 153], "pair": [7, 40, 131, 132, 148], "k1": [7, 8], "v1": [7, 8, 42], "k2": [7, 8], "v2": [7, 8, 148], "lora_finetune_single_devic": [7, 146, 147, 149, 150, 151, 152, 153], "checkpoint": [7, 8, 94, 105, 108, 109, 110, 111, 112, 130, 135, 144, 146, 151, 152, 153], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 26, 27, 28, 29, 30, 31, 34, 37, 87, 88, 89, 90, 91, 92, 93, 96, 97, 98, 100, 101, 104, 105, 106, 108, 109, 110, 111, 112, 113, 127, 128, 129, 130, 147, 148, 150, 152, 153], "assign": [7, 31], "nest": 7, "dot": 7, "notat": [7, 87, 91, 92], "certain": [7, 147], "flag": [7, 8, 28, 32, 33, 36, 38, 107, 110, 118, 146, 153], "built": [7, 9, 40, 143, 147, 150, 153], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 26], "specif": [7, 8, 10, 118, 147, 148, 149, 153], "pytorch": [7, 8, 92, 94, 102, 107, 124, 129, 133, 135, 137, 142, 143, 144, 151, 152, 153], "llama3": [7, 26, 39, 69, 70, 71, 72, 73, 74, 75, 111, 115, 118, 142, 146, 148], "8b_full": [7, 146, 148], "adamw": [7, 152], "lr": [7, 95], "2e": 7, "fuse": [7, 134], "nproc_per_nod": [7, 148, 151, 152], "full_finetune_distribut": [7, 146, 148, 149, 150], "core": [8, 144, 148, 150, 153], "i": [8, 19, 21, 87, 92, 93, 94, 101, 105, 112, 115, 148, 149, 151, 153], "structur": [8, 15, 16, 19, 21, 26, 147, 148, 149], "new": [8, 35, 78, 89, 111, 127, 129, 147, 149, 150, 151, 152, 153], "user": [8, 15, 16, 19, 20, 21, 23, 25, 26, 87, 104, 147, 148, 150], "thought": [8, 144, 150, 153], "target": [8, 144], "pipelin": [8, 144], "llm": [8, 142, 144, 148, 149, 152], "eg": [8, 92, 108, 144], "meaning": [8, 144, 149], "featur": [8, 9, 143, 144, 149, 150], "fsdp": [8, 107, 112, 118, 126, 144, 150, 151], "activ": [8, 88, 120, 125, 135, 144, 153], "gradient": [8, 126, 134, 144, 149, 151, 152, 153], "accumul": [8, 134, 144], "mix": [8, 146, 148, 149], "precis": [8, 94, 117, 144, 150, 153], "appli": [8, 26, 28, 30, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 90, 91, 92, 93, 102, 103, 135, 144, 153], "given": [8, 10, 18, 25, 98, 99, 106, 115, 116, 117, 121, 126, 134, 138, 144, 152], "complex": 8, "becom": [8, 143, 148], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 92, 111, 146, 148], "methodolog": 8, "reason": [8, 115, 149], "possibl": [8, 26, 29, 34, 146, 148], "trade": 8, "off": [8, 104, 149], "memori": [8, 27, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 94, 102, 118, 120, 125, 126, 142, 144, 149, 150, 151], "vs": [8, 150], "qualiti": [8, 149, 152], "believ": 8, "best": [8, 147], "suit": [8, 150], "b": [8, 87, 89, 91, 92, 93, 98, 126, 130, 152, 153], "fit": [8, 26, 28, 29, 30, 31, 32, 33, 35, 37, 41, 42, 148], "solut": 8, "result": [8, 104, 149, 151, 152, 153], "meant": [8, 94, 112], "depend": [8, 9, 14, 108, 146, 148, 149, 152, 153], "level": [8, 114, 119, 126, 144, 153], "expertis": 8, "routin": 8, "yourself": [8, 146, 151, 152], "exist": [8, 143, 146, 149, 150, 151, 153], "ad": [8, 104, 110, 111, 147, 152, 153], "ones": 8, "modular": [8, 144], "build": [8, 34, 37, 41, 144, 151, 152], "block": [8, 29, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 102, 103, 144], "wandb": [8, 9, 130, 150], "log": [8, 11, 96, 119, 120, 125, 127, 128, 129, 130, 149, 150, 151, 153], "fulli": [8, 27], "nativ": [8, 142, 144, 152, 153], "correct": [8, 17, 36, 90, 91, 92, 116, 144, 147, 148], "numer": [8, 144], "pariti": [8, 144], "verif": 8, "extens": [8, 110, 144], "comparison": [8, 152, 153], "benchmark": [8, 137, 144, 149, 151, 152], "limit": [8, 112, 148], "hidden": [8, 88], "behind": 8, "100": [8, 28, 32, 33, 36, 38, 39, 115, 131, 132, 133, 152, 153], "prefer": [8, 40, 96, 132, 144, 146, 148], "over": [8, 95, 113, 144, 149, 151, 152, 153], "unnecessari": 8, "abstract": [8, 15, 18, 106, 144, 150, 153], "No": [8, 110, 144], "inherit": [8, 113, 144, 148], "go": [8, 19, 21, 104, 144, 148, 149, 150, 153], "upon": [8, 27, 151], "figur": [8, 152, 153], "spectrum": 8, "decid": 8, "interact": [8, 142, 150], "start": [8, 9, 27, 105, 111, 143, 144, 147, 148, 149, 150], "paradigm": 8, "consist": [8, 42, 150], "configur": [8, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 93, 144, 147, 150, 151, 152, 153], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 83, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 107, 108, 109, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 144, 146, 147, 148, 149, 150, 151, 152, 153], "overrid": [8, 11, 12, 146, 149, 150, 151, 153], "togeth": [8, 29, 130, 150, 152], "valid": [8, 25, 102, 103, 139, 143, 149, 150], "environ": [8, 116, 124, 143, 146, 149, 150], "logic": [8, 111, 144, 150, 152], "api": [8, 9, 49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 102, 146, 147, 149, 150, 151, 153], "closer": [8, 152], "monolith": [8, 144], "trainer": [8, 96], "A": [8, 9, 23, 27, 29, 87, 92, 93, 94, 96, 98, 102, 104, 105, 107, 112, 113, 120, 121, 125, 126, 131, 132, 141, 142, 145, 146, 147, 149, 152, 153], "wrapper": [8, 104, 105, 112, 114, 146, 152], "around": [8, 26, 104, 105, 120, 133, 146, 147, 149, 152, 153], "extern": [8, 148], "primarili": [8, 27, 152], "eleutherai": [8, 144, 152], "har": [8, 144, 152], "control": [8, 28, 32, 33, 36, 38, 99, 137, 149], "multi": [8, 26, 87, 102, 151], "stage": 8, "distil": 8, "oper": [8, 27, 99, 133, 137], "turn": [8, 20, 25, 26, 105, 147], "dataload": [8, 29, 32, 33, 36, 38], "applic": [8, 87, 108, 109, 130], "clean": [8, 9, 32], "after": [8, 87, 89, 90, 92, 93, 102, 126, 127, 128, 129, 130, 147, 153], "process": [8, 9, 94, 122, 123, 137, 148, 150, 153], "group": [8, 87, 122, 123, 127, 128, 129, 130, 146, 151], "init_process_group": [8, 123], "backend": [8, 146], "gloo": 8, "els": [8, 113, 130, 144, 153], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 27, 110, 113, 118, 148, 150, 151, 152], "stuff": 8, "carri": 8, "relev": [8, 20, 146, 149, 152], "interfac": [8, 15, 18, 27, 97], "metric": [8, 150], "logger": [8, 119, 125, 127, 128, 129, 130, 150], "self": [8, 9, 29, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 87, 92, 93, 97, 102, 103, 108, 111, 112, 148, 152, 153], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 107, 118, 126, 133, 135, 147], "_model": [8, 112], "_setup_model": 8, "_token": [8, 148], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 112, 114, 134, 153], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 96, 137, 146, 151], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 29], "batch": [8, 29, 32, 33, 36, 38, 87, 89, 91, 92, 96, 104, 131, 132, 144, 148, 150, 151, 152], "enumer": 8, "_autocast": 8, "logit": [8, 115], "label": [8, 26, 28, 29, 30, 31, 32, 33, 34, 35, 37, 39, 40, 41, 42, 96, 131, 132], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 127, 128, 129, 130], "step": [8, 29, 92, 95, 105, 114, 127, 128, 129, 130, 133, 134, 142, 149, 152, 153], "learn": [8, 27, 95, 144, 147, 148, 150, 151, 152, 153], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 24, 25, 26, 28, 29, 30, 31, 34, 35, 37, 39, 41, 42, 87, 89, 91, 92, 93, 99, 101, 102, 103, 104, 105, 108, 109, 110, 111, 115, 116, 117, 119, 121, 125, 127, 128, 129, 130, 134, 135, 136, 137, 139, 147, 148, 149], "fullfinetunerecip": 8, "direct": [8, 96, 132, 143], "wandblogg": [9, 152, 153], "workspac": 9, "seen": [9, 152, 153], "screenshot": 9, "below": [9, 91, 107, 148, 151, 152, 153], "packag": [9, 129, 130, 143], "pip": [9, 129, 130, 143, 149, 151], "Then": [9, 99, 150], "login": [9, 130, 146, 149], "project": [9, 46, 47, 48, 63, 64, 65, 72, 73, 76, 77, 83, 87, 88, 102, 103, 118, 130, 142, 152, 153], "grab": [9, 151], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 124, 147], "exit": [9, 143, 146], "resourc": [9, 127, 128, 129, 130], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 28, 29, 30, 31, 37, 39, 41, 87, 91, 92, 93, 115, 147, 149], "desir": [9, 26, 136, 147], "suggest": 9, "approach": [9, 27, 148], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 149], "_output_dir": [9, 108, 109, 110], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 12, 20, 23, 24, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 83, 84, 85, 87, 89, 90, 91, 92, 93, 94, 96, 98, 100, 104, 105, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 126, 132, 133, 135, 136, 137, 138, 148, 149, 152, 153], "descript": [9, 34, 39, 146], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 23, 26, 29, 32, 33, 36, 38, 125, 148], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 26, 28, 30, 31, 34, 35, 37, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 78, 79, 81, 82, 83, 84, 85, 86, 87, 90, 91, 95, 96, 102, 107, 108, 109, 113, 119, 124, 129, 130, 133, 135, 137, 143, 148, 149], "com": [10, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 87, 90, 91, 95, 96, 102, 143], "facebookresearch": [10, 90, 91], "blob": [10, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83, 85, 87, 90, 91, 95, 96], "main": [10, 12, 85, 87, 90, 91, 143, 149, 151], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 92], "32": [10, 151, 152, 153], "num_head": [10, 87, 89, 91, 92], "num_kv_head": [10, 87, 89], "vocab_s": 10, "must": [10, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 97, 105, 113, 153], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 83, 84, 85, 87, 89, 90, 91, 92, 93, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 131, 132, 133, 136, 137, 138, 147, 148, 152, 153], "nn": [10, 87, 88, 89, 92, 93, 94, 97, 99, 100, 101, 107, 114, 126, 134, 135, 139, 152, 153], "parsed_yaml": 10, "embed_dim": [10, 87, 91, 93, 152], "valueerror": [10, 21, 25, 34, 39, 87, 89, 92, 96, 108, 109, 110, 117, 120, 137, 139], "recipe_nam": 11, "rank": [11, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 98, 122, 124, 137, 150, 152, 153], "zero": [11, 89, 90, 149, 151], "displai": 11, "callabl": [12, 26, 28, 30, 92, 99, 107, 115, 118, 121, 126, 135], "With": [12, 149, 152, 153], "my_recip": 12, "foo": 12, "bar": [12, 144, 150], "instanti": [13, 43, 44, 45, 46, 47, 48, 52, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 69, 70, 71, 72, 73, 76, 77, 78, 79, 80, 83, 84, 85, 112], "configerror": 13, "cannot": [13, 110, 151], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 120, 127, 128, 129, 130, 148, 149, 153], "prompt": [14, 15, 17, 18, 19, 21, 22, 23, 26, 28, 30, 32, 33, 34, 36, 37, 38, 39, 41, 92, 104, 115, 148, 149, 151], "templat": [14, 15, 17, 18, 22, 26, 28, 30, 32, 33, 36, 37, 38, 39], "style": [14, 29, 32, 33, 34, 39, 153], "slightli": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 148], "map": [14, 17, 18, 22, 23, 26, 27, 28, 29, 30, 37, 101, 108, 112, 114, 127, 128, 129, 130, 134, 147, 148, 149, 152], "column_map": [14, 17, 18, 22, 26, 28, 30, 37, 148], "placehold": [14, 15, 17, 18, 22, 26, 28, 30, 37, 148], "column": [14, 17, 18, 22, 26, 28, 30, 31, 37, 41, 87, 92, 93, 147, 148], "ident": [14, 17, 18, 21, 22, 28, 29, 30, 37, 149], "role": [15, 20, 23, 26, 104, 147, 148], "system": [15, 16, 19, 20, 21, 23, 25, 26, 85, 104, 147, 148], "assist": [15, 16, 19, 20, 23, 25, 26, 85, 104, 115, 147, 148], "messag": [15, 16, 19, 21, 23, 25, 26, 34, 85, 104, 105, 106, 143, 146, 147, 148], "accord": [15, 21, 147], "openai": [16, 34, 148], "markup": 16, "languag": [16, 98, 115, 152], "It": [16, 21, 146, 147, 148, 153], "huggingfac": [16, 26, 28, 30, 31, 34, 35, 37, 41, 42, 79, 84, 85, 95, 96, 108, 109, 146, 149], "im_start": 16, "context": [16, 84, 99, 133, 136, 148], "im_end": 16, "goe": [16, 99], "respons": [16, 96, 104, 148, 149, 150, 151], "appropri": [16, 19, 21, 27, 95, 108, 148, 153], "tag": [16, 19, 21, 26, 105, 127, 128, 129, 130, 147], "grammar": [17, 36, 148], "sentenc": [17, 29, 104], "alwai": [18, 113], "human": [19, 23, 147], "taken": [19, 152, 153], "inst": [19, 21, 26, 147, 148], "sy": [19, 147, 148], "respect": [19, 27, 101, 147, 148], "honest": [19, 147, 148], "am": [19, 21, 147, 148, 149, 151], "pari": [19, 21, 148], "capit": [19, 21, 148], "franc": [19, 21, 148], "known": [19, 21, 104, 121, 148], "its": [19, 21, 29, 87, 91, 92, 93, 134, 137, 146, 148, 149, 151, 152], "stun": [19, 21, 148], "liter": [20, 46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 102, 103], "mask": [20, 28, 29, 32, 33, 36, 38, 87, 92, 93, 104, 105, 147, 148], "ipython": 20, "eot": 20, "dataclass": [20, 147], "repres": [20, 132, 147], "individu": [20, 29, 120, 130, 135, 147, 148], "tiktoken": [20, 105, 151], "special": [20, 26, 85, 104, 105, 112, 148], "variabl": [20, 26, 27, 28, 30, 37, 124, 153], "writer": 20, "whether": [20, 23, 26, 28, 32, 33, 34, 36, 37, 38, 39, 41, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 94, 98, 102, 103, 104, 105, 107, 117, 120, 147, 148], "correspond": [20, 97, 100, 117, 132, 150, 151], "consecut": [20, 25], "from_dict": [20, 147], "construct": [20, 152], "dictionari": [20, 29, 120, 125, 127, 128, 129, 130, 132, 149], "mistral": [21, 26, 39, 76, 77, 78, 79, 80, 81, 82, 111, 146, 147, 149, 150], "llama2chatformat": [21, 147, 148], "summar": [22, 38, 147, 148], "task": [22, 27, 35, 147, 148, 149, 151, 152, 153], "dialogu": [22, 38, 147], "dialog": 22, "adher": 23, "sharegpt": [23, 34], "gpt": [23, 87, 149], "remain": [23, 95, 152], "unmask": 23, "eos_id": 24, "length": [24, 25, 27, 28, 29, 30, 31, 32, 33, 35, 37, 39, 41, 42, 84, 87, 89, 91, 92, 104, 105, 109, 131, 132], "last": [24, 29, 95, 148], "replac": [24, 28, 32, 33, 36, 38, 94, 152], "forth": [25, 148], "come": [25, 97, 152], "empti": [25, 146], "shorter": 25, "min": [25, 152], "invalid": 25, "convert_to_messag": [26, 147], "chat_format": [26, 34, 39, 147, 148], "chatformat": [26, 34, 148], "load_dataset_kwarg": [26, 28, 30, 31, 34, 35, 37, 41, 42], "multiturn": [26, 147], "prepar": [26, 147], "truncat": [26, 28, 29, 30, 31, 35, 37, 39, 41, 42, 104, 105, 148], "encod": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 96, 104, 105, 106, 147], "decod": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 92, 104, 105, 106, 115, 147], "anyth": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42], "load_dataset": [26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 147, 148], "co": [26, 28, 30, 31, 34, 35, 37, 41, 42, 79, 84, 85, 108, 109, 149], "doc": [26, 28, 30, 31, 34, 35, 37, 41, 42, 107, 113, 119, 124, 129, 130, 133, 137, 146, 149], "en": [26, 28, 30, 31, 34, 35, 37, 41, 42], "package_refer": [26, 28, 30, 31, 34, 35, 37, 41, 42], "loading_method": [26, 28, 30, 31, 34, 35, 37, 41, 42], "text": [26, 29, 31, 35, 41, 42, 104, 105, 106, 147, 149], "extra": [26, 143, 152, 153], "still": [26, 113, 152, 153], "where": [26, 27, 32, 33, 36, 38, 87, 92, 98, 104, 118, 126, 132, 148], "unless": 26, "check": [26, 34, 92, 102, 117, 124, 138, 142, 147, 149, 150, 152], "concaten": [27, 104, 106, 132], "sub": [27, 129], "unifi": [27, 79], "were": [27, 99, 147, 150], "simplifi": [27, 146, 152], "simultan": 27, "intern": [27, 113], "aggreg": 27, "transpar": 27, "index": [27, 87, 91, 92, 93, 95, 131, 132, 143, 147, 149], "howev": [27, 85, 143], "constitu": 27, "might": [27, 146, 149], "larg": [27, 98, 146, 153], "comput": [27, 87, 88, 91, 92, 96, 120, 137, 149, 153], "cumul": 27, "maintain": [27, 153], "indic": [27, 29, 87, 91, 92, 93, 107, 124, 147], "deleg": 27, "retriev": [27, 118], "lead": [27, 104], "high": [27, 144, 152], "scale": [27, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 98, 115, 152, 153], "consid": 27, "strategi": 27, "stream": [27, 119], "demand": 27, "deriv": [27, 88, 92, 93], "_dataset": 27, "_len": 27, "total": [27, 95, 122, 141, 145, 149, 151, 152], "combin": 27, "_index": 27, "lookup": 27, "dataset1": 27, "mycustomdataset": 27, "params1": 27, "dataset2": 27, "params2": 27, "concat_dataset": 27, "data_point": 27, "1500": 27, "element": [27, 105, 149], "focus": [27, 150], "enhanc": [27, 153], "divers": 27, "machin": [27, 116, 146, 149], "instructtempl": [28, 30, 148], "contribut": [28, 32, 33, 36, 38], "disabl": [28, 30, 31, 35, 37, 41, 42, 99, 137], "recommend": [28, 30, 31, 32, 33, 35, 37, 41, 42, 129, 147, 149, 153], "highest": [28, 30, 31, 32, 33, 35, 37, 41, 42], "sequenc": [28, 29, 30, 31, 32, 33, 35, 37, 39, 41, 42, 87, 89, 91, 92, 104, 105, 131, 132, 147], "ds": [29, 39], "max_pack": 29, "split_across_pack": 29, "greedi": 29, "pack": [29, 32, 33, 34, 36, 37, 38, 39, 41, 87, 91, 92, 93], "done": [29, 102, 117, 126, 152, 153], "preprocess": 29, "outsid": [29, 137, 149, 151, 152], "sampler": [29, 150], "part": [29, 147, 153], "buffer": 29, "long": [29, 147, 152], "enough": [29, 147], "attent": [29, 46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 84, 87, 89, 91, 92, 93, 102, 103, 151, 152, 153], "lower": [29, 152], "triangular": 29, "cross": 29, "attend": [29, 87, 92, 93], "rel": [29, 87, 91, 92, 93, 120, 152], "pad": [29, 115, 131, 132, 148], "max": [29, 39, 92, 95, 104, 146, 152], "wise": 29, "collat": [29, 131, 148], "made": [29, 34, 37, 41, 91, 149], "smaller": [29, 149, 151, 152, 153], "jam": 29, "vari": 29, "s1": [29, 104], "s2": [29, 104], "s3": 29, "s4": 29, "contamin": 29, "input_po": [29, 87, 89, 91, 92, 93], "matrix": 29, "causal": [29, 87, 92, 93], "continu": [29, 148], "increment": 29, "move": [29, 92], "entir": [29, 126, 147, 153], "avoid": [29, 90, 94, 137, 146, 153], "freeform": [31, 41], "unstructur": [31, 42], "corpu": [31, 35, 42], "local": [31, 41, 130, 137, 143, 146, 147, 149, 150], "tabular": [31, 41], "yahma": [32, 37], "codebas": [32, 33, 36, 38, 149], "prior": [32, 33, 34, 36, 37, 38, 39, 41], "alpaca_d": [32, 33], "batch_siz": [32, 33, 36, 38, 87, 89, 92, 93, 96, 149], "tatsu": 33, "lab": 33, "conversation_styl": [34, 148], "chatdataset": [34, 39, 147, 148], "friendli": [34, 37, 41, 115, 147], "huggingfaceh4": 34, "no_robot": 34, "chatmlformat": 34, "2096": [34, 37, 41], "accomplish": [34, 37, 41], "packeddataset": [34, 37, 41, 148], "ccdv": 35, "cnn_dailymail": 35, "textcompletiondataset": [35, 41, 42, 148], "similar": [35, 40, 42, 102, 148, 149, 151, 152, 153], "cnn": 35, "dailymail": 35, "articl": [35, 42], "extract": 35, "highlight": [35, 153], "liweili": 36, "c4_200m": 36, "variant": [36, 38], "mirror": [36, 38], "llama_recip": [36, 38], "grammar_d": 36, "alpaca_clean": 37, "alpacainstructtempl": [37, 148], "samsum": [38, 148], "summari": [38, 120, 148], "samsum_d": 38, "open": [39, 52, 53, 148, 149], "orca": 39, "slimorca": 39, "dedup": 39, "1024": [39, 40, 148], "prescrib": 39, "least": [39, 151, 152], "though": [39, 147], "10": [39, 131, 132, 149, 151, 153], "351": 39, "82": [39, 149], "391": 39, "221": 39, "220": 39, "193": 39, "12": [39, 132, 143], "471": 39, "lvwerra": [40, 148], "stack": [40, 148], "exchang": [40, 148], "preferencedataset": [40, 148], "stackexchangepair": 40, "textdataset": 41, "omit": [41, 152], "allenai": [41, 148], "c4": [41, 148], "data_dir": [41, 148], "realnewslik": [41, 148], "wikitext": 42, "subset": [42, 100], "103": [42, 149], "raw": 42, "wikipedia": 42, "page": [42, 143, 144, 146, 150, 151], "code_llama2": [43, 44, 45, 46, 47, 48, 49, 50, 51, 146], "transformerdecod": [43, 44, 45, 46, 47, 48, 49, 50, 51, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 79, 81, 82, 83, 84, 86, 115, 152], "w": [43, 44, 45, 52, 53, 59, 60, 61, 69, 70, 78, 79, 129, 130, 147, 149, 152, 153], "arxiv": [43, 44, 45, 49, 50, 51, 57, 58, 59, 60, 61, 66, 67, 68, 74, 75, 81, 82, 86, 87, 90, 91, 96], "org": [43, 44, 45, 49, 50, 51, 57, 58, 59, 60, 61, 66, 67, 68, 74, 75, 81, 82, 86, 87, 90, 91, 96, 107, 113, 119, 124, 129, 133, 135, 137, 143], "pdf": [43, 44, 45, 87, 90], "2308": [43, 44, 45], "12950": [43, 44, 45], "70b": [44, 47, 50, 60, 64, 67, 69, 72, 74, 151], "lora_attn_modul": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 102, 103, 152, 153], "q_proj": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 102, 103, 152, 153], "k_proj": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 102, 103, 152, 153], "v_proj": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 102, 103, 152, 153], "output_proj": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 102, 103, 152, 153], "apply_lora_to_mlp": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 102, 103, 152], "apply_lora_to_output": [46, 47, 48, 49, 50, 51, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 102, 103, 152], "lora_rank": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 152], "lora_alpha": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 152], "float": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 87, 90, 95, 96, 98, 115, 120, 125, 127, 128, 129, 130, 152, 153], "16": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 132, 152, 153], "lora_dropout": [46, 47, 48, 49, 50, 51, 63, 64, 65, 66, 67, 68], "05": [46, 47, 48, 49, 50, 51, 63, 64, 65, 66, 67, 68], "quantize_bas": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 98, 153], "lora": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 98, 99, 102, 103, 108, 126, 142, 144, 147, 150, 151], "code_llama2_13b": 46, "tloen": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83], "8bb8579e403dc78e37fe81ffbb253c413007323f": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83], "l41": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83], "l43": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 83], "linear": [46, 47, 48, 49, 50, 51, 55, 56, 57, 58, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 77, 81, 82, 83, 86, 92, 97, 98, 102, 103, 152, 153], "mlp": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 92, 93, 102, 103, 151, 152], "final": [46, 47, 48, 63, 64, 65, 72, 73, 76, 77, 83, 88, 92, 99, 102, 103, 105, 149, 151, 152, 153], "low": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 98, 149, 152, 153], "approxim": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 98, 152], "factor": [46, 47, 48, 55, 56, 63, 64, 65, 72, 73, 76, 77, 83, 98, 149], "code_llama2_70b": 47, "code_llama2_7b": 48, "qlora": [49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 94, 142, 144, 151, 152], "per": [49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 89, 94, 146, 151, 153], "paper": [49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 152, 153], "ab": [49, 50, 51, 57, 58, 59, 60, 61, 66, 67, 68, 74, 75, 81, 82, 86, 91, 96], "2305": [49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86, 87, 96], "14314": [49, 50, 51, 57, 58, 66, 67, 68, 74, 75, 81, 82, 86], "lora_code_llama2_13b": 49, "lora_code_llama2_70b": 50, "lora_code_llama2_7b": 51, "gemma": [52, 53, 54, 55, 56, 57, 58, 111], "gemmatransformerdecod": [52, 53, 55, 56, 57, 58], "blog": [52, 53], "technolog": [52, 53], "develop": [52, 53, 153], "sentencepiecetoken": [54, 62, 80, 147], "gemma_2b": 55, "gemma_7b": 56, "lora_gemma_2b": 57, "lora_gemma_7b": 58, "2307": [59, 60, 61], "09288": [59, 60, 61], "llama2_70b": 64, "llama2_7b": [65, 152], "lora_llama2_13b": 66, "lora_llama2_70b": 67, "lora_llama2_7b": [68, 152], "tiktokentoken": [71, 147], "llama3_70b": 72, "llama3_8b": [73, 115, 151], "lora_llama3_70b": 74, "lora_llama3_8b": 75, "classifi": [77, 79, 82, 148], "announc": 78, "ray2333": 79, "reward": [79, 96], "feedback": 79, "transformerclassifi": 79, "lora_mistral_7b": 81, "lora_mistral_classifier_7b": 82, "phi3": [83, 84, 85, 86, 111, 146], "phi3_mini": [83, 111], "ref": [84, 85, 130], "phi": [84, 85, 111], "128k": 84, "nor": 84, "slide": 84, "window": [84, 148], "phi3minisentencepiecetoken": 85, "tokenizer_config": 85, "spm": 85, "lm": 85, "eo": [85, 104, 105, 147, 148], "bo": [85, 104, 105, 147, 148], "unk": 85, "augment": [85, 153], "endoftext": 85, "opt": [85, 143, 150], "cite": 85, "better": [85, 144, 147, 148, 149], "51": 85, "lora_phi3_mini": 86, "head_dim": [87, 89, 92], "pos_embed": [87, 152], "kv_cach": 87, "kvcach": [87, 92], "attn_dropout": [87, 92], "head": [87, 89, 91, 92, 111, 151], "queri": [87, 89, 92, 93, 151], "gqa": 87, "introduc": [87, 90, 98, 147, 148, 152, 153], "13245v1": 87, "version": [87, 115, 138, 143, 147, 151, 153], "multihead": 87, "mha": [87, 92], "n": [87, 104, 105, 141, 145, 147, 148], "extrem": 87, "share": [87, 148, 149], "mqa": 87, "credit": 87, "document": [87, 107, 118, 126, 146, 148], "lightn": 87, "lit": 87, "lit_gpt": 87, "v": [87, 92, 152], "k": [87, 152], "q": [87, 152], "n_kv_head": 87, "dimens": [87, 89, 91, 92, 98, 151, 152, 153], "calcul": [87, 92, 151], "e": [87, 94, 97, 101, 108, 112, 120, 143, 149, 151, 152, 153], "g": [87, 97, 108, 120, 151, 152, 153], "rotarypositionalembed": [87, 152], "cach": [87, 89, 91, 92, 143, 146], "rope": [87, 91], "dropout": [87, 98, 152, 153], "onto": 87, "scaled_dot_product_attent": 87, "seq_length": [87, 93, 115], "boolean": [87, 92, 93, 107], "softmax": [87, 92, 93], "row": [87, 92, 93, 147], "j": [87, 92, 93], "seq_len": 87, "bigger": 87, "n_h": [87, 91], "num": [87, 91], "n_kv": 87, "kv": [87, 89, 92], "emb": [87, 92], "h_d": [87, 91], "gate_proj": 88, "down_proj": 88, "up_proj": 88, "silu": 88, "feed": [88, 93], "network": [88, 99, 152, 153], "fed": [88, 147], "multipli": 88, "subclass": [88, 113], "although": [88, 152], "afterward": 88, "former": 88, "regist": [88, 94, 134, 153], "hook": [88, 94, 134, 153], "latter": 88, "standalon": 89, "past": 89, "becaus": [89, 92, 110, 146, 147, 149, 151], "expand": 89, "dpython": [89, 92, 94], "reset": [89, 92, 120], "k_val": 89, "v_val": 89, "h": [89, 143, 146], "longer": [89, 148], "ep": 90, "1e": 90, "06": [90, 152], "root": [90, 129, 130], "squar": 90, "1910": 90, "07467": 90, "verfic": [90, 91], "small": [90, 149], "divis": 90, "10000": 91, "rotari": [91, 151], "propos": 91, "2104": 91, "09864": 91, "l450": 91, "upto": 91, "init": [91, 120, 130, 153], "exceed": 91, "freq": 91, "recomput": 91, "geometr": 91, "progress": [91, 150], "rotat": 91, "angl": 91, "todo": 91, "effici": [91, 102, 118, 142, 144, 149, 150, 152], "transformerdecoderlay": 92, "norm": [92, 93], "space": 92, "belong": [92, 114], "reduc": [92, 144, 148, 152, 153], "statement": 92, "improv": [92, 118, 149, 151, 152], "readabl": [92, 149], "At": 92, "arang": 92, "prompt_length": 92, "causal_mask": 92, "m_": 92, "seq": 92, "reset_cach": 92, "setup_cach": 92, "attn": [93, 152, 153], "causalselfattent": [93, 152], "sa_norm": 93, "mlp_norm": 93, "ff": 93, "common_util": 94, "bfloat16": [94, 136, 149, 150, 151, 152], "offload_to_cpu": 94, "nf4": [94, 153], "restor": 94, "higher": [94, 151, 153], "offload": [94, 153], "increas": [94, 95, 151, 152], "peak": [94, 120, 125, 149, 151, 152, 153], "gpu": [94, 146, 149, 150, 151, 152, 153], "_register_state_dict_hook": 94, "m": [94, 105, 115, 147], "mymodul": 94, "_after_": 94, "nf4tensor": [94, 153], "unquant": [94, 149, 153], "unus": 94, "num_warmup_step": 95, "num_training_step": 95, "num_cycl": 95, "last_epoch": 95, "lambdalr": 95, "rate": [95, 144, 150], "schedul": [95, 133, 150], "linearli": 95, "decreas": [95, 148, 152, 153], "cosin": 95, "v4": 95, "23": [95, 151], "src": 95, "l104": 95, "warmup": [95, 133], "phase": 95, "wave": 95, "half": 95, "lr_schedul": 95, "beta": 96, "label_smooth": 96, "loss_typ": 96, "sigmoid": 96, "dpo": [96, 99, 132], "18290": 96, "trl": 96, "librari": [96, 113, 117, 119, 137, 142, 144, 146, 148, 153], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 96, "dpo_train": 96, "l844": 96, "temperatur": [96, 115, 149], "uncertainti": 96, "hing": 96, "ipo": 96, "kto_pair": 96, "policy_chosen_logp": 96, "policy_rejected_logp": 96, "reference_chosen_logp": 96, "reference_rejected_logp": 96, "polici": [96, 99, 107, 118, 126, 135], "probabl": [96, 98, 115, 149], "chosen": [96, 148], "reject": [96, 148], "chosen_reward": 96, "rejected_reward": 96, "unknown": 96, "peft": [97, 98, 99, 100, 101, 102, 103, 108, 152, 153], "protocol": 97, "adapter_param": [97, 98, 99, 100, 101], "proj": 97, "in_dim": [97, 98, 152, 153], "out_dim": [97, 98, 152, 153], "bia": [97, 98, 152, 153], "loralinear": [97, 152, 153], "alpha": [98, 152, 153], "use_bia": 98, "perturb": 98, "decomposit": [98, 152], "matric": [98, 126, 152, 153], "trainabl": [98, 101, 126, 152, 153], "mapsto": 98, "w_0x": 98, "r": [98, 105, 152], "bax": 98, "lora_a": [98, 152, 153], "lora_b": [98, 152, 153], "temporarili": 99, "neural": [99, 152, 153], "treat": [99, 113, 147], "attribut": [99, 104, 114], "caller": 99, "whose": [99, 134], "yield": 99, "get_adapter_param": [101, 152], "base_miss": 102, "base_unexpect": 102, "lora_miss": 102, "lora_unexpect": 102, "validate_state_dict_for_lora": [102, 152], "unlik": [102, 149, 151], "reli": 102, "unexpect": 102, "strict": [102, 152], "pull": [102, 146], "120600": 102, "assertionerror": [102, 103, 132], "nonempti": 102, "full_model_state_dict_kei": 103, "lora_state_dict_kei": 103, "base_model_state_dict_kei": 103, "confirm": [103, 143], "determin": 103, "lora_modul": 103, "complement": 103, "disjoint": 103, "union": [103, 127, 128, 129, 130, 135, 137], "non": [103, 104], "overlap": 103, "sentencepieceprocessor": 104, "pretrain": [104, 105, 146, 147, 150, 152, 153], "spm_model": [104, 147], "tokenized_text": 104, "hello": [104, 147, 149, 151], "world": [104, 122, 124, 149], "add_bo": [104, 105, 106, 147], "add_eo": [104, 105, 106, 147], "31587": 104, "29644": 104, "102": 104, "trim_leading_whitespac": 104, "prefix": 104, "unbatch": 104, "prepend": [104, 105], "append": [104, 143], "trim": 104, "whitespac": 104, "underli": [104, 153], "sentencepiec": [104, 151], "due": [104, 152, 153], "tokenize_messag": [104, 105, 106, 147, 148], "problem": 104, "slice": 104, "tokenizer_path": 104, "separ": [104, 108, 147, 150, 151, 152, 153], "concat": 104, "1788": 104, "2643": 104, "13": [104, 132, 149, 151, 153], "1792": 104, "9508": 104, "465": 104, "22137": 104, "2933": 104, "join": 104, "llama3_tiktoken": 105, "p": [105, 107, 112, 152, 153], "l": 105, "all_special_token": 105, "bos_token": 105, "begin_of_text": [105, 147], "eos_token": 105, "end_of_text": 105, "start_header_id": [105, 147], "end_header_id": [105, 147], "step_id": 105, "eom_id": 105, "eot_id": [105, 147], "python_tag": 105, "identif": 105, "regex": 105, "second": [105, 149, 151, 152, 153], "uniqu": [105, 111], "256": [105, 149, 151], "header": [105, 147], "token_id": [105, 106], "truncate_at_eo": 105, "tokenize_head": 105, "datatyp": [107, 153], "denot": 107, "integ": [107, 131, 137], "auto_wrap_polici": [107, 118, 135], "submodul": [107, 126], "obei": 107, "contract": 107, "get_fsdp_polici": 107, "modules_to_wrap": [107, 118, 126], "min_num_param": 107, "my_fsdp_polici": 107, "recurs": [107, 126, 129], "isinst": [107, 148], "sum": [107, 152], "numel": [107, 152], "1000": 107, "functool": 107, "partial": 107, "stabl": [107, 124, 129, 133, 137, 143], "html": [107, 113, 119, 124, 129, 133, 135, 137, 142], "alia": 107, "from_pretrain": 108, "0001_of_0003": 108, "0002_of_0003": 108, "preserv": [108, 153], "weight_map": [108, 149], "convert_weight": 108, "_model_typ": [108, 111], "intermediate_checkpoint": [108, 109, 110], "_weight_map": 108, "shard": [109, 151], "wip": 109, "larger": [110, 149, 151], "present": 110, "down": [110, 148, 152, 153], "intermedi": [110, 135, 151, 153], "qualnam": 111, "boundari": 111, "distinguish": 111, "gate": [111, 146, 150], "my_new_model": 111, "my_custom_state_dict_map": 111, "mistral_reward": 111, "classif": 111, "mistral_classifi": 111, "optim_map": 112, "bare": 112, "bone": 112, "distribut": [112, 116, 123, 124, 135, 137, 144, 146, 150, 151], "optim_dict": [112, 114, 134], "cfg_optim": 112, "ckpt": 112, "optim_ckpt": 112, "placeholder_optim_dict": 112, "optiminbackwardwrapp": 112, "get_optim_kei": 112, "arbitrari": [112, 152], "hyperparamet": [112, 144, 150, 152, 153], "optim_ckpt_map": 112, "runtimeerror": [112, 117, 123], "loadabl": 112, "argpars": 113, "argumentpars": 113, "builtin": 113, "said": 113, "noth": 113, "consult": 113, "info": [113, 150], "parse_known_arg": 113, "namespac": 113, "act": 113, "precid": 113, "parse_arg": 113, "properti": [113, 152], "too": [113, 151], "optimizerinbackwardwrapp": 114, "top": [114, 149, 153], "named_paramet": 114, "max_generated_token": 115, "pad_id": 115, "top_k": [115, 149], "stop_token": 115, "custom_generate_next_token": 115, "condit": [115, 124, 146, 148], "bsz": 115, "predict": 115, "prune": [115, 153], "stop": 115, "compil": [115, 149, 151, 153], "generate_next_token": 115, "llama3_token": [115, 151], "hi": [115, 147], "my": [115, 146, 147, 148, 149, 151], "jeremi": 115, "float32": 117, "bf16": [117, 153], "request": [117, 148, 149], "inde": [117, 149], "kernel": 117, "isn": [117, 146], "hardwar": [117, 144, 148, 149, 152], "memory_efficient_fsdp_wrap": 118, "maxim": [118, 126, 142, 144], "been": [118, 151], "workload": 118, "15": [118, 132, 147, 149, 152, 153], "alongsid": 118, "ac": 118, "fullyshardeddataparallel": [118, 126], "fsdppolicytyp": [118, 126], "handler": 119, "reset_stat": 120, "track": 120, "alloc": [120, 125, 126, 151, 153], "reserv": [120, 125, 147, 153], "stat": [120, 125, 153], "int4": 121, "4w": [121, 149, 151], "recogn": 121, "mode": [121, 149], "aka": 122, "master": 124, "port": [124, 146], "address": 124, "hold": [124, 150], "peak_memory_act": 125, "peak_memory_alloc": 125, "peak_memory_reserv": 125, "get_memory_stat": 125, "own": [126, 137, 146, 147, 148, 149, 152], "unit": [126, 144], "hierarch": 126, "requires_grad": [126, 152, 153], "filenam": 127, "log_": 127, "unixtimestamp": 127, "txt": [127, 148, 150], "thread": 127, "safe": 127, "flush": [127, 128, 129, 130], "ndarrai": [127, 128, 129, 130], "scalar": [127, 128, 129, 130], "record": [127, 128, 129, 130], "payload": [127, 128, 129, 130], "organize_log": 129, "tensorboard": 129, "subdirectori": 129, "compar": [129, 138, 149, 152, 153], "logdir": 129, "startup": 129, "tree": [129, 148, 149], "tfevent": 129, "encount": 129, "frontend": 129, "organ": [129, 146], "accordingli": 129, "my_log_dir": 129, "view": [129, 149, 150], "my_metr": [129, 130], "termin": [129, 130], "entiti": 130, "bias": 130, "sent": 130, "usernam": 130, "my_project": 130, "my_ent": 130, "my_group": 130, "importerror": 130, "account": [130, 152, 153], "log_config": 130, "link": [130, 149], "capecap": 130, "6053ofw0": 130, "torchtune_config_j67sb73v": 130, "padding_idx": [131, 132], "ignore_idx": [131, 132], "longest": 131, "token_pair": 131, "input_id": 132, "chosen_input_id": [132, 148], "chosen_label": [132, 148], "rejected_input_id": [132, 148], "rejected_label": [132, 148], "14": [132, 153], "17": [132, 149, 152], "18": [132, 151], "19": [132, 149, 151, 153], "20": 132, "torchtune_perf_trac": 133, "contextmanag": [133, 136], "wait": 133, "trace": 133, "speed": [133, 151, 153], "reduct": [133, 152], "soon": 134, "readi": [134, 142, 147], "grad": 134, "achiev": [134, 149, 151, 152, 153], "acwrappolicytyp": 135, "describ": [135, 148], "author": [135, 144, 150, 153], "fsdp_adavnced_tutori": 135, "insid": 136, "debug_mod": 137, "pseudo": 137, "random": [137, 150], "commonli": [137, 149, 152, 153], "numpi": 137, "determinist": 137, "global": [137, 148], "warn": 137, "nondeterminist": 137, "addition": [137, 148, 152], "cudnn": 137, "set_deterministic_debug_mod": 137, "algorithm": 137, "greater": 138, "equal": 138, "against": [138, 153], "__version__": 138, "named_param": 139, "iter": [139, 153], "generated_examples_python": 140, "zip": 140, "galleri": [140, 145], "sphinx": 140, "000": [141, 145, 151], "execut": [141, 145], "generated_exampl": 141, "mem": [141, 145], "mb": [141, 145], "topic": 142, "gentl": 142, "introduct": 142, "first_finetune_tutori": 142, "workflow": [142, 148, 150, 152], "requisit": 143, "proper": [143, 150], "host": [143, 146, 150], "latest": [143, 150, 153], "And": [143, 149, 151], "ls": [143, 146, 149, 150, 151], "welcom": [143, 146], "show": [143, 146, 147, 152], "greatest": [143, 150], "contributor": 143, "cd": [143, 149], "even": [143, 146, 147, 148, 151, 152, 153], "commit": 143, "branch": 143, "url": 143, "whl": 143, "therebi": [143, 153], "forc": 143, "reinstal": 143, "suffix": 143, "cu121": 143, "On": [144, 152], "pointer": 144, "emphas": 144, "aspect": 144, "simplic": 144, "component": 144, "reus": 144, "prove": 144, "democrat": 144, "box": [144, 153], "zoo": 144, "varieti": [144, 152], "techniqu": [144, 149, 150, 152], "integr": [144, 149, 150, 151, 152, 153], "excit": 144, "checkout": 144, "quickstart": 144, "attain": 144, "chekckpoint": 144, "embodi": 144, "philosophi": 144, "usabl": 144, "composit": 144, "hard": [144, 148], "outlin": 144, "unecessari": 144, "never": 144, "thoroughli": 144, "short": 146, "subcommand": 146, "anytim": 146, "symlink": 146, "auto": 146, "wrote": 146, "readm": 146, "md": 146, "lot": [146, 149], "recent": 146, "releas": [146, 151], "agre": 146, "term": 146, "perman": 146, "eat": 146, "bandwith": 146, "storag": [146, 153], "00030": 146, "ootb": 146, "full_finetune_single_devic": [146, 148, 149, 150], "7b_full_low_memori": [146, 149, 150], "8b_full_single_devic": [146, 148], "mini_full_low_memori": 146, "7b_full": [146, 149, 150], "13b_full": [146, 149, 150], "70b_full": 146, "edit": 146, "destin": 146, "lora_finetune_distribut": [146, 151, 152], "torchrun": 146, "8b_lora_single_devic": [146, 147, 151], "launch": [146, 147, 150], "nproc": 146, "node": 146, "worker": 146, "nnode": [146, 152], "minimum_nod": 146, "maximum_nod": 146, "fail": 146, "rdzv": 146, "rendezv": 146, "endpoint": 146, "8b_lora": [146, 151], "bypass": 146, "vice": 146, "versa": 146, "fancy_lora": 146, "8b_fancy_lora": 146, "sai": [146, 147, 150], "further": [146, 148, 152, 153], "know": [147, 148, 149, 151, 152], "align": 147, "intend": 147, "nice": 147, "meet": 147, "overhaul": 147, "accompani": 147, "who": 147, "influenti": 147, "hip": 147, "hop": 147, "artist": [147, 151], "2pac": 147, "rakim": 147, "c": 147, "na": 147, "flavor": [147, 148], "msg": 147, "formatted_messag": [147, 148], "nyou": [147, 148], "nwho": 147, "why": [147, 150, 152], "user_messag": 147, "518": 147, "25580": 147, "29962": 147, "3532": 147, "14816": 147, "29903": 147, "6778": 147, "piece_to_id": 147, "vector": 147, "place": 147, "manual": [147, 153], "529": 147, "29879": 147, "29958": 147, "nhere": 147, "_encode_special_token": 147, "128000": 147, "128009": 147, "pure": 147, "That": 147, "won": [147, 149, 151], "mess": 147, "govern": 147, "prime": 147, "strictli": 147, "summarizetempl": [147, 148], "lightweight": 147, "ask": 147, "untouch": 147, "nsummari": 147, "robust": 147, "csv": [147, 148], "question": [147, 148, 149, 151], "answer": [147, 149, 151], "onlin": 147, "forum": 147, "panda": 147, "pd": 147, "df": 147, "read_csv": 147, "your_fil": 147, "nrow": 147, "tolist": 147, "iloc": 147, "gp": 147, "receiv": 147, "commun": [147, 148, 149], "satellit": 147, "thing": [147, 153], "message_convert": 147, "input_msg": 147, "output_msg": 147, "assistant_messag": 147, "But": [147, 149, 151, 152], "mistralchatformat": 147, "custom_dataset": 147, "2048": 147, "data_fil": [147, 148], "honor": 147, "copi": [147, 149, 150, 151, 153], "custom_8b_lora_single_devic": 147, "steer": 148, "wheel": 148, "publicli": 148, "great": [148, 149], "hood": [148, 149, 153], "text_completion_dataset": 148, "padded_col": 148, "upper": 148, "constraint": [148, 152], "slow": [148, 153], "signific": 148, "speedup": [148, 151], "minim": [148, 150, 152, 153], "my_data": 148, "instruct_dataset": 148, "fix": 148, "goal": 148, "agnost": 148, "respond": 148, "anim": 148, "plant": 148, "miner": 148, "oak": 148, "copper": 148, "ore": 148, "eleph": 148, "customtempl": 148, "cl": 148, "chat_dataset": 148, "quit": [148, 153], "similarli": 148, "incorpor": 148, "advanc": 148, "customchatformat": 148, "vicgal": 148, "gpt4": 148, "drive": 148, "rajpurkar": 148, "io": 148, "squad": 148, "explor": 148, "rlhf": 148, "few": [148, 151, 152, 153], "adjust": 148, "chosen_messag": 148, "transformed_sampl": 148, "key_chosen": 148, "rejected_messag": 148, "key_reject": 148, "c_mask": 148, "np": 148, "cross_entropy_ignore_idx": 148, "r_mask": 148, "stack_exchanged_paired_dataset": 148, "had": 148, "stackexchangedpairedtempl": 148, "response_j": 148, "response_k": 148, "rl": 148, "favorit": [149, 151, 152], "seemlessli": 149, "beyond": [149, 153], "connect": 149, "amount": 149, "natur": 149, "export": 149, "mobil": 149, "phone": 149, "leverag": [149, 151, 153], "plai": 149, "freez": [149, 152], "percentag": 149, "learnabl": 149, "keep": [149, 152], "16gb": [149, 152], "rtx": 149, "3090": 149, "4090": 149, "hour": 149, "7b_qlora_single_devic": [149, 150, 153], "473": 149, "98": [149, 153], "gb": [149, 151, 152, 153], "50": 149, "484": 149, "01": [149, 150], "fact": [149, 151, 152], "third": 149, "realli": 149, "eleuther_ev": [149, 151], "eleuther_evalu": [149, 151], "lm_eval": [149, 151], "plan": 149, "custom_eval_config": [149, 151], "truthfulqa_mc2": [149, 151, 152], "measur": [149, 151], "propens": [149, 151], "shot": [149, 151], "accuraci": [149, 151, 152, 153], "baselin": [149, 152], "324": 149, "loglikelihood": 149, "195": 149, "121": 149, "27": 149, "197": 149, "acc": 149, "388": 149, "38": 149, "shown": 149, "489": 149, "48": [149, 153], "seem": 149, "custom_generation_config": [149, 151], "kick": 149, "300": 149, "interest": 149, "site": 149, "visit": 149, "bai": 149, "area": 149, "92": [149, 151], "exploratorium": 149, "san": 149, "francisco": 149, "magazin": 149, "awesom": 149, "bridg": 149, "pretti": 149, "cool": 149, "96": [149, 153], "61": 149, "sec": [149, 151], "25": 149, "83": 149, "99": [149, 152], "72": 149, "littl": 149, "saw": 149, "took": [149, 151], "torchao": [149, 151, 153], "bit": [149, 151, 152, 153], "custom_quantization_config": [149, 151], "68": 149, "76": 149, "69": 149, "95": [149, 151], "67": 149, "engin": [149, 151], "fullmodeltorchtunecheckpoint": [149, 151], "int4weightonlyquant": [149, 151], "groupsiz": [149, 151], "did": [149, 151, 153], "park": 149, "sit": 149, "hill": 149, "beauti": 149, "62": [149, 151], "85": 149, "sped": 149, "almost": [149, 151, 152], "3x": [149, 151], "benefit": 149, "doesn": 149, "yet": 149, "fast": 149, "clone": [149, 152, 153], "assumpt": 149, "satisfi": 149, "new_dir": 149, "output_dict": 149, "sd_1": 149, "sd_2": 149, "dump": 149, "convert_hf_checkpoint": 149, "checkpoint_path": 149, "justin": 149, "school": 149, "math": 149, "teacher": 149, "ws": 149, "94": [149, 151], "28": 149, "bandwidth": [149, 151], "1391": 149, "84": 149, "thats": 149, "seamlessli": 149, "authent": [149, 150], "hopefulli": 149, "gave": 149, "grant": 150, "minut": 150, "agreement": 150, "altern": 150, "hackabl": 150, "singularli": 150, "technic": 150, "purpos": [150, 151], "depth": 150, "principl": 150, "boilerpl": 150, "substanti": [150, 152], "custom_config": 150, "replic": 150, "lorafinetunerecipesingledevic": 150, "lora_finetune_output": 150, "log_1713194212": 150, "52": 150, "3697006702423096": 150, "25880": [150, 153], "24": [150, 151], "55": 150, "83it": 150, "monitor": 150, "tqdm": 150, "interv": 150, "e2": 150, "focu": 151, "128": [151, 152], "theta": 151, "gain": 151, "illustr": 151, "basic": 151, "observ": 151, "consum": [151, 153], "vram": [151, 152], "overal": 151, "8b_qlora_single_devic": 151, "coupl": [151, 152, 153], "meta_model_0": 151, "122": 151, "sarah": 151, "busi": 151, "mum": 151, "young": 151, "children": 151, "live": 151, "north": 151, "east": 151, "england": 151, "135": 151, "88": 151, "138": 151, "346": 151, "09": 151, "139": 151, "31": 151, "far": 151, "drill": 151, "90": 151, "93": 151, "91": 151, "104": 151, "four": [151, 152], "again": 151, "jake": 151, "disciplin": 151, "passion": 151, "draw": 151, "paint": 151, "57": [151, 152, 153], "broader": 151, "teach": 152, "straight": 152, "jump": 152, "unfamiliar": 152, "oppos": [152, 153], "momentum": 152, "could": 152, "relat": 152, "aghajanyan": 152, "et": 152, "al": 152, "hypothes": 152, "intrins": 152, "often": 152, "eight": 152, "practic": 152, "imag": 152, "left": 152, "blue": 152, "rememb": 152, "approx": 152, "15m": 152, "8192": 152, "65k": 152, "frozen_out": [152, 153], "lora_out": [152, 153], "base_model": 152, "choos": 152, "lora_model": 152, "lora_llama_2_7b": [152, 153], "alon": 152, "in_featur": 152, "out_featur": 152, "inplac": 152, "feel": 152, "free": 152, "whenev": 152, "peft_util": 152, "set_trainable_param": 152, "fetch": 152, "lora_param": 152, "total_param": 152, "trainable_param": 152, "2f": 152, "6742609920": 152, "4194304": 152, "7b_lora": 152, "my_model_checkpoint_path": [152, 153], "tokenizer_checkpoint": [152, 153], "my_tokenizer_checkpoint_path": [152, 153], "factori": 152, "benefici": 152, "impact": 152, "minor": 152, "good": 152, "64": 152, "lora_experiment_1": 152, "smooth": [152, 153], "curv": [152, 153], "500": 152, "ran": 152, "footprint": 152, "commod": 152, "cogniz": 152, "ax": 152, "parallel": 152, "truthfulqa": 152, "previous": 152, "475": 152, "87": 152, "508": 152, "86": 152, "504": 152, "04": 152, "514": 152, "lowest": 152, "absolut": 152, "4gb": 152, "tradeoff": 152, "potenti": 152, "highli": 153, "vanilla": 153, "held": 153, "therefor": 153, "bespok": 153, "normalfloat": 153, "8x": 153, "retain": 153, "vast": 153, "major": 153, "degrad": 153, "normatfloat": 153, "doubl": 153, "themselv": 153, "deepdiv": 153, "idea": 153, "distinct": 153, "de": 153, "incur": 153, "counterpart": 153, "set_default_devic": 153, "qlora_linear": 153, "memory_alloc": 153, "177": 153, "152": 153, "byte": 153, "del": 153, "empty_cach": 153, "lora_linear": 153, "081": 153, "344": 153, "qlora_llama2_7b": 153, "qlora_model": 153, "essenti": 153, "reparametrize_as_dtype_state_dict_post_hook": 153, "35": 153, "40": 153, "29": 153, "slower": 153, "149": 153, "9157477021217346": 153, "02": 153, "08": 153, "15it": 153, "nightli": 153, "200": 153, "hundr": 153, "228": 153, "8158286809921265": 153, "59": 153, "95it": 153, "exercis": 153, "portion": 153, "linear_nf4": 153, "to_nf4": 153, "linear_weight": 153, "autograd": 153, "regular": 153, "incom": 153}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "SummarizeTemplate"], [23, 0, 1, "", "sharegpt_to_llama2_messages"], [24, 0, 1, "", "truncate"], [25, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[22, 2, 1, "", "format"]], "torchtune.datasets": [[26, 1, 1, "", "ChatDataset"], [27, 1, 1, "", "ConcatDataset"], [28, 1, 1, "", "InstructDataset"], [29, 1, 1, "", "PackedDataset"], [30, 1, 1, "", "PreferenceDataset"], [31, 1, 1, "", "TextCompletionDataset"], [32, 0, 1, "", "alpaca_cleaned_dataset"], [33, 0, 1, "", "alpaca_dataset"], [34, 0, 1, "", "chat_dataset"], [35, 0, 1, "", "cnn_dailymail_articles_dataset"], [36, 0, 1, "", "grammar_dataset"], [37, 0, 1, "", "instruct_dataset"], [38, 0, 1, "", "samsum_dataset"], [39, 0, 1, "", "slimorca_dataset"], [40, 0, 1, "", "stack_exchanged_paired_dataset"], [41, 0, 1, "", "text_completion_dataset"], [42, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[43, 0, 1, "", "code_llama2_13b"], [44, 0, 1, "", "code_llama2_70b"], [45, 0, 1, "", "code_llama2_7b"], [46, 0, 1, "", "lora_code_llama2_13b"], [47, 0, 1, "", "lora_code_llama2_70b"], [48, 0, 1, "", "lora_code_llama2_7b"], [49, 0, 1, "", "qlora_code_llama2_13b"], [50, 0, 1, "", "qlora_code_llama2_70b"], [51, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[52, 0, 1, "", "gemma_2b"], [53, 0, 1, "", "gemma_7b"], [54, 0, 1, "", "gemma_tokenizer"], [55, 0, 1, "", "lora_gemma_2b"], [56, 0, 1, "", "lora_gemma_7b"], [57, 0, 1, "", "qlora_gemma_2b"], [58, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[59, 0, 1, "", "llama2_13b"], [60, 0, 1, "", "llama2_70b"], [61, 0, 1, "", "llama2_7b"], [62, 0, 1, "", "llama2_tokenizer"], [63, 0, 1, "", "lora_llama2_13b"], [64, 0, 1, "", "lora_llama2_70b"], [65, 0, 1, "", "lora_llama2_7b"], [66, 0, 1, "", "qlora_llama2_13b"], [67, 0, 1, "", "qlora_llama2_70b"], [68, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[69, 0, 1, "", "llama3_70b"], [70, 0, 1, "", "llama3_8b"], [71, 0, 1, "", "llama3_tokenizer"], [72, 0, 1, "", "lora_llama3_70b"], [73, 0, 1, "", "lora_llama3_8b"], [74, 0, 1, "", "qlora_llama3_70b"], [75, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[76, 0, 1, "", "lora_mistral_7b"], [77, 0, 1, "", "lora_mistral_classifier_7b"], [78, 0, 1, "", "mistral_7b"], [79, 0, 1, "", "mistral_classifier_7b"], [80, 0, 1, "", "mistral_tokenizer"], [81, 0, 1, "", "qlora_mistral_7b"], [82, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.phi3": [[83, 0, 1, "", "lora_phi3_mini"], [84, 0, 1, "", "phi3_mini"], [85, 0, 1, "", "phi3_mini_tokenizer"], [86, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[87, 1, 1, "", "CausalSelfAttention"], [88, 1, 1, "", "FeedForward"], [89, 1, 1, "", "KVCache"], [90, 1, 1, "", "RMSNorm"], [91, 1, 1, "", "RotaryPositionalEmbeddings"], [92, 1, 1, "", "TransformerDecoder"], [93, 1, 1, "", "TransformerDecoderLayer"], [95, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[87, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[88, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[89, 2, 1, "", "reset"], [89, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[90, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[91, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[92, 2, 1, "", "forward"], [92, 2, 1, "", "reset_caches"], [92, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[93, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[94, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[96, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[96, 2, 1, "", "forward"]], "torchtune.modules.peft": [[97, 1, 1, "", "AdapterModule"], [98, 1, 1, "", "LoRALinear"], [99, 0, 1, "", "disable_adapter"], [100, 0, 1, "", "get_adapter_params"], [101, 0, 1, "", "set_trainable_params"], [102, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [103, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[97, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[98, 2, 1, "", "adapter_params"], [98, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[104, 1, 1, "", "SentencePieceTokenizer"], [105, 1, 1, "", "TikTokenTokenizer"], [106, 1, 1, "", "Tokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[104, 2, 1, "", "decode"], [104, 2, 1, "", "encode"], [104, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[105, 2, 1, "", "decode"], [105, 2, 1, "", "encode"], [105, 2, 1, "", "tokenize_message"], [105, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.Tokenizer": [[106, 2, 1, "", "decode"], [106, 2, 1, "", "encode"], [106, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[107, 4, 1, "", "FSDPPolicyType"], [108, 1, 1, "", "FullModelHFCheckpointer"], [109, 1, 1, "", "FullModelMetaCheckpointer"], [110, 1, 1, "", "FullModelTorchTuneCheckpointer"], [111, 1, 1, "", "ModelType"], [112, 1, 1, "", "OptimizerInBackwardWrapper"], [113, 1, 1, "", "TuneRecipeArgumentParser"], [114, 0, 1, "", "create_optim_in_bwd_wrapper"], [115, 0, 1, "", "generate"], [116, 0, 1, "", "get_device"], [117, 0, 1, "", "get_dtype"], [118, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [119, 0, 1, "", "get_logger"], [120, 0, 1, "", "get_memory_stats"], [121, 0, 1, "", "get_quantizer_mode"], [122, 0, 1, "", "get_world_size_and_rank"], [123, 0, 1, "", "init_distributed"], [124, 0, 1, "", "is_distributed"], [125, 0, 1, "", "log_memory_stats"], [126, 0, 1, "", "lora_fsdp_wrap_policy"], [131, 0, 1, "", "padded_collate"], [132, 0, 1, "", "padded_collate_dpo"], [133, 0, 1, "", "profiler"], [134, 0, 1, "", "register_optim_in_bwd_hooks"], [135, 0, 1, "", "set_activation_checkpointing"], [136, 0, 1, "", "set_default_dtype"], [137, 0, 1, "", "set_seed"], [138, 0, 1, "", "torch_version_ge"], [139, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[108, 2, 1, "", "load_checkpoint"], [108, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[109, 2, 1, "", "load_checkpoint"], [109, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[110, 2, 1, "", "load_checkpoint"], [110, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[111, 3, 1, "", "GEMMA"], [111, 3, 1, "", "LLAMA2"], [111, 3, 1, "", "LLAMA3"], [111, 3, 1, "", "MISTRAL"], [111, 3, 1, "", "MISTRAL_REWARD"], [111, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[112, 2, 1, "", "get_optim_key"], [112, 2, 1, "", "load_state_dict"], [112, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[113, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[127, 1, 1, "", "DiskLogger"], [128, 1, 1, "", "StdoutLogger"], [129, 1, 1, "", "TensorBoardLogger"], [130, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[127, 2, 1, "", "close"], [127, 2, 1, "", "log"], [127, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[128, 2, 1, "", "close"], [128, 2, 1, "", "log"], [128, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[129, 2, 1, "", "close"], [129, 2, 1, "", "log"], [129, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[130, 2, 1, "", "close"], [130, 2, 1, "", "log"], [130, 2, 1, "", "log_config"], [130, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 107, 142, 144, 146, 149, 151, 152, 153], "config": [0, 7, 8, 146, 150], "data": [1, 5, 147], "instruct": [1, 143, 148, 151], "templat": [1, 147, 148], "chat": [1, 147, 148], "format": [1, 6, 148], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 147, 148], "exampl": 2, "gener": [2, 115, 149, 151], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 146, 149, 150, 151, 152], "llama3": [3, 147, 151], "llama2": [3, 147, 149, 152, 153], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 143, 153], "block": 4, "token": [4, 106, 147], "peft": 4, "util": [4, 5, 107], "loss": 4, "checkpoint": [5, 6, 9, 149], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 148, 152, 153], "manag": 5, "perform": [5, 152], "profil": [5, 133], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 144, 149], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 149, 152, 153], "put": [6, 153], "thi": 6, "all": [6, 7, 153], "togeth": [6, 153], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 148], "us": [7, 8, 147, 149, 153], "instanti": [7, 10], "referenc": 7, "other": [7, 149], "field": 7, "interpol": 7, "valid": [7, 13, 146], "your": [7, 8, 149, 150], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 144, 152, 153], "ar": 8, "recip": [8, 146, 150, 152], "script": 8, "run": [8, 146, 149], "cli": [8, 146], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "summarizetempl": 22, "sharegpt_to_llama2_messag": 23, "truncat": 24, "validate_messag": 25, "chatdataset": 26, "concatdataset": 27, "instructdataset": 28, "packeddataset": 29, "preferencedataset": 30, "textcompletiondataset": 31, "alpaca_cleaned_dataset": 32, "alpaca_dataset": 33, "chat_dataset": 34, "cnn_dailymail_articles_dataset": 35, "grammar_dataset": 36, "instruct_dataset": 37, "samsum_dataset": 38, "slimorca_dataset": 39, "stack_exchanged_paired_dataset": 40, "text_completion_dataset": 41, "wikitext_dataset": 42, "code_llama2_13b": 43, "code_llama2_70b": 44, "code_llama2_7b": 45, "lora_code_llama2_13b": 46, "lora_code_llama2_70b": 47, "lora_code_llama2_7b": 48, "qlora_code_llama2_13b": 49, "qlora_code_llama2_70b": 50, "qlora_code_llama2_7b": 51, "gemma_2b": 52, "gemma_7b": 53, "gemma_token": 54, "lora_gemma_2b": 55, "lora_gemma_7b": 56, "qlora_gemma_2b": 57, "qlora_gemma_7b": 58, "llama2_13b": 59, "llama2_70b": 60, "llama2_7b": 61, "llama2_token": 62, "lora_llama2_13b": 63, "lora_llama2_70b": 64, "lora_llama2_7b": 65, "qlora_llama2_13b": 66, "qlora_llama2_70b": 67, "qlora_llama2_7b": 68, "llama3_70b": 69, "llama3_8b": 70, "llama3_token": 71, "lora_llama3_70b": 72, "lora_llama3_8b": 73, "qlora_llama3_70b": 74, "qlora_llama3_8b": 75, "lora_mistral_7b": 76, "lora_mistral_classifier_7b": 77, "mistral_7b": 78, "mistral_classifier_7b": 79, "mistral_token": 80, "qlora_mistral_7b": 81, "qlora_mistral_classifier_7b": 82, "lora_phi3_mini": 83, "phi3_mini": 84, "phi3_mini_token": 85, "qlora_phi3_mini": 86, "causalselfattent": 87, "todo": [87, 93], "feedforward": 88, "kvcach": 89, "rmsnorm": 90, "rotarypositionalembed": 91, "transformerdecod": 92, "transformerdecoderlay": 93, "reparametrize_as_dtype_state_dict_post_hook": 94, "get_cosine_schedule_with_warmup": 95, "dpoloss": 96, "adaptermodul": 97, "loralinear": 98, "disable_adapt": 99, "get_adapter_param": 100, "set_trainable_param": 101, "validate_missing_and_unexpected_for_lora": 102, "validate_state_dict_for_lora": 103, "sentencepiecetoken": 104, "tiktokentoken": 105, "fsdppolicytyp": 107, "fullmodelhfcheckpoint": 108, "fullmodelmetacheckpoint": 109, "fullmodeltorchtunecheckpoint": 110, "modeltyp": 111, "optimizerinbackwardwrapp": 112, "tunerecipeargumentpars": 113, "create_optim_in_bwd_wrapp": 114, "get_devic": 116, "get_dtyp": 117, "get_full_finetune_fsdp_wrap_polici": 118, "get_logg": 119, "get_memory_stat": 120, "get_quantizer_mod": 121, "get_world_size_and_rank": 122, "init_distribut": 123, "is_distribut": 124, "log_memory_stat": 125, "lora_fsdp_wrap_polici": 126, "disklogg": 127, "stdoutlogg": 128, "tensorboardlogg": 129, "wandblogg": 130, "padded_col": 131, "padded_collate_dpo": 132, "register_optim_in_bwd_hook": 134, "set_activation_checkpoint": 135, "set_default_dtyp": 136, "set_se": 137, "torch_version_g": 138, "validate_expected_param_dtyp": 139, "comput": [141, 145], "time": [141, 145], "welcom": 142, "document": 142, "get": [142, 146, 151], "start": [142, 146], "tutori": 142, "instal": 143, "via": [143, 151], "pypi": 143, "git": 143, "clone": 143, "nightli": 143, "kei": 144, "concept": 144, "design": 144, "principl": 144, "download": [146, 149, 150], "list": 146, "built": [146, 148], "copi": 146, "fine": [147, 148, 150, 151], "tune": [147, 148, 150, 151], "chang": 147, "from": [147, 153], "prompt": 147, "special": 147, "when": 147, "should": 147, "i": 147, "custom": [147, 148], "hug": [148, 149], "face": [148, 149], "set": 148, "max": 148, "sequenc": 148, "length": 148, "sampl": 148, "pack": 148, "unstructur": 148, "text": [148, 151], "corpu": 148, "multipl": 148, "local": 148, "remot": 148, "fulli": 148, "end": 149, "workflow": 149, "7b": 149, "finetun": [149, 152, 153], "evalu": [149, 151], "eleutherai": [149, 151], "s": [149, 151], "eval": [149, 151], "har": [149, 151], "speed": 149, "up": 149, "quantiz": [149, 151], "librari": 149, "upload": 149, "hub": 149, "first": 150, "llm": 150, "select": 150, "modifi": 150, "train": 150, "next": 150, "step": 150, "meta": 151, "8b": 151, "access": 151, "our": 151, "faster": 151, "how": 152, "doe": 152, "work": 152, "appli": 152, "trade": 152, "off": 152, "qlora": 153, "save": 153, "deep": 153, "dive": 153}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})