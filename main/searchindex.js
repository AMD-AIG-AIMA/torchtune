Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_generation", "api_ref_models", "api_ref_modules", "api_ref_rlhf", "api_ref_training", "api_ref_utilities", "basics/chat_datasets", "basics/instruct_datasets", "basics/prompt_templates", "basics/tokenizers", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Message", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.format_content_with_images", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.left_pad_sequence", "generated/torchtune.data.load_image", "generated/torchtune.data.padded_collate", "generated/torchtune.data.padded_collate_dpo", "generated/torchtune.data.padded_collate_sft", "generated/torchtune.data.padded_collate_tiled_images_and_mask", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.hh_rlhf_helpful_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.multimodal.llava_instruct_dataset", "generated/torchtune.datasets.multimodal.the_cauldron_dataset", "generated/torchtune.datasets.preference_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.generation.generate", "generated/torchtune.generation.generate_next_token", "generated/torchtune.generation.get_causal_mask_from_padding_mask", "generated/torchtune.generation.get_position_ids_from_padding_mask", "generated/torchtune.generation.sample", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_405b", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.MultiHeadAttention", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TanhGate", "generated/torchtune.modules.TiedLinear", "generated/torchtune.modules.TransformerCrossAttentionLayer", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerSelfAttentionLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss", "generated/torchtune.modules.model_fusion.DeepFusionModel", "generated/torchtune.modules.model_fusion.FusionEmbedding", "generated/torchtune.modules.model_fusion.FusionLayer", "generated/torchtune.modules.model_fusion.get_fusion_params", "generated/torchtune.modules.model_fusion.register_fusion_module", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.rlhf.estimate_advantages", "generated/torchtune.rlhf.get_rewards_ppo", "generated/torchtune.rlhf.loss.DPOLoss", "generated/torchtune.rlhf.loss.PPOLoss", "generated/torchtune.rlhf.loss.RSOLoss", "generated/torchtune.rlhf.loss.SimPOLoss", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.training.FSDPPolicyType", "generated/torchtune.training.FormattedCheckpointFiles", "generated/torchtune.training.FullModelHFCheckpointer", "generated/torchtune.training.FullModelMetaCheckpointer", "generated/torchtune.training.FullModelTorchTuneCheckpointer", "generated/torchtune.training.ModelType", "generated/torchtune.training.OptimizerInBackwardWrapper", "generated/torchtune.training.apply_selective_activation_checkpointing", "generated/torchtune.training.create_optim_in_bwd_wrapper", "generated/torchtune.training.get_dtype", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.training.get_memory_stats", "generated/torchtune.training.get_quantizer_mode", "generated/torchtune.training.get_unmasked_sequence_lengths", "generated/torchtune.training.get_world_size_and_rank", "generated/torchtune.training.init_distributed", "generated/torchtune.training.is_distributed", "generated/torchtune.training.log_memory_stats", "generated/torchtune.training.lora_fsdp_wrap_policy", "generated/torchtune.training.metric_logging.CometLogger", "generated/torchtune.training.metric_logging.DiskLogger", "generated/torchtune.training.metric_logging.StdoutLogger", "generated/torchtune.training.metric_logging.TensorBoardLogger", "generated/torchtune.training.metric_logging.WandBLogger", "generated/torchtune.training.register_optim_in_bwd_hooks", "generated/torchtune.training.set_activation_checkpointing", "generated/torchtune.training.set_default_dtype", "generated/torchtune.training.set_seed", "generated/torchtune.training.setup_torch_profiler", "generated/torchtune.training.update_state_dict_for_classifier", "generated/torchtune.training.validate_expected_param_dtype", "generated/torchtune.utils.batch_to_device", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.torch_version_ge", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "recipes/lora_finetune_single_device", "recipes/qat_distributed", "recipes/recipes_overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/memory_optimizations", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_generation.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_rlhf.rst", "api_ref_training.rst", "api_ref_utilities.rst", "basics/chat_datasets.rst", "basics/instruct_datasets.rst", "basics/prompt_templates.rst", "basics/tokenizers.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.format_content_with_images.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.left_pad_sequence.rst", "generated/torchtune.data.load_image.rst", "generated/torchtune.data.padded_collate.rst", "generated/torchtune.data.padded_collate_dpo.rst", "generated/torchtune.data.padded_collate_sft.rst", "generated/torchtune.data.padded_collate_tiled_images_and_mask.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.hh_rlhf_helpful_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.multimodal.llava_instruct_dataset.rst", "generated/torchtune.datasets.multimodal.the_cauldron_dataset.rst", "generated/torchtune.datasets.preference_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.generation.generate.rst", "generated/torchtune.generation.generate_next_token.rst", "generated/torchtune.generation.get_causal_mask_from_padding_mask.rst", "generated/torchtune.generation.get_position_ids_from_padding_mask.rst", "generated/torchtune.generation.sample.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.MultiHeadAttention.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TanhGate.rst", "generated/torchtune.modules.TiedLinear.rst", "generated/torchtune.modules.TransformerCrossAttentionLayer.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerSelfAttentionLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss.rst", "generated/torchtune.modules.model_fusion.DeepFusionModel.rst", "generated/torchtune.modules.model_fusion.FusionEmbedding.rst", "generated/torchtune.modules.model_fusion.FusionLayer.rst", "generated/torchtune.modules.model_fusion.get_fusion_params.rst", "generated/torchtune.modules.model_fusion.register_fusion_module.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.rlhf.estimate_advantages.rst", "generated/torchtune.rlhf.get_rewards_ppo.rst", "generated/torchtune.rlhf.loss.DPOLoss.rst", "generated/torchtune.rlhf.loss.PPOLoss.rst", "generated/torchtune.rlhf.loss.RSOLoss.rst", "generated/torchtune.rlhf.loss.SimPOLoss.rst", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.training.FSDPPolicyType.rst", "generated/torchtune.training.FormattedCheckpointFiles.rst", "generated/torchtune.training.FullModelHFCheckpointer.rst", "generated/torchtune.training.FullModelMetaCheckpointer.rst", "generated/torchtune.training.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.training.ModelType.rst", "generated/torchtune.training.OptimizerInBackwardWrapper.rst", "generated/torchtune.training.apply_selective_activation_checkpointing.rst", "generated/torchtune.training.create_optim_in_bwd_wrapper.rst", "generated/torchtune.training.get_dtype.rst", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.training.get_memory_stats.rst", "generated/torchtune.training.get_quantizer_mode.rst", "generated/torchtune.training.get_unmasked_sequence_lengths.rst", "generated/torchtune.training.get_world_size_and_rank.rst", "generated/torchtune.training.init_distributed.rst", "generated/torchtune.training.is_distributed.rst", "generated/torchtune.training.log_memory_stats.rst", "generated/torchtune.training.lora_fsdp_wrap_policy.rst", "generated/torchtune.training.metric_logging.CometLogger.rst", "generated/torchtune.training.metric_logging.DiskLogger.rst", "generated/torchtune.training.metric_logging.StdoutLogger.rst", "generated/torchtune.training.metric_logging.TensorBoardLogger.rst", "generated/torchtune.training.metric_logging.WandBLogger.rst", "generated/torchtune.training.register_optim_in_bwd_hooks.rst", "generated/torchtune.training.set_activation_checkpointing.rst", "generated/torchtune.training.set_default_dtype.rst", "generated/torchtune.training.set_seed.rst", "generated/torchtune.training.setup_torch_profiler.rst", "generated/torchtune.training.update_state_dict_for_classifier.rst", "generated/torchtune.training.validate_expected_param_dtype.rst", "generated/torchtune.utils.batch_to_device.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "recipes/lora_finetune_single_device.rst", "recipes/qat_distributed.rst", "recipes/recipes_overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/memory_optimizations.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.generation", "torchtune.models", "torchtune.modules", "torchtune.rlhf", "torchtune.training", "torchtune.utils", "Chat Datasets", "Instruct Datasets", "Prompt Templates", "Tokenizers", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "ChatFormat", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Message", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "format_content_with_images", "get_openai_messages", "get_sharegpt_messages", "left_pad_sequence", "load_image", "padded_collate", "padded_collate_dpo", "padded_collate_sft", "padded_collate_tiled_images_and_mask", "truncate", "validate_messages", "torchtune.datasets.ChatDataset", "ConcatDataset", "torchtune.datasets.InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "hh_rlhf_helpful_dataset", "instruct_dataset", "llava_instruct_dataset", "the_cauldron_dataset", "preference_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "wikitext_dataset", "generate", "generate_next_token", "get_causal_mask_from_padding_mask", "get_position_ids_from_padding_mask", "sample", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_405b", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_405b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralChatTemplate", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "FeedForward", "Fp32LayerNorm", "KVCache", "MultiHeadAttention", "RMSNorm", "RotaryPositionalEmbeddings", "TanhGate", "TiedLinear", "TransformerCrossAttentionLayer", "TransformerDecoder", "TransformerSelfAttentionLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "CEWithChunkedOutputLoss", "DeepFusionModel", "FusionEmbedding", "FusionLayer", "get_fusion_params", "register_fusion_module", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "estimate_advantages", "get_rewards_ppo", "DPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "truncate_sequence_at_first_stop_token", "torchtune.training.FSDPPolicyType", "FormattedCheckpointFiles", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "apply_selective_activation_checkpointing", "create_optim_in_bwd_wrapper", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_memory_stats", "get_quantizer_mode", "get_unmasked_sequence_lengths", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "update_state_dict_for_classifier", "validate_expected_param_dtype", "batch_to_device", "get_device", "get_logger", "torch_version_ge", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "LoRA Single Device Finetuning", "Distributed Quantization-Aware Training (QAT)", "Recipes Overview", "Computation times", "torchtune CLI", "Fine-Tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Fine-Tuning Llama2 with LoRA", "Memory Optimization Overview", "Fine-Tuning Llama3 with QAT", "Fine-Tuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 4, 9, 11, 12, 23, 24, 26, 27, 28, 34, 49, 50, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 67, 128, 136, 142, 143, 144, 151, 152, 153, 234, 237, 238, 241, 242, 245, 247, 249, 250], "prompt": [1, 9, 10, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 47, 49, 51, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 86, 92, 98, 111, 128, 137, 144, 154, 164, 170, 187, 243, 244, 246], "chat": [1, 2, 22, 23, 28, 34, 37, 38, 47, 52, 56, 92, 144, 237], "includ": [1, 9, 10, 11, 12, 13, 15, 16, 22, 27, 30, 31, 52, 73, 83, 93, 108, 117, 133, 144, 150, 164, 176, 182, 199, 200, 236, 239, 241, 242, 243, 244, 245, 246, 247, 250], "some": [1, 12, 13, 15, 23, 131, 171, 173, 178, 179, 234, 236, 237, 238, 241, 242, 243, 244, 245, 247, 248, 249, 250], "specif": [1, 5, 10, 11, 12, 15, 16, 18, 51, 52, 61, 62, 183, 207, 238, 242, 243, 244, 248, 249, 250], "format": [1, 2, 7, 11, 12, 22, 27, 29, 37, 39, 40, 47, 49, 51, 52, 55, 56, 59, 60, 63, 92, 128, 183, 198, 199, 200, 201, 202, 241, 242, 244, 245, 246, 247, 248], "differ": [1, 9, 11, 12, 15, 17, 42, 48, 49, 56, 60, 166, 184, 192, 202, 227, 236, 237, 238, 241, 242, 244, 246, 247, 248, 249, 250], "dataset": [1, 11, 15, 24, 26, 27, 28, 29, 34, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 192, 236, 245, 246, 249], "model": [1, 2, 9, 10, 11, 13, 14, 15, 16, 18, 23, 24, 26, 28, 29, 34, 47, 48, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 157, 158, 159, 160, 162, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 186, 187, 188, 190, 191, 192, 193, 194, 195, 199, 200, 201, 202, 204, 205, 207, 208, 215, 216, 221, 222, 226, 234, 236, 237, 238, 242, 243, 250], "from": [1, 2, 4, 11, 13, 14, 15, 16, 17, 18, 24, 27, 28, 29, 34, 38, 39, 40, 41, 44, 47, 48, 49, 50, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 73, 74, 75, 76, 84, 85, 92, 94, 95, 96, 97, 111, 134, 136, 144, 151, 152, 153, 154, 155, 158, 163, 164, 165, 166, 168, 169, 172, 173, 174, 175, 178, 181, 184, 186, 189, 192, 194, 195, 198, 199, 200, 201, 203, 205, 216, 219, 220, 221, 226, 233, 235, 238, 240, 241, 243, 244, 245, 246, 247, 248, 249], "common": [1, 2, 5, 9, 15, 187, 241, 242, 243, 246, 247, 248, 249], "json": [1, 10, 12, 13, 28, 34, 37, 38, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 111, 144, 154, 186, 199, 241, 242, 243, 244, 249], "schema": [1, 9, 10], "convers": [1, 11, 12, 13, 22, 24, 34, 37, 38, 46, 47, 51, 52, 56, 61, 63, 65, 199, 201, 202, 236, 242, 243, 244, 247, 248, 250], "list": [1, 9, 11, 12, 13, 15, 22, 24, 29, 30, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 56, 57, 61, 62, 63, 68, 69, 77, 78, 79, 80, 81, 82, 86, 87, 88, 89, 90, 91, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 137, 138, 139, 140, 141, 144, 145, 146, 147, 148, 149, 164, 166, 169, 170, 171, 172, 175, 176, 180, 181, 182, 183, 184, 185, 187, 189, 198, 199, 200, 201, 216, 230, 239, 242, 243, 244, 245, 246, 248, 249], "us": [1, 2, 4, 5, 9, 10, 12, 13, 14, 17, 18, 20, 22, 23, 26, 27, 29, 30, 36, 37, 38, 41, 44, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 92, 93, 99, 108, 111, 112, 117, 121, 140, 144, 146, 150, 154, 155, 157, 158, 159, 160, 162, 164, 165, 166, 167, 169, 170, 171, 174, 177, 180, 184, 185, 189, 190, 191, 192, 193, 195, 197, 199, 200, 202, 203, 206, 207, 208, 215, 216, 217, 218, 219, 220, 224, 226, 228, 229, 234, 235, 236, 237, 238, 239, 241, 243, 245, 246, 247, 248, 249], "collect": [1, 15, 245], "sampl": [1, 9, 10, 11, 12, 14, 17, 22, 24, 26, 27, 28, 29, 34, 36, 37, 38, 44, 47, 49, 50, 51, 52, 53, 58, 59, 61, 62, 63, 64, 65, 67, 69, 70, 158, 160, 164, 165, 166, 170, 188, 189, 194, 242, 244, 248], "batch": [1, 16, 41, 42, 43, 44, 50, 55, 58, 61, 62, 64, 157, 158, 160, 163, 164, 165, 166, 170, 172, 190, 191, 192, 194, 195, 210, 225, 228, 236, 243, 245, 246, 247, 248], "handl": [1, 15, 20, 48, 52, 184, 185, 242, 244, 247, 248, 250], "ani": [1, 5, 12, 13, 15, 16, 18, 20, 21, 24, 27, 28, 29, 30, 34, 36, 37, 38, 41, 44, 45, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 72, 156, 164, 167, 170, 172, 173, 178, 179, 180, 181, 182, 183, 184, 187, 199, 200, 201, 203, 212, 215, 216, 224, 227, 241, 242, 243, 245, 247, 248, 249], "pad": [1, 39, 41, 42, 43, 44, 50, 69, 71, 72, 164, 166, 189, 191, 193, 196, 210, 243], "miscellan": 1, "modifi": [1, 12, 15, 16, 17, 167, 236, 244, 246, 247, 248, 249, 250], "For": [2, 7, 9, 10, 11, 12, 13, 15, 16, 24, 28, 29, 30, 44, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 164, 166, 169, 170, 171, 174, 177, 188, 199, 205, 209, 216, 220, 222, 224, 235, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "detail": [2, 9, 10, 12, 13, 47, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 135, 157, 166, 169, 193, 197, 207, 215, 224, 237, 238, 241, 243, 244, 245, 246, 247, 248, 249, 250], "usag": [2, 12, 167, 169, 198, 202, 203, 225, 235, 241, 243, 244, 245, 246, 248, 249, 250], "guid": [2, 14, 15, 17, 24, 26, 28, 34, 56, 58, 59, 60, 61, 62, 63, 64, 65, 195, 216, 236, 242, 243, 245, 247], "pleas": [2, 7, 22, 25, 27, 32, 35, 37, 38, 47, 49, 80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 166, 169, 197, 207, 215, 222, 235, 238, 239, 244, 246, 250], "see": [2, 7, 9, 10, 11, 12, 13, 14, 17, 25, 32, 35, 41, 47, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 80, 81, 82, 90, 91, 92, 104, 105, 106, 107, 115, 116, 125, 126, 127, 128, 135, 138, 139, 145, 157, 166, 175, 182, 183, 188, 197, 202, 207, 215, 216, 220, 222, 224, 230, 235, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "our": [2, 10, 13, 16, 236, 237, 238, 239, 242, 243, 244, 245, 247, 248, 249, 250], "tutori": [2, 13, 222, 236, 237, 238, 242, 243, 244, 245, 246, 247, 248, 249, 250], "support": [2, 12, 13, 14, 16, 17, 18, 29, 47, 49, 50, 51, 52, 55, 56, 57, 58, 61, 62, 63, 64, 65, 68, 73, 87, 99, 112, 121, 128, 129, 131, 140, 143, 144, 146, 156, 158, 166, 171, 172, 176, 194, 200, 201, 203, 206, 208, 209, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "sever": [2, 248], "wide": [2, 9, 158], "onli": [2, 13, 14, 17, 29, 50, 51, 52, 57, 63, 69, 73, 87, 99, 112, 121, 128, 129, 131, 140, 146, 158, 162, 164, 166, 169, 173, 176, 178, 180, 184, 199, 200, 201, 203, 206, 207, 208, 209, 215, 241, 243, 244, 245, 247, 248, 249, 250], "help": [2, 11, 13, 59, 92, 164, 166, 170, 199, 216, 234, 235, 236, 241, 242, 243, 244, 245, 248, 249, 250], "quickli": [2, 15, 30, 53, 237, 242, 243, 248], "bootstrap": 2, "your": [2, 7, 9, 10, 12, 14, 17, 18, 30, 47, 53, 56, 60, 63, 166, 171, 216, 219, 220, 226, 234, 235, 236, 237, 238, 241, 242, 243, 246, 247, 248, 249, 250], "fine": [2, 9, 10, 11, 13, 14, 16, 17, 29, 50, 51, 52, 67, 226, 234, 236, 237, 238, 239, 244], "tune": [2, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 29, 50, 51, 52, 67, 226, 234, 235, 236, 237, 238, 239, 241, 244], "also": [2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 48, 56, 60, 63, 67, 69, 70, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 144, 146, 150, 158, 164, 176, 195, 207, 208, 215, 216, 220, 226, 229, 235, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "like": [2, 6, 10, 13, 14, 15, 16, 17, 47, 144, 166, 169, 171, 201, 235, 241, 242, 243, 244, 245, 247, 248, 249], "These": [2, 5, 11, 12, 13, 15, 16, 18, 50, 51, 63, 166, 189, 237, 239, 242, 243, 244, 245, 246, 247, 248, 249, 250], "ar": [2, 5, 9, 10, 11, 12, 13, 14, 15, 17, 18, 22, 27, 30, 31, 37, 38, 39, 41, 42, 46, 49, 50, 51, 52, 55, 56, 60, 61, 62, 63, 69, 71, 72, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 92, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 157, 163, 164, 165, 166, 170, 171, 172, 176, 177, 180, 181, 189, 191, 197, 199, 200, 202, 203, 205, 206, 208, 213, 215, 225, 226, 235, 236, 237, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "especi": [2, 236, 241, 244, 248], "specifi": [2, 10, 13, 15, 16, 18, 24, 26, 28, 34, 36, 56, 58, 59, 60, 61, 62, 63, 64, 65, 69, 71, 73, 86, 93, 98, 99, 108, 111, 112, 117, 121, 137, 144, 146, 150, 154, 158, 164, 165, 170, 197, 207, 209, 215, 220, 222, 225, 238, 239, 241, 242, 243, 244, 245, 246, 248, 249, 250], "yaml": [2, 15, 16, 18, 19, 20, 48, 56, 60, 63, 67, 220, 236, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250], "config": [2, 9, 10, 11, 12, 13, 14, 17, 18, 19, 20, 21, 48, 56, 60, 63, 67, 158, 180, 199, 203, 216, 220, 225, 236, 237, 238, 239, 242, 243, 244, 246, 247, 248, 249, 250], "represent": [2, 198, 247, 249, 250], "abov": [2, 4, 9, 13, 51, 167, 213, 235, 238, 244, 246, 247, 248, 249, 250], "all": [4, 5, 11, 12, 16, 21, 29, 30, 39, 41, 44, 48, 50, 51, 52, 111, 144, 154, 158, 162, 164, 166, 167, 170, 171, 172, 174, 177, 188, 199, 203, 205, 213, 221, 227, 228, 232, 234, 236, 237, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249], "famili": [4, 13, 16, 55, 57, 61, 62, 65, 66, 68, 202, 236, 241, 246], "import": [4, 9, 10, 11, 12, 13, 15, 18, 56, 60, 61, 62, 63, 67, 73, 166, 192, 216, 219, 220, 242, 243, 244, 245, 246, 247, 249, 250], "you": [4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 27, 29, 30, 47, 49, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 92, 157, 164, 166, 169, 172, 174, 202, 216, 219, 220, 226, 234, 235, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "need": [4, 9, 10, 11, 13, 14, 15, 16, 17, 27, 30, 47, 50, 52, 158, 164, 166, 170, 171, 195, 215, 216, 219, 220, 221, 235, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 250], "request": [4, 206, 243, 244], "access": [4, 13, 15, 16, 48, 199, 205, 237, 238, 241, 243, 244, 245], "hug": [4, 13, 23, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 111, 144, 154, 168, 186, 236, 241, 245, 246], "face": [4, 13, 23, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 111, 144, 154, 168, 186, 236, 241, 245, 246], "befor": [4, 11, 13, 30, 46, 49, 50, 61, 83, 87, 158, 163, 164, 165, 166, 169, 170, 172, 176, 185, 199, 216, 238, 241, 244, 248, 249], "download": [4, 13, 61, 232, 235, 237, 238, 242, 243, 246, 247, 249, 250], "To": [4, 9, 10, 11, 13, 15, 16, 17, 50, 61, 164, 166, 172, 199, 226, 235, 236, 238, 239, 241, 243, 244, 245, 246, 247, 248, 249, 250], "8b": [4, 12, 110, 114, 116, 120, 122, 124, 127, 141, 237, 238, 241, 242, 249], "meta": [4, 12, 13, 92, 160, 199, 200, 237, 238, 241, 242, 244, 245], "output": [4, 10, 12, 13, 26, 27, 39, 48, 49, 51, 52, 55, 58, 60, 64, 65, 69, 77, 78, 79, 83, 87, 93, 97, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 136, 140, 141, 146, 149, 150, 155, 156, 158, 160, 161, 163, 164, 165, 166, 169, 170, 171, 172, 176, 179, 180, 181, 189, 201, 207, 218, 225, 226, 235, 237, 238, 241, 243, 244, 245, 246, 247, 248, 250], "dir": [4, 12, 13, 220, 235, 237, 238, 241, 244, 245, 246, 249], "tmp": [4, 9, 10, 11, 12, 15, 203, 237, 238, 242, 245], "ignor": [4, 9, 10, 13, 67, 158, 162, 163, 165, 204, 226, 237, 238, 241], "pattern": [4, 11, 185, 237, 238, 241], "origin": [4, 12, 13, 54, 55, 59, 167, 171, 172, 176, 237, 238, 242, 244, 246, 247, 248, 249, 250], "consolid": [4, 13, 237, 238], "00": [4, 13, 56, 60, 233, 237, 238, 240, 245], "pth": [4, 13, 198, 237, 238, 244], "hf": [4, 9, 12, 13, 192, 194, 199, 241, 242, 244, 245, 246], "token": [4, 9, 10, 11, 13, 15, 16, 29, 41, 43, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 83, 86, 87, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 144, 146, 150, 154, 158, 160, 163, 164, 165, 166, 169, 170, 171, 172, 182, 183, 184, 185, 186, 187, 189, 191, 193, 196, 207, 210, 237, 241, 243, 244, 245, 246, 247, 248, 249, 250], "hf_token": [4, 12, 238], "70b": [4, 75, 78, 81, 95, 101, 105, 109, 113, 115, 119, 123, 126, 246], "405b": [4, 118, 122, 125], "weight": [4, 12, 13, 16, 44, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 162, 167, 175, 176, 180, 184, 192, 199, 200, 201, 202, 209, 220, 226, 234, 238, 241, 242, 244, 245, 246, 247, 248, 249, 250], "can": [4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 24, 26, 28, 29, 30, 31, 34, 44, 48, 49, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 159, 160, 162, 163, 164, 166, 169, 170, 172, 174, 177, 184, 185, 197, 199, 202, 204, 207, 215, 216, 219, 220, 222, 225, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "instead": [4, 9, 10, 13, 16, 37, 38, 39, 50, 51, 67, 157, 162, 166, 176, 195, 241, 246, 247, 248, 249], "The": [4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 23, 27, 29, 37, 38, 40, 41, 46, 47, 48, 49, 50, 51, 52, 56, 59, 60, 61, 62, 63, 66, 77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 131, 140, 141, 146, 147, 148, 149, 156, 159, 160, 161, 162, 166, 167, 168, 169, 170, 171, 172, 177, 182, 183, 184, 185, 186, 187, 189, 190, 192, 193, 194, 195, 197, 199, 201, 206, 209, 216, 220, 223, 225, 229, 230, 231, 235, 236, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "reus": [4, 236], "llama3_token": [4, 12, 61, 62, 69, 242, 246], "builder": [4, 9, 10, 13, 54, 56, 57, 60, 63, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89, 90, 91, 94, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 113, 114, 115, 116, 118, 119, 120, 122, 123, 124, 125, 126, 127, 130, 132, 134, 136, 138, 139, 141, 143, 145, 147, 148, 149, 151, 152, 153, 242, 243, 248, 250], "class": [4, 12, 15, 17, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 37, 38, 47, 48, 49, 50, 51, 52, 53, 61, 62, 86, 92, 97, 98, 111, 128, 131, 135, 136, 137, 144, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 175, 176, 178, 179, 182, 183, 184, 185, 188, 189, 192, 193, 194, 195, 198, 199, 200, 201, 202, 203, 216, 217, 218, 219, 220, 239, 242, 243, 245, 247, 248, 250], "7b": [4, 9, 10, 11, 12, 13, 49, 57, 68, 76, 79, 82, 85, 89, 96, 97, 102, 103, 106, 107, 130, 132, 134, 136, 139, 149, 153, 199, 200, 242, 245, 246, 247, 250], "13b": [4, 13, 74, 77, 80, 94, 100, 104], "codellama": 4, "size": [4, 13, 16, 18, 39, 44, 55, 58, 61, 62, 64, 157, 158, 159, 160, 163, 164, 165, 166, 169, 170, 171, 172, 189, 190, 191, 210, 211, 213, 236, 238, 241, 243, 244, 245, 246, 247, 248, 249], "0": [4, 9, 10, 13, 16, 39, 41, 42, 43, 44, 50, 56, 60, 63, 69, 70, 72, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 93, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 151, 152, 157, 158, 164, 166, 168, 171, 176, 187, 192, 193, 194, 195, 196, 209, 210, 216, 219, 220, 224, 229, 231, 233, 238, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250], "5b": [4, 147, 148, 151, 152, 248], "qwen2": [4, 146, 147, 148, 149, 151, 152, 153, 154, 202, 248], "exampl": [4, 11, 12, 13, 14, 15, 16, 17, 18, 20, 24, 28, 30, 36, 39, 40, 41, 42, 43, 44, 48, 49, 50, 52, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 67, 68, 69, 71, 72, 73, 157, 158, 166, 169, 170, 171, 172, 174, 175, 177, 182, 183, 184, 185, 187, 188, 192, 194, 195, 196, 197, 198, 199, 200, 202, 203, 209, 210, 216, 219, 220, 223, 226, 229, 230, 231, 232, 233, 235, 237, 238, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250], "none": [4, 9, 16, 17, 19, 21, 24, 26, 27, 28, 34, 44, 45, 46, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 86, 93, 98, 99, 108, 111, 112, 117, 121, 137, 144, 154, 155, 157, 158, 160, 163, 164, 165, 166, 170, 172, 177, 179, 180, 181, 184, 187, 189, 190, 191, 193, 199, 200, 201, 202, 204, 206, 209, 214, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 227, 228, 229, 230, 241, 243, 244, 249], "mini": [4, 12, 141, 142, 143, 144, 145], "4k": [4, 12, 142, 143, 144], "microsoft": [4, 143, 144], "ai": [4, 10, 11, 51, 52, 134, 220, 242, 246], "thi": [4, 9, 10, 11, 12, 14, 15, 16, 17, 18, 22, 24, 25, 26, 27, 28, 29, 34, 35, 36, 37, 38, 39, 41, 42, 44, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 83, 87, 93, 99, 108, 112, 117, 121, 128, 129, 131, 133, 135, 140, 142, 143, 144, 146, 150, 155, 157, 158, 160, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 174, 175, 177, 180, 181, 182, 183, 184, 185, 187, 188, 189, 191, 192, 193, 195, 197, 198, 199, 200, 201, 203, 206, 208, 210, 213, 215, 216, 217, 219, 220, 221, 222, 224, 226, 228, 229, 234, 235, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "v0": [4, 9, 11, 12, 128], "mistralai": [4, 12, 241], "2b": [4, 84, 88], "gemma2": 4, "googl": [4, 84, 85], "gguf": 4, "perform": [5, 10, 11, 12, 13, 50, 69, 166, 169, 177, 188, 195, 236, 237, 238, 242, 244, 246, 249, 250], "direct": [5, 16, 42, 77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141, 192, 235, 239], "encod": [5, 12, 44, 52, 69, 70, 158, 163, 164, 165, 170, 171, 172, 174, 182, 184, 185, 187, 189, 192, 195, 242], "text": [5, 9, 10, 11, 12, 29, 30, 31, 36, 44, 47, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 171, 172, 182, 184, 185, 187, 189, 242, 244, 249], "id": [5, 12, 13, 41, 42, 43, 44, 47, 49, 50, 57, 61, 62, 68, 69, 70, 72, 73, 158, 160, 164, 165, 170, 182, 183, 184, 185, 186, 187, 189, 199, 201, 216, 242, 243, 244], "decod": [5, 9, 10, 12, 56, 60, 63, 69, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 163, 164, 165, 170, 172, 174, 182, 184, 185, 242], "typic": [5, 9, 10, 15, 24, 28, 34, 44, 50, 51, 52, 53, 67, 144, 174, 192, 195, 243, 248, 249, 250], "byte": [5, 12, 185, 248, 250], "pair": [5, 12, 15, 42, 43, 59, 63, 66, 185, 243], "underli": [5, 12, 184, 248, 250], "helper": 5, "method": [5, 11, 12, 13, 15, 16, 17, 20, 40, 47, 49, 51, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 164, 167, 169, 170, 173, 174, 175, 178, 180, 182, 183, 203, 209, 235, 236, 243, 247, 250], "two": [5, 11, 13, 15, 26, 44, 46, 61, 62, 69, 70, 166, 171, 174, 189, 196, 198, 236, 238, 244, 245, 246, 247, 248, 249, 250], "pre": [5, 9, 10, 11, 50, 51, 52, 53, 61, 62, 67, 92, 166, 170, 172, 174, 238, 242, 243], "train": [5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 24, 26, 44, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 92, 156, 158, 160, 164, 165, 167, 168, 169, 170, 171, 172, 174, 192, 195, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 234, 236, 237, 239, 241, 242, 243, 244, 246, 247, 248, 249, 250], "function": [5, 13, 15, 16, 18, 20, 39, 40, 41, 42, 47, 49, 56, 60, 63, 69, 155, 158, 166, 167, 177, 180, 181, 192, 193, 197, 199, 211, 224, 226, 228, 229, 236, 243, 250], "preprocess": [5, 50, 166], "imag": [5, 29, 36, 40, 44, 52, 61, 62, 166, 171, 189, 247], "compon": [6, 12, 13, 16, 21, 42, 51, 52, 61, 62, 236, 239, 243, 245, 247, 250], "loss": [6, 9, 10, 15, 16, 29, 30, 49, 51, 52, 55, 56, 58, 60, 63, 64, 65, 169, 192, 193, 194, 195, 245, 247, 250], "algorithm": [6, 12, 190, 195, 224], "ppo": [6, 190, 191, 192, 193, 239], "dpo": [6, 42, 51, 177, 192, 194, 195, 239], "offer": 7, "allow": [7, 48, 172, 180, 219, 238, 241, 248, 249, 250], "seamless": 7, "transit": 7, "between": [7, 9, 11, 12, 13, 51, 56, 63, 163, 164, 170, 191, 193, 195, 199, 202, 216, 243, 244, 246, 247, 249, 250], "interoper": [7, 13, 16, 236, 244, 250], "rest": [7, 242, 248, 250], "ecosystem": [7, 13, 16, 236, 244, 246, 250], "comprehens": [7, 248], "overview": [7, 15, 17, 170, 234, 237, 238, 245, 247, 250], "deep": [7, 13, 14, 15, 16, 17, 172, 174, 236, 239, 245, 246, 248], "dive": [7, 13, 14, 15, 16, 17, 236, 238, 239, 245, 246, 248], "util": [7, 13, 15, 16, 18, 39, 41, 44, 204, 219, 221, 222, 228, 229, 230, 231, 236, 244, 245, 248, 250], "work": [7, 13, 16, 162, 171, 172, 236, 238, 241, 244, 246, 248, 250], "set": [7, 9, 10, 13, 14, 15, 16, 17, 24, 28, 29, 34, 49, 50, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 160, 163, 164, 170, 177, 179, 197, 207, 213, 215, 216, 222, 223, 224, 225, 228, 229, 236, 239, 241, 242, 244, 245, 246, 247, 248, 249], "enabl": [7, 12, 14, 15, 16, 17, 48, 77, 78, 79, 80, 81, 82, 88, 89, 90, 91, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 122, 123, 124, 125, 126, 127, 130, 132, 138, 139, 141, 145, 147, 148, 149, 151, 152, 158, 164, 172, 176, 224, 225, 238, 246, 247, 248, 250], "consumpt": [7, 48, 71, 237, 248], "dure": [7, 13, 49, 50, 55, 56, 58, 60, 63, 64, 65, 157, 158, 160, 164, 165, 166, 167, 170, 171, 189, 195, 208, 237, 238, 242, 244, 246, 247, 248, 249, 250], "variou": [7, 27], "provid": [7, 10, 13, 15, 16, 18, 23, 24, 26, 28, 34, 40, 41, 45, 48, 49, 50, 69, 71, 158, 162, 164, 166, 170, 177, 192, 201, 207, 216, 220, 225, 229, 236, 237, 238, 241, 242, 243, 244, 245, 246, 248], "debug": [7, 13, 15, 16, 216, 241], "finetun": [7, 13, 15, 16, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 170, 234, 236, 238, 245, 246, 248], "job": [7, 17, 224, 245], "involv": [9, 10, 52, 249], "multi": [9, 16, 47, 158, 246], "turn": [9, 16, 24, 28, 29, 34, 46, 47, 51, 63, 242, 248], "multipl": [9, 13, 15, 16, 24, 28, 29, 34, 42, 47, 48, 52, 63, 158, 164, 165, 166, 170, 176, 216, 217, 218, 219, 220, 225, 245, 246, 248], "back": [9, 12, 13, 46, 177, 199, 243, 247, 248, 250], "forth": [9, 46, 243], "user": [9, 10, 11, 12, 16, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 46, 47, 51, 52, 56, 60, 63, 86, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 144, 146, 150, 154, 158, 187, 239, 242, 243, 245, 249], "assist": [9, 10, 11, 12, 22, 23, 24, 26, 28, 29, 30, 31, 33, 34, 36, 37, 38, 46, 47, 51, 52, 56, 63, 69, 86, 92, 98, 111, 137, 144, 154, 187, 242, 243], "role": [9, 11, 12, 22, 24, 28, 29, 30, 31, 34, 36, 37, 38, 47, 51, 52, 56, 63, 86, 98, 111, 137, 144, 154, 187, 242, 243], "content": [9, 11, 12, 13, 22, 24, 28, 29, 30, 31, 34, 36, 37, 38, 47, 51, 52, 56, 63, 187, 242, 243], "what": [9, 13, 14, 15, 17, 29, 51, 52, 56, 60, 63, 92, 128, 166, 234, 239, 242, 243, 244, 245, 246, 248], "answer": [9, 11, 32, 60, 244, 246], "ultim": [9, 249], "question": [9, 11, 32, 60, 243, 244, 246], "life": 9, "42": [9, 69, 166], "That": [9, 242], "s": [9, 10, 11, 13, 15, 16, 17, 18, 20, 22, 23, 28, 34, 37, 38, 46, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 77, 78, 79, 92, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 128, 129, 130, 131, 132, 140, 141, 144, 146, 149, 150, 157, 158, 160, 164, 165, 166, 167, 170, 173, 174, 175, 178, 180, 181, 185, 192, 194, 195, 196, 197, 199, 200, 203, 207, 208, 210, 215, 216, 219, 222, 223, 226, 228, 229, 236, 241, 242, 243, 245, 247, 248, 249, 250], "ridicul": 9, "oh": 9, "i": [9, 10, 11, 16, 29, 63, 69, 92, 128, 158, 163, 164, 165, 166, 167, 170, 179, 198, 203, 243, 244, 246, 248, 249, 250], "know": [9, 242, 243, 244, 247], "more": [9, 10, 11, 12, 13, 15, 16, 30, 47, 49, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 157, 166, 169, 174, 180, 197, 198, 201, 216, 220, 222, 224, 228, 236, 237, 238, 239, 241, 243, 244, 245, 246, 247, 248, 249, 250], "structur": [9, 10, 11, 16, 22, 28, 31, 34, 37, 38, 47, 56, 111, 144, 154, 189, 242, 243, 244, 249], "than": [9, 10, 15, 46, 69, 71, 157, 158, 166, 192, 197, 201, 202, 227, 228, 231, 242, 243, 244, 245, 246, 247, 248, 250], "freeform": [9, 10, 53, 67], "associ": [9, 10, 13, 15, 16, 69, 70, 83, 93, 108, 117, 133, 150, 216, 244, 247], "where": [9, 10, 11, 29, 30, 39, 42, 47, 55, 69, 71, 72, 97, 136, 155, 158, 164, 166, 169, 170, 176, 184, 189, 190, 192, 193, 196, 207, 210, 215, 243, 248], "thei": [9, 10, 11, 12, 15, 16, 48, 61, 62, 164, 166, 172, 181, 207, 241, 242, 243, 247, 248, 249], "learn": [9, 10, 16, 48, 168, 171, 172, 174, 236, 237, 238, 239, 242, 243, 245, 246, 247, 248, 249, 250], "simpli": [9, 10, 13, 15, 28, 50, 52, 192, 241, 242, 243, 244, 246, 248, 250], "predict": [9, 10, 69, 70, 73, 190, 191, 193, 237], "next": [9, 10, 13, 50, 67, 69, 70, 166, 189, 237, 246, 250], "respond": [9, 243], "accur": 9, "primari": [9, 10, 13, 15, 16, 51, 52, 239, 245], "entri": [9, 10, 15, 16, 41, 44, 239, 245, 248], "point": [9, 10, 12, 15, 16, 37, 38, 40, 56, 187, 239, 243, 244, 245, 246, 247, 249, 250], "torchtun": [9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 235, 237, 238, 239, 242, 243, 245, 248], "chat_dataset": [9, 10, 47, 242, 243], "let": [9, 10, 13, 15, 17, 241, 242, 243, 244, 245, 246, 247, 248, 250], "follow": [9, 10, 11, 13, 16, 28, 29, 30, 34, 37, 38, 44, 47, 50, 51, 52, 60, 63, 158, 163, 168, 189, 193, 201, 202, 203, 213, 220, 225, 234, 235, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "data": [9, 10, 11, 12, 14, 22, 23, 24, 26, 27, 28, 29, 30, 31, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 92, 154, 166, 188, 192, 194, 208, 216, 217, 218, 219, 220, 228, 237, 238, 243, 244, 248, 249, 250], "directli": [9, 10, 13, 15, 16, 18, 51, 52, 56, 60, 61, 63, 67, 192, 197, 199, 241, 244, 245, 246, 247, 248, 249, 250], "llm": [9, 10, 12, 16, 170, 172, 234, 235, 236, 237, 239, 243, 244, 246, 247], "my_data": [9, 10, 242, 243], "human": [9, 29, 34, 38, 56, 92, 192, 193, 194, 242], "valu": [9, 13, 15, 24, 26, 28, 34, 38, 39, 41, 42, 55, 56, 58, 59, 60, 63, 64, 65, 66, 69, 70, 72, 73, 74, 75, 76, 83, 84, 85, 87, 93, 94, 95, 96, 97, 99, 108, 109, 110, 112, 117, 118, 119, 120, 121, 129, 131, 133, 134, 135, 136, 140, 142, 146, 150, 151, 152, 153, 157, 158, 159, 163, 164, 165, 168, 170, 172, 180, 190, 191, 193, 196, 199, 202, 203, 210, 216, 217, 218, 219, 220, 224, 238, 241, 242, 243, 245, 246, 247, 248, 249], "gpt": [9, 34, 38, 56, 70, 242, 244], "mistral": [9, 11, 12, 47, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 202, 241, 242, 244, 245], "mistral_token": [9, 11, 12], "m_token": [9, 11, 12], "path": [9, 10, 11, 12, 15, 16, 17, 18, 40, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 86, 98, 111, 137, 144, 154, 184, 185, 186, 199, 200, 201, 225, 241, 242, 243, 244, 246, 247], "1": [9, 11, 12, 13, 16, 39, 41, 42, 43, 44, 50, 65, 69, 70, 72, 73, 93, 99, 108, 112, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 133, 135, 140, 142, 146, 147, 148, 150, 151, 152, 157, 158, 164, 166, 168, 169, 184, 185, 187, 192, 193, 194, 195, 200, 202, 210, 213, 216, 219, 220, 223, 224, 236, 237, 241, 242, 244, 245, 247, 248, 249, 250], "prompt_templ": [9, 10, 11, 86, 98, 111, 137, 144, 154], "mistralchattempl": [9, 11, 137, 242], "max_seq_len": [9, 10, 12, 15, 18, 41, 44, 45, 47, 49, 50, 55, 56, 57, 58, 60, 61, 62, 64, 65, 67, 68, 83, 86, 87, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 144, 146, 150, 154, 157, 158, 160, 164, 187, 243, 249], "8192": [9, 10, 12, 247, 249], "ds": [9, 10, 50, 65, 242], "sourc": [9, 10, 13, 15, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 242, 243, 244, 249], "data_fil": [9, 10, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 242, 243], "split": [9, 10, 13, 36, 47, 48, 49, 50, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 185, 242, 243, 244, 249], "conversation_column": [9, 56, 242], "conversation_styl": [9, 56, 242, 243], "By": [9, 10, 13, 238, 241, 247, 248, 249, 250], "default": [9, 10, 13, 15, 23, 24, 26, 28, 29, 34, 37, 38, 39, 42, 43, 44, 45, 47, 49, 50, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 158, 159, 160, 163, 164, 165, 167, 168, 170, 176, 180, 184, 185, 187, 189, 190, 191, 192, 195, 199, 200, 201, 206, 211, 215, 216, 217, 220, 223, 224, 225, 235, 238, 241, 242, 243, 244, 246, 247, 248, 249, 250], "true": [9, 10, 13, 15, 29, 37, 38, 39, 48, 49, 50, 53, 54, 55, 56, 58, 60, 61, 62, 63, 64, 65, 67, 68, 71, 72, 80, 81, 82, 83, 87, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 158, 163, 164, 165, 167, 170, 172, 177, 184, 185, 187, 189, 190, 193, 196, 197, 199, 200, 201, 207, 208, 210, 212, 213, 216, 219, 225, 231, 237, 241, 242, 243, 244, 246, 247, 248, 249, 250], "train_on_input": [9, 10, 15, 24, 26, 28, 34, 37, 38, 47, 48, 49, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 243], "new_system_prompt": [9, 10, 24, 26, 28, 34, 56, 58, 59, 60, 61, 62, 63, 64, 65], "tokenized_dict": [9, 10], "label": [9, 10, 16, 41, 42, 43, 44, 47, 49, 50, 57, 65, 68, 169, 192, 195], "print": [9, 10, 11, 12, 13, 17, 36, 44, 48, 55, 58, 61, 62, 64, 65, 69, 166, 184, 185, 187, 231, 242, 243, 245, 247, 249, 250], "inst": [9, 11, 12, 47, 92, 128, 242, 243], "733": [9, 12], "16289": [9, 12], "28793": [9, 12], "1824": 9, "349": 9, "272": 9, "4372": 9, "In": [9, 10, 11, 12, 13, 15, 16, 47, 51, 160, 164, 166, 176, 197, 215, 219, 220, 238, 242, 244, 246, 247, 248, 249, 250], "_component_": [9, 10, 11, 12, 13, 14, 15, 17, 18, 48, 56, 60, 63, 67, 225, 238, 242, 243, 244, 246, 247, 248, 249], "null": [9, 13, 15, 249], "have": [9, 12, 13, 15, 18, 26, 29, 51, 56, 63, 71, 156, 157, 158, 159, 162, 164, 166, 169, 175, 181, 189, 195, 198, 201, 203, 207, 219, 227, 235, 242, 243, 244, 245, 246, 247, 248, 249, 250], "singl": [9, 11, 13, 15, 18, 22, 24, 26, 27, 28, 34, 37, 38, 41, 48, 50, 51, 52, 53, 56, 63, 67, 86, 97, 98, 111, 136, 137, 144, 158, 164, 166, 170, 199, 200, 201, 202, 203, 205, 239, 241, 242, 243, 244, 245, 246, 247, 248, 250], "name": [9, 10, 13, 14, 15, 17, 19, 24, 26, 27, 28, 34, 49, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 175, 179, 181, 185, 199, 200, 201, 202, 203, 205, 216, 217, 218, 219, 220, 226, 227, 229, 241, 242, 244, 246, 248, 249], "messag": [9, 10, 11, 12, 22, 23, 24, 26, 28, 30, 31, 34, 36, 37, 38, 46, 47, 51, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 86, 98, 111, 137, 144, 183, 187, 235, 241, 242, 243], "contain": [9, 13, 24, 26, 29, 37, 41, 42, 43, 44, 50, 51, 52, 53, 56, 61, 67, 111, 144, 154, 157, 158, 160, 164, 165, 170, 173, 175, 178, 179, 180, 185, 187, 190, 196, 199, 200, 201, 203, 205, 208, 214, 219, 225, 226, 228, 242, 244, 246, 247], "topic": [9, 234], "per": [9, 41, 80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 157, 166, 167, 189, 191, 192, 241, 248, 249, 250], "could": [9, 11, 37, 247], "system": [9, 10, 11, 22, 23, 24, 26, 28, 29, 30, 31, 33, 34, 36, 37, 38, 46, 47, 51, 52, 56, 58, 59, 60, 61, 62, 63, 64, 65, 86, 92, 98, 111, 128, 137, 144, 154, 187, 242, 243], "tool": [9, 11, 13, 29, 30, 52, 128, 216, 243, 244, 245], "call": [9, 12, 13, 18, 29, 30, 52, 61, 62, 128, 158, 164, 166, 167, 170, 180, 216, 217, 218, 219, 220, 221, 225, 226, 242, 243, 247, 250], "return": [9, 11, 12, 18, 20, 22, 27, 29, 30, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 223, 224, 225, 229, 230, 231, 243, 247, 250], "dai": 9, "todai": 9, "It": [9, 23, 29, 30, 51, 52, 56, 58, 60, 61, 62, 64, 66, 128, 162, 164, 166, 170, 192, 195, 216, 241, 242, 243, 250], "tuesdai": 9, "about": [9, 13, 16, 61, 62, 166, 192, 195, 216, 220, 236, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 249, 250], "tomorrow": 9, "wednesdai": 9, "As": [9, 10, 13, 15, 16, 17, 176, 236, 244, 248, 250], "an": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 40, 46, 48, 49, 53, 56, 58, 60, 61, 62, 63, 64, 67, 68, 99, 112, 121, 129, 131, 135, 140, 146, 147, 148, 151, 152, 158, 162, 164, 166, 170, 171, 172, 174, 175, 177, 178, 179, 183, 188, 189, 192, 197, 198, 199, 200, 201, 203, 204, 207, 216, 220, 225, 229, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "slimorca": [9, 65], "pass": [9, 10, 11, 12, 15, 18, 29, 30, 47, 48, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 164, 167, 170, 177, 181, 185, 193, 197, 201, 206, 207, 208, 212, 215, 216, 219, 220, 222, 225, 241, 242, 243, 247, 249, 250], "repo": [9, 10, 13, 61, 199, 200, 202, 241, 244], "select": [9, 204], "one": [9, 10, 13, 16, 24, 26, 28, 34, 41, 44, 46, 56, 62, 63, 166, 169, 187, 201, 216, 243, 244, 245, 246, 248, 250], "most": [9, 10, 13, 15, 29, 30, 242, 245, 247, 248, 250], "gemma": [9, 10, 84, 85, 86, 87, 88, 89, 90, 91, 162, 202, 248], "gemma_token": [9, 10], "g_token": [9, 10], "open": [9, 40, 65, 84, 85, 243, 244], "orca": [9, 65], "dedup": [9, 65], "recip": [9, 10, 13, 14, 15, 17, 18, 19, 20, 164, 170, 199, 200, 201, 236, 237, 238, 242, 243, 244, 246, 248, 250], "via": [9, 10, 14, 15, 17, 51, 56, 60, 63, 67, 158, 164, 165, 176, 199, 247, 250], "http": [9, 10, 18, 40, 47, 49, 53, 57, 59, 61, 67, 68, 70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 134, 136, 138, 139, 140, 141, 143, 144, 145, 147, 148, 149, 151, 152, 153, 158, 159, 160, 166, 168, 169, 189, 190, 192, 193, 194, 195, 197, 199, 200, 213, 216, 219, 220, 222, 224, 230, 235, 243, 244, 246], "ha": [9, 13, 60, 69, 161, 163, 164, 166, 169, 170, 173, 175, 177, 178, 181, 196, 201, 203, 226, 227, 242, 243, 244, 245, 246, 247, 248, 250], "addition": [9, 13, 184, 185, 195, 224, 242, 243, 247], "argument": [9, 10, 13, 15, 18, 25, 27, 32, 35, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 158, 197, 207, 212, 216, 217, 219, 220, 222, 241, 242, 243, 247, 249], "load_dataset": [9, 10, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 242, 243], "document": [9, 10, 158, 164, 165, 197, 207, 215, 237, 239, 241, 243, 248], "file": [9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 40, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 111, 144, 154, 184, 185, 186, 199, 200, 201, 217, 220, 225, 233, 236, 238, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "raw": [9, 12, 36], "vari": [9, 44, 50, 164], "field": [9, 18, 26, 27, 29, 36, 37, 38, 47, 50, 51, 52, 55, 61, 62, 214, 243], "indic": [9, 11, 44, 48, 49, 50, 71, 72, 158, 160, 164, 165, 166, 170, 171, 189, 190, 193, 196, 197, 210, 213, 242], "There": [9, 15, 46, 242, 245, 246, 247, 248], "few": [9, 172, 243, 246, 247, 250], "standard": [9, 10, 11, 13, 25, 37, 51, 52, 56, 59, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 218, 236, 242, 244, 246], "across": [9, 13, 16, 44, 48, 199, 219, 224, 244, 246, 249], "mani": [9, 11, 15, 50, 237, 238, 243, 244], "we": [9, 10, 11, 12, 13, 14, 15, 16, 17, 41, 44, 49, 50, 51, 52, 56, 57, 63, 68, 69, 73, 157, 158, 160, 164, 165, 166, 169, 170, 176, 192, 195, 199, 200, 201, 206, 209, 215, 221, 226, 236, 237, 238, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "convert": [9, 12, 13, 24, 26, 28, 34, 37, 38, 43, 47, 51, 52, 56, 61, 62, 63, 71, 199, 244, 249, 250], "ipython": [9, 11, 29, 30, 33, 51, 52, 86, 98, 111, 137, 144, 154], "transform": [9, 13, 16, 24, 26, 28, 47, 49, 51, 52, 55, 56, 58, 59, 61, 62, 63, 64, 65, 77, 78, 79, 83, 87, 88, 89, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 135, 140, 141, 142, 146, 147, 148, 149, 150, 163, 164, 165, 166, 168, 172, 189, 222, 247, 248, 249], "sharegpttomessag": [9, 38, 56, 65], "expect": [9, 10, 11, 13, 15, 18, 24, 26, 27, 28, 29, 34, 40, 44, 47, 49, 51, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 160, 170, 181, 203, 216, 220, 227, 242, 243, 247, 249], "code": [9, 10, 11, 12, 13, 16, 74, 75, 76, 77, 78, 79, 80, 81, 82, 164, 216, 232, 236, 243, 245], "jsontomessag": [9, 37, 56, 63], "If": [9, 11, 12, 13, 15, 21, 24, 26, 27, 28, 29, 34, 36, 37, 40, 41, 44, 45, 46, 47, 49, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 67, 69, 71, 73, 86, 93, 98, 99, 108, 111, 112, 117, 121, 137, 144, 146, 150, 154, 157, 158, 160, 162, 164, 165, 166, 167, 169, 170, 176, 181, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 212, 216, 219, 220, 224, 225, 227, 229, 235, 241, 242, 243, 244, 245, 246, 247, 248, 249], "doe": [9, 13, 36, 37, 44, 47, 50, 63, 67, 83, 128, 133, 143, 158, 162, 164, 165, 170, 175, 187, 199, 201, 203, 226, 241, 244, 249], "fit": [9, 16, 47, 49, 50, 57, 67, 68, 166, 192, 242, 243], "creat": [9, 11, 13, 15, 18, 28, 30, 50, 52, 56, 63, 71, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 88, 89, 90, 91, 94, 95, 96, 97, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 113, 114, 115, 116, 118, 119, 120, 122, 123, 124, 125, 126, 127, 130, 132, 134, 136, 138, 139, 141, 143, 145, 147, 148, 149, 151, 152, 153, 157, 158, 164, 165, 166, 168, 197, 199, 200, 201, 205, 216, 217, 219, 241, 243, 244, 250], "custom": [9, 12, 15, 16, 22, 27, 30, 47, 49, 51, 52, 56, 60, 61, 62, 63, 67, 86, 98, 111, 137, 144, 154, 222, 236, 237, 238, 241, 245, 246, 247, 248], "dialogu": [9, 35, 64, 242], "defin": [9, 13, 15, 16, 30, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 158, 163, 164, 170, 173, 175, 176, 178, 191, 243, 245, 247], "same": [9, 13, 15, 30, 39, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 156, 157, 159, 161, 162, 163, 165, 166, 170, 172, 187, 193, 195, 196, 203, 207, 220, 226, 228, 238, 241, 242, 244, 246, 247, 248, 249, 250], "wai": [9, 11, 13, 15, 47, 51, 52, 180, 198, 241, 243, 244, 245, 246], "instruct_dataset": [9, 10, 48, 49, 243], "info": [9, 230, 245], "slimorca_dataset": [9, 15], "task": [10, 11, 25, 32, 35, 48, 57, 237, 242, 243, 244, 246, 247, 248, 249, 250], "take": [10, 13, 15, 16, 18, 42, 51, 52, 61, 62, 63, 157, 166, 167, 172, 199, 201, 228, 229, 238, 242, 243, 244, 245, 246, 247, 248, 250], "form": [10, 13, 15, 16, 36, 46, 51, 52, 241], "command": [10, 12, 14, 16, 17, 235, 238, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250], "respons": [10, 11, 12, 23, 24, 26, 28, 29, 34, 51, 52, 56, 58, 59, 60, 61, 62, 63, 64, 65, 187, 190, 191, 192, 194, 195, 243, 244, 245, 246], "along": [10, 13, 247], "option": [10, 12, 13, 15, 16, 24, 26, 27, 28, 34, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 77, 78, 79, 86, 87, 88, 89, 93, 98, 99, 100, 101, 102, 103, 108, 111, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 137, 140, 141, 144, 146, 147, 148, 149, 150, 154, 155, 158, 160, 163, 164, 165, 166, 167, 170, 180, 181, 182, 184, 187, 189, 190, 191, 193, 199, 200, 201, 204, 206, 209, 216, 217, 220, 224, 225, 229, 230, 235, 236, 241, 242, 243, 244, 248], "describ": [10, 222, 243], "hand": [10, 29], "complet": [10, 13, 16, 50, 57, 67, 144, 242, 243, 244, 245, 246, 248], "here": [10, 12, 13, 14, 15, 17, 23, 58, 61, 62, 159, 160, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250], "grammar": [10, 11, 25, 58, 243], "correct": [10, 11, 16, 25, 58, 159, 160, 164, 229, 236, 242, 243], "head": [10, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 160, 164, 170, 174, 202, 246], "csv": [10, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 243], "incorrect": [10, 11], "cat": [10, 11, 189], "grammarerrorcorrectiontempl": [10, 11, 58], "prepend": [10, 11, 22, 24, 26, 28, 30, 31, 34, 56, 58, 59, 60, 61, 62, 63, 64, 65, 86, 98, 111, 137, 144, 154, 184], "everi": [10, 13, 16, 58, 59, 63, 64, 65, 166, 219, 225, 235, 241, 248, 250], "column_map": [10, 24, 26, 27, 28, 34, 48, 49, 54, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 243], "input": [10, 12, 13, 26, 27, 41, 42, 43, 44, 47, 49, 50, 51, 52, 55, 57, 58, 60, 61, 62, 64, 65, 68, 86, 98, 111, 137, 144, 146, 150, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 170, 171, 172, 176, 184, 185, 189, 199, 201, 224, 227, 242, 243, 247, 250], "english": [10, 11, 25], "ncorrect": [10, 25], "mask": [10, 11, 12, 29, 30, 44, 49, 50, 52, 55, 56, 58, 60, 61, 62, 63, 64, 65, 70, 71, 72, 158, 163, 164, 165, 170, 183, 187, 189, 190, 193, 210, 242, 243], "out": [10, 13, 15, 16, 49, 55, 56, 58, 60, 63, 64, 65, 71, 72, 189, 199, 200, 210, 234, 236, 237, 238, 239, 241, 242, 244, 245, 246, 247, 248, 250], "100": [10, 16, 42, 43, 44, 49, 55, 56, 58, 60, 63, 64, 65, 69, 169, 171, 247, 250], "27957": 10, "736": 10, "577": 10, "anoth": [10, 15, 52, 162, 216, 244, 248], "c4": [10, 67, 243, 249], "200m": 10, "which": [10, 11, 12, 13, 15, 16, 40, 41, 48, 49, 50, 53, 55, 56, 58, 60, 63, 64, 65, 67, 72, 73, 77, 78, 79, 86, 87, 88, 89, 98, 99, 100, 101, 102, 103, 111, 112, 113, 114, 121, 122, 123, 124, 128, 129, 130, 131, 132, 137, 140, 141, 144, 146, 147, 148, 149, 157, 158, 160, 164, 165, 166, 168, 170, 172, 180, 181, 184, 199, 200, 201, 203, 206, 217, 220, 222, 226, 236, 237, 238, 239, 241, 242, 243, 244, 245, 247, 248, 249, 250], "liweili": [10, 58], "c4_200m": [10, 58], "chang": [10, 12, 13, 14, 15, 17, 24, 26, 60, 62, 66, 201, 235, 241, 244, 245, 246, 247, 248, 249, 250], "remap": 10, "each": [10, 11, 13, 16, 22, 27, 30, 31, 41, 42, 44, 48, 50, 51, 52, 77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 158, 160, 164, 165, 166, 169, 170, 172, 180, 181, 187, 189, 190, 191, 192, 194, 195, 210, 224, 225, 236, 238, 239, 241, 243, 244, 245, 247, 248, 249], "them": [10, 11, 13, 15, 48, 49, 63, 166, 167, 172, 187, 228, 238, 241, 242, 243, 244, 247, 248, 249, 250], "someth": [10, 13, 16, 17, 242, 244, 249], "els": [10, 11, 16, 220, 236, 250], "hello": [10, 11, 12, 36, 184, 185, 230, 242, 244, 246], "world": [10, 11, 12, 36, 184, 185, 211, 213, 230, 244], "bye": 10, "robot": 10, "am": [10, 56, 60, 92, 128, 242, 243, 244, 246], "want": [10, 11, 13, 15, 16, 17, 18, 44, 47, 51, 52, 69, 174, 235, 241, 242, 243, 244, 245, 246, 247, 248], "add": [10, 11, 12, 14, 15, 17, 44, 47, 50, 53, 67, 128, 166, 174, 185, 187, 201, 202, 242, 243, 244, 246, 247, 250], "prompttempl": [10, 22, 25, 27, 32, 35], "relev": [10, 16, 163, 165, 241, 244, 247, 248], "inform": [10, 13, 216, 220, 222, 236, 241, 244, 245], "mai": [10, 15, 17, 56, 69, 166, 171, 207, 226, 237, 238, 242, 243, 245, 247, 248], "automat": [10, 11, 12, 14, 15, 17, 18, 55, 56, 241, 244, 250], "appli": [10, 11, 13, 16, 41, 47, 49, 51, 52, 55, 61, 62, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 93, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 138, 139, 140, 141, 145, 146, 147, 148, 149, 150, 158, 162, 163, 164, 165, 170, 180, 181, 222, 236, 237, 248, 250], "alpaca_dataset": [10, 15, 54, 243], "grammar_dataset": 10, "samsum_dataset": 10, "optim": [11, 13, 15, 16, 42, 48, 51, 83, 133, 143, 168, 192, 193, 194, 195, 201, 203, 205, 208, 221, 225, 237, 238, 239, 242, 244, 245, 246, 247, 250], "serv": [11, 15, 24, 26, 28, 34, 56, 58, 59, 60, 61, 62, 63, 64, 65, 187, 197, 243, 247], "purpos": [11, 61, 62, 245, 246], "requir": [11, 12, 13, 15, 41, 42, 48, 51, 52, 53, 61, 62, 63, 67, 162, 164, 171, 199, 201, 203, 209, 212, 213, 215, 216, 219, 220, 224, 225, 235, 238, 241, 242, 243, 245, 248, 249, 250], "whenev": [11, 169, 247], "tag": [11, 12, 22, 30, 36, 47, 86, 92, 98, 111, 128, 137, 144, 154, 216, 217, 218, 219, 220, 242], "llama2": [11, 13, 15, 16, 18, 47, 49, 57, 68, 74, 75, 76, 77, 78, 79, 80, 81, 82, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 155, 164, 165, 202, 234, 237, 241, 245, 246, 248, 249], "were": [11, 12, 48, 166, 177, 193, 245, 249], "infer": [11, 13, 44, 47, 52, 83, 92, 133, 157, 158, 160, 164, 165, 170, 189, 229, 234, 238, 239, 242, 244, 245, 246, 249, 250], "ensur": [11, 12, 13, 15, 21, 46, 51, 52, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 199, 201, 206, 236, 243, 245], "gear": 11, "particular": [11, 12, 15, 47, 48, 197, 243, 247, 250], "after": [11, 14, 16, 30, 52, 61, 62, 86, 98, 111, 137, 144, 157, 158, 161, 162, 164, 165, 170, 172, 196, 215, 216, 217, 218, 219, 220, 238, 242, 244, 246, 249, 250], "summar": [11, 35, 64, 242, 243, 248], "summarizetempl": [11, 48, 64, 242, 243], "questionanswertempl": [11, 66], "commun": [11, 243, 244, 248], "chatmltempl": [11, 154], "my": [11, 14, 63, 69, 241, 242, 243, 244, 246], "msg": [11, 12, 242], "gec_templ": 11, "templated_msg": 11, "text_cont": [11, 29, 242], "ad": [11, 12, 16, 30, 44, 135, 166, 170, 171, 174, 184, 201, 202, 242, 243, 247, 248, 249, 250], "special": [11, 29, 47, 111, 144, 154, 166, 171, 182, 183, 185, 186, 187, 189, 203, 243], "extend": [11, 12, 13, 16, 236], "discuss": [11, 12, 15, 244, 245, 246, 247], "A": [11, 16, 17, 24, 25, 28, 32, 34, 35, 37, 38, 41, 42, 43, 44, 48, 50, 63, 154, 158, 162, 163, 164, 165, 166, 167, 170, 176, 180, 184, 185, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 202, 203, 208, 209, 214, 215, 233, 234, 240, 241, 242, 247, 248, 249, 250], "e": [11, 29, 40, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 158, 166, 167, 170, 175, 179, 189, 198, 199, 203, 208, 225, 229, 235, 238, 244, 246, 247, 248, 249, 250], "customprompttempl": 11, "map": [11, 12, 13, 24, 26, 27, 28, 30, 34, 37, 38, 41, 47, 48, 49, 50, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 86, 98, 111, 137, 144, 154, 179, 185, 186, 199, 203, 205, 216, 217, 218, 219, 220, 221, 225, 243, 244, 247], "tupl": [11, 12, 15, 18, 30, 42, 69, 70, 86, 98, 111, 137, 144, 154, 157, 166, 167, 183, 187, 190, 191, 192, 193, 194, 195, 196, 197, 211, 225, 226, 227], "achiev": [11, 30, 221, 238, 244, 246, 247, 249, 250], "n": [11, 12, 25, 30, 32, 35, 158, 166, 187, 233, 240, 241, 242, 243, 249], "prepend_tag": [11, 30], "append_tag": [11, 30], "thu": [11, 30, 51, 52, 164, 248, 249], "would": [11, 13, 15, 17, 30, 50, 166, 170, 235, 242, 243, 244, 247, 248, 250], "now": [11, 13, 157, 203, 205, 238, 242, 243, 244, 245, 246, 247, 249, 250], "don": [11, 13, 15, 16, 220, 224, 241, 242, 243, 244, 245, 248, 250], "t": [11, 13, 15, 16, 39, 169, 172, 206, 220, 224, 241, 242, 243, 244, 245, 248, 250], "append": [11, 22, 30, 31, 86, 98, 111, 137, 144, 154, 164, 170, 184, 216, 235, 243], "just": [11, 13, 236, 238, 241, 242, 243, 245, 246, 247, 248, 249], "empti": [11, 41, 44, 46, 73, 241], "so": [11, 13, 15, 50, 61, 162, 166, 199, 235, 236, 242, 244, 245, 246, 247, 248, 249, 250], "standalon": [11, 157], "def": [11, 12, 15, 16, 17, 20, 61, 62, 197, 202, 243, 247, 250], "my_custom_templ": 11, "Is": 11, "overhyp": 11, "advanc": [11, 166, 243], "configur": [11, 12, 16, 49, 51, 52, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 87, 99, 112, 121, 129, 140, 146, 216, 236, 238, 239, 242, 245, 246, 247, 248, 249, 250], "doesn": [11, 244], "neatli": 11, "fall": 11, "new": [11, 12, 16, 28, 34, 37, 38, 55, 57, 58, 59, 61, 63, 64, 65, 134, 157, 171, 172, 202, 216, 217, 219, 242, 244, 245, 246, 247, 250], "inherit": [11, 16, 236, 243], "prompttemplateinterfac": [11, 86, 98, 111, 137, 144, 154], "implement": [11, 12, 13, 16, 47, 49, 51, 53, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 155, 159, 160, 161, 166, 168, 175, 176, 182, 183, 188, 192, 193, 194, 195, 199, 209, 219, 236, 238, 243, 247, 248, 249, 250], "__call__": [11, 61, 62], "protocol": [11, 12, 175, 182, 183, 188], "self": [11, 12, 16, 17, 50, 61, 62, 77, 78, 79, 83, 87, 88, 89, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 135, 140, 141, 142, 146, 147, 148, 149, 150, 158, 163, 164, 165, 169, 170, 172, 175, 180, 181, 199, 202, 203, 243, 247, 250], "bool": [11, 12, 15, 24, 26, 28, 29, 34, 37, 38, 39, 47, 49, 50, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 150, 158, 163, 164, 165, 167, 170, 172, 176, 180, 181, 183, 184, 185, 187, 190, 196, 197, 199, 200, 201, 207, 208, 212, 213, 215, 216, 219, 222, 225, 226, 231, 248, 250], "fals": [11, 13, 15, 24, 26, 28, 29, 34, 37, 38, 39, 47, 48, 49, 50, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 150, 158, 164, 165, 170, 171, 172, 176, 177, 180, 184, 196, 199, 200, 201, 210, 213, 225, 226, 241, 242, 243, 244, 246, 247, 249, 250], "accord": [11, 22, 61, 62, 72, 128, 242], "arg": [11, 12, 15, 18, 31, 156, 164, 167, 172, 175, 182, 183, 188, 218, 225, 238, 249], "object": [11, 12, 15, 18, 19, 22, 158, 192, 195, 197, 209], "whether": [11, 24, 26, 28, 29, 34, 37, 38, 41, 44, 47, 49, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 77, 78, 79, 83, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 150, 167, 170, 176, 180, 181, 184, 185, 197, 206, 208, 216, 226, 242, 243], "being": [11, 13, 52, 199, 200, 201, 205, 229, 248, 249, 250], "contriv": 11, "make": [11, 13, 14, 15, 16, 17, 166, 236, 241, 244, 245, 246, 247, 248, 249, 250], "sai": [11, 241, 242, 245], "eureka": 11, "eurekatempl": 11, "formatted_dialogu": 11, "eot": [11, 29], "look": [11, 13, 15, 16, 205, 219, 235, 242, 243, 244, 245, 246, 247, 249], "llama2chattempl": [11, 98, 128, 154, 242], "kei": [12, 13, 15, 17, 24, 26, 28, 34, 37, 41, 42, 47, 49, 51, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 163, 164, 165, 170, 172, 179, 180, 181, 195, 199, 201, 203, 216, 225, 241, 244, 245, 247, 248, 250], "index": [12, 42, 43, 44, 48, 50, 158, 160, 165, 168, 170, 191, 229, 235, 242, 244], "embed": [12, 13, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 159, 160, 163, 164, 166, 170, 171, 172, 174, 207, 242, 246, 248, 249], "vector": [12, 194, 242], "understood": 12, "plai": [12, 244, 248], "necessari": [12, 13, 51, 52, 216, 217, 218, 219, 220, 242, 247], "phi3": [12, 13, 140, 141, 143, 144, 145, 202, 241], "phi3_mini_token": 12, "p_token": 12, "phi": [12, 143, 144, 202], "3": [12, 13, 39, 41, 42, 43, 44, 50, 72, 73, 128, 141, 143, 144, 166, 202, 209, 210, 223, 230, 237, 238, 241, 242, 244, 245, 246, 249, 250], "tokenize_messag": [12, 29, 47, 49, 51, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 183, 187, 242, 243], "32010": 12, "29871": 12, "13": [12, 42, 166, 187, 196, 250], "1792": [12, 187], "9508": [12, 187], "32007": 12, "32001": 12, "4299": 12, "2933": [12, 187], "nuser": 12, "nmodel": 12, "usual": [12, 13, 160, 164, 196, 199, 210, 220, 241, 244, 247, 248], "sentencepiec": [12, 184, 246], "tiktoken": [12, 185, 246], "both": [12, 13, 44, 48, 59, 63, 155, 164, 170, 172, 174, 181, 241, 244, 247, 249, 250], "host": [12, 235, 241, 245, 248], "distribut": [12, 73, 203, 212, 213, 222, 224, 229, 236, 239, 241, 245, 246, 248], "alongsid": [12, 207], "when": [12, 13, 15, 16, 20, 48, 50, 51, 52, 53, 63, 67, 69, 71, 157, 158, 160, 164, 165, 166, 167, 168, 169, 170, 171, 177, 180, 191, 207, 219, 221, 226, 237, 241, 244, 246, 247, 248, 249, 250], "cd": [12, 235, 244], "ls": [12, 235, 239, 241, 244, 245, 246], "onc": [12, 15, 30, 244, 245, 246, 247, 248, 250], "ve": [12, 15, 157, 238, 241, 242, 243, 244, 246, 247, 248], "correspond": [12, 29, 42, 70, 71, 72, 173, 175, 178, 190, 193, 206, 238, 245, 246, 248, 249], "constructor": 12, "alreadi": [12, 15, 24, 28, 34, 58, 59, 61, 62, 63, 64, 65, 157, 158, 170, 202, 212, 235, 241, 243, 244, 247], "locat": [12, 15, 241, 243, 246, 247, 249, 250], "type": [12, 17, 18, 20, 29, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 176, 178, 182, 183, 184, 185, 186, 187, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 222, 223, 224, 225, 227, 229, 230, 231, 238, 243, 244, 247, 248, 249, 250], "_token": [12, 16, 243], "mistraltoken": [12, 137, 242], "give": [12, 15, 198, 243, 247, 248], "control": [12, 16, 29, 49, 55, 56, 58, 60, 63, 64, 65, 172, 177, 216, 224, 238, 244, 248], "over": [12, 16, 29, 52, 168, 192, 236, 238, 241, 244, 247, 248, 250], "memori": [12, 16, 48, 49, 50, 53, 57, 67, 68, 162, 164, 167, 169, 170, 180, 207, 208, 214, 215, 225, 234, 236, 237, 238, 244, 245, 246, 249], "adher": [12, 34, 37, 38], "arbitrarili": 12, "small": [12, 159, 244], "seq": [12, 164, 170], "len": [12, 13, 44, 48, 55, 58, 61, 62, 64, 164, 166, 170], "demonstr": [12, 248, 249], "7": [12, 13, 39, 41, 42, 43, 44, 157, 166, 189, 193], "6312": 12, "28709": 12, "2": [12, 13, 17, 39, 41, 42, 43, 44, 46, 50, 65, 72, 73, 128, 157, 158, 166, 184, 185, 187, 193, 195, 196, 199, 200, 209, 210, 223, 224, 225, 231, 238, 242, 244, 245, 246, 247, 248, 249], "becaus": [12, 51, 52, 87, 157, 164, 166, 170, 201, 241, 242, 249], "assign": [12, 15, 51, 52], "own": [12, 13, 30, 215, 224, 241, 242, 243, 244, 246, 247], "uniqu": [12, 51, 52, 202], "do": [12, 13, 14, 16, 29, 41, 49, 61, 63, 180, 187, 216, 220, 226, 241, 243, 244, 245, 246, 247, 248, 249], "addit": [12, 13, 15, 16, 18, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 128, 171, 172, 180, 192, 197, 199, 200, 201, 206, 207, 212, 215, 216, 217, 219, 220, 222, 236, 242, 245, 247, 248], "abil": 12, "experiment": [12, 15], "NOT": [12, 13, 83, 133], "correctli": [12, 13, 16, 21, 180, 199, 235, 239, 242, 245, 250], "note": [12, 13, 15, 27, 87, 170, 175, 203, 221, 224, 226, 238, 242, 243, 244, 247, 248, 249, 250], "presenc": 12, "certain": [12, 13, 15, 225, 242], "proper": [12, 235, 245], "eot_id": [12, 242], "llama3": [12, 15, 47, 61, 62, 69, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 140, 169, 202, 207, 234, 236, 237, 238, 241, 243, 244, 248], "begin_of_text": [12, 242], "end_of_text": 12, "special_token": [12, 185, 242], "added_token": 12, "128257": 12, "128258": 12, "remain": [12, 28, 34, 37, 38, 168, 247, 248], "llama": [12, 13, 47, 92, 159, 160, 199, 200, 237, 238, 241, 242, 244, 245, 246, 247], "special_tokens_path": [12, 111, 144, 154], "basetoken": 12, "modul": [12, 15, 18, 61, 62, 131, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 193, 194, 197, 202, 204, 205, 207, 215, 221, 222, 224, 243, 245, 247, 250], "actual": [12, 14, 15, 17, 24, 26, 47, 51, 52, 55, 58, 59, 60, 62, 63, 64, 66, 238, 242, 249], "string": [12, 13, 26, 29, 30, 36, 56, 57, 86, 98, 111, 137, 144, 154, 175, 182, 184, 185, 187, 198, 204, 206, 209, 216, 229, 241, 243, 248], "str": [12, 15, 18, 19, 24, 26, 27, 28, 29, 30, 34, 36, 37, 38, 40, 41, 42, 43, 44, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 86, 98, 111, 137, 144, 154, 167, 171, 172, 173, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 198, 199, 200, 201, 202, 203, 204, 206, 208, 209, 212, 214, 216, 217, 218, 219, 220, 224, 225, 226, 227, 229, 230, 231, 243, 248], "kwarg": [12, 15, 18, 31, 154, 156, 163, 165, 167, 172, 175, 182, 183, 188, 212, 216, 217, 218, 219, 220, 222, 225, 243], "dict": [12, 13, 15, 16, 17, 18, 24, 26, 27, 28, 29, 30, 34, 36, 37, 38, 41, 42, 43, 44, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 86, 98, 111, 137, 144, 154, 163, 165, 167, 170, 172, 173, 178, 179, 180, 181, 182, 183, 185, 186, 188, 199, 200, 201, 203, 205, 208, 212, 214, 216, 221, 226, 228, 243], "int": [12, 15, 17, 41, 42, 43, 44, 45, 47, 49, 50, 57, 61, 62, 68, 69, 70, 71, 73, 77, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 93, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 111, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 154, 157, 158, 159, 160, 163, 164, 165, 166, 168, 169, 170, 171, 172, 176, 182, 183, 184, 185, 186, 187, 189, 196, 197, 199, 200, 201, 204, 207, 211, 215, 216, 217, 218, 219, 220, 222, 224, 225, 241, 243, 247, 248, 250], "given": [12, 16, 18, 27, 36, 41, 46, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 176, 177, 182, 183, 191, 206, 209, 215, 221, 229, 231, 236, 247], "token_id": [12, 182, 185], "its": [12, 50, 92, 128, 131, 158, 160, 164, 165, 170, 172, 221, 224, 241, 242, 243, 244, 246, 247, 248], "sentencepiecebasetoken": [12, 182], "bpe": 12, "sp_token": 12, "1526": 12, "modeltoken": [12, 15, 29, 47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 187, 243], "concaten": [12, 42, 48, 183, 187], "reason": [12, 16, 69, 244, 248, 249], "28705": 12, "28748": 12, "walk": [13, 16, 219, 236, 242, 243, 244, 245, 249, 250], "through": [13, 14, 15, 16, 17, 51, 155, 157, 166, 172, 177, 236, 237, 238, 239, 241, 242, 243, 244, 245, 248, 249, 250], "design": [13, 16, 195], "behavior": [13, 215, 242, 243], "cover": [13, 14, 15, 16, 17, 242, 244, 250], "how": [13, 14, 15, 16, 17, 166, 197, 216, 222, 234, 237, 238, 241, 242, 243, 244, 245, 246, 248, 249, 250], "scenario": [13, 48], "full": [13, 15, 16, 25, 32, 35, 51, 68, 80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 170, 180, 181, 187, 204, 235, 236, 239, 241, 243, 244, 246, 247, 248, 249], "compos": [13, 166], "plug": [13, 248], "evalu": [13, 16, 234, 236, 238, 239, 245, 247, 250], "gener": [13, 16, 41, 47, 49, 50, 57, 67, 70, 71, 72, 73, 177, 190, 216, 223, 224, 225, 232, 234, 238, 242, 243, 247, 248, 249, 250], "easi": [13, 16, 236, 243, 247, 248], "understand": [13, 15, 16, 172, 234, 236, 237, 242, 243, 247, 248, 250], "concept": [13, 239, 244, 245, 248], "ll": [13, 15, 16, 69, 209, 236, 238, 242, 243, 244, 245, 246, 248, 249, 250], "talk": 13, "close": [13, 16, 216, 217, 218, 219, 220, 247], "veri": [13, 48, 164, 170, 241, 244, 248], "dictat": 13, "state_dict": [13, 167, 171, 172, 180, 199, 200, 201, 202, 203, 226, 247, 250], "store": [13, 51, 52, 216, 217, 220, 247, 248, 250], "disk": [13, 53, 217], "identifi": [13, 216], "state": [13, 16, 164, 166, 167, 173, 178, 179, 180, 181, 190, 192, 199, 200, 201, 203, 205, 226, 244, 246, 247, 248, 250], "match": [13, 36, 49, 181, 216, 226, 235, 241, 243, 244, 246, 247], "up": [13, 14, 16, 17, 44, 49, 50, 57, 68, 164, 170, 185, 189, 205, 216, 225, 237, 238, 239, 241, 242, 243, 245, 246, 247, 248, 250], "exactli": [13, 181, 198, 249], "those": [13, 202, 244, 246, 247], "definit": [13, 247], "either": [13, 41, 51, 52, 69, 158, 164, 165, 181, 199, 216, 222, 235, 241, 247, 248, 249, 250], "run": [13, 14, 15, 17, 20, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 164, 167, 169, 199, 200, 201, 203, 204, 205, 213, 216, 219, 220, 221, 235, 236, 237, 238, 239, 242, 243, 245, 246, 247, 248, 249, 250], "explicit": 13, "error": [13, 15, 25, 46, 199, 224, 241], "load": [13, 16, 40, 47, 48, 49, 50, 51, 52, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 170, 180, 199, 200, 201, 203, 219, 226, 243, 244, 246, 247], "rais": [13, 18, 21, 24, 26, 28, 34, 36, 37, 40, 41, 44, 46, 49, 55, 56, 58, 60, 61, 62, 64, 65, 67, 71, 146, 157, 158, 162, 163, 164, 166, 180, 181, 187, 199, 200, 201, 203, 206, 208, 212, 216, 220, 224, 226, 227, 228], "except": [13, 29, 128, 187, 243], "wors": [13, 248], "silent": 13, "succe": 13, "line": [13, 14, 16, 239, 241, 243, 245, 246], "shape": [13, 41, 44, 69, 70, 71, 72, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 169, 170, 171, 172, 176, 189, 190, 191, 192, 193, 194, 195, 196, 210, 225, 226], "popular": [13, 170, 236, 243, 244], "offici": [13, 92, 242, 245, 246], "websit": 13, "get": [13, 14, 15, 16, 17, 44, 47, 206, 208, 211, 216, 230, 235, 236, 237, 238, 242, 243, 244, 245, 247, 248, 249], "inspect": [13, 244, 247, 250], "easili": [13, 15, 236, 243, 247, 249, 250], "torch": [13, 15, 39, 41, 42, 43, 44, 69, 70, 71, 72, 73, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 176, 190, 191, 192, 193, 194, 195, 196, 201, 203, 205, 206, 208, 209, 210, 212, 213, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 231, 235, 238, 244, 245, 246, 247, 248, 250], "mmap": [13, 244], "weights_onli": [13, 201], "map_loc": [13, 244], "cpu": [13, 16, 167, 206, 225, 229, 235, 241, 244, 248, 250], "tensor": [13, 39, 41, 42, 43, 44, 69, 70, 71, 72, 73, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 167, 169, 170, 171, 172, 176, 190, 191, 192, 193, 194, 195, 196, 199, 210, 216, 217, 218, 219, 220, 223, 226, 228, 247, 248, 250], "item": 13, "f": [13, 17, 55, 58, 61, 62, 64, 198, 242, 244, 247, 250], "tok_embed": [13, 164, 170, 171], "32000": [13, 18, 247], "4096": [13, 18, 49, 57, 68, 158, 160, 243, 247, 249], "292": 13, "tabl": [13, 171, 242, 244, 246, 248, 250], "layer": [13, 16, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 93, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 136, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 158, 161, 162, 163, 164, 165, 166, 170, 172, 174, 176, 180, 181, 197, 204, 207, 236, 237, 246, 247, 248, 249, 250], "dim": [13, 44, 155, 158, 159, 160, 164, 169, 170], "within": [13, 15, 18, 47, 50, 69, 73, 87, 99, 112, 121, 129, 131, 140, 146, 166, 219, 224, 225, 241, 243, 247, 250], "hub": [13, 51, 52, 241, 243, 245], "first": [13, 15, 18, 46, 50, 61, 72, 164, 166, 170, 196, 199, 234, 236, 237, 242, 243, 244, 246, 247, 249, 250], "big": 13, "bin": [13, 241, 244], "piec": 13, "pytorch_model": [13, 244], "00001": [13, 241], "00002": [13, 241], "embed_token": 13, "241": 13, "Not": 13, "fewer": [13, 158], "sinc": [13, 15, 18, 51, 52, 199, 201, 242, 244, 246, 248, 249], "mismatch": 13, "caus": [13, 184], "try": [13, 15, 242, 244, 245, 246, 250], "re": [13, 15, 172, 195, 201, 236, 237, 238, 242, 244, 245, 247, 248], "care": [13, 199, 201, 244, 246, 247], "end": [13, 16, 29, 53, 67, 185, 187, 234, 236, 237, 242, 246, 247, 249], "number": [13, 16, 36, 44, 47, 49, 50, 57, 68, 69, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 164, 166, 168, 189, 199, 200, 201, 204, 211, 224, 225, 241, 245, 247, 248], "save": [13, 16, 17, 164, 167, 169, 170, 199, 200, 201, 203, 207, 215, 220, 234, 238, 241, 242, 243, 244, 246, 247, 248, 249], "less": [13, 69, 244, 245, 246, 248, 250], "prone": 13, "manag": [13, 48, 177, 216, 223, 242], "invari": 13, "accept": [13, 15, 197, 243, 245, 248, 250], "worri": [13, 242, 245], "explicitli": [13, 175, 236, 247], "time": [13, 56, 60, 83, 133, 169, 187, 190, 217, 219, 225, 238, 241, 242, 243, 244, 246, 250], "produc": [13, 203, 238, 249, 250], "One": [13, 44, 249], "advantag": [13, 190, 193, 238, 247], "should": [13, 15, 16, 22, 24, 26, 27, 28, 29, 30, 34, 37, 38, 41, 50, 55, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 77, 78, 79, 87, 88, 89, 92, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 128, 129, 130, 131, 132, 133, 135, 140, 141, 142, 146, 147, 148, 149, 150, 157, 158, 164, 166, 170, 175, 180, 181, 190, 193, 197, 198, 214, 216, 217, 218, 219, 220, 235, 236, 243, 244, 245, 246, 247, 248, 249, 250], "abl": [13, 16, 244, 245, 249], "post": [13, 166, 221, 225, 238, 244, 246, 249, 250], "quantiz": [13, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 176, 201, 209, 234, 235, 237, 239, 245, 250], "eval": [13, 234, 236, 249], "without": [13, 15, 17, 158, 162, 180, 235, 236, 238, 242, 244, 247, 248, 249], "OR": [13, 37], "script": [13, 17, 239, 241, 243, 244, 245, 246], "surround": [13, 16, 236], "load_checkpoint": [13, 16, 199, 200, 201, 202], "save_checkpoint": [13, 16, 17, 199, 200, 201], "permut": 13, "behav": 13, "further": [13, 166, 195, 241, 243, 247, 248, 249, 250], "illustr": [13, 61, 62, 246], "whilst": [13, 237, 248], "other": [13, 16, 18, 26, 30, 48, 201, 207, 225, 228, 237, 238, 242, 243, 245, 246, 247, 248, 249], "found": [13, 14, 15, 17, 159, 160, 199, 200, 201, 238, 241, 247, 250], "folder": 13, "three": [13, 16, 44, 192, 194, 195, 239, 245], "read": [13, 199, 200, 201, 236, 248], "write": [13, 16, 199, 200, 201, 217, 243, 245], "compat": [13, 199, 201, 248, 249], "framework": [13, 16, 236], "mention": [13, 244, 250], "assum": [13, 27, 39, 41, 49, 61, 86, 98, 111, 137, 144, 154, 157, 158, 160, 165, 168, 170, 171, 173, 178, 185, 203, 205, 206, 242, 244, 247], "checkpoint_dir": [13, 15, 199, 200, 201, 244, 246, 249], "easiest": [13, 244, 245], "sure": [13, 15, 244, 245, 246, 247, 248, 249, 250], "everyth": [13, 16, 236, 239, 245], "flow": [13, 47, 49, 50, 249, 250], "safetensor": [13, 198, 199, 241], "output_dir": [13, 15, 199, 200, 201, 225, 244, 246, 247, 249, 250], "snippet": 13, "explain": [13, 248], "setup": [13, 15, 16, 71, 157, 158, 163, 164, 165, 170, 172, 204, 225, 241, 243, 244, 247, 250], "fullmodelhfcheckpoint": [13, 244], "directori": [13, 15, 61, 199, 200, 201, 217, 219, 220, 225, 241, 243, 244, 245, 246], "sort": [13, 199, 201], "order": [13, 14, 16, 199, 201, 219, 220, 245, 248], "matter": [13, 199, 201, 241, 247], "checkpoint_fil": [13, 15, 17, 199, 200, 201, 244, 246, 247, 249, 250], "restart": [13, 241], "previou": [13, 50, 199, 200, 201], "section": [13, 16, 208, 234, 244, 246, 248, 250], "recipe_checkpoint": [13, 199, 200, 201, 249], "model_typ": [13, 199, 200, 201, 244, 246, 249], "resume_from_checkpoint": [13, 199, 200, 201], "param": [13, 16, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 173, 174, 176, 178, 179, 181, 199, 247, 249, 250], "case": [13, 16, 17, 29, 30, 51, 166, 199, 203, 206, 209, 215, 217, 222, 236, 241, 242, 243, 244, 246, 247, 248, 250], "discrep": [13, 199], "github": [13, 18, 70, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 159, 160, 168, 169, 192, 193, 194, 195, 235, 243, 244, 246], "repositori": [13, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 92, 237, 238, 244, 245], "fullmodelmetacheckpoint": [13, 246, 249], "current": [13, 50, 63, 71, 83, 87, 99, 112, 121, 129, 131, 133, 140, 143, 146, 157, 158, 160, 164, 165, 170, 193, 200, 201, 207, 209, 211, 217, 219, 221, 224, 238, 239, 243, 245, 246, 248, 249], "test": [13, 15, 16, 69, 236, 238, 242, 248], "written": [13, 15, 16, 199, 200, 216, 217, 218, 219, 220, 236], "begin": [13, 50, 67, 166, 185, 242, 246, 250], "partit": [13, 199, 250], "key_1": [13, 201], "weight_1": 13, "key_2": 13, "weight_2": 13, "mid": 13, "chekpoint": 13, "middl": [13, 172, 244, 248], "subsequ": [13, 16, 157, 164, 166, 189], "recipe_st": [13, 199, 200, 201], "pt": [13, 17, 199, 200, 201, 244, 246, 249], "epoch": [13, 16, 17, 168, 199, 200, 201, 241, 242, 244, 245, 246, 249], "etc": [13, 16, 199, 208, 245], "prevent": [13, 50, 192, 241, 248], "flood": 13, "overwritten": 13, "updat": [13, 15, 16, 30, 157, 158, 188, 192, 193, 199, 203, 225, 228, 235, 242, 244, 245, 246, 247, 248, 249, 250], "hf_model_0001_0": [13, 244], "hf_model_0002_0": [13, 244], "adapt": [13, 77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141, 170, 172, 173, 175, 176, 177, 178, 179, 199, 200, 201, 215, 237, 242, 244, 247, 250], "merg": [13, 18, 19, 154, 199, 244, 246, 250], "save_adapter_weights_onli": 13, "choos": [13, 56, 247], "resum": [13, 16, 168, 199, 200, 201, 250], "initi": [13, 16, 20, 48, 50, 74, 75, 76, 84, 85, 94, 95, 96, 97, 109, 110, 118, 119, 120, 134, 136, 151, 152, 153, 192, 203, 212, 213, 226, 238, 245, 247, 250], "frozen": [13, 171, 192, 247, 248, 250], "base": [13, 18, 29, 30, 49, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 93, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 160, 168, 176, 177, 179, 180, 181, 191, 192, 194, 195, 199, 207, 215, 217, 226, 229, 234, 242, 244, 245, 246, 247, 248, 250], "well": [13, 15, 16, 236, 241, 243, 244, 246, 248, 250], "learnt": [13, 242, 244], "refer": [13, 15, 16, 159, 160, 162, 166, 169, 177, 191, 192, 193, 194, 195, 216, 236, 247, 248, 249], "adapter_checkpoint": [13, 199, 200, 201], "adapter_0": [13, 244], "knowledg": 13, "simpl": [13, 16, 166, 195, 234, 243, 245, 247, 249, 250], "forward": [13, 16, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 169, 170, 171, 172, 176, 192, 193, 194, 195, 208, 225, 246, 247, 248, 250], "modeltyp": [13, 199, 200, 201], "llama2_13b": [13, 100], "right": [13, 41, 44, 72, 164, 199, 244, 246, 247], "pytorch_fil": 13, "00003": [13, 198], "torchtune_sd": 13, "load_state_dict": [13, 170, 171, 172, 180, 203, 226, 247], "successfulli": [13, 241, 245], "vocab": [13, 18, 154, 164, 170, 171, 246], "70": [13, 109], "x": [13, 39, 69, 70, 71, 155, 156, 158, 159, 160, 161, 163, 164, 165, 166, 170, 171, 172, 176, 210, 223, 247, 249, 250], "randint": 13, "no_grad": 13, "6": [13, 39, 41, 42, 43, 44, 50, 83, 87, 159, 166, 210, 238, 249, 250], "3989": 13, "9": [13, 39, 41, 42, 44, 157, 166, 210, 244, 249, 250], "0531": 13, "2375": 13, "5": [13, 15, 39, 41, 42, 43, 44, 71, 166, 168, 192, 195, 196, 244, 245, 246, 248], "2822": 13, "4": [13, 15, 39, 41, 42, 43, 44, 72, 157, 158, 166, 209, 210, 231, 236, 238, 241, 243, 244, 246, 247, 248, 249, 250], "4872": 13, "7469": 13, "8": [13, 39, 41, 42, 44, 55, 58, 61, 62, 64, 77, 78, 79, 80, 81, 82, 88, 89, 90, 91, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 117, 122, 123, 124, 125, 126, 127, 130, 132, 138, 139, 141, 145, 147, 148, 149, 157, 166, 169, 244, 247, 248, 249, 250], "6737": 13, "11": [13, 39, 41, 42, 166, 244, 249, 250], "0023": 13, "8235": 13, "6819": 13, "2424": 13, "0109": 13, "6915": 13, "3618": 13, "1628": 13, "8594": 13, "5857": 13, "1151": 13, "7808": 13, "2322": 13, "8850": 13, "9604": 13, "7624": 13, "6040": 13, "3159": 13, "5849": 13, "8039": 13, "9322": 13, "2010": [13, 166], "6824": 13, "8929": 13, "8465": 13, "3794": 13, "3500": 13, "6145": 13, "5931": 13, "find": [13, 14, 16, 17, 192, 241, 244, 245, 247, 248], "hope": 13, "deeper": [13, 237, 238, 245, 248], "insight": [13, 244], "happi": [13, 244], "start": [14, 16, 17, 40, 72, 187, 202, 216, 235, 236, 242, 243, 244, 245, 248, 249], "cometlogg": 14, "checkpoint": [14, 15, 16, 167, 170, 172, 185, 198, 199, 200, 201, 202, 203, 204, 220, 222, 226, 236, 238, 241, 246, 247, 249, 250], "workspac": [14, 17, 216], "seen": [14, 17, 247, 250], "screenshot": [14, 17], "below": [14, 17, 41, 197, 243, 246, 247, 250], "instal": [14, 15, 17, 213, 216, 219, 220, 234, 241, 243, 244, 245, 246, 247, 248, 249, 250], "comet_ml": [14, 216], "packag": [14, 17, 216, 219, 220, 235, 243], "featur": [14, 16, 17, 235, 236, 237, 238, 244, 245, 248], "pip": [14, 17, 216, 219, 220, 235, 244, 246, 248], "login": [14, 17, 216, 220, 241, 244], "built": [14, 15, 17, 59, 63, 66, 235, 242, 245, 250], "metric_logg": [14, 15, 16, 17], "metric_log": [14, 15, 17, 216, 217, 218, 219, 220], "project": [14, 17, 77, 78, 79, 83, 87, 93, 97, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 136, 140, 141, 146, 149, 150, 155, 158, 164, 166, 170, 174, 180, 181, 202, 207, 216, 220, 234, 247, 248, 250], "experiment_nam": [14, 216], "experi": [14, 15, 216, 220, 234, 236, 246, 247], "grab": [14, 17, 246], "hyperparamet": [14, 195, 203, 236, 245, 247, 250], "tab": [14, 17], "asset": 14, "artifact": [14, 17, 225], "click": [14, 17], "pars": [15, 18, 19, 186, 239, 245], "effect": [15, 195, 248, 249], "cli": [15, 17, 19, 20, 235, 237, 244, 245, 248], "prerequisit": [15, 242, 243, 244, 245, 246, 247, 249, 250], "Be": [15, 242, 244, 245, 246, 247, 248, 249, 250], "familiar": [15, 242, 244, 245, 246, 247, 249, 250], "fundament": [15, 249], "truth": [15, 169, 244, 246], "reproduc": [15, 216], "overridden": [15, 225], "quick": 15, "seed": [15, 16, 17, 224, 245, 249], "shuffl": [15, 50, 249], "devic": [15, 16, 203, 206, 208, 228, 229, 239, 241, 242, 244, 245, 246, 247, 248, 250], "cuda": [15, 206, 208, 225, 229, 235, 244, 248, 250], "dtype": [15, 16, 73, 157, 158, 163, 164, 165, 167, 170, 172, 206, 223, 227, 244, 248, 249, 250], "fp32": [15, 164, 169, 248, 249, 250], "enable_fsdp": 15, "keyword": [15, 18, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 167, 242, 243], "subfield": 15, "dotpath": [15, 86, 98, 111, 137, 144, 154, 243], "wish": [15, 157, 226, 243], "exact": [15, 18, 244], "normal": [15, 47, 50, 156, 158, 159, 163, 164, 165, 169, 184, 242, 243, 247, 249, 250], "python": [15, 216, 220, 224, 230, 232, 241, 243, 244, 249], "instanc": [15, 18, 48, 49, 99, 112, 121, 129, 131, 140, 146, 147, 148, 151, 152, 167, 173, 178, 179, 247], "cfg": [15, 16, 19, 20, 21], "under": [15, 225, 243, 248, 250], "preced": [15, 18, 241, 246, 247], "throw": 15, "notic": [15, 166, 242, 243, 247], "miss": [15, 180, 181, 225, 247], "posit": [15, 18, 50, 70, 72, 83, 87, 129, 131, 133, 135, 140, 142, 157, 158, 160, 163, 164, 165, 166, 170, 171, 246], "dictconfig": [15, 16, 18, 19, 20, 21, 216, 220, 225], "mean": [15, 158, 159, 163, 164, 165, 170, 190, 215, 241, 242, 243, 245, 247, 249], "d": [15, 29, 61, 157, 158, 164, 170, 241, 242, 247, 249], "llama2_token": [15, 242, 244], "llama2token": [15, 98], "512": [15, 243, 250], "instructdataset": [15, 243], "overwrit": [15, 201, 226, 235, 241], "duplic": [15, 16, 236, 241], "sometim": 15, "resolv": [15, 19, 245], "alpaca": [15, 48, 54, 55, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 243], "disklogg": 15, "log_dir": [15, 217, 219, 220], "conveni": [15, 16, 40, 241], "verifi": [15, 206, 207, 229, 242, 245, 247], "properli": [15, 180, 213, 241], "wa": [15, 44, 71, 166, 180, 242, 247, 249, 250], "cp": [15, 235, 241, 242, 244, 245, 246, 249], "7b_lora_single_devic": [15, 244, 245, 247, 250], "my_config": [15, 241], "guidelin": 15, "while": [15, 16, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 171, 236, 238, 244, 249, 250], "tempt": 15, "put": [15, 16, 239, 245, 247, 249], "much": [15, 171, 195, 244, 246, 247, 249, 250], "maximum": [15, 41, 44, 45, 47, 49, 50, 57, 68, 71, 83, 86, 87, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 144, 146, 150, 157, 158, 160, 163, 164, 165, 170, 172, 189, 198, 241], "flexibl": [15, 48, 243, 248], "switch": 15, "encourag": [15, 195, 247, 248], "clariti": 15, "significantli": [15, 192, 237, 238, 248], "easier": [15, 244, 245], "dont": 15, "privat": 15, "expos": [15, 16, 201, 239, 245], "parent": [15, 241], "__init__": [15, 16, 61, 62, 247, 250], "py": [15, 18, 70, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 157, 159, 160, 168, 192, 193, 194, 195, 241, 244, 246], "guarante": 15, "stabil": [15, 169, 236, 238, 248, 249, 250], "underscor": 15, "_alpaca": 15, "itself": 15, "k1": [15, 16], "v1": [15, 16, 68], "k2": [15, 16], "v2": [15, 16, 216, 243], "lora": [15, 77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 176, 177, 180, 181, 199, 215, 234, 236, 239, 242, 245, 246], "lora_finetune_single_devic": [15, 237, 241, 242, 244, 245, 246, 247, 248, 250], "home": [15, 40], "my_model_checkpoint": 15, "file_1": 15, "file_2": 15, "my_tokenizer_path": 15, "nest": [15, 228], "dot": 15, "notat": [15, 44, 158, 160, 164, 170, 190, 191, 210], "flag": [15, 16, 29, 49, 55, 56, 58, 60, 63, 64, 65, 197, 201, 207, 241, 248, 250], "bitsandbyt": [15, 248], "pagedadamw8bit": [15, 248], "delet": 15, "foreach": [15, 47], "pytorch": [15, 16, 70, 164, 167, 169, 197, 213, 219, 222, 224, 225, 234, 235, 236, 238, 244, 246, 247, 248, 249, 250], "8b_full": [15, 241, 243], "adamw": [15, 247, 248], "lr": [15, 168, 248], "2e": [15, 248], "fuse": [15, 170, 171, 172, 173, 221, 249], "nproc_per_nod": [15, 238, 243, 246, 247, 249], "full_finetune_distribut": [15, 241, 243, 244, 245], "core": [16, 51, 52, 236, 239, 243, 245, 250], "thought": [16, 236, 239, 245, 250], "target": [16, 71, 195, 236], "pipelin": [16, 236, 238], "eg": [16, 164, 170, 199, 236], "meaning": [16, 236, 244], "fsdp": [16, 162, 197, 203, 207, 215, 245, 246], "activ": [16, 155, 204, 208, 214, 222, 225, 236, 238, 249, 250], "gradient": [16, 215, 221, 225, 236, 238, 244, 246, 247, 250], "accumul": [16, 221, 225, 236, 238], "mix": [16, 156, 241, 243, 244, 248], "precis": [16, 156, 167, 206, 236, 238, 245, 250], "complex": 16, "becom": [16, 166, 235, 243], "harder": 16, "anticip": 16, "architectur": [16, 92, 128, 164, 166, 170, 172, 202, 241, 243], "methodolog": 16, "possibl": [16, 50, 198, 241, 243, 248], "trade": [16, 248], "off": [16, 30, 63, 237, 238, 244, 249], "vs": [16, 245], "qualiti": [16, 244, 247, 249], "believ": 16, "best": [16, 238, 242, 248], "suit": [16, 245, 248], "b": [16, 39, 41, 157, 158, 160, 164, 165, 170, 176, 190, 191, 195, 210, 220, 247, 250], "solut": 16, "result": [16, 61, 166, 187, 189, 225, 238, 244, 246, 247, 248, 249, 250], "meant": [16, 167, 203], "depend": [16, 17, 199, 225, 241, 243, 244, 247, 248, 250], "level": [16, 51, 52, 169, 188, 205, 215, 230, 236, 250], "expertis": 16, "routin": 16, "yourself": [16, 241, 246, 247], "exist": [16, 172, 216, 235, 241, 244, 245, 246, 250], "ones": [16, 44, 157], "modular": [16, 236], "build": [16, 67, 83, 93, 108, 117, 133, 135, 150, 198, 236, 246, 247], "block": [16, 50, 77, 78, 79, 83, 87, 88, 89, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 140, 141, 146, 147, 148, 149, 150, 158, 164, 165, 180, 181, 236], "wandb": [16, 17, 220, 245], "log": [16, 19, 192, 193, 194, 195, 208, 214, 216, 217, 218, 219, 220, 230, 244, 245, 246, 247, 248, 250], "fulli": [16, 48], "nativ": [16, 234, 236, 247, 249, 250], "numer": [16, 62, 236, 238, 249], "pariti": [16, 236], "verif": [16, 159], "extens": [16, 201, 236], "comparison": [16, 247, 250], "benchmark": [16, 224, 236, 244, 246, 247, 249], "limit": [16, 203, 243, 248, 249], "hidden": [16, 155, 164, 166], "behind": 16, "prefer": [16, 42, 51, 59, 63, 66, 192, 193, 194, 195, 236, 239, 241, 243, 248], "unnecessari": 16, "abstract": [16, 22, 27, 182, 183, 236, 245, 250], "No": [16, 201, 236], "go": [16, 92, 128, 166, 187, 236, 243, 244, 245, 248, 250], "upon": [16, 48, 246], "figur": [16, 247, 250], "spectrum": 16, "decid": 16, "interact": [16, 51, 63, 234, 239, 245], "avail": [16, 68, 170, 172, 206, 213, 229, 236, 241, 244, 246, 247], "paradigm": [16, 237, 248], "consist": [16, 24, 28, 34, 61, 62, 68, 239, 245], "paramet": [16, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210, 212, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 234, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 249, 250], "overrid": [16, 19, 20, 24, 28, 34, 58, 59, 61, 62, 63, 64, 65, 226, 239, 241, 244, 245, 246, 250], "togeth": [16, 50, 169, 220, 239, 245, 247, 248, 249], "valid": [16, 46, 72, 180, 181, 191, 226, 227, 235, 239, 244, 245], "environ": [16, 213, 216, 229, 235, 239, 241, 243, 244, 245, 249], "logic": [16, 52, 183, 202, 236, 239, 245, 247], "api": [16, 17, 25, 32, 35, 37, 51, 52, 55, 61, 62, 80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 180, 216, 235, 239, 241, 242, 245, 246, 250], "closer": [16, 247], "monolith": [16, 236], "trainer": [16, 192, 194, 195], "wrapper": [16, 156, 184, 185, 203, 205, 241, 247], "around": [16, 47, 156, 184, 185, 208, 241, 242, 244, 247, 248, 249, 250], "extern": [16, 243], "primarili": [16, 48, 247], "eleutherai": [16, 68, 236, 247, 249], "har": [16, 236, 247, 249], "stage": [16, 166], "distil": 16, "oper": [16, 166, 177, 188, 224, 249], "dataload": [16, 50, 55, 58, 61, 62, 64], "applic": [16, 199, 200, 220], "clean": [16, 17, 54], "process": [16, 17, 51, 52, 53, 59, 61, 62, 67, 166, 167, 211, 212, 224, 243, 245, 249, 250], "group": [16, 158, 211, 212, 216, 217, 218, 219, 220, 241, 246, 249], "init_process_group": [16, 212], "backend": [16, 241, 249], "gloo": 16, "nccl": 16, "fullfinetunerecipedistribut": 16, "cleanup": 16, "stuff": 16, "carri": [16, 52], "interfac": [16, 22, 27, 30, 31, 48, 175, 188, 243], "metric": [16, 245, 248, 249], "logger": [16, 214, 216, 217, 218, 219, 220, 230, 245], "_devic": 16, "get_devic": 16, "_dtype": 16, "get_dtyp": 16, "ckpt_dict": 16, "wrap": [16, 172, 197, 204, 207, 215, 222, 242], "_model": [16, 203], "_setup_model": 16, "_setup_token": 16, "_optim": 16, "_setup_optim": 16, "_loss_fn": 16, "_setup_loss": 16, "_sampler": 16, "_dataload": 16, "_setup_data": 16, "backward": [16, 203, 205, 221, 225, 250], "zero_grad": 16, "curr_epoch": 16, "rang": [16, 171, 192, 193, 195, 224, 241, 246, 249], "epochs_run": [16, 17], "total_epoch": [16, 17], "idx": [16, 50], "enumer": 16, "_autocast": 16, "logit": [16, 69, 70, 73, 169, 210], "global_step": 16, "_log_every_n_step": 16, "_metric_logg": 16, "log_dict": [16, 216, 217, 218, 219, 220], "step": [16, 50, 51, 52, 61, 62, 164, 168, 170, 190, 205, 216, 217, 218, 219, 220, 221, 225, 234, 238, 244, 247, 249, 250], "decor": [16, 20], "recipe_main": [16, 20], "fullfinetunerecip": 16, "wandblogg": [17, 247, 250], "Then": [17, 177, 245, 248], "tip": 17, "straggler": 17, "background": 17, "crash": 17, "otherwis": [17, 39, 41, 44, 162, 164, 166, 213, 216, 242, 249], "exit": [17, 177, 235, 241], "resourc": [17, 216, 217, 218, 219, 220, 248, 249], "kill": 17, "ps": 17, "aux": 17, "grep": 17, "awk": 17, "xarg": 17, "desir": [17, 47, 51, 52, 223, 242, 248], "suggest": 17, "approach": [17, 48, 243], "full_finetun": 17, "joinpath": 17, "_checkpoint": [17, 244], "_output_dir": [17, 199, 200, 201], "torchtune_model_": 17, "with_suffix": 17, "wandb_at": 17, "descript": [17, 241], "whatev": 17, "metadata": [17, 249], "seed_kei": 17, "epochs_kei": 17, "total_epochs_kei": 17, "max_steps_kei": 17, "max_steps_per_epoch": [17, 249], "add_fil": 17, "log_artifact": 17, "hydra": 18, "facebook": 18, "research": 18, "com": [18, 70, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149, 159, 160, 168, 169, 192, 193, 194, 195, 216, 235, 244, 246], "facebookresearch": [18, 159], "blob": [18, 70, 77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 144, 147, 148, 149, 159, 160, 168, 192, 193, 194, 195], "main": [18, 20, 144, 159, 160, 235, 238, 244, 246], "_intern": 18, "_instantiate2": 18, "l148": 18, "omegaconf": 18, "num_lay": [18, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 164, 166, 170, 172], "32": [18, 157, 166, 170, 172, 216, 246, 247, 248, 249, 250], "num_head": [18, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 160, 164], "num_kv_head": [18, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158], "vocab_s": [18, 69, 70, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 169, 171], "must": [18, 30, 48, 61, 62, 158, 175, 198, 216, 250], "nn": [18, 39, 41, 44, 155, 157, 158, 162, 163, 164, 165, 166, 167, 170, 171, 172, 173, 174, 175, 177, 178, 179, 197, 204, 205, 215, 221, 222, 226, 227, 247, 250], "parsed_yaml": 18, "embed_dim": [18, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 160, 163, 164, 165, 166, 171, 172, 226, 247], "valueerror": [18, 24, 26, 28, 34, 36, 37, 40, 41, 44, 46, 49, 55, 56, 58, 60, 61, 62, 64, 65, 67, 146, 157, 158, 166, 199, 200, 201, 206, 208, 224, 227], "recipe_nam": 19, "rank": [19, 77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 176, 211, 213, 224, 237, 245, 247, 250], "zero": [19, 44, 157, 159, 198, 244, 246, 249], "displai": 19, "callabl": [20, 47, 49, 51, 52, 53, 61, 62, 67, 69, 164, 177, 197, 207, 209, 215, 222], "With": [20, 244, 247, 249, 250], "my_recip": 20, "foo": 20, "bar": [20, 236, 245, 248], "instanti": [21, 30, 74, 75, 76, 77, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 135, 136, 137, 140, 141, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 203], "configerror": 21, "cannot": [21, 40, 201, 246], "deprec": [22, 27, 37, 38, 47, 49], "remov": [22, 27, 37, 38, 47, 49, 248], "futur": [22, 27, 37, 38, 47, 49, 238, 249], "releas": [22, 27, 37, 38, 47, 49, 241, 246], "classmethod": [22, 27, 29, 243], "openai": [23, 37, 193, 243], "markup": 23, "languag": [23, 69, 171, 172, 176, 192, 226, 247, 248], "templat": [23, 25, 27, 30, 31, 32, 35, 47, 48, 49, 51, 52, 55, 58, 64, 86, 92, 98, 111, 128, 137, 144, 154], "im_start": 23, "context": [23, 143, 177, 223, 225, 243, 248], "im_end": 23, "goe": [23, 177], "chosen": [24, 51, 59, 63, 66, 192, 194, 195, 225, 243], "reject": [24, 51, 59, 63, 66, 192, 194, 195, 243], "column": [24, 26, 27, 28, 34, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 158, 164, 165, 170, 243, 249], "q1": [24, 51, 56, 63], "a1": [24, 51, 56], "a2": [24, 51], "keep": [24, 26, 28, 34, 59, 60, 62, 63, 66, 171, 244, 247, 248], "present": [24, 28, 34, 58, 59, 61, 62, 63, 64, 65, 185, 201, 226], "functool": [25, 32, 35, 197], "partial": [25, 32, 35, 197], "_prompt_templ": [25, 32, 35], "user_messag": [25, 32, 35, 242], "assistant_messag": [25, 32, 35], "equival": [26, 37, 38, 194, 195], "respect": [26, 48, 92, 157, 179, 225, 242, 243], "placehold": [27, 49, 198, 243], "ident": [27, 28, 39, 41, 49, 50, 61, 63, 128, 164, 244, 249], "alwai": [27, 216, 226, 242, 248], "dataclass": 28, "unmask": [28, 34, 37, 38], "liter": [29, 30, 33, 77, 78, 79, 80, 81, 82, 86, 87, 88, 89, 90, 91, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 137, 138, 139, 140, 141, 144, 145, 146, 147, 148, 149, 154, 180, 181], "union": [29, 40, 41, 54, 55, 56, 58, 60, 64, 65, 67, 68, 86, 98, 111, 137, 144, 154, 164, 170, 181, 199, 204, 216, 217, 218, 219, 220, 222, 224], "repres": [29, 42, 166, 198, 204, 242, 248, 249], "individu": [29, 50, 170, 208, 220, 222, 242, 243], "interleav": [29, 189], "attach": 29, "appropri": [29, 48, 72, 92, 168, 171, 199, 243, 250], "writer": 29, "multimod": [29, 52, 61, 62, 170, 235], "dictionari": [29, 30, 36, 41, 42, 43, 50, 51, 52, 86, 98, 111, 137, 144, 154, 208, 214, 216, 217, 218, 219, 220, 228, 244], "pil": [29, 36, 40], "calcul": [29, 30, 72, 158, 163, 165, 166, 190, 191, 193, 246], "consecut": [29, 46, 157, 189], "last": [29, 45, 50, 67, 164, 168, 191, 243], "properti": [29, 163, 165, 172, 247, 248], "contains_media": 29, "media": [29, 52], "from_dict": [29, 242], "construct": [29, 59, 189, 239, 247], "get_media": 29, "consid": [30, 48, 51, 52, 166, 248], "come": [30, 46, 175, 247, 248], "nanswer": 32, "alia": [33, 197], "sharegpt": [34, 38, 56, 242], "nsummari": [35, 242], "summari": [35, 48, 64, 166, 208, 243], "image_tag": 36, "transformed_sampl": [37, 38, 243], "sequenc": [39, 41, 42, 43, 44, 49, 50, 53, 57, 61, 62, 67, 68, 71, 72, 83, 86, 87, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 144, 146, 150, 154, 157, 158, 160, 163, 164, 165, 166, 170, 172, 185, 187, 189, 191, 195, 196, 210, 242], "batch_first": 39, "padding_valu": 39, "float": [39, 69, 70, 73, 77, 78, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 93, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 117, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 135, 138, 139, 140, 141, 142, 145, 146, 147, 148, 149, 150, 158, 159, 168, 176, 190, 191, 192, 193, 194, 195, 208, 214, 216, 217, 218, 219, 220, 247, 248, 249, 250], "rnn": [39, 41, 44], "pad_sequ": [39, 41, 44], "variabl": [39, 49, 202, 213, 216, 243, 248, 250], "length": [39, 41, 43, 44, 45, 46, 48, 49, 50, 57, 68, 71, 83, 86, 87, 93, 98, 99, 108, 111, 112, 117, 121, 129, 131, 133, 135, 137, 140, 142, 143, 144, 146, 150, 154, 157, 158, 160, 163, 164, 165, 169, 170, 172, 185, 187, 189, 190, 191, 200, 210, 216, 248], "left": [39, 41, 44, 164, 247], "longest": [39, 43, 44], "trail": 39, "dimens": [39, 44, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 155, 157, 158, 160, 164, 166, 171, 176, 246, 247, 248, 250], "element": [39, 41, 44, 48, 210, 244], "c": [39, 41, 44, 61, 242], "10": [39, 41, 42, 43, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 166, 171, 244, 246, 248, 249, 250], "12": [39, 41, 42, 65, 166, 235, 249], "image_loc": 40, "local": [40, 47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 111, 144, 154, 216, 220, 224, 235, 241, 242, 244, 245], "remot": [40, 51, 52], "url": [40, 235], "g": [40, 47, 49, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 158, 166, 170, 175, 189, 198, 199, 208, 225, 229, 238, 246, 247, 248, 249, 250], "www": [40, 216], "wikipedia": [40, 68], "org": [40, 61, 74, 75, 76, 77, 78, 80, 81, 82, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 101, 102, 104, 105, 106, 107, 112, 113, 114, 115, 116, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 158, 159, 160, 166, 189, 190, 192, 193, 194, 195, 197, 213, 219, 222, 224, 230, 235], "en": [40, 47, 49, 53, 57, 59, 67, 68, 249], "bird": 40, "jpg": 40, "pad_direct": [41, 44], "keys_to_pad": 41, "padding_idx": [41, 42, 43, 44, 50], "collat": [41, 43, 44, 50, 243], "left_pad_sequ": [41, 44], "subset": [41, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 87, 99, 112, 121, 129, 131, 140, 146, 173, 178], "integ": [41, 43, 171, 197, 198, 204, 224], "batch_siz": [41, 55, 58, 61, 62, 64, 157, 158, 163, 164, 165, 169, 170, 171, 172, 192, 194, 196, 244, 248, 249], "ignore_idx": [42, 43, 44], "input_id": [42, 210], "chosen_input_id": [42, 63, 243], "chosen_label": [42, 243], "rejected_input_id": [42, 63, 243], "rejected_label": [42, 243], "14": [42, 166, 249, 250], "15": [42, 166, 207, 242, 244, 247, 250], "16": [42, 77, 78, 79, 80, 81, 82, 88, 89, 90, 91, 100, 101, 102, 103, 104, 105, 106, 107, 113, 114, 115, 116, 122, 123, 124, 125, 126, 127, 130, 132, 138, 139, 141, 145, 147, 148, 149, 157, 166, 247, 250], "17": [42, 166, 247], "18": [42, 166, 246], "19": [42, 166, 250], "20": [42, 166, 196, 249], "token_pair": 43, "padded_col": [43, 243], "pad_max_imag": 44, "tile": [44, 166, 189], "aspect": [44, 236], "ratio": [44, 192, 193], "cross": [44, 50, 163, 169, 170, 172, 189], "attent": [44, 50, 70, 71, 72, 77, 78, 79, 83, 87, 88, 89, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 135, 140, 141, 142, 143, 146, 147, 148, 149, 150, 157, 158, 160, 163, 164, 165, 170, 172, 180, 181, 189, 246, 247, 248, 250], "text_seq_len": [44, 189], "encoder_input": [44, 163, 164, 170], "n_tile": [44, 166], "h": [44, 157, 166, 169, 235, 241], "w": [44, 74, 75, 76, 84, 85, 94, 95, 96, 97, 109, 110, 118, 119, 120, 134, 136, 151, 152, 153, 166, 216, 219, 220, 242, 244, 247, 250], "aspect_ratio": [44, 166], "h_ratio": 44, "w_ratio": 44, "encoder_mask": [44, 163, 164, 170], "image_seq_len": [44, 189], "channel": [44, 166, 249], "height": 44, "largest": 44, "max": [44, 50, 154, 164, 166, 168, 170, 185, 187, 198, 241, 247], "bsz": [44, 69, 70, 71, 72, 166, 169], "max_num_imag": 44, "max_num_til": [44, 166, 189], "tokens_per_til": 44, "image_id": 44, "four": [44, 247], "model_input": 44, "max_text_seq_len": 44, "40": [44, 166, 189, 248, 250], "got": 44, "did": [44, 246, 250], "extra": [44, 47, 170, 235, 242, 247, 248, 249, 250], "second": [44, 158, 171, 244, 247, 248, 250], "eos_id": [45, 185, 187], "replac": [45, 49, 55, 56, 58, 60, 63, 64, 65, 167, 171, 226, 247], "shorter": [46, 164], "min": [46, 247], "invalid": 46, "sftdataset": [47, 49, 51, 54, 55, 56, 58, 60, 61, 62, 64, 65], "multiturn": [47, 242], "convert_to_messag": 47, "prepar": [47, 242, 249], "truncat": [47, 49, 50, 57, 67, 68, 86, 98, 111, 137, 144, 154, 185, 187, 196, 243], "filepath": [47, 49, 51, 52, 53, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68], "huggingfac": [47, 49, 53, 57, 59, 67, 68, 136, 143, 144, 151, 152, 153, 168, 192, 194, 195, 199, 200, 241, 244], "co": [47, 49, 53, 57, 59, 67, 68, 136, 143, 144, 151, 152, 153, 199, 200, 244], "doc": [47, 49, 52, 53, 57, 59, 67, 68, 197, 213, 216, 219, 220, 224, 230, 241, 243, 244], "package_refer": [47, 49, 53, 57, 59, 67, 68], "loading_method": [47, 49, 53, 57, 59, 67, 68], "chat_format": [47, 243], "chatformat": [47, 243], "still": [47, 169, 171, 172, 237, 247, 249, 250], "unless": 47, "load_dataset_kwarg": [47, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68], "sub": [48, 219], "unifi": [48, 136], "simplifi": [48, 192, 241, 247], "simultan": 48, "intern": 48, "aggreg": 48, "transpar": 48, "howev": [48, 144, 235, 248], "constitu": 48, "might": [48, 171, 174, 241, 244, 248], "larg": [48, 169, 176, 225, 241, 248, 250], "comput": [48, 51, 52, 93, 99, 108, 112, 117, 121, 146, 150, 158, 160, 164, 165, 169, 170, 189, 192, 194, 195, 208, 224, 238, 244, 248, 249, 250], "cumul": 48, "maintain": [48, 172, 237, 248, 250], "deleg": 48, "retriev": [48, 51, 52, 164, 207], "lead": [48, 184, 198, 238], "high": [48, 51, 52, 236, 247, 248], "scale": [48, 69, 70, 73, 77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 159, 161, 163, 165, 176, 191, 195, 247, 248, 249, 250], "strategi": [48, 238], "stream": [48, 230, 248], "demand": 48, "deriv": [48, 155, 164, 165], "dataset1": 48, "mycustomdataset": 48, "params1": 48, "dataset2": 48, "params2": 48, "concat_dataset": 48, "total": [48, 168, 191, 193, 211, 233, 240, 244, 246, 247, 248], "data_point": 48, "1500": 48, "accomplish": [48, 56, 60, 63, 67], "vicgal": [48, 243], "gpt4": [48, 243], "alpacainstructtempl": [48, 243], "samsum": [48, 64, 243], "focus": [48, 239, 245, 248], "enhanc": [48, 166, 195, 248, 250], "divers": 48, "machin": [48, 194, 229, 241, 244], "instructtempl": [49, 243], "contribut": [49, 55, 56, 58, 60, 63, 64, 65, 191, 193], "disabl": [49, 57, 68, 177, 224, 249], "recommend": [49, 56, 57, 58, 63, 64, 66, 68, 128, 164, 169, 216, 219, 242, 244, 248, 250], "highest": [49, 57, 68], "max_pack": 50, "split_across_pack": [50, 67], "greedi": 50, "pack": [50, 54, 55, 56, 58, 60, 61, 62, 64, 65, 67, 68, 158, 160, 164, 165, 170, 249], "done": [50, 164, 180, 206, 215, 226, 247, 249, 250], "outsid": [50, 224, 225, 247], "sampler": [50, 245], "part": [50, 171, 194, 242, 250], "style": [50, 54, 55, 56, 65, 172, 250], "buffer": [50, 248], "long": [50, 185, 242, 243, 247], "enough": [50, 242], "lower": [50, 238, 247], "triangular": 50, "attend": [50, 158, 163, 164, 165, 170, 189], "rel": [50, 158, 160, 164, 165, 170, 192, 208, 247], "wise": 50, "made": [50, 56, 60, 63, 67, 244], "smaller": [50, 171, 244, 246, 247, 248, 249, 250], "jam": 50, "s1": [50, 184], "s2": [50, 184], "s3": 50, "s4": 50, "contamin": 50, "input_po": [50, 70, 158, 160, 164, 165, 170], "matrix": [50, 163, 164, 170], "causal": [50, 71, 158, 164, 165, 170], "continu": [50, 166, 216, 243], "increment": 50, "move": [50, 67, 164, 228, 248], "entir": [50, 67, 169, 174, 215, 242, 250], "avoid": [50, 67, 159, 166, 167, 224, 241, 249, 250], "sentenc": [50, 67], "message_transform": [51, 52], "techniqu": [51, 236, 237, 238, 244, 245, 246, 247, 248, 249], "rlhf": [51, 59, 190, 191, 192, 193, 194, 195, 196, 243], "separ": [51, 172, 187, 199, 242, 245, 246, 247, 250], "repons": 51, "At": [51, 52, 164, 170], "extract": [51, 52, 57, 186], "against": [51, 52, 195, 231, 249, 250], "unit": [51, 52, 215, 236], "row": [51, 52, 158, 164, 165, 170], "final": [51, 52, 77, 78, 79, 83, 87, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 140, 141, 146, 149, 150, 155, 164, 170, 180, 181, 244, 246, 247, 248, 250], "model_transform": [51, 52, 58, 61, 62, 64, 65], "ref": [51, 52, 55, 61, 62, 143, 144, 220], "filter_fn": [52, 53, 67], "supervis": 52, "round": [52, 249], "incorpor": [52, 192, 243], "happen": [52, 169], "ti": [52, 87, 146, 150, 162, 248], "vision": [52, 171], "agnost": [52, 243], "treat": [52, 166, 177, 242], "modal": [52, 172], "minimum": [52, 61, 62], "filter": [52, 53, 67, 249], "prior": [52, 53, 55, 56, 58, 60, 61, 62, 64, 65, 67, 68, 226], "add_eo": [53, 67, 184, 185, 242], "unstructur": [53, 67, 68], "corpu": [53, 57, 67, 68], "tabular": [53, 67], "txt": [53, 67, 154, 217, 243, 245], "eo": [53, 67, 144, 184, 187, 242, 243], "yahma": 54, "packeddataset": [54, 55, 56, 58, 60, 64, 65, 67, 68, 243], "variant": [54, 58, 64], "version": [54, 69, 87, 99, 112, 121, 129, 131, 140, 146, 158, 231, 235, 246, 248, 249, 250], "page": [54, 68, 235, 236, 241, 245, 246, 248], "tatsu": 55, "lab": [55, 70], "codebas": [55, 244], "independ": 55, "alpacatomessag": 55, "alpaca_d": 55, "altern": [56, 60, 63, 245, 248], "friendli": [56, 60, 63, 67, 69, 242], "similar": [56, 57, 59, 61, 62, 63, 66, 67, 68, 180, 192, 243, 244, 246, 247, 248, 250], "toward": [56, 195], "my_dataset": [56, 60], "london": [56, 60], "ccdv": 57, "cnn_dailymail": 57, "textcompletiondataset": [57, 67, 68, 243], "cnn": 57, "dailymail": 57, "articl": [57, 68], "highlight": [57, 250], "anyth": [57, 228], "conjunct": [58, 64, 66, 164, 248], "inputoutputtomessag": [58, 64], "grammar_d": 58, "rlhflow": 59, "hh": 59, "preferencedataset": [59, 63, 66, 243], "anthrop": 59, "harmless": 59, "chosenrejectedtomessag": [59, 63], "liuhaotian": 61, "llava": 61, "150k": 61, "images_dir": 61, "coco": 61, "llava_instruct_150k": 61, "2017": 61, "visit": [61, 244], "cocodataset": 61, "wget": 61, "zip": [61, 232], "train2017": 61, "unzip": 61, "minim": [61, 62, 243, 245, 247, 249, 250], "clip": [61, 62, 166, 193], "clipimagetransform": [61, 62, 166], "mymodeltransform": [61, 62], "tokenizer_path": [61, 62], "image_transform": [61, 62], "align": [61, 62, 192, 242], "yet": [61, 62, 128, 242, 244], "llava_instruct_d": 61, "huggingfacem4": 62, "the_cauldron": 62, "cauldron": 62, "card": 62, "cauldron_d": 62, "ai2d": 62, "compris": 63, "share": [63, 158, 162, 243, 244], "c1": 63, "r1": 63, "chosen_messag": [63, 243], "rejected_messag": [63, 243], "my_preference_dataset": 63, "chosen_convers": 63, "hole": 63, "trouser": 63, "fix": [63, 243, 249], "rejected_convers": 63, "skip_special_token": [63, 185], "samsung": 64, "samsum_d": 64, "351": 65, "82": 65, "391": 65, "221": 65, "220": 65, "193": 65, "471": 65, "lvwerra": [66, 243], "stack": [66, 166, 225, 243], "exchang": [66, 243], "allenai": [67, 243, 249], "data_dir": [67, 243], "realnewslik": [67, 243], "wikitext_document_level": 68, "wikitext": [68, 249], "103": [68, 244], "transformerdecod": [69, 70, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 170, 171, 172, 247], "max_generated_token": 69, "pad_id": [69, 196], "temperatur": [69, 70, 73, 192, 194, 195, 244], "top_k": [69, 70, 73, 244], "stop_token": [69, 196], "rng": 69, "custom_generate_next_token": 69, "condit": [69, 213, 241, 243], "seq_length": [69, 70, 71, 163, 165, 171, 172], "prune": [69, 73, 250], "probabl": [69, 73, 77, 78, 79, 87, 88, 89, 99, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 176, 192, 193, 194, 195, 244], "stop": [69, 196], "random": [69, 166, 224, 245], "compil": [69, 169, 244, 246, 248, 250], "generate_next_token": 69, "been": [69, 71, 157, 164, 196, 207, 242, 249], "llama3_8b": [69, 114, 122, 170, 246, 248, 249], "hi": [69, 242], "manual_se": 69, "tolist": 69, "jeremi": 69, "m": [69, 167, 242, 249], "seq_len": [69, 71, 72, 164], "num_generated_token": 69, "q": [70, 73, 158, 247], "randomli": [70, 73, 226], "softmax": [70, 73, 158, 164, 165, 170], "trick": [70, 73], "fast": [70, 244], "32971d3129541c5bfb4f715abc33d1c5f408d204": 70, "l40": 70, "top": [70, 73, 205, 248, 250], "k": [70, 73, 158, 247], "padding_mask": [71, 72, 193, 196], "target_seq_len": 71, "suitabl": 71, "scaled_dot_product_attent": [71, 83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158], "static": 71, "kv": [71, 157, 158, 164, 165, 170, 249], "cach": [71, 157, 158, 160, 163, 164, 165, 170, 172, 235, 241], "longer": [71, 157, 243, 248], "boolean": [71, 72, 158, 163, 164, 165, 170, 172, 197, 210], "assertionerror": [71, 163, 164, 180, 181, 226], "shift": [72, 164], "uniform_": 73, "int32": 73, "code_llama2": [74, 75, 76, 77, 78, 79, 80, 81, 82, 241], "arxiv": [74, 75, 76, 77, 78, 80, 81, 82, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 101, 102, 104, 105, 106, 107, 112, 113, 114, 115, 116, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 158, 159, 160, 166, 189, 190, 192, 193, 194, 195], "pdf": [74, 75, 76, 189, 190], "2308": [74, 75, 76], "12950": [74, 75, 76], "lora_attn_modul": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 180, 181, 237, 247, 248, 250], "q_proj": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 158, 180, 181, 237, 247, 248, 249, 250], "k_proj": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 158, 180, 181, 237, 247, 248, 249, 250], "v_proj": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 158, 180, 181, 237, 247, 248, 249, 250], "output_proj": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 158, 180, 181, 247, 248, 249, 250], "apply_lora_to_mlp": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 180, 181, 237, 247, 248], "apply_lora_to_output": [77, 78, 79, 80, 81, 82, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 149, 180, 181, 247, 248], "lora_rank": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 237, 247, 248], "lora_alpha": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 237, 247, 248], "lora_dropout": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 248], "use_dora": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 123, 124, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149], "quantize_bas": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 176, 250], "code_llama2_13b": 77, "tloen": [77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149], "8bb8579e403dc78e37fe81ffbb253c413007323f": [77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149], "l41": [77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149], "l43": [77, 78, 79, 88, 89, 100, 101, 102, 103, 113, 114, 122, 123, 124, 141, 147, 148, 149], "linear": [77, 78, 79, 80, 81, 82, 87, 88, 89, 90, 91, 99, 100, 101, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 146, 147, 148, 149, 162, 164, 175, 176, 180, 181, 247, 248, 249, 250], "mlp": [77, 78, 79, 83, 87, 88, 89, 93, 99, 100, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 135, 140, 141, 142, 146, 147, 148, 149, 150, 163, 164, 165, 180, 181, 246, 247, 248], "low": [77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 176, 237, 244, 247, 250], "approxim": [77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 176, 247], "factor": [77, 78, 79, 87, 88, 89, 99, 100, 101, 102, 103, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 140, 141, 146, 147, 148, 149, 176, 190, 244], "dropout": [77, 78, 79, 83, 87, 88, 89, 93, 99, 101, 102, 103, 108, 112, 113, 114, 117, 121, 122, 123, 124, 129, 130, 131, 132, 133, 135, 140, 141, 142, 146, 147, 148, 149, 150, 158, 176, 247, 248, 250], "decompos": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141], "magnitud": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141, 248], "introduc": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141, 158, 159, 172, 176, 195, 238, 242, 243, 247, 248, 249, 250], "dora": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141], "ab": [77, 78, 80, 81, 82, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 101, 102, 104, 105, 106, 107, 112, 113, 114, 115, 116, 123, 124, 125, 126, 127, 129, 130, 131, 132, 138, 139, 140, 141, 145, 158, 159, 160, 166, 192, 193, 194, 195], "2402": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141], "09353": [77, 78, 87, 88, 89, 99, 100, 101, 102, 112, 113, 114, 123, 124, 129, 130, 131, 132, 140, 141], "code_llama2_70b": 78, "code_llama2_7b": 79, "qlora": [80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 167, 234, 236, 237, 246, 247], "paper": [80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 189, 192, 194, 195, 247, 250], "2305": [80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145, 158, 192, 194], "14314": [80, 81, 82, 90, 91, 104, 105, 106, 107, 115, 116, 125, 126, 127, 138, 139, 145], "lora_code_llama2_13b": 80, "lora_code_llama2_70b": 81, "lora_code_llama2_7b": 82, "head_dim": [83, 87, 157, 158, 164], "intermediate_dim": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150], "attn_dropout": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 164], "norm_ep": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150], "1e": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 159], "06": [83, 87, 159, 247], "rope_bas": [83, 87, 93, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150], "10000": [83, 87, 93, 129, 131, 133, 135, 140, 142, 160], "norm_embed": [83, 87], "transformerselfattentionlay": [83, 93, 108, 117, 133, 150, 163, 164, 170, 172], "rm": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150], "norm": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 164], "space": [83, 93, 108, 117, 133, 150, 164, 174, 248], "slide": [83, 133, 143], "window": [83, 133, 143, 243], "vocabulari": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 169, 247, 248], "queri": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 157, 158, 164, 165, 170, 246, 248], "mha": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 164], "intermedi": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 166, 201, 222, 246, 250], "onto": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158, 174], "epsilon": [83, 87, 93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 193], "rotari": [83, 87, 93, 129, 131, 133, 135, 140, 142, 160, 246], "10_000": [83, 87, 129, 131, 133, 135, 142], "blog": [84, 85], "technolog": [84, 85], "develop": [84, 85, 235, 250], "gemmatoken": 86, "_templatetyp": [86, 98, 111, 137, 144, 154], "gemma_2b": 88, "gemma_7b": 89, "lora_gemma_2b": 90, "lora_gemma_7b": 91, "taken": [92, 247, 250], "sy": [92, 242, 243], "honest": [92, 242, 243], "pari": [92, 128, 243], "capit": [92, 128, 243], "franc": [92, 128, 243], "known": [92, 128, 209, 243, 249], "stun": [92, 128, 243], "05": [93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150], "gqa": [93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158], "mqa": [93, 99, 108, 112, 117, 121, 129, 131, 133, 135, 140, 142, 146, 150, 158], "kvcach": [93, 99, 108, 112, 117, 121, 140, 146, 150, 158, 164], "scale_hidden_dim_for_mlp": [93, 99, 108, 112, 117, 121, 146, 150], "2307": [94, 95, 96, 97], "09288": [94, 95, 96, 97], "classif": [97, 131, 135, 136, 202], "reward": [97, 103, 107, 132, 136, 139, 190, 191, 192, 194, 195, 202], "llama2_70b": 101, "llama2_7b": [102, 247], "classifi": [103, 131, 135, 136, 226, 243, 248], "llama2_reward_7b": [103, 202], "lora_llama2_13b": 104, "lora_llama2_70b": 105, "lora_llama2_7b": [106, 247], "lora_llama2_reward_7b": 107, "500000": [108, 112, 117, 121], "llama3token": [111, 183], "regist": [111, 144, 154, 167, 221, 250], "similarli": [111, 144, 154, 243, 249], "canon": [111, 144, 154], "llama3_70b": 113, "lora_llama3_70b": 115, "lora_llama3_8b": 116, "scale_factor": 117, "rope": [117, 146, 150, 158, 160], "llama3_1": [118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 237], "llama3_1_70b": 123, "llama3_1_8b": 124, "lora_llama3_1_405b": 125, "lora_llama3_1_70b": 126, "lora_llama3_1_8b": 127, "num_class": [131, 135, 226], "announc": 134, "ray2333": 136, "feedback": [136, 192], "lora_mistral_7b": 138, "lora_mistral_reward_7b": 139, "phi3_mini": [141, 202], "128k": 143, "nor": 143, "phi3minitoken": 144, "tokenizer_config": 144, "spm": 144, "lm": [144, 193], "bo": [144, 184, 187, 242, 243], "unk": 144, "augment": [144, 250], "endoftext": 144, "phi3minisentencepiecebasetoken": 144, "lora_phi3_mini": 145, "1000000": [146, 150], "tie_word_embed": [146, 147, 148, 150, 151, 152], "qwen2transformerdecod": 146, "period": [146, 150], "word": [146, 150, 249], "qwen2_0_5b": [147, 162], "qwen2_1_5b": [148, 162], "qwen2_7b": 149, "qwen": [151, 152, 153], "merges_fil": 154, "qwen2token": 154, "gate_proj": 155, "down_proj": 155, "up_proj": 155, "silu": 155, "feed": [155, 163, 165], "network": [155, 177, 247, 250], "fed": [155, 242], "multipli": [155, 248], "in_dim": [155, 175, 176, 247, 248, 250], "out_dim": [155, 164, 175, 176, 247, 248, 250], "layernorm": 156, "past": 157, "expand": 157, "dpython": [157, 158, 163, 164, 165, 167, 170, 172, 223, 227], "reset": [157, 158, 163, 164, 165, 170, 172, 208], "k_val": 157, "v_val": 157, "fill": 157, "th": 157, "bfloat16": [157, 167, 223, 244, 245, 246, 247, 248, 249], "greater": [157, 166, 231], "pos_embed": [158, 163, 247, 249], "q_norm": 158, "k_norm": 158, "kv_cach": 158, "is_caus": 158, "13245v1": 158, "multihead": 158, "extrem": 158, "credit": 158, "litgpt": 158, "v": [158, 164, 170, 247], "n_kv_head": 158, "rotarypositionalembed": [158, 247, 249], "rmsnorm": 158, "vice": [158, 241], "versa": [158, 241], "y": 158, "s_x": 158, "s_y": 158, "_masktyp": [158, 164, 165], "score": [158, 164, 165, 191], "encoder_max_cache_seq_len": [158, 164, 165], "j": [158, 163, 164, 165, 170], "blockmask": [158, 164, 165], "create_block_mask": [158, 164, 165], "flex_attent": [158, 164, 165], "n_h": [158, 160], "num": [158, 160], "n_kv": 158, "emb": [158, 163, 164, 170], "h_d": [158, 160], "reset_cach": [158, 163, 164, 165, 170, 172], "setup_cach": [158, 163, 164, 165, 170, 172], "skip": [158, 185], "ep": 159, "root": [159, 219, 220], "squar": 159, "1910": 159, "07467": 159, "divis": 159, "propos": [160, 248], "2104": 160, "09864": 160, "verfic": 160, "l80": 160, "upto": 160, "init": [160, 208, 220, 250], "exceed": 160, "freq": 160, "recomput": [160, 248], "geometr": 160, "progress": [160, 239, 245, 248], "rotat": 160, "angl": 160, "basic": [161, 246], "learnabl": [161, 170, 172, 244], "gate": [161, 202, 237, 238, 241, 245], "tied_modul": 162, "bia": [162, 175, 176, 226, 247, 249, 250], "lost": 162, "whose": [162, 177, 216, 221], "attributeerror": [162, 228], "attribut": [162, 177, 187, 195, 205], "attn": [163, 165, 247, 249, 250], "multiheadattent": [163, 165, 247, 249], "ca_norm": 163, "mlp_norm": [163, 165], "ca_scal": 163, "mlp_scale": [163, 165], "convent": 163, "ff": [163, 165], "cache_en": [163, 165, 172], "check": [163, 164, 165, 166, 170, 172, 180, 206, 213, 231, 234, 236, 237, 238, 239, 242, 244, 245, 247, 248], "token_sequ": 163, "relat": [163, 164, 170, 247], "embed_sequ": 163, "encoder_max_seq_len": [163, 164, 165, 170, 172], "decoder_max_seq_len": [163, 164, 165, 170, 172], "modulelist": 164, "output_hidden_st": [164, 170], "belong": [164, 205], "reduc": [164, 192, 236, 237, 238, 243, 247, 248, 249, 250], "statement": 164, "improv": [164, 185, 194, 207, 238, 246, 247, 248], "readabl": [164, 244], "chunked_output": 164, "last_hidden_st": 164, "chunk": [164, 169, 185], "cewithchunkedoutputloss": [164, 170], "upcast": [164, 169], "set_num_output_chunk": [164, 170], "num_chunk": [164, 169], "decoder_caches_are_en": 164, "transformercrossattentionlay": [164, 170, 172], "encoder_caches_are_en": 164, "fusionlay": [164, 170], "s_e": [164, 170], "d_e": [164, 170], "arang": [164, 170], "prompt_length": [164, 170], "correspondingli": 164, "padded_prompt_length": 164, "m_": [164, 170], "num_output_chunk": [164, 169, 170], "combin": [164, 170, 172, 174, 191], "sa_norm": 165, "sa_scal": 165, "patch_siz": [166, 189], "tile_s": [166, 189], "token_pos_embed": 166, "pre_tile_pos_emb": 166, "post_tile_pos_emb": 166, "cls_project": 166, "out_indic": 166, "in_channel": 166, "vit": 166, "11929": 166, "crop": 166, "cl": [166, 243], "divid": [166, 189], "patch": [166, 189], "convolut": 166, "flatten": 166, "downscal": 166, "800x400": 166, "400x400": 166, "400": [166, 189], "_transform": 166, "broken": [166, 172], "down": [166, 201, 243, 247, 248, 250], "grid": [166, 189], "10x10": [166, 189], "whole": 166, "num_til": 166, "n_token": 166, "101": 166, "pool": 166, "_position_embed": 166, "tiledtokenpositionalembed": 166, "tilepositionalembed": 166, "tile_pos_emb": 166, "tokenpositionalembed": 166, "even": [166, 226, 235, 241, 242, 243, 246, 247, 248, 250], "8x8": 166, "21": 166, "22": 166, "23": [166, 168], "24": [166, 245, 246], "25": [166, 244], "26": 166, "27": [166, 244], "28": [166, 244], "29": [166, 250], "30": [166, 196, 249], "31": [166, 246], "33": 166, "34": 166, "35": [166, 250], "36": 166, "37": 166, "38": [166, 244], "39": 166, "41": 166, "43": 166, "44": 166, "45": 166, "46": 166, "47": 166, "48": [166, 244, 250], "49": 166, "50": [166, 196, 216, 244], "51": 166, "52": [166, 245], "53": 166, "54": 166, "55": [166, 245], "56": 166, "57": [166, 247, 250], "58": 166, "59": [166, 250], "60": 166, "61": [166, 244], "62": 166, "63": 166, "64": [166, 237, 247], "num_patches_per_til": 166, "emb_dim": 166, "dimension": 166, "cls_output_dim": 166, "fourth": 166, "n_img": 166, "constain": 166, "anim": [166, 243], "max_n_img": 166, "n_channel": 166, "hidden_st": 166, "vision_util": 166, "tile_crop": 166, "num_channel": 166, "image_s": 166, "800": 166, "patch_grid_s": 166, "rand": 166, "nch": 166, "tile_cropped_imag": 166, "batch_imag": 166, "unsqueez": 166, "batch_aspect_ratio": 166, "clip_vision_encod": 166, "common_util": 167, "offload_to_cpu": 167, "hook": [167, 221, 248, 250], "nf4": [167, 248, 250], "restor": 167, "higher": [167, 246, 248, 249, 250], "offload": [167, 250], "increas": [167, 168, 192, 246, 247, 248, 249], "peak": [167, 208, 214, 244, 246, 247, 250], "gpu": [167, 238, 241, 244, 245, 246, 247, 248, 249, 250], "_register_state_dict_hook": 167, "mymodul": 167, "_after_": 167, "nf4tensor": [167, 250], "unquant": [167, 249, 250], "unus": 167, "num_warmup_step": 168, "num_training_step": 168, "num_cycl": [168, 225], "last_epoch": 168, "lambdalr": 168, "rate": [168, 236, 245, 248], "schedul": [168, 225, 245, 248], "linearli": 168, "decreas": [168, 243, 247, 248, 249, 250], "cosin": 168, "v4": 168, "src": 168, "l104": 168, "warmup": [168, 225], "phase": 168, "wave": 168, "half": [168, 248], "lr_schedul": 168, "ignore_index": 169, "entropi": 169, "bf16": [169, 206, 248, 250], "ce": 169, "better": [169, 195, 236, 242, 243, 244, 248, 249], "accuraci": [169, 238, 244, 246, 247, 248, 249, 250], "doubl": [169, 250], "therefor": [169, 250], "num_token": 169, "consider": 169, "compute_cross_entropi": 169, "gain": [169, 238, 246], "won": [169, 242], "realiz": 169, "pull": [169, 237, 238, 241], "1390": 169, "ground": [169, 248], "loss_fn": 169, "chunkedcrossentropyloss": 169, "output_chunk": 169, "model_fus": [170, 171, 172, 173, 174], "decoder_train": 170, "encoder_train": 170, "fusion_train": 170, "deepfus": 170, "pretrain": [170, 172, 174, 184, 185, 241, 242, 245, 247, 250], "evolut": 170, "signatur": 170, "interchang": 170, "fusion_param": [170, 171, 172, 173, 174], "fusionembed": 170, "fusion_lay": [170, 172], "clip_vit_224": [170, 174], "projection_head": [170, 174], "feedforward": [170, 174], "register_fusion_modul": 170, "sequenti": [170, 174], "flamingo": [170, 172, 189], "Or": [170, 235], "strict": [170, 171, 172, 180, 247], "freez": [170, 244, 247], "fusion": [170, 171, 172, 173, 174], "caches_are_en": 170, "fusion_vocab_s": 171, "necessit": 171, "rout": 171, "drop": [171, 249], "128": [171, 237, 246, 247, 248], "fusion_first": 172, "visual": 172, "shot": [172, 244, 246, 249], "infus": 172, "interpret": [172, 243], "enocd": 172, "isn": [172, 206, 241], "trainabl": [172, 176, 179, 215, 247, 248, 250], "fused_lay": 172, "insert": [172, 249], "mark": [174, 242], "earli": 174, "peft": [175, 176, 177, 178, 179, 180, 181, 199, 237, 247, 250], "adapter_param": [175, 176, 177, 178, 179], "proj": 175, "loralinear": [175, 247, 250], "alpha": [176, 247, 248, 250], "use_bia": 176, "perturb": 176, "decomposit": [176, 247, 248], "matric": [176, 247, 250], "mapsto": 176, "w_0x": 176, "r": [176, 247], "bax": 176, "lora_a": [176, 247, 250], "lora_b": [176, 247, 250], "temporarili": [177, 248], "polici": [177, 191, 192, 193, 194, 195, 197, 207, 215, 222, 239], "neural": [177, 247, 250], "caller": 177, "yield": 177, "get_adapter_param": [179, 247], "base_miss": 180, "base_unexpect": 180, "lora_miss": 180, "lora_unexpect": 180, "effici": [180, 207, 234, 236, 237, 244, 245, 247, 249], "validate_state_dict_for_lora": [180, 247], "unlik": 180, "reli": [180, 187, 244, 246], "unexpect": 180, "nonempti": 180, "full_model_state_dict_kei": 181, "lora_state_dict_kei": 181, "base_model_state_dict_kei": 181, "confirm": [181, 235], "determin": 181, "lora_modul": 181, "complement": 181, "disjoint": 181, "non": [181, 191], "overlap": [181, 248], "tiktokenbasetoken": 182, "light": 184, "sentencepieceprocessor": 184, "trim": 184, "whitespac": 184, "spm_model": [184, 242], "tokenized_text": [184, 185], "add_bo": [184, 185, 242], "31587": [184, 185], "29644": [184, 185], "102": [184, 185], "trim_leading_whitespac": 184, "prefix": [184, 248], "unbatch": 184, "due": [184, 247, 248, 250], "bos_id": [185, 187], "lightweight": [185, 242], "break": 185, "substr": 185, "repetit": 185, "speed": [185, 225, 246, 248, 249, 250], "identif": 185, "regex": 185, "absent": 185, "tt_model": 185, "truncate_at_eo": 185, "show": [185, 189, 235, 237, 238, 241, 242, 247], "tokenizer_json_path": 186, "heavili": 187, "concat": 187, "1788": 187, "2643": 187, "465": 187, "22137": 187, "join": 187, "beggin": 187, "runtimeerror": [187, 203, 206, 212], "satisfi": [187, 244], "loos": 188, "visioncrossattentionmask": 188, "image_token_id": 189, "particip": [189, 190], "laid": 189, "fig": 189, "2204": 189, "14198": 189, "immedi": [189, 248], "until": [189, 248], "img1": 189, "img2": 189, "img3": 189, "dog": 189, "equal": [189, 231], "gamma": [190, 194, 195], "lmbda": 190, "estim": [190, 191], "1506": 190, "02438": 190, "response_len": [190, 191], "receiv": 190, "discount": 190, "gae": 190, "lambda": 190, "logprob": [191, 195], "ref_logprob": 191, "kl_coeff": 191, "valid_score_idx": 191, "kl": 191, "coeffici": [191, 193], "total_reward": 191, "diverg": 191, "kl_reward": 191, "beta": [192, 195], "label_smooth": [192, 195], "18290": 192, "intuit": [192, 194, 195], "dispref": 192, "dynam": [192, 249], "degener": 192, "occur": [192, 238], "naiv": 192, "trl": [192, 194, 195], "librari": [192, 194, 206, 224, 230, 234, 235, 236, 241, 243, 248, 250], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 192, "dpo_train": [192, 194], "l844": 192, "retain": [192, 248, 250], "2009": 192, "01325": 192, "regular": [192, 195, 248, 249, 250], "baselin": [192, 193, 244, 247], "rather": [192, 248], "overhead": [192, 238, 248, 249], "uncertainti": [192, 195], "policy_chosen_logp": [192, 194, 195], "policy_rejected_logp": [192, 194, 195], "reference_chosen_logp": [192, 194], "reference_rejected_logp": [192, 194], "chosen_reward": [192, 194, 195], "rejected_reward": [192, 194, 195], "value_clip_rang": 193, "value_coeff": 193, "proxim": [193, 239], "1707": 193, "06347": 193, "eqn": 193, "vwxyzjn": 193, "ccc19538e817e98a60d3253242ac15e2a562cb49": 193, "lm_human_preference_detail": 193, "train_policy_acceler": 193, "l719": 193, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 193, "ppo2": 193, "l68": 193, "l75": 193, "pi_old_logprob": 193, "pi_logprob": 193, "phi_old_valu": 193, "phi_valu": 193, "value_padding_mask": 193, "old": 193, "participag": 193, "five": 193, "policy_loss": 193, "value_loss": 193, "clipfrac": 193, "fraction": 193, "statist": [194, 248], "rso": 194, "hing": 194, "2309": 194, "06657": 194, "logist": 194, "regress": 194, "slic": 194, "10425": 194, "almost": [194, 247], "svm": 194, "counter": 194, "4dce042a3863db1d375358e8c8092b874b02934b": 194, "l1141": 194, "simpo": 195, "free": [195, 239, 247], "2405": 195, "14734": 195, "averag": 195, "implicit": 195, "margin": 195, "bradlei": 195, "terri": 195, "larger": [195, 201, 244, 246, 248], "win": 195, "lose": 195, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 195, "cpo_train": 195, "l603": 195, "pretti": [195, 244], "identitc": 195, "elimin": 195, "kind": 195, "ipoloss": 195, "fill_valu": 196, "sequence_length": 196, "stop_token_id": 196, "869": 196, "eos_mask": 196, "truncated_sequ": 196, "datatyp": [197, 248, 250], "denot": 197, "auto_wrap_polici": [197, 207, 222], "submodul": [197, 215], "obei": 197, "contract": 197, "get_fsdp_polici": 197, "modules_to_wrap": [197, 207, 215], "min_num_param": 197, "my_fsdp_polici": 197, "recurs": [197, 215, 219], "isinst": [197, 243], "sum": [197, 247], "p": [197, 203, 247, 249, 250], "numel": [197, 247], "1000": [197, 249], "stabl": [197, 213, 219, 224, 235, 248], "html": [197, 213, 219, 222, 224, 230, 234], "filename_format": 198, "max_filenam": 198, "concis": 198, "filenam": [198, 217], "file_": 198, "_of_": 198, "n_file": 198, "build_checkpoint_filenam": 198, "file_00001_of_00003": 198, "file_00002_of_00003": 198, "file_00003_of_00003": 198, "safe_seri": 199, "from_pretrain": 199, "0001_of_0003": 199, "0002_of_0003": 199, "todo": 199, "preserv": [199, 250], "weight_map": [199, 244], "convert_weight": 199, "_model_typ": [199, 202], "intermediate_checkpoint": [199, 200, 201], "adapter_onli": [199, 200, 201], "_weight_map": 199, "shard": [200, 246], "wip": 200, "qualnam": 202, "boundari": 202, "distinguish": 202, "mistral_reward_7b": 202, "my_new_model": 202, "my_custom_state_dict_map": 202, "optim_map": 203, "bare": 203, "bone": 203, "optim_dict": [203, 205, 221], "cfg_optim": 203, "ckpt": 203, "optim_ckpt": 203, "placeholder_optim_dict": 203, "optiminbackwardwrapp": 203, "get_optim_kei": 203, "arbitrari": [203, 247, 248], "optim_ckpt_map": 203, "loadabl": 203, "ac_mod": 204, "ac_opt": 204, "mode": [204, 209, 216, 244], "op": [204, 249], "ac": [204, 207], "optimizerinbackwardwrapp": 205, "named_paramet": [205, 226], "float32": 206, "inde": [206, 244], "kernel": 206, "hardwar": [206, 236, 243, 244, 247, 248], "memory_efficient_fsdp_wrap": [207, 249], "maxim": [207, 215, 234, 236], "workload": [207, 238, 248, 249], "fullyshardeddataparallel": [207, 215], "fsdppolicytyp": [207, 215], "reset_stat": 208, "track": [208, 216], "alloc": [208, 214, 215, 246, 250], "reserv": [208, 214, 242, 250], "stat": [208, 214, 250], "int4": [209, 249], "4w": 209, "recogn": 209, "int8dynactint4weightquant": [209, 238, 249], "8da4w": [209, 249], "int8dynactint4weightqatquant": [209, 238, 249], "qat": [209, 234, 239], "exclud": 210, "aka": 211, "master": 213, "port": [213, 241], "address": [213, 248], "hold": [213, 245], "peak_memory_act": 214, "peak_memory_alloc": 214, "peak_memory_reserv": 214, "get_memory_stat": 214, "hierarch": 215, "api_kei": 216, "experiment_kei": 216, "onlin": 216, "log_cod": 216, "comet": 216, "site": [216, 243, 244], "ml": 216, "team": 216, "compar": [216, 219, 231, 244, 246, 247, 249, 250], "sdk": 216, "uncategor": 216, "alphanumer": 216, "charact": 216, "get_or_cr": 216, "fresh": 216, "persist": 216, "hpo": 216, "sweep": 216, "server": 216, "offlin": 216, "auto": [216, 241], "creation": 216, "experimentconfig": 216, "project_nam": 216, "my_project": [216, 220], "my_workspac": 216, "my_metr": [216, 219, 220], "importerror": [216, 220], "termin": [216, 219, 220], "comet_api_kei": 216, "flush": [216, 217, 218, 219, 220], "ndarrai": [216, 217, 218, 219, 220], "scalar": [216, 217, 218, 219, 220], "record": [216, 217, 218, 219, 220, 225], "log_config": [216, 220], "payload": [216, 217, 218, 219, 220], "log_": 217, "unixtimestamp": 217, "thread": 217, "safe": 217, "organize_log": 219, "tensorboard": 219, "subdirectori": 219, "logdir": 219, "startup": 219, "tree": [219, 243, 244, 246], "tfevent": 219, "encount": 219, "frontend": 219, "organ": [219, 241], "accordingli": [219, 249], "my_log_dir": 219, "view": 219, "entiti": 220, "bias": [220, 247, 250], "sent": 220, "usernam": 220, "my_ent": 220, "my_group": 220, "account": [220, 247, 250], "link": [220, 244, 246], "capecap": 220, "6053ofw0": 220, "torchtune_config_j67sb73v": 220, "soon": [221, 248], "readi": [221, 234, 242, 249], "grad": 221, "acwrappolicytyp": 222, "author": [222, 236, 245, 248, 250], "fsdp_adavnced_tutori": 222, "insid": 223, "contextmanag": 223, "debug_mod": 224, "pseudo": 224, "commonli": [224, 247, 248, 250], "numpi": 224, "determinist": 224, "global": [224, 243, 248], "warn": 224, "nondeterminist": 224, "cudnn": 224, "set_deterministic_debug_mod": 224, "profile_memori": 225, "with_stack": 225, "record_shap": 225, "with_flop": 225, "wait_step": 225, "warmup_step": 225, "active_step": 225, "profil": 225, "layout": 225, "trace": 225, "profileract": 225, "gradient_accumul": 225, "sensibl": 225, "default_schedul": 225, "reduct": [225, 238, 247], "iter": [225, 226, 227, 250], "scope": 225, "flop": 225, "wait": 225, "cycl": 225, "repeat": [225, 248], "model_named_paramet": 226, "force_overrid": 226, "behaviour": 226, "concret": [226, 248], "vocab_dim": 226, "place": [226, 242, 243, 248], "named_param": 227, "intend": [228, 242], "inplac": [228, 247], "too": [228, 238, 246], "handler": 230, "_log": 230, "__version__": 231, "generated_examples_python": 232, "galleri": [232, 240], "sphinx": 232, "000": [233, 240, 246], "execut": [233, 240], "generated_exampl": 233, "mem": [233, 240], "mb": [233, 240], "gentl": 234, "introduct": 234, "first_finetune_tutori": 234, "workflow": [234, 237, 243, 245, 247], "torchvis": 235, "torchao": [235, 238, 244, 246, 249, 250], "latest": [235, 238, 245, 248, 250], "whl": 235, "cu121": 235, "cu118": 235, "cu124": 235, "And": [235, 244], "welcom": [235, 241], "greatest": [235, 245], "contributor": 235, "dev": 235, "commit": 235, "branch": 235, "therebi": [235, 248, 249, 250], "forc": 235, "reinstal": 235, "opt": [235, 245], "suffix": 235, "On": [236, 247], "pointer": 236, "emphas": 236, "simplic": 236, "component": 236, "prove": 236, "democrat": 236, "box": [236, 238, 250], "zoo": 236, "varieti": [236, 247], "integr": [236, 244, 245, 246, 247, 249, 250], "fsdp2": 236, "excit": 236, "checkout": 236, "quickstart": 236, "attain": 236, "embodi": 236, "philosophi": 236, "usabl": 236, "composit": 236, "hard": [236, 243], "outlin": 236, "unecessari": 236, "never": 236, "thoroughli": 236, "competit": 237, "grant": [237, 238, 245], "interest": [237, 238, 244], "8b_lora_single_devic": [237, 241, 242, 246, 248], "adjust": [237, 238, 243, 248, 249], "lever": [237, 238], "action": [237, 238], "degrad": [238, 248, 249, 250], "simul": [238, 248, 249], "compromis": 238, "blogpost": [238, 248], "qat_distribut": [238, 249], "8b_qat_ful": [238, 249], "least": [238, 246, 247, 249], "vram": [238, 246, 247, 248, 249], "80gb": [238, 249], "a100": 238, "h100": 238, "delai": 238, "fake": [238, 249], "empir": [238, 249], "potenti": [238, 247, 248], "fake_quant_after_n_step": [238, 249], "idea": [238, 250], "roughli": 238, "total_step": 238, "plan": [238, 244], "un": 238, "groupsiz": [238, 249], "256": [238, 246, 249], "hackabl": [239, 245], "singularli": [239, 245], "technic": [239, 245], "awar": [239, 248, 249], "feel": [239, 247], "tracker": 239, "issu": [239, 249], "short": 241, "subcommand": 241, "anytim": 241, "symlink": 241, "wrote": 241, "readm": [241, 244, 246], "md": 241, "lot": [241, 244, 248], "recent": 241, "agre": 241, "term": [241, 248], "perman": 241, "eat": 241, "bandwith": 241, "storag": [241, 250], "00030": 241, "ootb": 241, "full_finetune_single_devic": [241, 243, 244, 245], "7b_full_low_memori": [241, 244, 245], "8b_full_single_devic": [241, 243], "mini_full_low_memori": 241, "7b_full": [241, 244, 245], "13b_full": [241, 244, 245], "70b_full": 241, "edit": 241, "clobber": 241, "destin": 241, "lora_finetune_distribut": [241, 246, 247], "torchrun": 241, "launch": [241, 242, 245], "nproc": 241, "node": 241, "worker": 241, "nnode": [241, 247, 249], "minimum_nod": 241, "maximum_nod": 241, "fail": 241, "rdzv": 241, "rendezv": 241, "endpoint": 241, "8b_lora": [241, 246], "bypass": 241, "fancy_lora": 241, "8b_fancy_lora": 241, "nice": 242, "meet": 242, "overhaul": 242, "start_header_id": 242, "end_header_id": 242, "untrain": 242, "accompani": 242, "who": 242, "influenti": 242, "hip": 242, "hop": 242, "artist": 242, "2pac": 242, "rakim": 242, "na": 242, "flavor": [242, 243], "formatted_messag": [242, 243], "nyou": [242, 243], "nwho": 242, "why": [242, 245, 247], "518": 242, "25580": 242, "29962": 242, "3532": 242, "14816": 242, "29903": 242, "6778": 242, "_spm_model": 242, "piece_to_id": 242, "manual": [242, 250], "529": 242, "29879": 242, "29958": 242, "nhere": 242, "128000": [242, 249], "128009": 242, "pure": 242, "mess": 242, "govern": 242, "prime": 242, "strictli": 242, "ask": [242, 248], "untouch": 242, "though": 242, "robust": 242, "pretend": 242, "zuckerberg": 242, "seem": [242, 244], "good": [242, 247, 248], "altogeth": 242, "honor": 242, "copi": [242, 244, 245, 246, 249, 250], "custom_8b_lora_single_devic": 242, "steer": 243, "wheel": 243, "publicli": 243, "great": [243, 244, 248], "hood": [243, 250], "text_completion_dataset": [243, 249], "upper": 243, "constraint": [243, 247], "slow": [243, 248, 250], "signific": [243, 248, 249], "speedup": [243, 244, 246], "goal": [243, 249], "plant": 243, "miner": 243, "oak": 243, "copper": 243, "ore": 243, "eleph": 243, "customtempl": 243, "importlib": 243, "import_modul": 243, "mechan": 243, "search": 243, "often": [243, 247, 248], "runtim": [243, 248], "pythonpath": 243, "quit": [243, 248, 250], "llama2chatformat": 243, "customchatformat": 243, "concatdataset": 243, "drive": 243, "rajpurkar": 243, "io": 243, "squad": 243, "explor": 243, "chatdataset": 243, "key_chosen": 243, "key_reject": 243, "c_mask": 243, "np": 243, "cross_entropy_ignore_idx": 243, "r_mask": 243, "stack_exchanged_paired_dataset": 243, "had": 243, "1024": [243, 249], "stackexchangedpairedtempl": 243, "response_j": 243, "response_k": 243, "rl": 243, "favorit": [244, 247], "seemlessli": 244, "beyond": [244, 248, 250], "connect": [244, 249], "amount": 244, "natur": 244, "export": 244, "mobil": 244, "phone": 244, "leverag": [244, 246, 250], "percentag": 244, "16gb": [244, 247], "rtx": 244, "3090": 244, "4090": 244, "hour": 244, "7b_qlora_single_devic": [244, 245, 250], "473": 244, "98": [244, 250], "gb": [244, 246, 247, 249, 250], "484": 244, "01": [244, 245], "fact": [244, 246, 247, 248], "third": 244, "But": [244, 247], "realli": 244, "eleuther_ev": [244, 246, 249], "eleuther_evalu": [244, 246, 249], "lm_eval": [244, 246], "custom_eval_config": [244, 246], "truthfulqa_mc2": [244, 246, 247], "measur": [244, 246], "propens": [244, 246], "324": 244, "loglikelihood": 244, "195": 244, "121": 244, "197": 244, "acc": [244, 249], "388": 244, "shown": [244, 249], "489": 244, "custom_generation_config": [244, 246], "kick": 244, "300": 244, "bai": 244, "area": 244, "92": 244, "exploratorium": 244, "san": 244, "francisco": 244, "magazin": 244, "awesom": 244, "bridg": 244, "cool": 244, "96": [244, 250], "sec": [244, 246], "83": 244, "99": [244, 247], "72": 244, "littl": 244, "int8_weight_onli": [244, 246], "int8_dynamic_activation_int8_weight": [244, 246], "ao": [244, 246], "quant_api": [244, 246], "quantize_": [244, 246], "int4_weight_onli": [244, 246], "previous": [244, 246, 247], "benefit": 244, "clone": [244, 247, 249, 250], "assumpt": 244, "new_dir": 244, "output_dict": 244, "sd_1": 244, "sd_2": 244, "dump": 244, "convert_hf_checkpoint": 244, "checkpoint_path": 244, "justin": 244, "school": 244, "math": 244, "teacher": 244, "ws": 244, "94": [244, 246], "bandwidth": [244, 246], "1391": 244, "84": 244, "thats": 244, "seamlessli": 244, "authent": [244, 245], "hopefulli": 244, "gave": 244, "minut": 245, "agreement": 245, "depth": 245, "principl": 245, "boilerpl": 245, "substanti": [245, 247], "custom_config": 245, "replic": 245, "lorafinetunerecipesingledevic": 245, "lora_finetune_output": 245, "log_1713194212": 245, "3697006702423096": 245, "25880": [245, 250], "83it": 245, "monitor": 245, "tqdm": 245, "interv": 245, "e2": 245, "focu": 246, "theta": 246, "observ": [246, 249], "consum": [246, 250], "overal": 246, "8b_qlora_single_devic": [246, 248], "coupl": [246, 247, 250], "meta_model_0": [246, 249], "122": 246, "sarah": 246, "busi": 246, "mum": 246, "young": 246, "children": 246, "live": 246, "north": 246, "east": 246, "england": 246, "135": 246, "88": 246, "138": 246, "346": 246, "09": 246, "139": 246, "broader": 246, "teach": 247, "straight": 247, "jump": 247, "unfamiliar": 247, "oppos": [247, 250], "momentum": [247, 248], "aghajanyan": 247, "et": 247, "al": 247, "hypothes": 247, "intrins": 247, "eight": 247, "practic": 247, "blue": 247, "although": [247, 249], "rememb": 247, "approx": 247, "15m": 247, "65k": 247, "requires_grad": [247, 250], "frozen_out": [247, 250], "lora_out": [247, 250], "omit": [247, 248], "base_model": 247, "lora_model": 247, "lora_llama_2_7b": [247, 250], "alon": 247, "bit": [247, 248, 249, 250], "in_featur": [247, 249], "out_featur": [247, 249], "validate_missing_and_unexpected_for_lora": 247, "peft_util": 247, "set_trainable_param": 247, "fetch": 247, "lora_param": 247, "total_param": 247, "trainable_param": 247, "2f": 247, "6742609920": 247, "4194304": 247, "7b_lora": 247, "my_model_checkpoint_path": [247, 249, 250], "tokenizer_checkpoint": [247, 249, 250], "my_tokenizer_checkpoint_path": [247, 249, 250], "factori": 247, "benefici": 247, "impact": [247, 248], "minor": 247, "lora_experiment_1": 247, "smooth": [247, 250], "curv": [247, 250], "500": 247, "ran": 247, "footprint": [247, 249], "commod": 247, "cogniz": 247, "ax": 247, "parallel": 247, "truthfulqa": 247, "475": 247, "87": 247, "508": 247, "86": 247, "504": 247, "04": 247, "514": 247, "lowest": 247, "absolut": 247, "4gb": 247, "tradeoff": 247, "salman": 248, "mohammadi": 248, "brief": 248, "glossari": 248, "thing": [248, 250], "leav": 248, "struggl": 248, "constrain": [248, 249], "particularli": 248, "gradient_accumulation_step": 248, "throughput": 248, "cost": 248, "sebastian": 248, "raschka": 248, "fp16": 248, "sound": 248, "quot": 248, "aliv": 248, "region": 248, "enable_activation_checkpoint": 248, "bring": 248, "autograd": [248, 250], "saved_tensors_hook": 248, "offload_with_stream": 248, "hide": 248, "later": 248, "brought": 248, "enable_activation_offload": 248, "dev20240907": 248, "total_batch_s": 248, "count": 248, "suppos": 248, "log_every_n_step": 248, "translat": 248, "appear": 248, "frequent": 248, "slowli": 248, "num_devic": 248, "adamw8bit": 248, "pagedadamw": 248, "modern": 248, "converg": 248, "stateless": 248, "stochast": 248, "descent": 248, "sacrif": 248, "optimizer_in_bwd": 248, "greatli": 248, "lora_": 248, "lora_llama3": 248, "aim": 248, "_lora": 248, "firstli": 248, "secondli": 248, "affect": 248, "fashion": 248, "slower": [248, 250], "jointli": 248, "sens": 248, "novel": 248, "normalfloat": [248, 250], "8x": [248, 250], "worth": 248, "cast": [248, 249], "incur": [248, 249, 250], "penalti": 248, "qlora_": 248, "qlora_llama3_8b": 248, "_qlora": 248, "perplex": 249, "ptq": 249, "kept": 249, "width": 249, "nois": 249, "henc": 249, "x_q": 249, "int8": 249, "zp": 249, "x_float": 249, "qmin": 249, "qmax": 249, "clamp": 249, "x_fq": 249, "dequant": 249, "proce": 249, "prepared_model": 249, "swap": 249, "int8dynactint4weightqatlinear": 249, "int8dynactint4weightlinear": 249, "train_loop": 249, "converted_model": 249, "qat_distributed_recipe_label": 249, "recov": 249, "modif": 249, "custom_8b_qat_ful": 249, "2000": 249, "led": 249, "presum": 249, "mutat": 249, "5gb": 249, "custom_quant": 249, "poorli": 249, "custom_eleuther_evalu": 249, "fullmodeltorchtunecheckpoint": 249, "hellaswag": 249, "max_seq_length": 249, "my_eleuther_evalu": 249, "stderr": 249, "word_perplex": 249, "9148": 249, "byte_perplex": 249, "5357": 249, "bits_per_byt": 249, "6189": 249, "5687": 249, "0049": 249, "acc_norm": 249, "7536": 249, "0043": 249, "portion": [249, 250], "74": 249, "048": 249, "190": 249, "7735": 249, "5598": 249, "6413": 249, "5481": 249, "0050": 249, "7390": 249, "0044": 249, "7251": 249, "4994": 249, "5844": 249, "5740": 249, "7610": 249, "outperform": 249, "importantli": 249, "characterist": 249, "187": 249, "958": 249, "halv": 249, "int4weightonlyquant": 249, "motiv": 249, "edg": 249, "smartphon": 249, "executorch": 249, "xnnpack": 249, "export_llama": 249, "use_sdpa_with_kv_cach": 249, "qmode": 249, "group_siz": 249, "get_bos_id": 249, "get_eos_id": 249, "128001": 249, "output_nam": 249, "llama3_8da4w": 249, "pte": 249, "881": 249, "oneplu": 249, "709": 249, "tok": 249, "815": 249, "316": 249, "364": 249, "highli": 250, "vanilla": 250, "held": 250, "bespok": 250, "vast": 250, "major": 250, "normatfloat": 250, "themselv": 250, "deepdiv": 250, "distinct": 250, "de": 250, "counterpart": 250, "set_default_devic": 250, "qlora_linear": 250, "memory_alloc": 250, "177": 250, "152": 250, "del": 250, "empty_cach": 250, "lora_linear": 250, "081": 250, "344": 250, "qlora_llama2_7b": 250, "qlora_model": 250, "essenti": 250, "reparametrize_as_dtype_state_dict_post_hook": 250, "149": 250, "9157477021217346": 250, "02": 250, "08": 250, "15it": 250, "nightli": 250, "200": 250, "hundr": 250, "228": 250, "8158286809921265": 250, "95it": 250, "exercis": 250, "linear_nf4": 250, "to_nf4": 250, "linear_weight": 250, "incom": 250}, "objects": {"torchtune.config": [[18, 0, 1, "", "instantiate"], [19, 0, 1, "", "log_config"], [20, 0, 1, "", "parse"], [21, 0, 1, "", "validate"]], "torchtune.data": [[22, 1, 1, "", "ChatFormat"], [23, 1, 1, "", "ChatMLTemplate"], [24, 1, 1, "", "ChosenRejectedToMessages"], [25, 3, 1, "", "GrammarErrorCorrectionTemplate"], [26, 1, 1, "", "InputOutputToMessages"], [27, 1, 1, "", "InstructTemplate"], [28, 1, 1, "", "JSONToMessages"], [29, 1, 1, "", "Message"], [30, 1, 1, "", "PromptTemplate"], [31, 1, 1, "", "PromptTemplateInterface"], [32, 3, 1, "", "QuestionAnswerTemplate"], [33, 3, 1, "", "Role"], [34, 1, 1, "", "ShareGPTToMessages"], [35, 3, 1, "", "SummarizeTemplate"], [36, 0, 1, "", "format_content_with_images"], [37, 0, 1, "", "get_openai_messages"], [38, 0, 1, "", "get_sharegpt_messages"], [39, 0, 1, "", "left_pad_sequence"], [40, 0, 1, "", "load_image"], [41, 0, 1, "", "padded_collate"], [42, 0, 1, "", "padded_collate_dpo"], [43, 0, 1, "", "padded_collate_sft"], [44, 0, 1, "", "padded_collate_tiled_images_and_mask"], [45, 0, 1, "", "truncate"], [46, 0, 1, "", "validate_messages"]], "torchtune.data.ChatFormat": [[22, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[27, 2, 1, "", "format"]], "torchtune.data.Message": [[29, 4, 1, "", "contains_media"], [29, 2, 1, "", "from_dict"], [29, 2, 1, "", "get_media"], [29, 4, 1, "", "text_content"]], "torchtune.datasets": [[47, 3, 1, "", "ChatDataset"], [48, 1, 1, "", "ConcatDataset"], [49, 3, 1, "", "InstructDataset"], [50, 1, 1, "", "PackedDataset"], [51, 1, 1, "", "PreferenceDataset"], [52, 1, 1, "", "SFTDataset"], [53, 1, 1, "", "TextCompletionDataset"], [54, 0, 1, "", "alpaca_cleaned_dataset"], [55, 0, 1, "", "alpaca_dataset"], [56, 0, 1, "", "chat_dataset"], [57, 0, 1, "", "cnn_dailymail_articles_dataset"], [58, 0, 1, "", "grammar_dataset"], [59, 0, 1, "", "hh_rlhf_helpful_dataset"], [60, 0, 1, "", "instruct_dataset"], [63, 0, 1, "", "preference_dataset"], [64, 0, 1, "", "samsum_dataset"], [65, 0, 1, "", "slimorca_dataset"], [66, 0, 1, "", "stack_exchange_paired_dataset"], [67, 0, 1, "", "text_completion_dataset"], [68, 0, 1, "", "wikitext_dataset"]], "torchtune.datasets.multimodal": [[61, 0, 1, "", "llava_instruct_dataset"], [62, 0, 1, "", "the_cauldron_dataset"]], "torchtune.generation": [[69, 0, 1, "", "generate"], [70, 0, 1, "", "generate_next_token"], [71, 0, 1, "", "get_causal_mask_from_padding_mask"], [72, 0, 1, "", "get_position_ids_from_padding_mask"], [73, 0, 1, "", "sample"]], "torchtune.models.code_llama2": [[74, 0, 1, "", "code_llama2_13b"], [75, 0, 1, "", "code_llama2_70b"], [76, 0, 1, "", "code_llama2_7b"], [77, 0, 1, "", "lora_code_llama2_13b"], [78, 0, 1, "", "lora_code_llama2_70b"], [79, 0, 1, "", "lora_code_llama2_7b"], [80, 0, 1, "", "qlora_code_llama2_13b"], [81, 0, 1, "", "qlora_code_llama2_70b"], [82, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[83, 0, 1, "", "gemma"], [84, 0, 1, "", "gemma_2b"], [85, 0, 1, "", "gemma_7b"], [86, 0, 1, "", "gemma_tokenizer"], [87, 0, 1, "", "lora_gemma"], [88, 0, 1, "", "lora_gemma_2b"], [89, 0, 1, "", "lora_gemma_7b"], [90, 0, 1, "", "qlora_gemma_2b"], [91, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[92, 1, 1, "", "Llama2ChatTemplate"], [93, 0, 1, "", "llama2"], [94, 0, 1, "", "llama2_13b"], [95, 0, 1, "", "llama2_70b"], [96, 0, 1, "", "llama2_7b"], [97, 0, 1, "", "llama2_reward_7b"], [98, 0, 1, "", "llama2_tokenizer"], [99, 0, 1, "", "lora_llama2"], [100, 0, 1, "", "lora_llama2_13b"], [101, 0, 1, "", "lora_llama2_70b"], [102, 0, 1, "", "lora_llama2_7b"], [103, 0, 1, "", "lora_llama2_reward_7b"], [104, 0, 1, "", "qlora_llama2_13b"], [105, 0, 1, "", "qlora_llama2_70b"], [106, 0, 1, "", "qlora_llama2_7b"], [107, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama3": [[108, 0, 1, "", "llama3"], [109, 0, 1, "", "llama3_70b"], [110, 0, 1, "", "llama3_8b"], [111, 0, 1, "", "llama3_tokenizer"], [112, 0, 1, "", "lora_llama3"], [113, 0, 1, "", "lora_llama3_70b"], [114, 0, 1, "", "lora_llama3_8b"], [115, 0, 1, "", "qlora_llama3_70b"], [116, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3_1": [[117, 0, 1, "", "llama3_1"], [118, 0, 1, "", "llama3_1_405b"], [119, 0, 1, "", "llama3_1_70b"], [120, 0, 1, "", "llama3_1_8b"], [121, 0, 1, "", "lora_llama3_1"], [122, 0, 1, "", "lora_llama3_1_405b"], [123, 0, 1, "", "lora_llama3_1_70b"], [124, 0, 1, "", "lora_llama3_1_8b"], [125, 0, 1, "", "qlora_llama3_1_405b"], [126, 0, 1, "", "qlora_llama3_1_70b"], [127, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[128, 1, 1, "", "MistralChatTemplate"], [129, 0, 1, "", "lora_mistral"], [130, 0, 1, "", "lora_mistral_7b"], [131, 0, 1, "", "lora_mistral_classifier"], [132, 0, 1, "", "lora_mistral_reward_7b"], [133, 0, 1, "", "mistral"], [134, 0, 1, "", "mistral_7b"], [135, 0, 1, "", "mistral_classifier"], [136, 0, 1, "", "mistral_reward_7b"], [137, 0, 1, "", "mistral_tokenizer"], [138, 0, 1, "", "qlora_mistral_7b"], [139, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.phi3": [[140, 0, 1, "", "lora_phi3"], [141, 0, 1, "", "lora_phi3_mini"], [142, 0, 1, "", "phi3"], [143, 0, 1, "", "phi3_mini"], [144, 0, 1, "", "phi3_mini_tokenizer"], [145, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.qwen2": [[146, 0, 1, "", "lora_qwen2"], [147, 0, 1, "", "lora_qwen2_0_5b"], [148, 0, 1, "", "lora_qwen2_1_5b"], [149, 0, 1, "", "lora_qwen2_7b"], [150, 0, 1, "", "qwen2"], [151, 0, 1, "", "qwen2_0_5b"], [152, 0, 1, "", "qwen2_1_5b"], [153, 0, 1, "", "qwen2_7b"], [154, 0, 1, "", "qwen2_tokenizer"]], "torchtune.modules": [[155, 1, 1, "", "FeedForward"], [156, 1, 1, "", "Fp32LayerNorm"], [157, 1, 1, "", "KVCache"], [158, 1, 1, "", "MultiHeadAttention"], [159, 1, 1, "", "RMSNorm"], [160, 1, 1, "", "RotaryPositionalEmbeddings"], [161, 1, 1, "", "TanhGate"], [162, 1, 1, "", "TiedLinear"], [163, 1, 1, "", "TransformerCrossAttentionLayer"], [164, 1, 1, "", "TransformerDecoder"], [165, 1, 1, "", "TransformerSelfAttentionLayer"], [166, 1, 1, "", "VisionTransformer"], [168, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.FeedForward": [[155, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[156, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[157, 2, 1, "", "reset"], [157, 2, 1, "", "update"]], "torchtune.modules.MultiHeadAttention": [[158, 2, 1, "", "forward"], [158, 2, 1, "", "reset_cache"], [158, 2, 1, "", "setup_cache"]], "torchtune.modules.RMSNorm": [[159, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[160, 2, 1, "", "forward"]], "torchtune.modules.TanhGate": [[161, 2, 1, "", "forward"]], "torchtune.modules.TransformerCrossAttentionLayer": [[163, 4, 1, "", "cache_enabled"], [163, 2, 1, "", "forward"], [163, 2, 1, "", "reset_cache"], [163, 2, 1, "", "setup_cache"]], "torchtune.modules.TransformerDecoder": [[164, 2, 1, "", "chunked_output"], [164, 2, 1, "", "decoder_caches_are_enabled"], [164, 2, 1, "", "encoder_caches_are_enabled"], [164, 2, 1, "", "forward"], [164, 2, 1, "", "reset_caches"], [164, 2, 1, "", "set_num_output_chunks"], [164, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerSelfAttentionLayer": [[165, 4, 1, "", "cache_enabled"], [165, 2, 1, "", "forward"], [165, 2, 1, "", "reset_cache"], [165, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[166, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[167, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[169, 1, 1, "", "CEWithChunkedOutputLoss"]], "torchtune.modules.loss.CEWithChunkedOutputLoss": [[169, 2, 1, "", "compute_cross_entropy"], [169, 2, 1, "", "forward"]], "torchtune.modules.model_fusion": [[170, 1, 1, "", "DeepFusionModel"], [171, 1, 1, "", "FusionEmbedding"], [172, 1, 1, "", "FusionLayer"], [173, 0, 1, "", "get_fusion_params"], [174, 0, 1, "", "register_fusion_module"]], "torchtune.modules.model_fusion.DeepFusionModel": [[170, 2, 1, "", "caches_are_enabled"], [170, 2, 1, "", "forward"], [170, 2, 1, "", "reset_caches"], [170, 2, 1, "", "set_num_output_chunks"], [170, 2, 1, "", "setup_caches"]], "torchtune.modules.model_fusion.FusionEmbedding": [[171, 2, 1, "", "forward"], [171, 2, 1, "", "fusion_params"]], "torchtune.modules.model_fusion.FusionLayer": [[172, 4, 1, "", "cache_enabled"], [172, 2, 1, "", "forward"], [172, 2, 1, "", "fusion_params"], [172, 2, 1, "", "reset_cache"], [172, 2, 1, "", "setup_cache"]], "torchtune.modules.peft": [[175, 1, 1, "", "AdapterModule"], [176, 1, 1, "", "LoRALinear"], [177, 0, 1, "", "disable_adapter"], [178, 0, 1, "", "get_adapter_params"], [179, 0, 1, "", "set_trainable_params"], [180, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [181, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[175, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[176, 2, 1, "", "adapter_params"], [176, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[182, 1, 1, "", "BaseTokenizer"], [183, 1, 1, "", "ModelTokenizer"], [184, 1, 1, "", "SentencePieceBaseTokenizer"], [185, 1, 1, "", "TikTokenBaseTokenizer"], [186, 0, 1, "", "parse_hf_tokenizer_json"], [187, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[182, 2, 1, "", "decode"], [182, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[183, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[184, 2, 1, "", "decode"], [184, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[185, 2, 1, "", "decode"], [185, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[188, 1, 1, "", "Transform"], [189, 1, 1, "", "VisionCrossAttentionMask"]], "torchtune.rlhf": [[190, 0, 1, "", "estimate_advantages"], [191, 0, 1, "", "get_rewards_ppo"], [196, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.rlhf.loss": [[192, 1, 1, "", "DPOLoss"], [193, 1, 1, "", "PPOLoss"], [194, 1, 1, "", "RSOLoss"], [195, 1, 1, "", "SimPOLoss"]], "torchtune.rlhf.loss.DPOLoss": [[192, 2, 1, "", "forward"]], "torchtune.rlhf.loss.PPOLoss": [[193, 2, 1, "", "forward"]], "torchtune.rlhf.loss.RSOLoss": [[194, 2, 1, "", "forward"]], "torchtune.rlhf.loss.SimPOLoss": [[195, 2, 1, "", "forward"]], "torchtune.training": [[197, 3, 1, "", "FSDPPolicyType"], [198, 1, 1, "", "FormattedCheckpointFiles"], [199, 1, 1, "", "FullModelHFCheckpointer"], [200, 1, 1, "", "FullModelMetaCheckpointer"], [201, 1, 1, "", "FullModelTorchTuneCheckpointer"], [202, 1, 1, "", "ModelType"], [203, 1, 1, "", "OptimizerInBackwardWrapper"], [204, 0, 1, "", "apply_selective_activation_checkpointing"], [205, 0, 1, "", "create_optim_in_bwd_wrapper"], [206, 0, 1, "", "get_dtype"], [207, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [208, 0, 1, "", "get_memory_stats"], [209, 0, 1, "", "get_quantizer_mode"], [210, 0, 1, "", "get_unmasked_sequence_lengths"], [211, 0, 1, "", "get_world_size_and_rank"], [212, 0, 1, "", "init_distributed"], [213, 0, 1, "", "is_distributed"], [214, 0, 1, "", "log_memory_stats"], [215, 0, 1, "", "lora_fsdp_wrap_policy"], [221, 0, 1, "", "register_optim_in_bwd_hooks"], [222, 0, 1, "", "set_activation_checkpointing"], [223, 0, 1, "", "set_default_dtype"], [224, 0, 1, "", "set_seed"], [225, 0, 1, "", "setup_torch_profiler"], [226, 0, 1, "", "update_state_dict_for_classifier"], [227, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.training.FormattedCheckpointFiles": [[198, 2, 1, "", "build_checkpoint_filenames"]], "torchtune.training.FullModelHFCheckpointer": [[199, 2, 1, "", "load_checkpoint"], [199, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelMetaCheckpointer": [[200, 2, 1, "", "load_checkpoint"], [200, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelTorchTuneCheckpointer": [[201, 2, 1, "", "load_checkpoint"], [201, 2, 1, "", "save_checkpoint"]], "torchtune.training.OptimizerInBackwardWrapper": [[203, 2, 1, "", "get_optim_key"], [203, 2, 1, "", "load_state_dict"], [203, 2, 1, "", "state_dict"]], "torchtune.training.metric_logging": [[216, 1, 1, "", "CometLogger"], [217, 1, 1, "", "DiskLogger"], [218, 1, 1, "", "StdoutLogger"], [219, 1, 1, "", "TensorBoardLogger"], [220, 1, 1, "", "WandBLogger"]], "torchtune.training.metric_logging.CometLogger": [[216, 2, 1, "", "close"], [216, 2, 1, "", "log"], [216, 2, 1, "", "log_config"], [216, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.DiskLogger": [[217, 2, 1, "", "close"], [217, 2, 1, "", "log"], [217, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.StdoutLogger": [[218, 2, 1, "", "close"], [218, 2, 1, "", "log"], [218, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.TensorBoardLogger": [[219, 2, 1, "", "close"], [219, 2, 1, "", "log"], [219, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.WandBLogger": [[220, 2, 1, "", "close"], [220, 2, 1, "", "log"], [220, 2, 1, "", "log_config"], [220, 2, 1, "", "log_dict"]], "torchtune.utils": [[228, 0, 1, "", "batch_to_device"], [229, 0, 1, "", "get_device"], [230, 0, 1, "", "get_logger"], [231, 0, 1, "", "torch_version_ge"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 25, 32, 33, 35, 47, 49, 197, 234, 236, 241, 244, 246, 247, 249, 250], "config": [0, 15, 16, 241, 245], "data": [1, 25, 32, 33, 35, 242], "text": [1, 2, 243, 246], "templat": [1, 9, 10, 11, 12, 242, 243], "type": 1, "convert": 1, "messag": [1, 29], "transform": [1, 5, 188], "collat": 1, "helper": 1, "function": 1, "dataset": [2, 9, 10, 47, 49, 242, 243], "imag": 2, "gener": [2, 3, 69, 244, 246], "builder": 2, "class": [2, 11, 16], "model": [4, 5, 12, 17, 241, 244, 245, 246, 247, 248, 249], "llama3": [4, 108, 242, 246, 249], "1": 4, "llama2": [4, 93, 242, 244, 247, 250], "code": 4, "llama": 4, "qwen": 4, "2": 4, "phi": 4, "3": 4, "mistral": [4, 133], "gemma": [4, 83], "modul": 5, "compon": [5, 15, 248], "build": [5, 235, 250], "block": 5, "loss": 5, "base": [5, 12], "token": [5, 12, 242], "util": [5, 8], "peft": [5, 248], "fusion": 5, "vision": 5, "rlhf": 6, "train": [7, 197, 238, 245], "checkpoint": [7, 13, 17, 244, 248], "reduc": 7, "precis": [7, 248], "distribut": [7, 238], "memori": [7, 243, 247, 248, 250], "manag": 7, "metric": [7, 14, 17], "log": [7, 14, 17], "perform": [7, 247], "profil": 7, "miscellan": [7, 8], "chat": [9, 242, 243], "exampl": [9, 10], "format": [9, 10, 13, 243], "load": [9, 10, 12], "from": [9, 10, 12, 242, 250], "hug": [9, 10, 12, 243, 244], "face": [9, 10, 12, 243, 244], "local": [9, 10, 243], "remot": [9, 10, 243], "specifi": 9, "convers": 9, "style": 9, "sharegpt": 9, "json": 9, "renam": [9, 10], "column": [9, 10], "built": [9, 10, 241, 243], "instruct": [10, 235, 243, 246], "prompt": [11, 12, 242], "us": [11, 15, 16, 242, 244, 250], "defin": 11, "via": [11, 235, 246], "dotpath": 11, "string": 11, "dictionari": 11, "prompttempl": [11, 30], "custom": [11, 242, 243], "download": [12, 241, 244, 245], "file": 12, "set": [12, 243], "max": [12, 243], "sequenc": [12, 243], "length": [12, 243], "special": [12, 242], "overview": [13, 236, 239, 244, 248], "handl": 13, "differ": 13, "hfcheckpoint": 13, "metacheckpoint": 13, "torchtunecheckpoint": 13, "intermedi": 13, "vs": 13, "final": 13, "lora": [13, 237, 244, 247, 248, 250], "put": [13, 250], "thi": 13, "all": [13, 15, 250], "togeth": [13, 250], "comet": 14, "logger": [14, 17], "about": 15, "where": 15, "do": 15, "paramet": [15, 248], "live": 15, "write": 15, "configur": [15, 243], "instanti": [15, 18], "referenc": 15, "other": [15, 244], "field": 15, "interpol": 15, "valid": [15, 21, 241], "your": [15, 16, 244, 245], "best": 15, "practic": 15, "airtight": 15, "public": 15, "api": 15, "onli": 15, "command": 15, "line": 15, "overrid": 15, "remov": 15, "what": [16, 236, 247, 249, 250], "ar": 16, "recip": [16, 239, 241, 245, 247, 249], "script": 16, "run": [16, 241, 244], "cli": [16, 241], "pars": [16, 20], "weight": 17, "bias": 17, "w": 17, "b": 17, "log_config": 19, "chatformat": 22, "chatmltempl": 23, "chosenrejectedtomessag": 24, "grammarerrorcorrectiontempl": 25, "inputoutputtomessag": 26, "instructtempl": 27, "jsontomessag": 28, "prompttemplateinterfac": 31, "questionanswertempl": 32, "role": 33, "sharegpttomessag": 34, "summarizetempl": 35, "format_content_with_imag": 36, "get_openai_messag": 37, "get_sharegpt_messag": 38, "left_pad_sequ": 39, "load_imag": 40, "padded_col": 41, "padded_collate_dpo": 42, "padded_collate_sft": 43, "padded_collate_tiled_images_and_mask": 44, "truncat": 45, "validate_messag": 46, "chatdataset": 47, "concatdataset": 48, "instructdataset": 49, "packeddataset": 50, "preferencedataset": 51, "sftdataset": 52, "textcompletiondataset": 53, "alpaca_cleaned_dataset": 54, "alpaca_dataset": 55, "chat_dataset": 56, "cnn_dailymail_articles_dataset": 57, "grammar_dataset": 58, "hh_rlhf_helpful_dataset": 59, "instruct_dataset": 60, "llava_instruct_dataset": 61, "the_cauldron_dataset": 62, "preference_dataset": 63, "samsum_dataset": 64, "slimorca_dataset": 65, "stack_exchange_paired_dataset": 66, "text_completion_dataset": 67, "wikitext_dataset": 68, "generate_next_token": 70, "get_causal_mask_from_padding_mask": 71, "get_position_ids_from_padding_mask": 72, "sampl": [73, 243], "code_llama2_13b": 74, "code_llama2_70b": 75, "code_llama2_7b": 76, "lora_code_llama2_13b": 77, "lora_code_llama2_70b": 78, "lora_code_llama2_7b": 79, "qlora_code_llama2_13b": 80, "qlora_code_llama2_70b": 81, "qlora_code_llama2_7b": 82, "gemma_2b": 84, "gemma_7b": 85, "gemma_token": 86, "lora_gemma": 87, "lora_gemma_2b": 88, "lora_gemma_7b": 89, "qlora_gemma_2b": 90, "qlora_gemma_7b": 91, "llama2chattempl": 92, "llama2_13b": 94, "llama2_70b": 95, "llama2_7b": 96, "llama2_reward_7b": 97, "llama2_token": 98, "lora_llama2": 99, "lora_llama2_13b": 100, "lora_llama2_70b": 101, "lora_llama2_7b": 102, "lora_llama2_reward_7b": 103, "qlora_llama2_13b": 104, "qlora_llama2_70b": 105, "qlora_llama2_7b": 106, "qlora_llama2_reward_7b": 107, "llama3_70b": 109, "llama3_8b": 110, "llama3_token": 111, "lora_llama3": 112, "lora_llama3_70b": 113, "lora_llama3_8b": 114, "qlora_llama3_70b": 115, "qlora_llama3_8b": 116, "llama3_1": 117, "llama3_1_405b": 118, "llama3_1_70b": 119, "llama3_1_8b": 120, "lora_llama3_1": 121, "lora_llama3_1_405b": 122, "lora_llama3_1_70b": 123, "lora_llama3_1_8b": 124, "qlora_llama3_1_405b": 125, "qlora_llama3_1_70b": 126, "qlora_llama3_1_8b": 127, "mistralchattempl": 128, "lora_mistr": 129, "lora_mistral_7b": 130, "lora_mistral_classifi": 131, "lora_mistral_reward_7b": 132, "mistral_7b": 134, "mistral_classifi": 135, "mistral_reward_7b": 136, "mistral_token": 137, "qlora_mistral_7b": 138, "qlora_mistral_reward_7b": 139, "lora_phi3": 140, "lora_phi3_mini": 141, "phi3": 142, "phi3_mini": 143, "phi3_mini_token": 144, "qlora_phi3_mini": 145, "lora_qwen2": 146, "lora_qwen2_0_5b": 147, "lora_qwen2_1_5b": 148, "lora_qwen2_7b": 149, "qwen2": 150, "qwen2_0_5b": 151, "qwen2_1_5b": 152, "qwen2_7b": 153, "qwen2_token": 154, "feedforward": 155, "fp32layernorm": 156, "kvcach": 157, "multiheadattent": 158, "rmsnorm": 159, "rotarypositionalembed": 160, "tanhgat": 161, "tiedlinear": 162, "transformercrossattentionlay": 163, "transformerdecod": 164, "transformerselfattentionlay": 165, "visiontransform": 166, "reparametrize_as_dtype_state_dict_post_hook": 167, "get_cosine_schedule_with_warmup": 168, "cewithchunkedoutputloss": 169, "deepfusionmodel": 170, "fusionembed": 171, "fusionlay": 172, "get_fusion_param": 173, "register_fusion_modul": 174, "adaptermodul": 175, "loralinear": 176, "disable_adapt": 177, "get_adapter_param": 178, "set_trainable_param": 179, "validate_missing_and_unexpected_for_lora": 180, "validate_state_dict_for_lora": 181, "basetoken": 182, "modeltoken": 183, "sentencepiecebasetoken": 184, "tiktokenbasetoken": 185, "parse_hf_tokenizer_json": 186, "tokenize_messages_no_special_token": 187, "visioncrossattentionmask": 189, "estimate_advantag": 190, "get_rewards_ppo": 191, "dpoloss": 192, "ppoloss": 193, "rsoloss": 194, "simpoloss": 195, "truncate_sequence_at_first_stop_token": 196, "fsdppolicytyp": 197, "formattedcheckpointfil": 198, "fullmodelhfcheckpoint": 199, "fullmodelmetacheckpoint": 200, "fullmodeltorchtunecheckpoint": 201, "modeltyp": 202, "optimizerinbackwardwrapp": 203, "apply_selective_activation_checkpoint": 204, "create_optim_in_bwd_wrapp": 205, "get_dtyp": 206, "get_full_finetune_fsdp_wrap_polici": 207, "get_memory_stat": 208, "get_quantizer_mod": 209, "get_unmasked_sequence_length": 210, "get_world_size_and_rank": 211, "init_distribut": 212, "is_distribut": 213, "log_memory_stat": 214, "lora_fsdp_wrap_polici": 215, "cometlogg": 216, "disklogg": 217, "stdoutlogg": 218, "tensorboardlogg": 219, "wandblogg": 220, "register_optim_in_bwd_hook": 221, "set_activation_checkpoint": 222, "set_default_dtyp": 223, "set_se": 224, "setup_torch_profil": 225, "update_state_dict_for_classifi": 226, "validate_expected_param_dtyp": 227, "batch_to_devic": 228, "get_devic": 229, "get_logg": 230, "torch_version_g": 231, "comput": [233, 240], "time": [233, 240], "welcom": 234, "document": 234, "get": [234, 241, 246], "start": [234, 241], "tutori": 234, "instal": 235, "pre": 235, "requisit": 235, "pypi": 235, "git": 235, "clone": 235, "nightli": 235, "kei": 236, "concept": 236, "design": 236, "principl": 236, "singl": 237, "devic": [237, 249], "finetun": [237, 239, 244, 247, 249, 250], "quantiz": [238, 244, 246, 248, 249], "awar": 238, "qat": [238, 249], "list": 241, "copi": 241, "fine": [242, 243, 245, 246, 247, 248, 249, 250], "tune": [242, 243, 245, 246, 247, 248, 249, 250], "chang": 242, "when": 242, "should": 242, "i": 242, "pack": 243, "unstructur": 243, "corpu": 243, "multipl": 243, "fulli": 243, "end": 244, "workflow": 244, "7b": 244, "evalu": [244, 246, 249], "eleutherai": [244, 246], "s": [244, 246], "eval": [244, 246], "har": [244, 246], "speed": 244, "up": 244, "librari": 244, "upload": 244, "hub": 244, "first": 245, "llm": 245, "select": 245, "modifi": 245, "next": 245, "step": [245, 248], "meta": 246, "8b": 246, "access": 246, "our": 246, "faster": 246, "how": 247, "doe": 247, "work": 247, "appli": [247, 249], "trade": 247, "off": 247, "optim": 248, "activ": 248, "offload": 248, "gradient": 248, "accumul": 248, "lower": [248, 249], "fuse": 248, "backward": 248, "pass": 248, "effici": 248, "low": 248, "rank": 248, "adapt": 248, "qlora": [248, 250], "option": 249, "save": 250, "deep": 250, "dive": 250}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})