Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.Role", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "torchtune.data.Role", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2Tokenizer", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "Llama3Tokenizer", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "MistralTokenizer", "lora_mistral_7b", "lora_mistral_classifier_7b", "mistral_7b", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "Phi3MiniTokenizer", "lora_phi3_mini", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 21, 23, 31, 32, 33, 35, 36, 40, 44, 78, 90, 96, 97, 162, 166, 167, 170, 172, 173], "prompt": [1, 14, 17, 18, 19, 21, 23, 24, 25, 26, 29, 31, 33, 35, 36, 37, 39, 40, 41, 42, 44, 59, 67, 78, 86, 94, 105, 121, 135, 168, 169, 171], "chat": [1, 2, 15, 16, 19, 25, 26, 29, 37, 42, 67, 97], "includ": [1, 6, 7, 8, 15, 18, 49, 67, 97, 112, 124, 128, 129, 133, 164, 166, 167, 168, 169, 170, 171, 172, 173], "some": [1, 6, 7, 16, 114, 115, 162, 164, 166, 167, 168, 169, 170, 172, 173], "specif": [1, 4, 7, 8, 10, 138, 167, 168, 169, 173], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29, 31, 33, 35, 36, 37, 40, 42, 67, 78, 125, 128, 129, 130, 131, 166, 167, 169, 170, 171, 172], "differ": [1, 7, 9, 29, 30, 31, 33, 46, 47, 48, 86, 107, 118, 131, 152, 159, 164, 166, 167, 169, 171, 172, 173], "dataset": [1, 5, 7, 14, 17, 18, 20, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 164, 170, 171], "model": [1, 2, 6, 7, 8, 10, 16, 21, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 120, 121, 128, 129, 130, 131, 134, 135, 138, 140, 146, 153, 154, 162, 164, 167, 168, 173], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 23, 24, 26, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 60, 61, 68, 69, 70, 81, 86, 89, 90, 97, 100, 105, 106, 107, 109, 111, 114, 117, 118, 120, 122, 124, 125, 128, 129, 130, 132, 133, 134, 135, 149, 150, 153, 161, 163, 165, 166, 168, 169, 170, 171, 172], "common": [1, 2, 4, 7, 121, 166, 167, 168, 171, 172], "json": [1, 6, 25, 26, 81, 97, 120, 128, 166, 168, 169], "messag": [1, 15, 16, 19, 21, 25, 26, 28, 29, 37, 59, 67, 78, 86, 94, 121, 163, 166, 167, 168], "miscellan": 1, "function": [1, 4, 7, 8, 10, 12, 29, 47, 48, 49, 99, 100, 101, 107, 108, 110, 113, 116, 117, 127, 128, 135, 136, 142, 146, 152, 156, 164, 167, 168, 173], "us": [1, 2, 3, 4, 6, 9, 10, 12, 16, 19, 20, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 67, 81, 97, 99, 100, 103, 104, 105, 106, 107, 108, 110, 113, 116, 118, 119, 122, 123, 125, 127, 128, 129, 131, 132, 133, 135, 136, 137, 138, 140, 146, 147, 148, 149, 150, 156, 162, 163, 164, 166, 168, 170, 171, 172], "modifi": [1, 7, 8, 9, 108, 164, 169, 171, 172, 173], "For": [2, 5, 6, 7, 8, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 44, 45, 46, 47, 48, 49, 67, 99, 105, 107, 123, 124, 128, 133, 134, 141, 150, 154, 156, 163, 166, 167, 168, 169, 170, 171, 172, 173], "detail": [2, 6, 37, 42, 46, 47, 48, 49, 67, 102, 107, 127, 138, 146, 156, 166, 169, 170, 171, 172, 173], "usag": [2, 108, 131, 132, 157, 163, 166, 168, 169, 170, 171, 173], "guid": [2, 7, 9, 164, 167, 168, 170, 172], "pleas": [2, 5, 46, 47, 48, 49, 56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 107, 127, 138, 146, 154, 163, 173], "see": [2, 5, 6, 9, 19, 21, 37, 42, 45, 56, 57, 58, 65, 66, 67, 75, 76, 77, 84, 85, 92, 93, 98, 102, 107, 111, 127, 131, 133, 138, 139, 146, 150, 154, 156, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "our": [2, 6, 8, 164, 167, 168, 169, 170, 172, 173], "tutori": [2, 6, 67, 154, 164, 167, 168, 169, 170, 171, 172, 173], "support": [2, 3, 6, 8, 9, 10, 20, 21, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 96, 97, 99, 101, 107, 112, 123, 125, 129, 130, 132, 137, 140, 164, 166, 167, 168, 169, 170, 171, 172, 173], "sever": 2, "wide": 2, "help": [2, 6, 19, 105, 107, 128, 133, 162, 163, 164, 166, 167, 168, 169, 170, 171, 173], "quickli": [2, 7, 34, 167, 168], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 23, 24, 29, 34, 47, 48, 49, 67, 107, 149, 150, 162, 163, 164, 166, 167, 168, 171, 172, 173], "fine": [2, 6, 8, 9, 20, 32, 67, 162, 164, 169, 172], "tune": [2, 3, 6, 7, 8, 9, 12, 20, 32, 67, 162, 163, 164, 166, 169, 172, 173], "also": [2, 6, 7, 8, 9, 10, 37, 40, 44, 97, 99, 105, 112, 136, 138, 140, 146, 150, 163, 166, 167, 168, 169, 170, 171, 172, 173], "like": [2, 6, 7, 8, 9, 29, 97, 107, 130, 163, 166, 167, 168, 169, 170, 172], "These": [2, 4, 6, 7, 8, 10, 32, 107, 122, 133, 167, 168, 169, 170, 171, 172, 173], "ar": [2, 4, 6, 7, 9, 10, 14, 15, 17, 18, 19, 21, 23, 24, 28, 31, 32, 33, 35, 36, 37, 39, 40, 41, 47, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 105, 107, 112, 113, 116, 117, 122, 124, 127, 128, 129, 131, 132, 134, 135, 137, 140, 144, 146, 152, 157, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "especi": [2, 164, 166, 169], "specifi": [2, 6, 7, 8, 10, 37, 99, 105, 106, 110, 127, 135, 138, 141, 146, 150, 154, 157, 166, 167, 168, 169, 170, 171, 173], "yaml": [2, 7, 8, 10, 11, 12, 37, 40, 44, 133, 150, 164, 166, 167, 168, 169, 170, 171, 172, 173], "config": [2, 6, 9, 10, 11, 12, 13, 37, 40, 44, 99, 116, 128, 132, 133, 150, 157, 164, 167, 168, 169, 171, 172, 173], "represent": [2, 172, 173], "abov": [2, 6, 108, 124, 144, 163, 169, 171, 172, 173], "all": [3, 4, 8, 13, 29, 30, 32, 37, 49, 81, 97, 99, 100, 105, 107, 108, 113, 123, 124, 128, 132, 133, 134, 144, 153, 159, 160, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172], "famili": [3, 8, 35, 36, 38, 42, 43, 45, 131, 164, 166, 171], "To": [3, 6, 7, 8, 9, 32, 107, 128, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "download": [3, 6, 160, 163, 167, 168, 171, 172, 173], "8b": [3, 80, 83, 85, 95, 166, 167], "meta": [3, 6, 19, 67, 78, 128, 129, 166, 167, 169, 170], "hf": [3, 6, 94, 110, 128, 166, 167, 169, 170, 171], "token": [3, 6, 7, 8, 20, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 59, 62, 67, 71, 78, 81, 86, 91, 94, 97, 99, 104, 105, 106, 107, 118, 119, 120, 121, 122, 135, 138, 151, 166, 168, 169, 170, 171, 172, 173], "hf_token": 3, "70b": [3, 51, 54, 57, 69, 73, 76, 79, 82, 84, 171], "ignor": [3, 6, 94, 99, 100, 166], "pattern": [3, 119, 166], "origin": [3, 6, 35, 36, 108, 112, 167, 169, 171, 172, 173], "consolid": [3, 6, 171], "7b": [3, 6, 31, 33, 34, 35, 36, 38, 40, 44, 45, 52, 55, 58, 61, 64, 70, 74, 77, 87, 88, 89, 90, 128, 129, 167, 170, 171, 172, 173], "2": [3, 6, 9, 28, 32, 42, 46, 47, 59, 67, 78, 86, 94, 99, 107, 118, 119, 121, 123, 124, 125, 128, 129, 151, 152, 155, 156, 157, 158, 167, 169, 170, 171, 172], "13b": [3, 6, 50, 53, 56, 68, 72, 75], "codellama": 3, "mini": [3, 94, 95, 96, 97, 98], "4k": [3, 96, 97], "microsoft": [3, 96, 97], "ai": [3, 89, 99, 150, 167, 171], "v0": 3, "1": [3, 6, 8, 32, 42, 46, 47, 59, 67, 78, 86, 94, 99, 105, 107, 109, 110, 118, 119, 121, 124, 125, 129, 131, 135, 144, 149, 150, 151, 152, 155, 156, 166, 167, 169, 170, 171, 172, 173], "mistralai": [3, 166], "size": [3, 6, 8, 10, 35, 36, 39, 41, 47, 48, 49, 99, 102, 103, 104, 105, 107, 122, 123, 125, 126, 142, 144, 164, 166, 168, 169, 170, 171, 172], "2b": [3, 60, 63], "googl": [3, 60, 61], "gguf": 3, "vision": [3, 49], "compon": [3, 6, 8, 13, 152, 164, 168, 170, 172, 173], "multimod": [3, 20], "encod": [3, 4, 49, 59, 67, 78, 86, 94, 110, 118, 119, 121, 122, 167], "perform": [4, 6, 32, 67, 100, 107, 113, 135, 164, 167, 169, 171, 173], "direct": [4, 8, 110, 152, 163], "text": [4, 20, 29, 32, 34, 37, 38, 44, 45, 67, 78, 86, 94, 118, 119, 122, 167, 169], "id": [4, 6, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 67, 78, 86, 94, 99, 104, 105, 106, 118, 119, 120, 122, 128, 130, 135, 151, 152, 167, 168, 169], "decod": [4, 78, 86, 94, 105, 118, 119, 135, 167], "typic": [4, 7, 32, 34, 44, 97, 110, 168, 173], "byte": [4, 119, 173], "pair": [4, 7, 14, 43, 119, 151, 152, 168], "underli": [4, 86, 118, 173], "helper": 4, "method": [4, 6, 7, 8, 9, 12, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 108, 111, 114, 116, 125, 132, 133, 141, 163, 164, 168, 169, 171, 172, 173], "can": [4, 6, 7, 8, 9, 10, 13, 20, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 44, 45, 46, 47, 49, 67, 86, 103, 104, 107, 113, 118, 119, 123, 125, 127, 128, 131, 133, 138, 146, 149, 150, 154, 157, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "ani": [4, 6, 7, 8, 10, 12, 13, 14, 17, 18, 23, 24, 25, 26, 27, 29, 31, 33, 34, 37, 38, 40, 44, 45, 67, 86, 101, 108, 114, 115, 116, 117, 118, 121, 128, 129, 130, 132, 135, 143, 146, 156, 159, 166, 167, 168, 169, 170, 171, 172], "preprocess": [4, 32, 107], "imag": [4, 20, 46, 47, 48, 49, 107, 122, 123, 124, 125, 126, 172], "offer": 5, "allow": [5, 30, 116, 124, 149, 166, 173], "seamless": 5, "transit": 5, "between": [5, 6, 128, 131, 168, 169, 171, 172, 173], "train": [5, 6, 8, 9, 19, 29, 30, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 67, 99, 101, 104, 105, 106, 108, 109, 128, 129, 130, 137, 140, 146, 157, 162, 164, 166, 167, 168, 169, 171, 172, 173], "interoper": [5, 6, 8, 164, 169, 173], "rest": [5, 167, 173], "ecosystem": [5, 6, 8, 164, 169, 171, 173], "comprehens": 5, "overview": [5, 7, 9, 162, 170, 172, 173], "deep": [5, 6, 7, 8, 9, 164, 170, 171], "dive": [5, 6, 7, 8, 9, 164, 170, 171], "enabl": [5, 7, 8, 9, 30, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 112, 156, 157, 171, 172, 173], "work": [5, 6, 8, 133, 164, 166, 169, 171, 173], "set": [5, 6, 7, 8, 9, 20, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 44, 45, 78, 94, 99, 104, 105, 113, 115, 125, 127, 136, 138, 144, 146, 154, 155, 156, 157, 164, 166, 167, 169, 170, 171, 172], "consumpt": [5, 30], "dure": [5, 6, 30, 31, 32, 35, 36, 39, 41, 99, 102, 104, 105, 106, 107, 108, 140, 167, 169, 171, 172, 173], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 27, 29, 30, 31, 32, 33, 42, 49, 105, 107, 113, 130, 133, 136, 138, 150, 157, 164, 166, 167, 168, 169, 170, 171], "debug": [5, 6, 7, 8, 166], "finetun": [5, 6, 7, 8, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 162, 164, 170, 171], "job": [5, 9, 156, 170], "variou": [5, 18], "walk": [6, 8, 149, 164, 167, 168, 169, 170, 173], "you": [6, 7, 8, 9, 10, 18, 19, 20, 24, 29, 31, 33, 34, 35, 36, 38, 40, 44, 45, 107, 124, 131, 133, 135, 149, 150, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "through": [6, 7, 8, 9, 49, 100, 107, 113, 164, 166, 167, 168, 169, 170, 173], "design": [6, 8], "behavior": [6, 146, 167, 168], "associ": [6, 7, 8, 49, 135, 169, 172], "util": [6, 7, 8, 9, 10, 30, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 164, 169, 170, 171, 173], "what": [6, 7, 9, 19, 21, 23, 34, 39, 41, 107, 162, 167, 168, 169, 170, 171], "cover": [6, 7, 8, 9, 167, 169, 173], "how": [6, 7, 8, 9, 24, 107, 127, 154, 162, 166, 167, 168, 169, 170, 171, 173], "we": [6, 7, 8, 9, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 67, 86, 99, 102, 104, 105, 107, 112, 123, 124, 128, 129, 130, 135, 137, 141, 146, 153, 164, 166, 167, 168, 169, 170, 171, 172, 173], "them": [6, 7, 29, 30, 31, 33, 40, 59, 67, 86, 94, 100, 107, 108, 121, 166, 167, 168, 169, 172, 173], "scenario": [6, 30], "full": [6, 7, 8, 37, 40, 56, 57, 58, 59, 65, 66, 67, 75, 76, 77, 84, 85, 86, 92, 93, 94, 98, 116, 117, 121, 164, 166, 168, 171, 172], "compos": [6, 107], "which": [6, 7, 8, 30, 31, 32, 34, 35, 36, 39, 41, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 86, 87, 88, 95, 99, 104, 105, 106, 107, 109, 116, 117, 118, 124, 128, 129, 130, 132, 137, 147, 150, 154, 164, 166, 167, 168, 169, 170, 171, 172, 173], "plug": 6, "recip": [6, 7, 9, 10, 11, 12, 100, 116, 128, 129, 130, 164, 167, 168, 169, 171, 173], "evalu": [6, 8, 162, 164, 170, 172, 173], "gener": [6, 8, 14, 17, 23, 24, 29, 31, 32, 33, 38, 42, 67, 86, 113, 155, 156, 157, 160, 162, 167, 168, 172, 173], "each": [6, 8, 15, 18, 30, 32, 46, 47, 48, 49, 53, 54, 55, 59, 63, 64, 67, 72, 73, 74, 82, 83, 86, 87, 88, 94, 95, 99, 104, 105, 106, 107, 110, 116, 117, 121, 122, 124, 126, 152, 156, 157, 164, 166, 168, 169, 170, 171, 172], "make": [6, 7, 8, 9, 99, 106, 107, 164, 166, 169, 170, 171, 172, 173], "easi": [6, 8, 164, 168, 172], "understand": [6, 7, 8, 162, 164, 167, 168, 172, 173], "extend": [6, 8, 164], "befor": [6, 28, 31, 32, 33, 46, 47, 49, 99, 105, 106, 107, 112, 119, 128, 166, 169], "let": [6, 7, 9, 166, 167, 168, 169, 170, 171, 172, 173], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 25, 26, 28, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 53, 54, 55, 59, 67, 72, 73, 74, 78, 82, 83, 86, 87, 88, 94, 95, 97, 99, 102, 104, 105, 106, 107, 108, 110, 111, 114, 116, 117, 124, 127, 128, 129, 132, 136, 138, 140, 146, 149, 154, 155, 164, 166, 167, 168, 170, 172, 173], "defin": [6, 7, 8, 100, 111, 112, 114, 168, 170, 172], "concept": [6, 169, 170], "In": [6, 7, 8, 29, 47, 48, 49, 104, 107, 112, 124, 127, 146, 149, 150, 167, 169, 171, 172, 173], "ll": [6, 7, 8, 135, 141, 164, 167, 168, 169, 170, 171, 173], "talk": 6, "about": [6, 8, 107, 110, 150, 164, 166, 167, 169, 170, 171, 172, 173], "take": [6, 7, 8, 10, 100, 102, 107, 108, 128, 130, 133, 136, 152, 167, 168, 169, 170, 171, 172, 173], "close": [6, 8, 147, 148, 149, 150, 172], "look": [6, 7, 8, 134, 149, 163, 167, 168, 169, 170, 171, 172], "veri": [6, 30, 105, 166, 169], "simpli": [6, 7, 32, 166, 167, 168, 169, 171, 173], "dictat": 6, "state_dict": [6, 108, 116, 128, 129, 130, 131, 132, 172, 173], "store": [6, 30, 147, 150, 172, 173], "file": [6, 7, 8, 9, 10, 11, 12, 59, 67, 78, 81, 86, 94, 97, 118, 119, 120, 128, 129, 130, 133, 147, 150, 157, 161, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173], "disk": [6, 34, 147], "weight": [6, 8, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 108, 111, 112, 116, 118, 128, 129, 130, 131, 141, 146, 150, 162, 166, 167, 169, 170, 171, 172, 173], "string": [6, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 67, 78, 86, 94, 111, 118, 119, 121, 136, 137, 141, 166, 168], "kei": [6, 7, 9, 14, 17, 23, 24, 25, 29, 31, 33, 40, 99, 102, 105, 106, 115, 116, 117, 128, 130, 132, 152, 157, 166, 169, 170, 172, 173], "identifi": 6, "state": [6, 8, 107, 108, 114, 115, 116, 117, 128, 129, 130, 132, 134, 169, 171, 172, 173], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 37, 38, 40, 44, 45, 78, 94, 108, 114, 115, 116, 117, 119, 120, 128, 129, 130, 132, 134, 140, 143, 145, 151, 152, 153, 168], "If": [6, 7, 13, 14, 17, 18, 20, 21, 23, 24, 25, 27, 28, 29, 31, 33, 35, 36, 39, 40, 41, 42, 49, 78, 94, 99, 104, 105, 106, 107, 108, 110, 112, 117, 124, 125, 128, 129, 130, 131, 132, 135, 136, 137, 138, 140, 141, 143, 149, 150, 156, 159, 163, 166, 167, 168, 169, 170, 171, 172], "don": [6, 7, 8, 150, 156, 166, 167, 168, 169, 170, 171, 173], "t": [6, 7, 8, 137, 150, 156, 166, 167, 168, 169, 170, 171, 173], "match": [6, 29, 31, 33, 40, 94, 117, 124, 163, 166, 168, 169, 171, 172], "up": [6, 8, 9, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 119, 122, 123, 125, 134, 157, 166, 167, 168, 170, 171, 172, 173], "exactli": [6, 117], "those": [6, 131, 172], "definit": [6, 172], "either": [6, 117, 128, 135, 154, 166, 172, 173], "run": [6, 7, 9, 12, 100, 102, 105, 108, 128, 129, 130, 132, 134, 144, 149, 150, 153, 163, 164, 167, 168, 170, 171, 172, 173], "explicit": 6, "error": [6, 7, 28, 128, 156, 166], "load": [6, 8, 29, 30, 31, 32, 33, 34, 116, 128, 129, 130, 132, 133, 149, 167, 168, 169, 171, 172], "rais": [6, 10, 13, 21, 25, 28, 37, 42, 94, 99, 102, 105, 107, 110, 116, 117, 128, 129, 130, 132, 137, 140, 143, 150, 152, 156, 159], "an": [6, 7, 8, 9, 10, 14, 28, 30, 34, 39, 41, 44, 45, 46, 47, 48, 99, 105, 107, 110, 111, 113, 114, 115, 122, 123, 124, 125, 127, 128, 129, 130, 132, 136, 138, 150, 157, 164, 166, 167, 168, 169, 170, 171, 172, 173], "except": [6, 20, 21, 121, 168], "wors": 6, "silent": [6, 100], "succe": 6, "infer": [6, 19, 29, 67, 99, 102, 104, 105, 106, 136, 162, 167, 169, 170, 171, 173], "expect": [6, 7, 10, 14, 17, 18, 23, 24, 29, 31, 33, 37, 40, 104, 117, 132, 150, 159, 167, 168, 172], "addit": [6, 7, 8, 10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 67, 116, 127, 128, 129, 130, 137, 138, 143, 146, 147, 149, 150, 154, 157, 164, 167, 170, 172], "line": [6, 8, 14, 133, 166, 168, 170, 171], "need": [6, 7, 8, 9, 18, 29, 32, 42, 99, 100, 105, 107, 146, 149, 150, 153, 163, 166, 167, 168, 169, 170, 171, 172, 173], "shape": [6, 46, 47, 48, 49, 99, 102, 104, 105, 106, 107, 110, 112, 122, 124, 126, 135, 157], "valu": [6, 7, 26, 42, 50, 51, 52, 60, 61, 68, 69, 70, 79, 80, 89, 90, 99, 102, 103, 105, 106, 109, 116, 128, 131, 132, 133, 135, 147, 148, 149, 150, 152, 156, 166, 168, 170, 171, 172], "two": [6, 7, 28, 47, 107, 122, 124, 164, 169, 170, 171, 172, 173], "popular": [6, 164, 168, 169], "llama2": [6, 7, 8, 10, 19, 29, 31, 33, 34, 35, 36, 38, 40, 42, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 100, 105, 106, 131, 162, 164, 166, 170, 171], "offici": [6, 19, 167, 170, 171], "implement": [6, 8, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 67, 86, 94, 100, 103, 104, 107, 109, 110, 111, 112, 128, 141, 149, 164, 168, 172, 173], "when": [6, 7, 8, 12, 30, 32, 34, 67, 99, 104, 105, 106, 107, 108, 109, 116, 123, 125, 135, 138, 149, 153, 166, 169, 171, 172, 173], "llama": [6, 19, 29, 67, 78, 103, 104, 128, 129, 166, 167, 169, 170, 171, 172], "websit": 6, "get": [6, 7, 8, 9, 29, 67, 86, 137, 139, 140, 142, 163, 164, 167, 168, 169, 170, 172], "access": [6, 7, 8, 30, 128, 134, 166, 169, 170], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 23, 24, 25, 26, 30, 32, 34, 47, 48, 49, 99, 107, 116, 128, 129, 130, 132, 134, 166, 167, 168, 169, 170, 171, 172, 173], "pth": [6, 169], "inspect": [6, 169, 172, 173], "content": [6, 15, 20, 25, 26, 29, 59, 67, 86, 94, 121, 167, 168], "easili": [6, 7, 164, 168, 172, 173], "torch": [6, 7, 30, 46, 47, 48, 101, 102, 105, 107, 108, 109, 110, 124, 125, 126, 130, 132, 134, 135, 136, 137, 140, 143, 144, 152, 153, 154, 155, 156, 157, 158, 159, 169, 170, 171, 172, 173], "import": [6, 7, 10, 37, 40, 44, 107, 149, 150, 167, 168, 169, 170, 172, 173], "00": [6, 161, 165, 170], "mmap": [6, 169], "true": [6, 7, 20, 31, 32, 34, 35, 36, 37, 39, 40, 41, 44, 49, 56, 57, 58, 59, 65, 66, 67, 75, 76, 77, 78, 84, 85, 86, 92, 93, 94, 98, 99, 105, 106, 108, 113, 118, 119, 121, 122, 124, 127, 128, 129, 130, 138, 140, 143, 144, 146, 149, 157, 158, 166, 167, 168, 169, 171, 172, 173], "weights_onli": [6, 130], "map_loc": [6, 169], "cpu": [6, 8, 108, 137, 157, 163, 166, 169, 173], "tensor": [6, 46, 47, 48, 49, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 124, 125, 126, 128, 135, 147, 148, 149, 150, 151, 152, 155, 172, 173], "item": 6, "print": [6, 9, 30, 35, 36, 39, 41, 42, 59, 67, 78, 86, 94, 107, 118, 119, 121, 135, 158, 167, 168, 170, 172, 173], "f": [6, 9, 35, 36, 39, 41, 167, 169, 172, 173], "tok_embed": [6, 105], "32000": [6, 10, 172], "4096": [6, 10, 31, 33, 34, 35, 36, 38, 40, 44, 45, 99, 104, 168, 172], "len": [6, 30, 35, 36, 39, 41, 105, 107], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 29, 30, 31, 32, 33, 39, 41, 42, 43, 46, 47, 48, 49, 53, 54, 55, 59, 63, 64, 67, 72, 73, 74, 78, 82, 83, 86, 94, 95, 101, 103, 104, 107, 108, 109, 110, 113, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 130, 133, 136, 137, 139, 141, 150, 155, 157, 158, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "contain": [6, 20, 25, 30, 32, 34, 44, 59, 67, 78, 81, 86, 94, 97, 99, 102, 104, 105, 106, 111, 114, 115, 116, 119, 121, 123, 128, 129, 130, 132, 133, 134, 140, 145, 149, 151, 152, 157, 167, 169, 171, 172], "input": [6, 14, 17, 18, 23, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 86, 94, 99, 100, 101, 103, 104, 105, 106, 107, 112, 118, 119, 122, 125, 126, 128, 130, 151, 152, 156, 159, 167, 168, 172, 173], "embed": [6, 46, 47, 48, 49, 99, 102, 103, 104, 105, 107, 138, 167, 171], "tabl": [6, 167, 173], "call": [6, 10, 20, 100, 107, 108, 116, 133, 147, 148, 149, 150, 153, 157, 167, 168, 172, 173], "layer": [6, 8, 49, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 105, 106, 107, 112, 116, 117, 127, 138, 164, 171, 172, 173], "have": [6, 7, 10, 47, 48, 49, 99, 102, 107, 111, 117, 122, 124, 130, 132, 133, 138, 146, 149, 159, 163, 167, 168, 169, 170, 171, 172, 173], "dim": [6, 99, 100, 103, 104, 105], "most": [6, 7, 123, 167, 170, 172, 173], "within": [6, 7, 10, 29, 32, 42, 46, 100, 107, 135, 149, 156, 157, 166, 168, 169, 171, 172, 173], "hug": [6, 16, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 81, 97, 109, 120, 164, 166, 170, 171], "face": [6, 16, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 81, 97, 109, 120, 164, 166, 170, 171], "hub": [6, 166, 168, 170], "default": [6, 7, 16, 20, 25, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 63, 64, 67, 68, 69, 70, 72, 73, 74, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 94, 95, 97, 99, 100, 103, 104, 105, 106, 108, 109, 110, 112, 116, 118, 119, 121, 128, 129, 130, 133, 135, 137, 142, 146, 147, 150, 151, 152, 155, 156, 157, 163, 166, 167, 168, 169, 171, 172, 173], "everi": [6, 8, 46, 47, 48, 100, 107, 149, 157, 163, 166, 173], "repo": [6, 128, 129, 131, 166, 169], "first": [6, 7, 10, 28, 32, 49, 102, 105, 107, 128, 133, 162, 164, 167, 168, 169, 171, 172, 173], "big": [6, 169], "split": [6, 32, 119, 167, 168, 169], "across": [6, 8, 30, 128, 149, 156, 169, 171], "bin": [6, 166, 169], "correctli": [6, 8, 13, 116, 128, 163, 167, 170, 173], "piec": 6, "one": [6, 8, 28, 59, 67, 86, 94, 100, 107, 110, 121, 124, 130, 167, 168, 169, 170, 171, 173], "pytorch_model": [6, 169], "00001": [6, 166], "00002": [6, 166], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 32, 38, 49, 107, 112, 114, 116, 118, 129, 130, 132, 133, 135, 137, 138, 140, 141, 146, 166, 168, 169, 170, 171, 172, 173], "doe": [6, 21, 25, 29, 32, 67, 96, 99, 105, 106, 111, 121, 128, 130, 132, 133, 166, 167, 169], "fewer": [6, 99], "sinc": [6, 7, 10, 100, 124, 125, 128, 130, 167, 169, 171], "instead": [6, 8, 32, 37, 40, 44, 49, 100, 102, 107, 112, 166, 169, 171, 172], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 42, 44, 45, 111, 115, 117, 119, 128, 129, 130, 131, 132, 133, 134, 135, 136, 147, 148, 149, 150, 159, 166, 167, 169, 171], "caus": [6, 86, 118, 125], "try": [6, 7, 167, 169, 170, 171, 173], "same": [6, 7, 46, 47, 53, 54, 55, 59, 63, 64, 67, 72, 73, 74, 82, 83, 86, 94, 95, 102, 106, 107, 121, 132, 133, 138, 150, 166, 167, 169, 171, 172, 173], "As": [6, 7, 8, 9, 112, 164, 169, 171, 173], "re": [6, 7, 130, 164, 167, 169, 170, 171, 172], "care": [6, 100, 128, 130, 169, 171, 172], "end": [6, 8, 20, 30, 34, 44, 78, 86, 119, 162, 164, 167, 171, 172], "number": [6, 8, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 99, 102, 105, 107, 109, 122, 123, 128, 129, 130, 135, 142, 156, 157, 166, 170, 172], "just": [6, 14, 164, 166, 167, 168, 170, 171, 172], "save": [6, 8, 9, 108, 128, 129, 130, 132, 138, 146, 150, 162, 166, 167, 168, 169, 171, 172], "less": [6, 42, 169, 170, 171, 173], "prone": 6, "manag": [6, 30, 113, 155, 167], "invari": 6, "accept": [6, 7, 42, 127, 168, 170, 173], "multipl": [6, 7, 8, 20, 29, 30, 99, 105, 106, 107, 112, 123, 124, 147, 148, 149, 150, 152, 157, 170, 171], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 167, 168, 169], "worri": [6, 167, 170], "explicitli": [6, 111, 164, 172], "convert": [6, 25, 26, 29, 128, 151, 167, 169, 173], "time": [6, 59, 67, 86, 94, 121, 147, 149, 157, 166, 167, 168, 169, 171, 173], "produc": [6, 132, 173], "back": [6, 28, 113, 128, 168, 172, 173], "form": [6, 7, 8, 28, 166], "One": [6, 169], "advantag": [6, 172], "being": [6, 128, 129, 130, 134, 136, 173], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 25, 26, 32, 37, 40, 44, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 99, 100, 107, 110, 111, 116, 117, 126, 127, 133, 145, 147, 148, 149, 150, 163, 164, 168, 169, 170, 171, 172, 173], "abl": [6, 8, 169, 170, 171], "post": [6, 107, 153, 157, 173], "tool": [6, 20, 168, 169, 170], "quantiz": [6, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 112, 130, 141, 162, 170, 173], "eval": [6, 162, 164], "without": [6, 7, 9, 14, 116, 124, 125, 163, 164, 167, 169, 172], "code": [6, 8, 50, 51, 52, 53, 54, 55, 56, 57, 58, 105, 160, 164, 168, 170], "chang": [6, 7, 9, 14, 130, 163, 169, 170, 171, 172, 173], "OR": [6, 25], "convers": [6, 15, 16, 19, 21, 25, 26, 28, 29, 37, 42, 128, 130, 131, 164, 167, 168, 169, 171, 172, 173], "script": [6, 9, 166, 169, 170, 171], "wai": [6, 7, 29, 116, 166, 167, 168, 169, 170, 171], "surround": [6, 8, 164], "load_checkpoint": [6, 8, 128, 129, 130, 131], "save_checkpoint": [6, 8, 9, 128, 129, 130], "convertor": 6, "avail": [6, 8, 45, 133, 136, 137, 144, 164, 166, 169, 171, 172], "here": [6, 7, 9, 14, 16, 17, 23, 24, 39, 103, 104, 166, 167, 168, 169, 170, 171, 172, 173], "three": [6, 8, 110, 170], "hfcheckpoint": 6, "read": [6, 128, 129, 130, 164], "write": [6, 8, 14, 128, 129, 130, 147, 167, 168, 170], "compat": [6, 128, 130], "transform": [6, 8, 29, 31, 33, 49, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 105, 106, 107, 109, 122, 123, 124, 125, 126, 154, 172], "framework": [6, 8, 164], "mention": [6, 169, 173], "assum": [6, 14, 17, 18, 23, 24, 31, 33, 40, 99, 104, 105, 106, 109, 114, 119, 132, 134, 137, 146, 169, 172], "checkpoint_dir": [6, 7, 128, 129, 130, 169, 171], "necessari": [6, 42, 147, 148, 149, 150, 167, 172], "easiest": [6, 169, 170], "sure": [6, 7, 169, 170, 171, 172, 173], "everyth": [6, 8, 133, 164, 170], "follow": [6, 8, 20, 25, 26, 29, 32, 99, 109, 122, 123, 130, 131, 132, 144, 150, 157, 162, 163, 166, 168, 169, 170, 171, 172, 173], "flow": [6, 29, 31, 32, 33, 173], "By": [6, 166, 171, 172, 173], "safetensor": [6, 128, 166], "output": [6, 18, 35, 36, 39, 42, 49, 53, 54, 55, 72, 73, 74, 82, 83, 87, 88, 95, 99, 100, 101, 103, 104, 105, 106, 107, 112, 115, 116, 117, 122, 125, 130, 135, 138, 148, 157, 163, 166, 167, 168, 169, 170, 171, 172, 173], "dir": [6, 150, 163, 166, 169, 170, 171], "output_dir": [6, 7, 128, 129, 130, 157, 169, 171, 172, 173], "argument": [6, 7, 10, 18, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 99, 127, 133, 138, 143, 147, 149, 150, 154, 166, 167, 168, 171, 172], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 105, 157, 166, 168, 169, 172, 173], "_component_": [6, 7, 9, 10, 37, 40, 44, 167, 168, 169, 171, 172], "fullmodelhfcheckpoint": [6, 169], "directori": [6, 7, 128, 129, 130, 147, 149, 150, 157, 166, 169, 170, 171], "sort": [6, 128, 130], "so": [6, 7, 32, 107, 124, 128, 133, 163, 164, 167, 169, 170, 171, 172, 173], "order": [6, 8, 128, 130, 149, 150, 170], "matter": [6, 128, 130, 166, 172], "checkpoint_fil": [6, 7, 9, 128, 129, 130, 169, 171, 172, 173], "restart": [6, 166], "previou": [6, 32, 128, 129, 130], "more": [6, 7, 8, 37, 42, 67, 102, 104, 107, 116, 127, 130, 133, 150, 154, 156, 164, 166, 168, 169, 170, 171, 172, 173], "next": [6, 32, 49, 107, 122, 135, 171, 173], "section": [6, 8, 140, 162, 169, 171, 173], "recipe_checkpoint": [6, 128, 129, 130], "null": [6, 7], "usual": [6, 104, 128, 150, 166, 169, 172], "model_typ": [6, 128, 129, 130, 169, 171], "resume_from_checkpoint": [6, 128, 129, 130], "fals": [6, 7, 20, 25, 26, 29, 31, 32, 35, 36, 37, 39, 40, 41, 42, 44, 49, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 98, 99, 105, 106, 112, 113, 116, 118, 124, 128, 129, 130, 144, 157, 166, 167, 168, 169, 171, 172, 173], "requir": [6, 7, 30, 34, 42, 44, 67, 128, 130, 132, 143, 144, 146, 149, 150, 152, 156, 157, 163, 166, 167, 168, 170, 173], "param": [6, 8, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 112, 114, 115, 117, 128, 146, 172, 173], "directli": [6, 7, 8, 10, 37, 40, 44, 127, 128, 166, 169, 170, 171, 172, 173], "ensur": [6, 7, 13, 28, 42, 99, 128, 130, 137, 164, 168, 170], "out": [6, 7, 8, 29, 31, 35, 36, 37, 39, 41, 122, 128, 129, 162, 164, 166, 167, 169, 170, 171, 172, 173], "case": [6, 8, 9, 20, 47, 48, 49, 107, 128, 132, 137, 141, 146, 147, 154, 164, 166, 167, 168, 169, 171, 172, 173], "discrep": [6, 128], "along": [6, 171, 172], "found": [6, 7, 9, 103, 104, 166, 172, 173], "metacheckpoint": 6, "github": [6, 10, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 99, 103, 104, 109, 110, 116, 163, 168, 170], "repositori": [6, 19, 169, 170], "fullmodelmetacheckpoint": [6, 171], "torchtunecheckpoint": 6, "current": [6, 32, 96, 99, 102, 104, 105, 106, 129, 130, 138, 142, 147, 149, 153, 156, 169, 170, 171], "test": [6, 7, 8, 164, 167], "complet": [6, 8, 14, 32, 38, 97, 167, 168, 169, 170, 171], "written": [6, 7, 8, 128, 129, 147, 148, 149, 150, 164], "begin": [6, 32, 67, 86, 107, 119, 167, 171, 173], "partit": [6, 128, 173], "ha": [6, 67, 86, 107, 111, 113, 114, 117, 130, 132, 159, 168, 169, 170, 171, 172, 173], "standard": [6, 17, 25, 99, 148, 164, 167, 169, 171], "key_1": [6, 130], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 169], "inform": [6, 150, 154, 164, 166, 169, 170, 171], "subsequ": [6, 8, 107, 122], "recipe_st": [6, 128, 129, 130], "pt": [6, 9, 128, 129, 130, 169, 171], "epoch": [6, 8, 9, 109, 128, 129, 130, 166, 167, 169, 170, 171], "optim": [6, 7, 8, 30, 67, 96, 109, 110, 130, 132, 134, 140, 152, 153, 157, 167, 169, 170, 171, 172, 173], "etc": [6, 8, 128, 140, 170], "prevent": [6, 32, 166], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 105, 111, 132, 153, 156, 157, 167, 168, 169, 172, 173], "updat": [6, 7, 8, 102, 132, 157, 163, 167, 169, 170, 171, 172, 173], "hf_model_0001_0": [6, 169], "hf_model_0002_0": [6, 169], "both": [6, 30, 117, 166, 169, 172, 173], "adapt": [6, 111, 112, 113, 114, 115, 128, 129, 130, 167, 169, 172, 173], "merg": [6, 10, 11, 128, 169, 171, 173], "would": [6, 7, 9, 32, 105, 107, 163, 167, 168, 169, 172, 173], "primari": [6, 7, 8, 170], "want": [6, 7, 8, 9, 10, 29, 123, 124, 135, 163, 166, 167, 168, 169, 170, 171, 172], "resum": [6, 8, 109, 128, 129, 130, 173], "initi": [6, 8, 12, 30, 32, 50, 51, 52, 60, 61, 68, 69, 70, 79, 80, 89, 90, 132, 143, 144, 170, 172, 173], "frozen": [6, 172, 173], "base": [6, 10, 20, 31, 33, 42, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 104, 109, 110, 112, 113, 115, 116, 117, 128, 133, 136, 138, 146, 147, 162, 167, 169, 170, 171, 172, 173], "well": [6, 7, 8, 164, 166, 168, 169, 171, 173], "learnt": [6, 167, 169], "someth": [6, 8, 9, 167, 169], "NOT": 6, "refer": [6, 7, 8, 103, 104, 107, 110, 113, 164, 172], "adapter_checkpoint": [6, 128, 129, 130], "adapter_0": [6, 169], "now": [6, 132, 134, 167, 168, 169, 170, 171, 172, 173], "knowledg": 6, "creat": [6, 7, 10, 32, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 92, 93, 95, 96, 98, 102, 107, 109, 127, 128, 129, 130, 134, 147, 149, 166, 167, 168, 169, 171, 173], "simpl": [6, 8, 14, 17, 23, 24, 107, 162, 168, 170, 172, 173], "forward": [6, 8, 46, 47, 48, 99, 100, 101, 103, 104, 105, 106, 107, 110, 112, 140, 157, 171, 172, 173], "modeltyp": [6, 128, 129, 130], "llama2_13b": [6, 72], "right": [6, 128, 169, 171, 172], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 116, 132, 172], "successfulli": [6, 166, 170], "vocab": [6, 10, 105, 171], "70": [6, 79], "x": [6, 46, 47, 48, 99, 100, 101, 103, 104, 105, 106, 107, 112, 135, 155, 172, 173], "randint": 6, "0": [6, 8, 32, 49, 53, 54, 55, 56, 57, 58, 59, 67, 72, 73, 74, 75, 76, 77, 86, 94, 99, 105, 107, 109, 110, 112, 121, 124, 135, 149, 150, 151, 152, 156, 158, 161, 165, 167, 168, 169, 170, 171, 172, 173], "no_grad": 6, "6": [6, 32, 103, 107, 126, 151, 152, 169, 173], "3989": 6, "9": [6, 107, 152, 169, 173], "0531": 6, "3": [6, 32, 49, 78, 95, 96, 97, 107, 124, 125, 126, 131, 133, 139, 151, 152, 155, 166, 167, 169, 170, 171, 173], "2375": 6, "5": [6, 7, 14, 107, 109, 110, 124, 151, 152, 169, 170, 171], "2822": 6, "4": [6, 7, 42, 49, 99, 107, 123, 151, 152, 158, 164, 166, 168, 169, 171, 172, 173], "4872": 6, "7469": 6, "8": [6, 35, 36, 39, 41, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 107, 152, 169, 172, 173], "6737": 6, "11": [6, 107, 152, 169, 171, 173], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 107, 122, 151, 152], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 107], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 20, 29, 31, 33, 40, 42, 116, 121, 150, 166, 167, 168, 169, 170, 171, 172], "find": [6, 8, 9, 166, 169, 170, 172], "list": [6, 7, 15, 16, 19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 49, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 98, 107, 111, 112, 116, 117, 118, 119, 121, 122, 123, 124, 128, 129, 130, 133, 135, 139, 151, 152, 167, 168, 170, 171], "builder": [6, 38, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 92, 93, 95, 96, 98, 167, 168, 173], "hope": 6, "deeper": [6, 170], "insight": [6, 169], "happi": [6, 169], "thi": [7, 8, 9, 10, 17, 20, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 44, 45, 47, 48, 49, 67, 78, 86, 94, 96, 97, 99, 100, 104, 105, 106, 107, 108, 109, 111, 113, 116, 117, 118, 119, 121, 122, 127, 128, 129, 130, 132, 133, 135, 136, 137, 140, 144, 146, 147, 149, 150, 152, 153, 154, 156, 162, 163, 164, 166, 167, 168, 169, 170, 171, 172, 173], "pars": [7, 10, 11, 120, 133, 167, 170], "effect": 7, "cli": [7, 9, 11, 12, 163, 169, 170], "prerequisit": [7, 167, 168, 169, 170, 171, 172, 173], "Be": [7, 167, 169, 170, 171, 172, 173], "familiar": [7, 167, 169, 170, 171, 172, 173], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 163, 167, 168, 170], "instal": [7, 9, 144, 149, 150, 162, 166, 169, 170, 171, 172, 173], "fundament": 7, "There": [7, 28, 47, 124, 167, 169, 170, 171, 172], "entri": [7, 8, 170], "point": [7, 8, 25, 26, 121, 168, 169, 170, 171, 172, 173], "locat": [7, 166, 171, 172, 173], "thei": [7, 8, 30, 49, 105, 107, 117, 133, 138, 166, 167, 168, 172], "truth": [7, 169, 171], "reproduc": 7, "overridden": [7, 100, 133, 157], "quick": [7, 30], "experiment": 7, "serv": [7, 121, 127, 168, 172], "particular": [7, 29, 30, 42, 127, 168, 172, 173], "seed": [7, 8, 9, 156, 170], "shuffl": [7, 32], "devic": [7, 8, 116, 132, 136, 137, 140, 166, 167, 169, 170, 171, 172], "cuda": [7, 136, 137, 140, 157, 163, 169, 173], "dtype": [7, 8, 102, 105, 108, 137, 155, 159, 169, 173], "fp32": [7, 173], "enable_fsdp": 7, "mani": [7, 32, 168, 169], "object": [7, 10, 11, 15, 16, 19, 21, 49, 99, 127, 141, 167], "keyword": [7, 10, 29, 31, 33, 34, 37, 38, 40, 42, 44, 45, 108, 167, 168], "loss": [7, 8, 20, 31, 35, 36, 39, 41, 110, 170, 172, 173], "exampl": [7, 8, 9, 10, 12, 14, 17, 23, 24, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 49, 59, 67, 78, 86, 94, 99, 107, 110, 111, 113, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 131, 132, 135, 141, 149, 150, 151, 152, 155, 158, 160, 161, 163, 165, 166, 167, 168, 169, 171, 172, 173], "subfield": 7, "dotpath": 7, "wish": [7, 168], "exact": [7, 10, 169], "path": [7, 8, 9, 10, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 62, 67, 71, 78, 81, 86, 91, 94, 97, 118, 119, 120, 128, 129, 130, 133, 157, 166, 167, 168, 169, 171, 172], "normal": [7, 29, 32, 67, 86, 101, 103, 105, 106, 118, 167, 168, 172, 173], "python": [7, 133, 139, 150, 156, 160, 166, 169], "alpaca_dataset": [7, 35, 168], "custom": [7, 8, 29, 31, 33, 37, 40, 44, 154, 164, 166, 169, 170, 171, 172], "train_on_input": [7, 25, 26, 29, 31, 35, 36, 37, 39, 40, 41, 42, 167, 168], "onc": [7, 113, 169, 170, 171, 172, 173], "ve": [7, 102, 167, 168, 169, 171, 172], "instanc": [7, 10, 30, 100, 108, 114, 115, 172], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 37, 166, 169, 173], "under": [7, 157, 168, 169, 171, 173], "preced": [7, 10, 166, 171, 172], "actual": [7, 9, 14, 17, 23, 24, 29, 167], "throw": 7, "notic": [7, 46, 47, 48, 107, 167, 168, 172], "miss": [7, 116, 117, 157, 172], "posit": [7, 10, 32, 46, 47, 48, 49, 99, 102, 104, 105, 106, 107, 171], "anoth": [7, 169], "handl": [7, 12, 30, 67, 86, 118, 119, 167, 169, 172, 173], "def": [7, 8, 9, 12, 127, 131, 167, 168, 172, 173], "dictconfig": [7, 8, 10, 11, 12, 13, 150, 157], "arg": [7, 10, 48, 101, 105, 108, 111, 133, 148, 157], "tupl": [7, 10, 30, 42, 59, 67, 78, 86, 94, 102, 107, 108, 110, 121, 123, 124, 125, 127, 133, 142, 151, 152, 157, 159], "kwarg": [7, 10, 101, 108, 111, 133, 143, 147, 148, 149, 150, 154, 157, 168], "str": [7, 10, 11, 14, 17, 18, 20, 23, 24, 25, 26, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 62, 67, 71, 78, 81, 86, 91, 94, 97, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 128, 129, 130, 132, 133, 136, 137, 139, 140, 141, 143, 145, 147, 148, 149, 150, 151, 152, 156, 157, 158, 159, 167, 168], "mean": [7, 99, 103, 105, 106, 146, 166, 167, 168, 170, 172], "pass": [7, 10, 20, 29, 30, 31, 33, 34, 37, 38, 40, 44, 45, 99, 100, 108, 113, 117, 119, 127, 130, 137, 138, 140, 143, 146, 149, 150, 154, 157, 166, 167, 168, 172, 173], "add": [7, 9, 29, 32, 34, 44, 67, 107, 121, 130, 131, 133, 168, 169, 171, 172, 173], "d": [7, 20, 99, 102, 105, 166, 167, 172], "llama2_token": [7, 167, 169], "tmp": [7, 132, 167, 170, 171], "llama2token": [7, 71], "option": [7, 8, 14, 17, 18, 23, 24, 27, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 53, 54, 55, 59, 63, 64, 67, 72, 73, 74, 78, 81, 82, 83, 86, 87, 88, 94, 95, 97, 99, 104, 105, 106, 107, 108, 116, 117, 118, 121, 124, 125, 128, 129, 130, 135, 136, 137, 139, 141, 147, 150, 156, 157, 163, 164, 166, 168, 169], "modeltoken": [7, 20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 121, 167, 168], "bool": [7, 20, 25, 26, 29, 31, 32, 34, 35, 36, 37, 39, 40, 41, 42, 44, 49, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 98, 108, 112, 116, 117, 118, 119, 121, 124, 127, 128, 129, 130, 138, 140, 143, 144, 146, 149, 154, 157, 158, 167, 173], "max_seq_len": [7, 10, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 67, 78, 86, 94, 99, 102, 104, 105, 121, 167, 168], "int": [7, 9, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 98, 99, 102, 103, 104, 105, 107, 109, 112, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 135, 138, 142, 146, 147, 148, 149, 150, 151, 152, 154, 156, 157, 166, 167, 168, 172, 173], "512": [7, 35, 36, 49, 168, 173], "instructdataset": [7, 35, 36, 39, 40, 41, 168], "alreadi": [7, 131, 143, 146, 163, 166, 168, 169, 172], "overwrit": [7, 130, 163], "duplic": [7, 8, 164, 166], "sometim": 7, "than": [7, 28, 42, 99, 102, 107, 127, 130, 131, 158, 159, 167, 168, 169, 170, 171, 172, 173], "resolv": [7, 11, 170], "alpaca": [7, 14, 35, 36, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 168], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 147, 148, 149, 150], "disklogg": 7, "log_dir": [7, 147, 149, 150], "conveni": [7, 8, 166], "verifi": [7, 136, 137, 138, 167, 170, 172], "properli": [7, 116, 144, 166], "experi": [7, 150, 162, 164, 167, 171, 172], "wa": [7, 47, 48, 49, 107, 116, 167, 169, 171, 172, 173], "cp": [7, 163, 166, 167, 169, 170, 171], "7b_lora_single_devic": [7, 169, 170, 172, 173], "my_config": 7, "discuss": [7, 170, 172], "guidelin": 7, "while": [7, 8, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 100, 164, 169, 173], "mai": [7, 9, 107, 138, 167, 168, 170, 172], "tempt": 7, "put": [7, 8, 170, 172], "much": [7, 169, 171, 172, 173], "give": [7, 168, 172], "maximum": [7, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 49, 78, 99, 102, 104, 105, 123, 124, 125, 166], "flexibl": [7, 30, 168], "switch": 7, "encourag": [7, 67, 172], "clariti": 7, "significantli": 7, "easier": [7, 169, 170], "dont": 7, "slimorca_dataset": 7, "privat": 7, "expos": [7, 8, 130, 167, 170], "parent": [7, 166], "modul": [7, 10, 46, 47, 48, 49, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 131, 134, 138, 146, 153, 154, 156, 170, 172, 173], "__init__": [7, 8, 172, 173], "py": [7, 10, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 99, 102, 103, 104, 109, 110, 166, 169, 171], "guarante": 7, "stabil": [7, 164, 173], "underscor": 7, "_alpaca": 7, "collect": [7, 135, 170], "itself": 7, "via": [7, 9, 37, 40, 44, 112, 128, 172, 173], "k1": [7, 8], "v1": [7, 8, 45], "k2": [7, 8], "v2": [7, 8, 168], "lora_finetune_single_devic": [7, 166, 167, 169, 170, 171, 172, 173], "checkpoint": [7, 8, 108, 119, 128, 129, 130, 131, 132, 150, 154, 164, 166, 171, 172, 173], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 29, 30, 31, 32, 33, 34, 37, 40, 46, 47, 48, 59, 67, 78, 86, 94, 99, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 114, 115, 118, 119, 122, 128, 129, 130, 131, 132, 133, 147, 148, 149, 150, 167, 168, 170, 172, 173], "assign": [7, 34], "nest": 7, "dot": 7, "notat": [7, 99, 104, 105], "certain": [7, 157, 167], "flag": [7, 8, 20, 31, 35, 36, 39, 41, 127, 130, 138, 166, 173], "built": [7, 9, 43, 163, 167, 170, 173], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 29], "pytorch": [7, 8, 67, 105, 108, 116, 127, 144, 149, 154, 156, 157, 162, 163, 164, 171, 172, 173], "llama3": [7, 29, 42, 78, 79, 80, 81, 82, 83, 84, 85, 131, 135, 138, 162, 166, 168], "8b_full": [7, 166, 168], "adamw": [7, 172], "lr": [7, 109], "2e": 7, "fuse": [7, 153], "nproc_per_nod": [7, 168, 171, 172], "full_finetune_distribut": [7, 166, 168, 169, 170], "core": [8, 164, 168, 170, 173], "i": [8, 19, 20, 21, 99, 105, 106, 107, 108, 115, 132, 135, 168, 169, 171, 173], "structur": [8, 15, 16, 19, 21, 25, 26, 29, 37, 81, 97, 122, 167, 168, 169], "new": [8, 38, 89, 102, 131, 147, 149, 167, 169, 170, 171, 172, 173], "user": [8, 15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 67, 86, 94, 99, 121, 125, 167, 168, 170], "thought": [8, 164, 170, 173], "target": [8, 164], "pipelin": [8, 164], "llm": [8, 162, 164, 168, 169, 172], "eg": [8, 105, 128, 164], "meaning": [8, 164, 169], "featur": [8, 9, 163, 164, 169, 170], "fsdp": [8, 127, 132, 138, 146, 164, 170, 171], "activ": [8, 100, 140, 145, 154, 157, 164, 173], "gradient": [8, 146, 153, 157, 164, 169, 171, 172, 173], "accumul": [8, 153, 157, 164], "mix": [8, 101, 166, 168, 169], "precis": [8, 101, 108, 137, 164, 170, 173], "appli": [8, 29, 31, 33, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 103, 104, 105, 106, 116, 117, 154, 164, 173], "given": [8, 10, 14, 17, 18, 23, 24, 28, 112, 113, 135, 136, 137, 141, 146, 153, 158, 164, 172], "complex": 8, "becom": [8, 107, 163, 168], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 105, 107, 131, 166, 168], "methodolog": 8, "reason": [8, 135, 169], "possibl": [8, 29, 32, 37, 123, 124, 166, 168], "trade": 8, "off": [8, 67, 86, 169], "memori": [8, 30, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 108, 116, 138, 140, 145, 146, 157, 162, 164, 169, 170, 171], "vs": [8, 170], "qualiti": [8, 169, 172], "believ": 8, "best": [8, 124, 167], "suit": [8, 170], "b": [8, 99, 102, 104, 105, 106, 112, 146, 150, 172, 173], "fit": [8, 29, 31, 32, 33, 34, 35, 36, 38, 40, 44, 45, 107, 124, 125, 168], "solut": 8, "result": [8, 49, 59, 67, 86, 94, 107, 121, 122, 157, 169, 171, 172, 173], "meant": [8, 108, 132], "depend": [8, 9, 14, 128, 157, 166, 168, 169, 172, 173], "level": [8, 134, 139, 146, 164, 173], "expertis": 8, "routin": 8, "yourself": [8, 166, 171, 172], "exist": [8, 163, 166, 169, 170, 171, 173], "ad": [8, 46, 47, 48, 86, 107, 118, 130, 131, 167, 172, 173], "ones": 8, "modular": [8, 164], "build": [8, 37, 40, 44, 49, 164, 171, 172], "block": [8, 32, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 116, 117, 164], "wandb": [8, 9, 150, 170], "log": [8, 11, 110, 139, 140, 145, 147, 148, 149, 150, 169, 170, 171, 173], "fulli": [8, 30], "nativ": [8, 162, 164, 172, 173], "correct": [8, 17, 39, 103, 104, 105, 136, 164, 167, 168], "numer": [8, 164], "pariti": [8, 164], "verif": 8, "extens": [8, 130, 164], "comparison": [8, 172, 173], "benchmark": [8, 156, 164, 169, 171, 172], "limit": [8, 124, 125, 132, 168], "hidden": [8, 49, 100, 107], "behind": 8, "100": [8, 31, 35, 36, 39, 41, 42, 135, 151, 152, 172, 173], "prefer": [8, 23, 43, 110, 152, 164, 166, 168], "over": [8, 109, 133, 164, 169, 171, 172, 173], "unnecessari": 8, "abstract": [8, 15, 18, 164, 170, 173], "No": [8, 130, 164], "inherit": [8, 133, 164, 168], "go": [8, 19, 21, 49, 59, 67, 86, 94, 107, 121, 164, 168, 169, 170, 173], "upon": [8, 30, 171], "figur": [8, 172, 173], "spectrum": 8, "decid": 8, "interact": [8, 162, 170], "start": [8, 9, 30, 121, 131, 163, 164, 167, 168, 169, 170], "paradigm": 8, "consist": [8, 45, 170], "configur": [8, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 78, 94, 106, 164, 167, 170, 171, 172, 173], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 94, 95, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 134, 135, 136, 137, 138, 139, 140, 141, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 162, 164, 166, 167, 168, 169, 170, 171, 172, 173], "command": [8, 9, 133, 163, 166, 167, 168, 169, 170, 171, 172, 173], "overrid": [8, 11, 12, 166, 169, 170, 171, 173], "togeth": [8, 32, 150, 170, 172], "valid": [8, 28, 116, 117, 159, 163, 169, 170], "environ": [8, 136, 144, 163, 166, 169, 170], "logic": [8, 131, 164, 170, 172], "api": [8, 9, 25, 56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 116, 166, 167, 169, 170, 171, 173], "closer": [8, 172], "monolith": [8, 164], "trainer": [8, 110], "A": [8, 9, 25, 26, 30, 32, 49, 59, 67, 86, 94, 99, 105, 106, 107, 108, 110, 112, 116, 118, 119, 121, 122, 124, 127, 132, 133, 140, 141, 145, 146, 151, 152, 161, 162, 165, 166, 167, 169, 172, 173], "wrapper": [8, 101, 118, 119, 132, 134, 166, 172], "around": [8, 29, 67, 86, 101, 118, 119, 140, 166, 167, 169, 172, 173], "extern": [8, 168], "primarili": [8, 30, 172], "eleutherai": [8, 164, 172], "har": [8, 164, 172], "control": [8, 31, 35, 36, 39, 41, 113, 156, 169], "multi": [8, 29, 99, 116, 171], "stage": [8, 107], "distil": 8, "oper": [8, 30, 107, 113, 156], "turn": [8, 20, 28, 29, 167], "dataload": [8, 32, 35, 36, 39, 41], "applic": [8, 99, 128, 129, 150], "clean": [8, 9, 35], "after": [8, 94, 99, 102, 103, 105, 106, 116, 146, 147, 148, 149, 150, 167, 173], "process": [8, 9, 49, 107, 108, 142, 143, 156, 168, 170, 173], "group": [8, 99, 142, 143, 147, 148, 149, 150, 166, 171], "init_process_group": [8, 143], "backend": [8, 166], "gloo": 8, "els": [8, 133, 150, 164, 173], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 30, 130, 133, 138, 157, 168, 170, 171, 172], "stuff": 8, "carri": 8, "relev": [8, 166, 169, 172], "interfac": [8, 15, 18, 30, 111], "metric": [8, 170], "logger": [8, 139, 145, 147, 148, 149, 150, 170], "self": [8, 9, 32, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 99, 105, 106, 111, 116, 117, 128, 131, 132, 168, 172, 173], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 127, 138, 146, 154, 167], "_model": [8, 132], "_setup_model": 8, "_token": [8, 168], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 132, 134, 153, 157, 173], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 110, 156, 166, 171], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 32], "batch": [8, 32, 35, 36, 39, 41, 47, 99, 102, 104, 105, 107, 110, 151, 152, 157, 164, 168, 170, 171, 172], "enumer": 8, "_autocast": 8, "logit": [8, 135], "label": [8, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 110, 151, 152], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 147, 148, 149, 150], "step": [8, 32, 105, 109, 134, 147, 148, 149, 150, 153, 157, 162, 169, 172, 173], "learn": [8, 30, 109, 164, 167, 168, 170, 171, 172, 173], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 23, 24, 27, 28, 29, 31, 32, 33, 34, 37, 38, 40, 42, 44, 45, 49, 59, 67, 78, 81, 86, 94, 97, 99, 102, 104, 105, 106, 107, 113, 115, 116, 117, 118, 121, 125, 128, 129, 130, 131, 135, 136, 137, 139, 141, 145, 147, 148, 149, 150, 153, 154, 155, 156, 157, 159, 167, 168, 169], "fullfinetunerecip": 8, "wandblogg": [9, 172, 173], "workspac": 9, "seen": [9, 172, 173], "screenshot": 9, "below": [9, 14, 104, 127, 168, 171, 172, 173], "packag": [9, 149, 150, 163], "pip": [9, 149, 150, 163, 169, 171], "Then": [9, 113, 170], "login": [9, 150, 166, 169], "project": [9, 49, 53, 54, 55, 72, 73, 74, 82, 83, 87, 88, 95, 99, 100, 107, 116, 117, 138, 150, 162, 172, 173], "grab": [9, 171], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 47, 48, 49, 107, 144, 167], "exit": [9, 163, 166], "resourc": [9, 147, 148, 149, 150], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 29, 31, 32, 33, 34, 40, 42, 44, 99, 104, 105, 106, 107, 122, 135, 167, 169], "desir": [9, 29, 125, 155, 167], "suggest": 9, "approach": [9, 30, 168], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 169], "_output_dir": [9, 128, 129, 130], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 157], "type": [9, 10, 12, 20, 25, 26, 27, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 107, 108, 110, 112, 114, 118, 119, 120, 121, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 146, 152, 154, 155, 156, 157, 158, 168, 169, 172, 173], "descript": [9, 37, 42, 166], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 25, 26, 29, 32, 35, 36, 39, 41, 145, 168], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 29, 31, 33, 34, 37, 38, 40, 44, 45, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 89, 90, 92, 93, 95, 96, 97, 98, 99, 103, 104, 107, 109, 110, 116, 122, 127, 128, 129, 133, 139, 144, 149, 150, 154, 156, 163, 168, 169], "com": [10, 53, 54, 55, 63, 64, 67, 72, 73, 74, 78, 82, 83, 95, 99, 103, 104, 109, 110, 116, 163], "facebookresearch": [10, 103, 104], "blob": [10, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95, 97, 99, 103, 104, 109, 110], "main": [10, 12, 67, 97, 99, 103, 104, 163, 169, 171], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 49, 105, 107], "32": [10, 107, 171, 172, 173], "num_head": [10, 49, 99, 102, 104, 105], "num_kv_head": [10, 99, 102], "vocab_s": 10, "must": [10, 30, 111, 133, 173], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 59, 60, 61, 62, 63, 64, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 99, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 151, 152, 155, 156, 157, 158, 167, 168, 172, 173], "nn": [10, 99, 100, 101, 102, 105, 106, 107, 108, 111, 113, 114, 115, 127, 134, 146, 153, 154, 159, 172, 173], "parsed_yaml": 10, "embed_dim": [10, 46, 47, 48, 49, 99, 104, 106, 107, 172], "valueerror": [10, 21, 25, 28, 37, 42, 94, 99, 102, 105, 107, 110, 128, 129, 130, 137, 140, 156, 159], "recipe_nam": 11, "rank": [11, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 112, 142, 144, 156, 170, 172, 173], "zero": [11, 102, 103, 169, 171], "displai": 11, "callabl": [12, 29, 31, 33, 105, 113, 127, 135, 138, 141, 146, 154], "With": [12, 169, 172, 173], "my_recip": 12, "foo": 12, "bar": [12, 164, 170], "instanti": [13, 50, 51, 52, 53, 54, 55, 60, 61, 62, 63, 64, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 87, 88, 89, 90, 91, 95, 96, 97, 132], "configerror": 13, "cannot": [13, 130, 171], "data": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 30, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 67, 107, 140, 147, 148, 149, 150, 168, 169, 173], "templat": [14, 17, 18, 23, 24, 29, 31, 33, 35, 36, 37, 39, 40, 41, 42, 67], "style": [14, 32, 35, 36, 37, 42, 173], "slightli": 14, "describ": [14, 67, 78, 154, 168], "task": [14, 24, 30, 38, 167, 168, 169, 171, 172, 173], "further": [14, 107, 166, 168, 172, 173], "context": [14, 16, 96, 113, 155, 157, 168], "respons": [14, 16, 59, 67, 86, 94, 110, 121, 168, 169, 170, 171], "appropri": [14, 16, 19, 20, 21, 30, 109, 128, 168, 173], "request": [14, 137, 168, 169], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 168], "map": [14, 17, 18, 23, 24, 25, 26, 29, 30, 31, 32, 33, 40, 78, 94, 115, 119, 120, 128, 132, 134, 147, 148, 149, 150, 153, 157, 167, 168, 169, 172], "column_map": [14, 17, 18, 23, 24, 29, 31, 33, 40, 168], "placehold": [14, 17, 18, 23, 24, 29, 31, 33, 40, 168], "column": [14, 17, 18, 23, 24, 29, 31, 33, 34, 40, 44, 99, 105, 106, 167, 168], "ident": [14, 17, 18, 21, 23, 24, 31, 32, 33, 40, 169], "poem": 14, "n": [14, 23, 59, 67, 86, 94, 99, 107, 121, 124, 161, 165, 167, 168], "nwrite": 14, "long": [14, 32, 119, 167, 172], "where": [14, 17, 23, 24, 29, 30, 35, 36, 39, 41, 47, 67, 86, 99, 105, 107, 112, 118, 122, 124, 138, 146, 152, 168], "me": 14, "tag": [15, 16, 19, 21, 29, 147, 148, 149, 150, 167], "system": [15, 16, 19, 20, 21, 22, 25, 26, 28, 29, 59, 67, 86, 94, 121, 167, 168], "assist": [15, 16, 19, 20, 22, 25, 26, 28, 29, 59, 67, 86, 94, 97, 121, 135, 167, 168], "role": [15, 20, 25, 26, 29, 59, 67, 86, 94, 121, 167, 168], "prepend": [15, 67, 78, 86, 118], "append": [15, 78, 86, 94, 118, 163], "accord": [15, 21, 167], "openai": [16, 25, 37, 168], "markup": 16, "languag": [16, 112, 135, 172], "It": [16, 20, 21, 107, 166, 167, 168, 173], "im_start": 16, "im_end": 16, "goe": [16, 113], "grammar": [17, 39, 168], "english": 17, "sentenc": [17, 32, 86], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 172], "lazi": 17, "dog": [17, 122], "alwai": [18, 133], "human": [19, 26, 167], "pre": [19, 32, 67, 107, 163, 167, 168], "taken": [19, 172, 173], "inst": [19, 21, 29, 67, 167, 168], "sy": [19, 67, 167, 168], "respect": [19, 30, 115, 124, 157, 167, 168], "honest": [19, 167, 168], "am": [19, 21, 167, 168, 169, 171], "pari": [19, 21, 24, 168], "capit": [19, 21, 23, 24, 168], "franc": [19, 21, 23, 24, 168], "known": [19, 21, 67, 86, 141, 168], "its": [19, 21, 32, 99, 104, 105, 106, 153, 156, 166, 168, 169, 171, 172], "stun": [19, 21, 168], "liter": [20, 22, 53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 116, 117], "ipython": [20, 22], "union": [20, 117, 147, 148, 149, 150, 154, 156], "mask": [20, 31, 32, 35, 36, 39, 41, 59, 67, 78, 86, 94, 99, 105, 106, 121, 122, 167, 168], "eot": [20, 78], "repres": [20, 46, 47, 107, 124, 152, 167], "individu": [20, 32, 140, 150, 154, 167, 168], "interleav": [20, 122], "tokenize_messag": [20, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 59, 67, 78, 86, 94, 121, 167, 168], "attach": 20, "special": [20, 29, 67, 78, 81, 86, 94, 97, 107, 119, 120, 121, 122, 132, 168], "variabl": [20, 29, 30, 31, 33, 40, 144, 173], "writer": 20, "dictionari": [20, 32, 140, 145, 147, 148, 149, 150, 152, 169], "hello": [20, 24, 59, 67, 78, 86, 94, 118, 119, 167, 169, 171], "world": [20, 59, 67, 78, 86, 94, 118, 119, 142, 144, 169], "whether": [20, 25, 26, 29, 31, 34, 35, 36, 37, 39, 40, 41, 42, 44, 53, 54, 55, 63, 64, 72, 73, 74, 78, 82, 83, 86, 87, 88, 94, 95, 108, 112, 116, 117, 118, 119, 127, 137, 140, 167, 168], "calcul": [20, 99, 105, 107, 124, 171], "correspond": [20, 111, 114, 137, 152, 170, 171], "consecut": [20, 28, 122], "e": [20, 46, 47, 48, 49, 99, 107, 108, 111, 115, 122, 124, 128, 132, 140, 157, 163, 169, 171, 172, 173], "properti": [20, 133, 172], "contains_media": 20, "non": [20, 117], "from_dict": [20, 167], "construct": [20, 122, 172], "text_cont": 20, "mistral": [21, 29, 42, 86, 87, 88, 89, 90, 91, 92, 93, 131, 166, 167, 169, 170], "llama2chatformat": [21, 67, 167, 168], "alia": [22, 127], "similar": [23, 38, 43, 45, 116, 168, 169, 171, 172, 173], "stackexchangedpair": 23, "question": [23, 167, 168, 169, 171], "answer": [23, 167, 169, 171], "nanswer": 23, "summar": [24, 41, 167, 168], "dialogu": [24, 41, 167], "summari": [24, 41, 107, 140, 168], "dialog": 24, "did": [24, 169, 171, 173], "know": [24, 167, 168, 169, 171, 172], "adher": [25, 26], "could": [25, 172], "remain": [25, 26, 109, 172], "unmask": [25, 26], "sharegpt": [26, 37], "gpt": [26, 99, 169], "eos_id": [27, 119, 121], "length": [27, 28, 30, 31, 32, 33, 34, 35, 36, 38, 40, 42, 44, 45, 59, 67, 78, 86, 94, 96, 99, 102, 104, 105, 119, 121, 122, 129, 151, 152], "last": [27, 32, 109, 168], "replac": [27, 31, 35, 36, 39, 41, 108, 172], "forth": [28, 168], "come": [28, 111, 172], "empti": [28, 166], "shorter": 28, "min": [28, 124, 172], "invalid": 28, "convert_to_messag": [29, 167], "chat_format": [29, 37, 42, 167, 168], "chatformat": [29, 37, 168], "load_dataset_kwarg": [29, 31, 33, 34, 37, 38, 40, 44, 45], "multiturn": [29, 167], "prepar": [29, 167], "truncat": [29, 31, 32, 33, 34, 38, 40, 42, 44, 45, 59, 67, 78, 86, 94, 119, 121, 168], "anyth": [29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "load_dataset": [29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 167, 168], "huggingfac": [29, 31, 33, 34, 37, 38, 40, 44, 45, 90, 96, 97, 109, 110, 128, 129, 166, 169], "co": [29, 31, 33, 34, 37, 38, 40, 44, 45, 90, 96, 97, 128, 129, 169], "doc": [29, 31, 33, 34, 37, 38, 40, 44, 45, 67, 78, 127, 133, 139, 144, 149, 150, 156, 166, 169], "en": [29, 31, 33, 34, 37, 38, 40, 44, 45], "package_refer": [29, 31, 33, 34, 37, 38, 40, 44, 45], "loading_method": [29, 31, 33, 34, 37, 38, 40, 44, 45], "extra": [29, 163, 172, 173], "still": [29, 133, 172, 173], "unless": 29, "check": [29, 37, 46, 47, 48, 49, 105, 107, 116, 137, 144, 158, 162, 167, 169, 170, 172], "concaten": [30, 59, 67, 86, 94, 121, 152], "sub": [30, 149], "unifi": [30, 90], "were": [30, 107, 113, 167, 170], "simplifi": [30, 166, 172], "simultan": 30, "intern": [30, 133], "aggreg": 30, "transpar": 30, "index": [30, 32, 99, 104, 105, 106, 109, 151, 152, 163, 167, 169], "howev": [30, 97, 163], "constitu": 30, "might": [30, 166, 169], "larg": [30, 112, 157, 166, 173], "comput": [30, 99, 100, 104, 105, 110, 122, 123, 140, 156, 169, 173], "cumul": 30, "maintain": [30, 173], "indic": [30, 32, 49, 99, 104, 105, 106, 107, 122, 127, 144, 167], "deleg": 30, "retriev": [30, 138], "lead": [30, 86, 118], "high": [30, 164, 172], "scale": [30, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 112, 124, 135, 172, 173], "consid": [30, 47, 48, 49, 107], "strategi": 30, "stream": [30, 139], "demand": 30, "deriv": [30, 100, 105, 106], "_dataset": 30, "_len": 30, "total": [30, 109, 142, 161, 165, 169, 171, 172], "combin": [30, 123], "_index": 30, "lookup": 30, "dataset1": 30, "mycustomdataset": 30, "params1": 30, "dataset2": 30, "params2": 30, "concat_dataset": 30, "data_point": 30, "1500": 30, "element": [30, 169], "focus": [30, 170], "enhanc": [30, 107, 173], "divers": 30, "machin": [30, 136, 166, 169], "instructtempl": [31, 33, 168], "contribut": [31, 35, 36, 39, 41], "disabl": [31, 33, 34, 38, 40, 44, 45, 113, 156], "recommend": [31, 33, 34, 35, 36, 38, 40, 44, 45, 149, 167, 169, 173], "highest": [31, 33, 34, 35, 36, 38, 40, 44, 45], "sequenc": [31, 32, 33, 34, 35, 36, 38, 40, 42, 44, 45, 59, 67, 78, 86, 94, 99, 102, 104, 105, 107, 119, 121, 122, 151, 152, 167], "ds": [32, 42], "padding_idx": [32, 151, 152], "max_pack": 32, "split_across_pack": 32, "greedi": 32, "pack": [32, 35, 36, 37, 39, 40, 41, 42, 44, 99, 104, 105, 106], "done": [32, 116, 137, 146, 172, 173], "outsid": [32, 156, 157, 169, 171, 172], "sampler": [32, 170], "part": [32, 167, 173], "buffer": 32, "enough": [32, 167], "attent": [32, 49, 53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 96, 99, 102, 104, 105, 106, 116, 117, 122, 171, 172, 173], "lower": [32, 172], "triangular": 32, "cross": [32, 122], "attend": [32, 99, 105, 106, 122], "rel": [32, 99, 104, 105, 106, 140, 172], "pad": [32, 107, 124, 125, 135, 151, 152, 168], "max": [32, 42, 59, 67, 86, 94, 105, 107, 109, 119, 121, 166, 172], "wise": 32, "collat": [32, 151, 168], "made": [32, 37, 40, 44, 104, 169], "smaller": [32, 169, 171, 172, 173], "jam": 32, "vari": 32, "s1": [32, 67, 86, 118], "s2": [32, 67, 86, 118], "s3": 32, "s4": 32, "contamin": 32, "input_po": [32, 99, 102, 104, 105, 106], "matrix": 32, "causal": [32, 99, 105, 106], "continu": [32, 107, 168], "increment": 32, "move": [32, 105], "entir": [32, 146, 167, 173], "avoid": [32, 103, 107, 108, 156, 166, 173], "add_eo": [34, 44, 59, 67, 78, 86, 94, 118, 119, 167], "freeform": [34, 44], "unstructur": [34, 45], "corpu": [34, 38, 45], "local": [34, 44, 81, 97, 150, 156, 163, 166, 167, 169, 170], "tabular": [34, 44], "eo": [34, 44, 86, 94, 97, 118, 121, 167, 168], "yahma": [35, 40], "codebas": [35, 36, 39, 41, 169], "prior": [35, 36, 37, 39, 40, 41, 42, 44], "alpaca_d": [35, 36], "batch_siz": [35, 36, 39, 41, 99, 102, 105, 106, 110, 169], "tatsu": 36, "lab": 36, "conversation_styl": [37, 168], "chatdataset": [37, 42, 167, 168], "friendli": [37, 40, 44, 135, 167], "huggingfaceh4": 37, "no_robot": 37, "chatmlformat": 37, "2096": [37, 40, 44], "accomplish": [37, 40, 44], "packeddataset": [37, 40, 44, 168], "ccdv": 38, "cnn_dailymail": 38, "textcompletiondataset": [38, 44, 45, 168], "cnn": 38, "dailymail": 38, "articl": [38, 45], "extract": [38, 120], "highlight": [38, 173], "liweili": 39, "c4_200m": 39, "variant": [39, 41], "mirror": [39, 41], "llama_recip": [39, 41], "grammar_d": 39, "alpaca_clean": 40, "alpacainstructtempl": [40, 168], "samsum": [41, 168], "samsum_d": 41, "open": [42, 60, 61, 168, 169], "orca": 42, "slimorca": 42, "dedup": 42, "1024": [42, 43, 168], "prescrib": 42, "least": [42, 171, 172], "though": [42, 167], "10": [42, 107, 151, 152, 169, 171, 173], "351": 42, "82": [42, 169], "391": 42, "221": 42, "220": 42, "193": 42, "12": [42, 107, 152, 163], "471": 42, "lvwerra": [43, 168], "stack": [43, 107, 157, 168], "exchang": [43, 168], "preferencedataset": [43, 168], "stackexchangepair": 43, "textdataset": 44, "omit": [44, 172], "allenai": [44, 168], "c4": [44, 168], "data_dir": [44, 168], "realnewslik": [44, 168], "wikitext": 45, "subset": [45, 114], "103": [45, 169], "raw": 45, "wikipedia": 45, "page": [45, 163, 164, 166, 170, 171], "clip": [46, 47, 48, 49, 107], "max_num_til": [46, 47, 49, 107, 123], "tile": [46, 47, 48, 49, 107, 122, 123, 126], "patch": [46, 47, 48, 49, 107, 122], "document": [46, 47, 48, 49, 99, 127, 138, 146, 166, 168], "vision_transform": [46, 47, 48, 49], "visiontransform": [46, 47, 48, 49], "divid": [46, 47, 48, 49, 107, 122, 123, 126], "dimension": [46, 47, 48, 49, 107], "aspect_ratio": [46, 47, 107], "bsz": [46, 47, 107, 135], "n_img": [46, 47, 107], "n_tile": [46, 47, 107], "n_token": [46, 47, 48, 107], "aspect": [46, 47, 164], "ratio": [46, 47], "crop": [46, 47, 48, 49, 107, 126], "g": [46, 47, 48, 49, 99, 107, 111, 122, 124, 128, 140, 157, 171, 172, 173], "tile_s": [47, 48, 49, 107, 122, 123, 126], "patch_siz": [47, 48, 49, 107, 122], "local_token_positional_embed": 47, "equival": 47, "_position_embed": [47, 107], "tokenpositionalembed": [47, 107], "gate": [47, 131, 166, 170], "global_token_positional_embed": 47, "advanc": [47, 48, 49, 107, 168], "40": [47, 48, 49, 107, 122, 173], "400": [47, 48, 49, 107, 122, 126], "10x10": [47, 48, 49, 107, 122], "grid": [47, 48, 49, 107, 122], "k": [47, 99, 172], "th": 47, "cls_output_dim": [49, 107], "out_indic": [49, 107], "output_cls_project": 49, "in_channel": [49, 107], "transformerencoderlay": 49, "cl": [49, 107, 168], "head": [49, 99, 102, 104, 105, 131, 171], "intermedi": [49, 107, 130, 154, 171, 173], "fourth": [49, 107], "determin": [49, 117, 124], "channel": [49, 107], "code_llama2": [50, 51, 52, 53, 54, 55, 56, 57, 58, 166], "transformerdecod": [50, 51, 52, 53, 54, 55, 56, 57, 58, 68, 69, 70, 72, 73, 74, 75, 76, 77, 79, 80, 82, 83, 84, 85, 87, 88, 89, 90, 92, 93, 95, 96, 98, 135, 172], "w": [50, 51, 52, 60, 61, 68, 69, 70, 79, 80, 89, 90, 107, 125, 149, 150, 167, 169, 172, 173], "arxiv": [50, 51, 52, 56, 57, 58, 65, 66, 68, 69, 70, 75, 76, 77, 84, 85, 92, 93, 98, 99, 103, 104, 107, 110, 122], "org": [50, 51, 52, 56, 57, 58, 65, 66, 67, 68, 69, 70, 75, 76, 77, 84, 85, 92, 93, 98, 99, 103, 104, 107, 110, 122, 127, 133, 139, 144, 149, 154, 156, 163], "pdf": [50, 51, 52, 99, 103, 122], "2308": [50, 51, 52], "12950": [50, 51, 52], "lora_attn_modul": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 116, 117, 172, 173], "q_proj": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 116, 117, 172, 173], "k_proj": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 116, 117, 172, 173], "v_proj": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 116, 117, 172, 173], "output_proj": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 116, 117, 172, 173], "apply_lora_to_mlp": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 116, 117, 172], "apply_lora_to_output": [53, 54, 55, 56, 57, 58, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 116, 117, 172], "lora_rank": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 172], "lora_alpha": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 172], "float": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 99, 103, 109, 110, 112, 135, 140, 145, 147, 148, 149, 150, 172, 173], "16": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 107, 152, 172, 173], "lora_dropout": [53, 54, 55, 56, 57, 58, 72, 73, 74, 75, 76, 77], "05": [53, 54, 55, 56, 57, 58, 72, 73, 74, 75, 76, 77], "quantize_bas": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 112, 173], "lora": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 112, 113, 116, 117, 128, 146, 162, 164, 167, 170, 171], "code_llama2_13b": 53, "tloen": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95], "8bb8579e403dc78e37fe81ffbb253c413007323f": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95], "l41": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95], "l43": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 95], "linear": [53, 54, 55, 56, 57, 58, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 87, 88, 92, 93, 95, 98, 105, 111, 112, 116, 117, 172, 173], "mlp": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 105, 106, 116, 117, 171, 172], "final": [53, 54, 55, 72, 73, 74, 82, 83, 87, 88, 95, 100, 105, 113, 116, 117, 169, 171, 172, 173], "low": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 112, 169, 172, 173], "approxim": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 112, 172], "factor": [53, 54, 55, 63, 64, 72, 73, 74, 82, 83, 87, 88, 95, 112, 124, 169], "dropout": [53, 54, 55, 72, 73, 74, 99, 112, 172, 173], "probabl": [53, 54, 55, 72, 73, 74, 110, 112, 135, 169], "code_llama2_70b": 54, "code_llama2_7b": 55, "qlora": [56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 108, 162, 164, 171, 172], "per": [56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 102, 107, 108, 122, 123, 166, 171, 173], "paper": [56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 122, 172, 173], "ab": [56, 57, 58, 65, 66, 68, 69, 70, 75, 76, 77, 84, 85, 92, 93, 98, 104, 107, 110], "2305": [56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98, 99, 110], "14314": [56, 57, 58, 65, 66, 75, 76, 77, 84, 85, 92, 93, 98], "lora_code_llama2_13b": 56, "lora_code_llama2_70b": 57, "lora_code_llama2_7b": 58, "gemma": [59, 60, 61, 62, 63, 64, 65, 66, 131], "sentencepiec": [59, 67, 86, 94, 118, 171], "pretrain": [59, 67, 78, 86, 94, 118, 119, 166, 167, 170, 172, 173], "spm_model": [59, 67, 86, 94, 118, 167], "tokenized_text": [59, 67, 78, 86, 94, 118, 119], "add_bo": [59, 67, 78, 86, 94, 118, 119, 167], "31587": [59, 67, 78, 86, 94, 118, 119], "29644": [59, 67, 78, 86, 94, 118, 119], "102": [59, 67, 78, 86, 94, 118, 119], "tokenizer_path": [59, 67, 86, 94], "separ": [59, 67, 86, 94, 121, 128, 167, 170, 171, 172, 173], "concat": [59, 67, 86, 94, 121], "1788": [59, 67, 86, 94, 121], "2643": [59, 67, 86, 94, 121], "13": [59, 67, 86, 94, 107, 121, 152, 169, 171, 173], "1792": [59, 67, 86, 94, 121], "9508": [59, 67, 86, 94, 121], "465": [59, 67, 86, 94, 121], "22137": [59, 67, 86, 94, 121], "2933": [59, 67, 86, 94, 121], "join": [59, 67, 86, 94, 121], "attribut": [59, 67, 86, 94, 113, 121, 134], "gemmatransformerdecod": [60, 61, 63, 64, 65, 66], "blog": [60, 61], "technolog": [60, 61], "develop": [60, 61, 173], "gemmatoken": 62, "gemma_2b": 63, "gemma_7b": 64, "lora_gemma_2b": 65, "lora_gemma_7b": 66, "card": [67, 78], "regist": [67, 78, 81, 94, 97, 100, 108, 153, 173], "uniqu": [67, 131], "strongli": 67, "beforehand": 67, "html": [67, 127, 133, 139, 144, 149, 154, 156, 162], "problem": [67, 86], "due": [67, 86, 118, 172, 173], "whitespac": [67, 86, 118], "slice": [67, 86], "2307": [68, 69, 70], "09288": [68, 69, 70], "llama2_70b": 73, "llama2_7b": [74, 172], "lora_llama2_13b": 75, "lora_llama2_70b": 76, "lora_llama2_7b": [77, 172], "special_token": [78, 94, 119], "tiktoken": [78, 119, 171], "left": [78, 94, 172], "canon": [78, 81, 94, 97], "tt_model": [78, 119], "token_id": [78, 86, 119], "truncate_at_eo": [78, 119], "tokenize_head": 78, "tokenize_end": 78, "header": [78, 167], "eom": 78, "special_tokens_path": [81, 97], "llama3token": [81, 167], "similarli": [81, 97, 157, 168], "llama3_70b": 82, "llama3_8b": [83, 135, 171], "lora_llama3_70b": 84, "lora_llama3_8b": 85, "trim_leading_whitespac": [86, 118], "unbatch": [86, 118], "bo": [86, 97, 118, 121, 167, 168], "trim": [86, 118], "classifi": [88, 90, 93, 168], "announc": 89, "ray2333": 90, "reward": [90, 110], "feedback": 90, "mistraltoken": [91, 167], "lora_mistral_7b": 92, "lora_mistral_classifier_7b": 93, "phi3": [94, 95, 96, 97, 98, 131, 166], "ignore_system_prompt": 94, "phi3_mini": [95, 131], "ref": [96, 97, 150], "phi": [96, 97, 131], "128k": 96, "nor": 96, "slide": 96, "window": [96, 168], "phi3minitoken": 97, "tokenizer_config": 97, "spm": 97, "lm": 97, "unk": 97, "augment": [97, 173], "endoftext": 97, "phi3minisentencepiecebasetoken": 97, "lora_phi3_mini": 98, "head_dim": [99, 102, 105], "pos_embed": [99, 172], "kv_cach": 99, "kvcach": [99, 105], "attn_dropout": [99, 105], "queri": [99, 102, 105, 106, 171], "gqa": 99, "introduc": [99, 103, 112, 167, 168, 172, 173], "13245v1": 99, "version": [99, 135, 158, 163, 167, 171, 173], "multihead": 99, "mha": [99, 105], "extrem": 99, "share": [99, 168, 169], "mqa": 99, "credit": 99, "lightn": 99, "lit": 99, "lit_gpt": 99, "v": [99, 105, 172], "q": [99, 172], "n_kv_head": 99, "dimens": [99, 102, 104, 105, 107, 112, 171, 172, 173], "rotarypositionalembed": [99, 172], "cach": [99, 102, 104, 105, 163, 166], "rope": [99, 104], "onto": 99, "scaled_dot_product_attent": 99, "seq_length": [99, 106, 135], "boolean": [99, 105, 106, 127], "softmax": [99, 105, 106], "row": [99, 105, 106, 124, 167], "j": [99, 105, 106], "seq_len": 99, "bigger": 99, "n_h": [99, 104], "num": [99, 104], "n_kv": 99, "kv": [99, 102, 105], "emb": [99, 105], "h_d": [99, 104], "gate_proj": 100, "down_proj": 100, "up_proj": 100, "silu": 100, "feed": [100, 106], "network": [100, 113, 172, 173], "fed": [100, 167], "multipli": 100, "subclass": [100, 133], "although": [100, 172], "afterward": 100, "former": 100, "hook": [100, 108, 153, 173], "latter": 100, "layer_norm": 101, "standalon": 102, "past": 102, "becaus": [102, 105, 107, 130, 166, 167, 169, 171], "expand": 102, "dpython": [102, 105, 108], "reset": [102, 105, 140], "k_val": 102, "v_val": 102, "h": [102, 107, 125, 163, 166], "longer": [102, 168], "ep": 103, "1e": 103, "06": [103, 172], "root": [103, 149, 150], "squar": 103, "1910": 103, "07467": 103, "verfic": [103, 104], "small": [103, 169], "divis": [103, 126], "10000": 104, "rotari": [104, 171], "propos": 104, "2104": 104, "09864": 104, "l450": 104, "upto": 104, "init": [104, 140, 150, 173], "exceed": 104, "freq": 104, "recomput": 104, "geometr": 104, "progress": [104, 170], "rotat": 104, "angl": 104, "todo": 104, "effici": [104, 116, 138, 162, 164, 169, 170, 172], "transformerdecoderlay": 105, "norm": [105, 106], "space": 105, "belong": [105, 134], "reduc": [105, 164, 168, 172, 173], "statement": 105, "improv": [105, 119, 138, 169, 171, 172], "readabl": [105, 169], "At": 105, "arang": 105, "prompt_length": 105, "causal_mask": 105, "m_": 105, "seq": 105, "reset_cach": 105, "setup_cach": 105, "attn": [106, 172, 173], "causalselfattent": [106, 172], "sa_norm": 106, "mlp_norm": 106, "ff": 106, "token_pos_embed": 107, "pre_tile_pos_emb": 107, "post_tile_pos_emb": 107, "cls_project": 107, "vit": 107, "11929": 107, "convolut": 107, "flatten": 107, "treat": [107, 113, 133, 167], "downscal": [107, 124, 125], "800x400": 107, "400x400": 107, "_transform": 107, "clipimagetransform": 107, "broken": 107, "down": [107, 130, 168, 172, 173], "whole": 107, "num_til": [107, 126], "101": 107, "pool": 107, "tiledtokenpositionalembed": 107, "tilepositionalembed": 107, "tile_pos_emb": 107, "even": [107, 163, 166, 167, 168, 171, 172, 173], "8x8": 107, "14": [107, 152, 173], "15": [107, 138, 152, 167, 169, 172, 173], "17": [107, 152, 169, 172], "18": [107, 152, 171], "19": [107, 152, 169, 171, 173], "20": [107, 152], "21": 107, "22": 107, "23": [107, 109, 171], "24": [107, 126, 170, 171], "25": [107, 169], "26": 107, "27": [107, 169], "28": [107, 169], "29": [107, 173], "30": 107, "31": [107, 171], "33": 107, "34": 107, "35": [107, 173], "36": 107, "37": 107, "38": [107, 169], "39": 107, "41": 107, "42": 107, "43": 107, "44": 107, "45": 107, "46": 107, "47": 107, "48": [107, 169, 173], "49": 107, "50": [107, 126, 169], "51": 107, "52": [107, 170], "53": 107, "54": 107, "55": [107, 170], "56": 107, "57": [107, 171, 172, 173], "58": 107, "59": [107, 173], "60": 107, "61": [107, 169], "62": [107, 169, 171], "63": 107, "64": [107, 172], "num_patches_per_til": 107, "emb_dim": 107, "greater": [107, 158], "constain": 107, "anim": [107, 168], "max_n_img": 107, "n_channel": 107, "hidden_st": 107, "vision_util": 107, "tile_crop": 107, "num_channel": 107, "image_s": [107, 125], "800": [107, 125], "patch_grid_s": 107, "random": [107, 156, 170], "rand": [107, 124, 125, 126], "nch": 107, "tile_cropped_imag": 107, "batch_imag": 107, "unsqueez": 107, "batch_aspect_ratio": 107, "clip_vision_encod": 107, "common_util": 108, "bfloat16": [108, 155, 169, 170, 171, 172], "offload_to_cpu": 108, "nf4": [108, 173], "restor": 108, "higher": [108, 171, 173], "offload": [108, 173], "increas": [108, 109, 171, 172], "peak": [108, 140, 145, 169, 171, 172, 173], "gpu": [108, 166, 169, 170, 171, 172, 173], "_register_state_dict_hook": 108, "m": [108, 135, 167], "mymodul": 108, "_after_": 108, "nf4tensor": [108, 173], "unquant": [108, 169, 173], "unus": 108, "num_warmup_step": 109, "num_training_step": 109, "num_cycl": [109, 157], "last_epoch": 109, "lambdalr": 109, "rate": [109, 164, 170], "schedul": [109, 157, 170], "linearli": 109, "decreas": [109, 168, 172, 173], "cosin": 109, "v4": 109, "src": 109, "l104": 109, "warmup": [109, 157], "phase": 109, "wave": 109, "half": 109, "lr_schedul": 109, "beta": 110, "label_smooth": 110, "loss_typ": 110, "sigmoid": 110, "dpo": [110, 113, 152], "18290": 110, "trl": 110, "librari": [110, 133, 137, 139, 156, 162, 164, 166, 168, 173], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 110, "dpo_train": 110, "l844": 110, "temperatur": [110, 135, 169], "uncertainti": 110, "hing": 110, "ipo": 110, "kto_pair": 110, "policy_chosen_logp": 110, "policy_rejected_logp": 110, "reference_chosen_logp": 110, "reference_rejected_logp": 110, "polici": [110, 113, 127, 138, 146, 154], "chosen": [110, 157, 168], "reject": [110, 168], "chosen_reward": 110, "rejected_reward": 110, "unknown": 110, "peft": [111, 112, 113, 114, 115, 116, 117, 128, 172, 173], "protocol": 111, "adapter_param": [111, 112, 113, 114, 115], "proj": 111, "in_dim": [111, 112, 172, 173], "out_dim": [111, 112, 172, 173], "bia": [111, 112, 172, 173], "loralinear": [111, 172, 173], "alpha": [112, 172, 173], "use_bia": 112, "perturb": 112, "decomposit": [112, 172], "matric": [112, 146, 172, 173], "trainabl": [112, 115, 146, 172, 173], "mapsto": 112, "w_0x": 112, "r": [112, 172], "bax": 112, "lora_a": [112, 172, 173], "lora_b": [112, 172, 173], "temporarili": 113, "neural": [113, 172, 173], "caller": 113, "whose": [113, 153], "yield": 113, "get_adapter_param": [115, 172], "base_miss": 116, "base_unexpect": 116, "lora_miss": 116, "lora_unexpect": 116, "validate_state_dict_for_lora": [116, 172], "unlik": [116, 169, 171], "reli": [116, 121], "unexpect": 116, "strict": [116, 172], "pull": [116, 166], "120600": 116, "assertionerror": [116, 117, 152], "nonempti": 116, "full_model_state_dict_kei": 117, "lora_state_dict_kei": 117, "base_model_state_dict_kei": 117, "confirm": [117, 163], "lora_modul": 117, "complement": 117, "disjoint": 117, "overlap": 117, "light": 118, "sentencepieceprocessor": 118, "addition": [118, 119, 156, 168, 172], "prefix": 118, "bos_id": [119, 121], "lightweight": [119, 167], "break": 119, "substr": 119, "repetit": 119, "speed": [119, 157, 171, 173], "identif": 119, "regex": 119, "chunk": 119, "present": [119, 130], "absent": 119, "tokenizer_json_path": 120, "heavili": 121, "image_token_id": 122, "particip": 122, "show": [122, 163, 166, 167, 172], "laid": 122, "fig": 122, "flamingo": 122, "2204": 122, "14198": 122, "immedi": 122, "until": 122, "img1": 122, "img2": 122, "img3": 122, "cat": 122, "text_seq_len": 122, "image_seq_len": 122, "equal": [122, 126, 158], "resolut": [123, 124, 125], "1x1": 123, "1x2": 123, "2x1": 123, "side": [123, 124, 125], "height": [123, 124, 125], "width": [123, 124, 125], "224": [123, 124, 125], "896": 123, "448": [123, 124, 125], "672": [123, 124], "possible_resolut": 124, "resize_to_max_canva": 124, "canva": 124, "resiz": [124, 125], "distort": [124, 125], "select": 124, "smallest": 124, "upscal": [124, 125], "2x": 124, "5x": 124, "canvas": 124, "satisfi": [124, 169], "condit": [124, 135, 144, 166, 168], "pick": 124, "lowest": [124, 172], "area": [124, 169], "minim": [124, 168, 170, 172, 173], "200": [124, 126, 173], "300": [124, 125, 126, 169], "scale_height": 124, "1200": 124, "3600": 124, "2400": 124, "scale_width": 124, "7467": 124, "4933": 124, "scaling_factor": 124, "upscaling_opt": 124, "selected_scal": 124, "150528": 124, "100352": 124, "optimal_canva": 124, "target_s": 125, "resampl": 125, "interpolationmod": 125, "max_upscaling_s": 125, "exce": 125, "torchvis": 125, "nearest": 125, "nearest_exact": 125, "bilinear": 125, "bicub": 125, "1194": 125, "1344": 125, "stai": 125, "600": [125, 126], "500": [125, 172], "1000": [125, 127], "488": 125, "channel_s": 126, "4x6": 126, "2x3": 126, "datatyp": [127, 173], "denot": 127, "integ": [127, 151, 156], "auto_wrap_polici": [127, 138, 154], "submodul": [127, 146], "obei": 127, "contract": 127, "get_fsdp_polici": 127, "modules_to_wrap": [127, 138, 146], "min_num_param": 127, "my_fsdp_polici": 127, "recurs": [127, 146, 149], "isinst": [127, 168], "sum": [127, 172], "p": [127, 132, 172, 173], "numel": [127, 172], "functool": 127, "partial": 127, "stabl": [127, 144, 149, 156, 163], "safe_seri": 128, "from_pretrain": 128, "0001_of_0003": 128, "0002_of_0003": 128, "preserv": [128, 173], "weight_map": [128, 169], "convert_weight": 128, "_model_typ": [128, 131], "intermediate_checkpoint": [128, 129, 130], "_weight_map": 128, "shard": [129, 171], "wip": 129, "larger": [130, 169, 171], "qualnam": 131, "boundari": 131, "distinguish": 131, "my_new_model": 131, "my_custom_state_dict_map": 131, "mistral_reward": 131, "classif": 131, "mistral_classifi": 131, "optim_map": 132, "bare": 132, "bone": 132, "distribut": [132, 136, 143, 144, 154, 156, 164, 166, 170, 171], "optim_dict": [132, 134, 153], "cfg_optim": 132, "ckpt": 132, "optim_ckpt": 132, "placeholder_optim_dict": 132, "optiminbackwardwrapp": 132, "get_optim_kei": 132, "arbitrari": [132, 172], "hyperparamet": [132, 164, 170, 172, 173], "optim_ckpt_map": 132, "runtimeerror": [132, 137, 143], "loadabl": 132, "argpars": 133, "argumentpars": 133, "builtin": 133, "said": 133, "noth": 133, "consult": 133, "info": [133, 170], "parse_known_arg": 133, "namespac": 133, "act": 133, "precid": 133, "parse_arg": 133, "too": [133, 171], "optimizerinbackwardwrapp": 134, "top": [134, 169, 173], "named_paramet": 134, "max_generated_token": 135, "pad_id": 135, "top_k": [135, 169], "stop_token": 135, "custom_generate_next_token": 135, "predict": 135, "prune": [135, 173], "stop": 135, "compil": [135, 169, 171, 173], "generate_next_token": 135, "llama3_token": [135, 167, 171], "hi": [135, 167], "my": [135, 166, 167, 168, 169, 171], "jeremi": 135, "float32": 137, "bf16": [137, 173], "inde": [137, 169], "kernel": 137, "isn": [137, 166], "hardwar": [137, 164, 168, 169, 172], "memory_efficient_fsdp_wrap": 138, "maxim": [138, 146, 162, 164], "been": [138, 171], "workload": 138, "alongsid": 138, "ac": 138, "fullyshardeddataparallel": [138, 146], "fsdppolicytyp": [138, 146], "handler": 139, "reset_stat": 140, "track": 140, "alloc": [140, 145, 146, 171, 173], "reserv": [140, 145, 167, 173], "stat": [140, 145, 173], "int4": 141, "4w": [141, 169, 171], "recogn": 141, "mode": [141, 169], "aka": 142, "master": 144, "port": [144, 166], "address": 144, "hold": [144, 170], "peak_memory_act": 145, "peak_memory_alloc": 145, "peak_memory_reserv": 145, "get_memory_stat": 145, "own": [146, 156, 166, 167, 168, 169, 172], "unit": [146, 164], "hierarch": 146, "requires_grad": [146, 172, 173], "filenam": 147, "log_": 147, "unixtimestamp": 147, "txt": [147, 168, 170], "thread": 147, "safe": 147, "flush": [147, 148, 149, 150], "ndarrai": [147, 148, 149, 150], "scalar": [147, 148, 149, 150], "record": [147, 148, 149, 150, 157], "payload": [147, 148, 149, 150], "organize_log": 149, "tensorboard": 149, "subdirectori": 149, "compar": [149, 158, 169, 172, 173], "logdir": 149, "startup": 149, "tree": [149, 168, 169], "tfevent": 149, "encount": 149, "frontend": 149, "organ": [149, 166], "accordingli": 149, "my_log_dir": 149, "view": [149, 169, 170], "my_metr": [149, 150], "termin": [149, 150], "entiti": 150, "bias": 150, "sent": 150, "usernam": 150, "my_project": 150, "my_ent": 150, "my_group": 150, "importerror": 150, "account": [150, 172, 173], "log_config": 150, "link": [150, 169], "capecap": 150, "6053ofw0": 150, "torchtune_config_j67sb73v": 150, "ignore_idx": [151, 152], "longest": 151, "token_pair": 151, "input_id": 152, "chosen_input_id": [152, 168], "chosen_label": [152, 168], "rejected_input_id": [152, 168], "rejected_label": [152, 168], "soon": 153, "readi": [153, 162, 167], "grad": 153, "achiev": [153, 169, 171, 172, 173], "acwrappolicytyp": 154, "author": [154, 164, 170, 173], "fsdp_adavnced_tutori": 154, "insid": 155, "contextmanag": 155, "debug_mod": 156, "pseudo": 156, "commonli": [156, 169, 172, 173], "numpi": 156, "determinist": 156, "global": [156, 168], "warn": 156, "nondeterminist": 156, "cudnn": 156, "set_deterministic_debug_mod": 156, "algorithm": 156, "profile_memori": 157, "with_stack": 157, "record_shap": 157, "with_flop": 157, "wait_step": 157, "warmup_step": 157, "active_step": 157, "profil": 157, "layout": 157, "trace": 157, "profileract": 157, "flop": 157, "wait": 157, "cycl": 157, "repeat": 157, "reduct": [157, 172], "iter": [157, 159, 173], "scope": 157, "gradient_accumul": 157, "sensibl": 157, "default_schedul": 157, "against": [158, 173], "__version__": 158, "named_param": 159, "generated_examples_python": 160, "zip": 160, "galleri": [160, 165], "sphinx": 160, "000": [161, 165, 171], "execut": [161, 165], "generated_exampl": 161, "mem": [161, 165], "mb": [161, 165], "topic": 162, "gentl": 162, "introduct": 162, "first_finetune_tutori": 162, "workflow": [162, 168, 170, 172], "requisit": 163, "proper": [163, 170], "host": [163, 166, 170], "latest": [163, 170, 173], "And": [163, 169, 171], "ls": [163, 166, 169, 170, 171], "welcom": [163, 166], "greatest": [163, 170], "contributor": 163, "cd": [163, 169], "commit": 163, "branch": 163, "url": 163, "whl": 163, "therebi": [163, 173], "forc": 163, "reinstal": 163, "opt": [163, 170], "suffix": 163, "cu121": 163, "On": [164, 172], "pointer": 164, "emphas": 164, "simplic": 164, "component": 164, "reus": 164, "prove": 164, "democrat": 164, "box": [164, 173], "zoo": 164, "varieti": [164, 172], "techniqu": [164, 169, 170, 172], "integr": [164, 169, 170, 171, 172, 173], "excit": 164, "checkout": 164, "quickstart": 164, "attain": 164, "better": [164, 167, 168, 169], "chekckpoint": 164, "embodi": 164, "philosophi": 164, "usabl": 164, "composit": 164, "hard": [164, 168], "outlin": 164, "unecessari": 164, "never": 164, "thoroughli": 164, "short": 166, "subcommand": 166, "anytim": 166, "symlink": 166, "auto": 166, "wrote": 166, "readm": 166, "md": 166, "lot": [166, 169], "recent": 166, "releas": [166, 171], "agre": 166, "term": 166, "perman": 166, "eat": 166, "bandwith": 166, "storag": [166, 173], "00030": 166, "ootb": 166, "full_finetune_single_devic": [166, 168, 169, 170], "7b_full_low_memori": [166, 169, 170], "8b_full_single_devic": [166, 168], "mini_full_low_memori": 166, "7b_full": [166, 169, 170], "13b_full": [166, 169, 170], "70b_full": 166, "edit": 166, "destin": 166, "lora_finetune_distribut": [166, 171, 172], "torchrun": 166, "8b_lora_single_devic": [166, 167, 171], "launch": [166, 167, 170], "nproc": 166, "node": 166, "worker": 166, "nnode": [166, 172], "minimum_nod": 166, "maximum_nod": 166, "fail": 166, "rdzv": 166, "rendezv": 166, "endpoint": 166, "8b_lora": [166, 171], "bypass": 166, "vice": 166, "versa": 166, "fancy_lora": 166, "8b_fancy_lora": 166, "sai": [166, 167, 170], "align": 167, "intend": 167, "nice": 167, "meet": 167, "overhaul": 167, "begin_of_text": 167, "start_header_id": 167, "end_header_id": 167, "eot_id": 167, "accompani": 167, "who": 167, "influenti": 167, "hip": 167, "hop": 167, "artist": [167, 171], "2pac": 167, "rakim": 167, "c": 167, "na": 167, "flavor": [167, 168], "msg": 167, "formatted_messag": [167, 168], "nyou": [167, 168], "nwho": 167, "why": [167, 170, 172], "user_messag": 167, "518": 167, "25580": 167, "29962": 167, "3532": 167, "14816": 167, "29903": 167, "6778": 167, "piece_to_id": 167, "vector": 167, "place": 167, "manual": [167, 173], "529": 167, "29879": 167, "29958": 167, "nhere": 167, "_encode_special_token": 167, "128000": 167, "128009": 167, "pure": 167, "That": 167, "won": [167, 169, 171], "mess": 167, "govern": 167, "prime": 167, "strictli": 167, "summarizetempl": [167, 168], "ask": 167, "untouch": 167, "nsummari": 167, "robust": 167, "csv": [167, 168], "onlin": 167, "forum": 167, "panda": 167, "pd": 167, "df": 167, "read_csv": 167, "your_fil": 167, "nrow": 167, "tolist": 167, "iloc": 167, "gp": 167, "receiv": 167, "commun": [167, 168, 169], "satellit": 167, "thing": [167, 173], "dataclass": 167, "message_convert": 167, "input_msg": 167, "output_msg": 167, "assistant_messag": 167, "But": [167, 169, 171, 172], "mistralchatformat": 167, "custom_dataset": 167, "2048": 167, "data_fil": [167, 168], "honor": 167, "copi": [167, 169, 170, 171, 173], "custom_8b_lora_single_devic": 167, "steer": 168, "wheel": 168, "publicli": 168, "great": [168, 169], "hood": [168, 169, 173], "text_completion_dataset": 168, "padded_col": 168, "upper": 168, "constraint": [168, 172], "slow": [168, 173], "signific": 168, "speedup": [168, 171], "my_data": 168, "instruct_dataset": 168, "fix": 168, "goal": 168, "agnost": 168, "respond": 168, "plant": 168, "miner": 168, "oak": 168, "copper": 168, "ore": 168, "eleph": 168, "customtempl": 168, "chat_dataset": 168, "quit": [168, 173], "incorpor": 168, "customchatformat": 168, "vicgal": 168, "gpt4": 168, "drive": 168, "rajpurkar": 168, "io": 168, "squad": 168, "explor": 168, "rlhf": 168, "few": [168, 171, 172, 173], "adjust": 168, "chosen_messag": 168, "transformed_sampl": 168, "key_chosen": 168, "rejected_messag": 168, "key_reject": 168, "c_mask": 168, "np": 168, "cross_entropy_ignore_idx": 168, "r_mask": 168, "stack_exchanged_paired_dataset": 168, "had": 168, "stackexchangedpairedtempl": 168, "response_j": 168, "response_k": 168, "rl": 168, "favorit": [169, 171, 172], "seemlessli": 169, "beyond": [169, 173], "connect": 169, "amount": 169, "natur": 169, "export": 169, "mobil": 169, "phone": 169, "leverag": [169, 171, 173], "plai": 169, "freez": [169, 172], "percentag": 169, "learnabl": 169, "keep": [169, 172], "16gb": [169, 172], "rtx": 169, "3090": 169, "4090": 169, "hour": 169, "7b_qlora_single_devic": [169, 170, 173], "473": 169, "98": [169, 173], "gb": [169, 171, 172, 173], "484": 169, "01": [169, 170], "fact": [169, 171, 172], "third": 169, "realli": 169, "eleuther_ev": [169, 171], "eleuther_evalu": [169, 171], "lm_eval": [169, 171], "plan": 169, "custom_eval_config": [169, 171], "truthfulqa_mc2": [169, 171, 172], "measur": [169, 171], "propens": [169, 171], "shot": [169, 171], "accuraci": [169, 171, 172, 173], "baselin": [169, 172], "324": 169, "loglikelihood": 169, "195": 169, "121": 169, "second": [169, 171, 172, 173], "197": 169, "acc": 169, "388": 169, "shown": 169, "489": 169, "seem": 169, "custom_generation_config": [169, 171], "kick": 169, "interest": 169, "site": 169, "visit": 169, "bai": 169, "92": [169, 171], "exploratorium": 169, "san": 169, "francisco": 169, "magazin": 169, "awesom": 169, "bridg": 169, "pretti": 169, "cool": 169, "96": [169, 173], "sec": [169, 171], "83": 169, "99": [169, 172], "72": 169, "littl": 169, "saw": 169, "took": [169, 171], "torchao": [169, 171, 173], "bit": [169, 171, 172, 173], "custom_quantization_config": [169, 171], "68": 169, "76": 169, "69": 169, "95": [169, 171], "67": 169, "engin": [169, 171], "fullmodeltorchtunecheckpoint": [169, 171], "int4weightonlyquant": [169, 171], "groupsiz": [169, 171], "256": [169, 171], "park": 169, "sit": 169, "hill": 169, "beauti": 169, "85": 169, "sped": 169, "almost": [169, 171, 172], "3x": [169, 171], "benefit": 169, "doesn": 169, "yet": 169, "fast": 169, "clone": [169, 172, 173], "assumpt": 169, "new_dir": 169, "output_dict": 169, "sd_1": 169, "sd_2": 169, "dump": 169, "convert_hf_checkpoint": 169, "checkpoint_path": 169, "justin": 169, "school": 169, "math": 169, "teacher": 169, "ws": 169, "94": [169, 171], "bandwidth": [169, 171], "1391": 169, "84": 169, "thats": 169, "seamlessli": 169, "authent": [169, 170], "hopefulli": 169, "gave": 169, "grant": 170, "minut": 170, "agreement": 170, "altern": 170, "hackabl": 170, "singularli": 170, "technic": 170, "purpos": [170, 171], "depth": 170, "principl": 170, "boilerpl": 170, "substanti": [170, 172], "custom_config": 170, "replic": 170, "lorafinetunerecipesingledevic": 170, "lora_finetune_output": 170, "log_1713194212": 170, "3697006702423096": 170, "25880": [170, 173], "83it": 170, "monitor": 170, "tqdm": 170, "interv": 170, "e2": 170, "focu": 171, "128": [171, 172], "theta": 171, "gain": 171, "illustr": 171, "basic": 171, "observ": 171, "consum": [171, 173], "vram": [171, 172], "overal": 171, "8b_qlora_single_devic": 171, "coupl": [171, 172, 173], "meta_model_0": 171, "122": 171, "sarah": 171, "busi": 171, "mum": 171, "young": 171, "children": 171, "live": 171, "north": 171, "east": 171, "england": 171, "135": 171, "88": 171, "138": 171, "346": 171, "09": 171, "139": 171, "far": 171, "drill": 171, "90": 171, "93": 171, "91": 171, "104": 171, "four": [171, 172], "again": 171, "jake": 171, "disciplin": 171, "passion": 171, "draw": 171, "paint": 171, "broader": 171, "teach": 172, "straight": 172, "unfamiliar": 172, "oppos": [172, 173], "momentum": 172, "relat": 172, "aghajanyan": 172, "et": 172, "al": 172, "hypothes": 172, "intrins": 172, "often": 172, "eight": 172, "practic": 172, "blue": 172, "rememb": 172, "approx": 172, "15m": 172, "8192": 172, "65k": 172, "frozen_out": [172, 173], "lora_out": [172, 173], "base_model": 172, "choos": 172, "lora_model": 172, "lora_llama_2_7b": [172, 173], "alon": 172, "in_featur": 172, "out_featur": 172, "inplac": 172, "feel": 172, "free": 172, "whenev": 172, "peft_util": 172, "set_trainable_param": 172, "fetch": 172, "lora_param": 172, "total_param": 172, "trainable_param": 172, "2f": 172, "6742609920": 172, "4194304": 172, "7b_lora": 172, "my_model_checkpoint_path": [172, 173], "tokenizer_checkpoint": [172, 173], "my_tokenizer_checkpoint_path": [172, 173], "factori": 172, "benefici": 172, "impact": 172, "minor": 172, "good": 172, "lora_experiment_1": 172, "smooth": [172, 173], "curv": [172, 173], "ran": 172, "footprint": 172, "commod": 172, "cogniz": 172, "ax": 172, "parallel": 172, "truthfulqa": 172, "previous": 172, "475": 172, "87": 172, "508": 172, "86": 172, "504": 172, "04": 172, "514": 172, "absolut": 172, "4gb": 172, "tradeoff": 172, "potenti": 172, "highli": 173, "vanilla": 173, "held": 173, "therefor": 173, "bespok": 173, "normalfloat": 173, "8x": 173, "retain": 173, "vast": 173, "major": 173, "degrad": 173, "normatfloat": 173, "doubl": 173, "themselv": 173, "deepdiv": 173, "idea": 173, "distinct": 173, "de": 173, "incur": 173, "counterpart": 173, "set_default_devic": 173, "qlora_linear": 173, "memory_alloc": 173, "177": 173, "152": 173, "del": 173, "empty_cach": 173, "lora_linear": 173, "081": 173, "344": 173, "qlora_llama2_7b": 173, "qlora_model": 173, "essenti": 173, "reparametrize_as_dtype_state_dict_post_hook": 173, "slower": 173, "149": 173, "9157477021217346": 173, "02": 173, "08": 173, "15it": 173, "nightli": 173, "hundr": 173, "228": 173, "8158286809921265": 173, "95it": 173, "exercis": 173, "portion": 173, "linear_nf4": 173, "to_nf4": 173, "linear_weight": 173, "autograd": 173, "regular": 173, "incom": 173}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 4, 1, "", "Role"], [23, 1, 1, "", "StackExchangedPairedTemplate"], [24, 1, 1, "", "SummarizeTemplate"], [25, 0, 1, "", "get_openai_messages"], [26, 0, 1, "", "get_sharegpt_messages"], [27, 0, 1, "", "truncate"], [28, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 3, 1, "", "contains_media"], [20, 2, 1, "", "from_dict"], [20, 3, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[23, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[24, 2, 1, "", "format"]], "torchtune.datasets": [[29, 1, 1, "", "ChatDataset"], [30, 1, 1, "", "ConcatDataset"], [31, 1, 1, "", "InstructDataset"], [32, 1, 1, "", "PackedDataset"], [33, 1, 1, "", "PreferenceDataset"], [34, 1, 1, "", "TextCompletionDataset"], [35, 0, 1, "", "alpaca_cleaned_dataset"], [36, 0, 1, "", "alpaca_dataset"], [37, 0, 1, "", "chat_dataset"], [38, 0, 1, "", "cnn_dailymail_articles_dataset"], [39, 0, 1, "", "grammar_dataset"], [40, 0, 1, "", "instruct_dataset"], [41, 0, 1, "", "samsum_dataset"], [42, 0, 1, "", "slimorca_dataset"], [43, 0, 1, "", "stack_exchanged_paired_dataset"], [44, 0, 1, "", "text_completion_dataset"], [45, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[46, 1, 1, "", "TilePositionalEmbedding"], [47, 1, 1, "", "TiledTokenPositionalEmbedding"], [48, 1, 1, "", "TokenPositionalEmbedding"], [49, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[46, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[47, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[48, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[50, 0, 1, "", "code_llama2_13b"], [51, 0, 1, "", "code_llama2_70b"], [52, 0, 1, "", "code_llama2_7b"], [53, 0, 1, "", "lora_code_llama2_13b"], [54, 0, 1, "", "lora_code_llama2_70b"], [55, 0, 1, "", "lora_code_llama2_7b"], [56, 0, 1, "", "qlora_code_llama2_13b"], [57, 0, 1, "", "qlora_code_llama2_70b"], [58, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[59, 1, 1, "", "GemmaTokenizer"], [60, 0, 1, "", "gemma_2b"], [61, 0, 1, "", "gemma_7b"], [62, 0, 1, "", "gemma_tokenizer"], [63, 0, 1, "", "lora_gemma_2b"], [64, 0, 1, "", "lora_gemma_7b"], [65, 0, 1, "", "qlora_gemma_2b"], [66, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[59, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[67, 1, 1, "", "Llama2Tokenizer"], [68, 0, 1, "", "llama2_13b"], [69, 0, 1, "", "llama2_70b"], [70, 0, 1, "", "llama2_7b"], [71, 0, 1, "", "llama2_tokenizer"], [72, 0, 1, "", "lora_llama2_13b"], [73, 0, 1, "", "lora_llama2_70b"], [74, 0, 1, "", "lora_llama2_7b"], [75, 0, 1, "", "qlora_llama2_13b"], [76, 0, 1, "", "qlora_llama2_70b"], [77, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[67, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[78, 1, 1, "", "Llama3Tokenizer"], [79, 0, 1, "", "llama3_70b"], [80, 0, 1, "", "llama3_8b"], [81, 0, 1, "", "llama3_tokenizer"], [82, 0, 1, "", "lora_llama3_70b"], [83, 0, 1, "", "lora_llama3_8b"], [84, 0, 1, "", "qlora_llama3_70b"], [85, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[78, 2, 1, "", "decode"], [78, 2, 1, "", "tokenize_message"], [78, 2, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[86, 1, 1, "", "MistralTokenizer"], [87, 0, 1, "", "lora_mistral_7b"], [88, 0, 1, "", "lora_mistral_classifier_7b"], [89, 0, 1, "", "mistral_7b"], [90, 0, 1, "", "mistral_classifier_7b"], [91, 0, 1, "", "mistral_tokenizer"], [92, 0, 1, "", "qlora_mistral_7b"], [93, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.mistral.MistralTokenizer": [[86, 2, 1, "", "decode"], [86, 2, 1, "", "encode"], [86, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[94, 1, 1, "", "Phi3MiniTokenizer"], [95, 0, 1, "", "lora_phi3_mini"], [96, 0, 1, "", "phi3_mini"], [97, 0, 1, "", "phi3_mini_tokenizer"], [98, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[94, 2, 1, "", "decode"], [94, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[99, 1, 1, "", "CausalSelfAttention"], [100, 1, 1, "", "FeedForward"], [101, 1, 1, "", "Fp32LayerNorm"], [102, 1, 1, "", "KVCache"], [103, 1, 1, "", "RMSNorm"], [104, 1, 1, "", "RotaryPositionalEmbeddings"], [105, 1, 1, "", "TransformerDecoder"], [106, 1, 1, "", "TransformerDecoderLayer"], [107, 1, 1, "", "VisionTransformer"], [109, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[99, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[100, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[101, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[102, 2, 1, "", "reset"], [102, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[103, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[104, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[105, 2, 1, "", "forward"], [105, 2, 1, "", "reset_caches"], [105, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[106, 2, 1, "", "forward"]], "torchtune.modules.VisionTransformer": [[107, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[108, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[110, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[110, 2, 1, "", "forward"]], "torchtune.modules.peft": [[111, 1, 1, "", "AdapterModule"], [112, 1, 1, "", "LoRALinear"], [113, 0, 1, "", "disable_adapter"], [114, 0, 1, "", "get_adapter_params"], [115, 0, 1, "", "set_trainable_params"], [116, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [117, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[111, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[112, 2, 1, "", "adapter_params"], [112, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[118, 1, 1, "", "SentencePieceBaseTokenizer"], [119, 1, 1, "", "TikTokenBaseTokenizer"], [120, 0, 1, "", "parse_hf_tokenizer_json"], [121, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[118, 2, 1, "", "decode"], [118, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[119, 2, 1, "", "decode"], [119, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[122, 1, 1, "", "VisionCrossAttentionMask"], [123, 0, 1, "", "find_supported_resolutions"], [124, 0, 1, "", "get_canvas_best_fit"], [125, 0, 1, "", "resize_with_pad"], [126, 0, 1, "", "tile_crop"]], "torchtune.utils": [[127, 4, 1, "", "FSDPPolicyType"], [128, 1, 1, "", "FullModelHFCheckpointer"], [129, 1, 1, "", "FullModelMetaCheckpointer"], [130, 1, 1, "", "FullModelTorchTuneCheckpointer"], [131, 1, 1, "", "ModelType"], [132, 1, 1, "", "OptimizerInBackwardWrapper"], [133, 1, 1, "", "TuneRecipeArgumentParser"], [134, 0, 1, "", "create_optim_in_bwd_wrapper"], [135, 0, 1, "", "generate"], [136, 0, 1, "", "get_device"], [137, 0, 1, "", "get_dtype"], [138, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [139, 0, 1, "", "get_logger"], [140, 0, 1, "", "get_memory_stats"], [141, 0, 1, "", "get_quantizer_mode"], [142, 0, 1, "", "get_world_size_and_rank"], [143, 0, 1, "", "init_distributed"], [144, 0, 1, "", "is_distributed"], [145, 0, 1, "", "log_memory_stats"], [146, 0, 1, "", "lora_fsdp_wrap_policy"], [151, 0, 1, "", "padded_collate"], [152, 0, 1, "", "padded_collate_dpo"], [153, 0, 1, "", "register_optim_in_bwd_hooks"], [154, 0, 1, "", "set_activation_checkpointing"], [155, 0, 1, "", "set_default_dtype"], [156, 0, 1, "", "set_seed"], [157, 0, 1, "", "setup_torch_profiler"], [158, 0, 1, "", "torch_version_ge"], [159, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[128, 2, 1, "", "load_checkpoint"], [128, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[129, 2, 1, "", "load_checkpoint"], [129, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[130, 2, 1, "", "load_checkpoint"], [130, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[131, 5, 1, "", "GEMMA"], [131, 5, 1, "", "LLAMA2"], [131, 5, 1, "", "LLAMA3"], [131, 5, 1, "", "MISTRAL"], [131, 5, 1, "", "MISTRAL_REWARD"], [131, 5, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[132, 2, 1, "", "get_optim_key"], [132, 2, 1, "", "load_state_dict"], [132, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[133, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[147, 1, 1, "", "DiskLogger"], [148, 1, 1, "", "StdoutLogger"], [149, 1, 1, "", "TensorBoardLogger"], [150, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[147, 2, 1, "", "close"], [147, 2, 1, "", "log"], [147, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[148, 2, 1, "", "close"], [148, 2, 1, "", "log"], [148, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[149, 2, 1, "", "close"], [149, 2, 1, "", "log"], [149, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[150, 2, 1, "", "close"], [150, 2, 1, "", "log"], [150, 2, 1, "", "log_config"], [150, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:property", "4": "py:data", "5": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"], "4": ["py", "data", "Python data"], "5": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 22, 127, 162, 164, 166, 169, 171, 172, 173], "config": [0, 7, 8, 166, 170], "data": [1, 5, 22, 167], "text": [1, 168, 171], "templat": [1, 167, 168], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 167, 168], "exampl": 2, "gener": [2, 135, 169, 171], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 166, 169, 170, 171, 172], "llama3": [3, 167, 171], "llama2": [3, 167, 169, 172, 173], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "clip": 3, "modul": 4, "compon": [4, 7], "build": [4, 163, 173], "block": 4, "base": 4, "token": [4, 167], "util": [4, 5, 127], "peft": 4, "loss": 4, "vision": 4, "transform": 4, "checkpoint": [5, 6, 9, 169], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 168, 172, 173], "manag": 5, "perform": [5, 172], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 164, 169], "format": [6, 168], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 169, 172, 173], "put": [6, 173], "thi": 6, "all": [6, 7, 173], "togeth": [6, 173], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 168], "us": [7, 8, 167, 169, 173], "instanti": [7, 10], "referenc": 7, "other": [7, 169], "field": 7, "interpol": 7, "valid": [7, 13, 166], "your": [7, 8, 169, 170], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 164, 172, 173], "ar": 8, "recip": [8, 166, 170, 172], "script": 8, "run": [8, 166, 169], "cli": [8, 166], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "role": 22, "stackexchangedpairedtempl": 23, "summarizetempl": 24, "get_openai_messag": 25, "get_sharegpt_messag": 26, "truncat": 27, "validate_messag": 28, "chatdataset": 29, "concatdataset": 30, "instructdataset": 31, "packeddataset": 32, "preferencedataset": 33, "textcompletiondataset": 34, "alpaca_cleaned_dataset": 35, "alpaca_dataset": 36, "chat_dataset": 37, "cnn_dailymail_articles_dataset": 38, "grammar_dataset": 39, "instruct_dataset": 40, "samsum_dataset": 41, "slimorca_dataset": 42, "stack_exchanged_paired_dataset": 43, "text_completion_dataset": 44, "wikitext_dataset": 45, "tilepositionalembed": 46, "tiledtokenpositionalembed": 47, "tokenpositionalembed": 48, "clip_vision_encod": 49, "code_llama2_13b": 50, "code_llama2_70b": 51, "code_llama2_7b": 52, "lora_code_llama2_13b": 53, "lora_code_llama2_70b": 54, "lora_code_llama2_7b": 55, "qlora_code_llama2_13b": 56, "qlora_code_llama2_70b": 57, "qlora_code_llama2_7b": 58, "gemmatoken": 59, "gemma_2b": 60, "gemma_7b": 61, "gemma_token": 62, "lora_gemma_2b": 63, "lora_gemma_7b": 64, "qlora_gemma_2b": 65, "qlora_gemma_7b": 66, "llama2token": 67, "llama2_13b": 68, "llama2_70b": 69, "llama2_7b": 70, "llama2_token": 71, "lora_llama2_13b": 72, "lora_llama2_70b": 73, "lora_llama2_7b": 74, "qlora_llama2_13b": 75, "qlora_llama2_70b": 76, "qlora_llama2_7b": 77, "llama3token": 78, "llama3_70b": 79, "llama3_8b": 80, "llama3_token": 81, "lora_llama3_70b": 82, "lora_llama3_8b": 83, "qlora_llama3_70b": 84, "qlora_llama3_8b": 85, "mistraltoken": 86, "lora_mistral_7b": 87, "lora_mistral_classifier_7b": 88, "mistral_7b": 89, "mistral_classifier_7b": 90, "mistral_token": 91, "qlora_mistral_7b": 92, "qlora_mistral_classifier_7b": 93, "phi3minitoken": 94, "lora_phi3_mini": 95, "phi3_mini": 96, "phi3_mini_token": 97, "qlora_phi3_mini": 98, "causalselfattent": 99, "todo": [99, 106], "feedforward": 100, "fp32layernorm": 101, "kvcach": 102, "rmsnorm": 103, "rotarypositionalembed": 104, "transformerdecod": 105, "transformerdecoderlay": 106, "visiontransform": 107, "reparametrize_as_dtype_state_dict_post_hook": 108, "get_cosine_schedule_with_warmup": 109, "dpoloss": 110, "adaptermodul": 111, "loralinear": 112, "disable_adapt": 113, "get_adapter_param": 114, "set_trainable_param": 115, "validate_missing_and_unexpected_for_lora": 116, "validate_state_dict_for_lora": 117, "sentencepiecebasetoken": 118, "tiktokenbasetoken": 119, "parse_hf_tokenizer_json": 120, "tokenize_messages_no_special_token": 121, "visioncrossattentionmask": 122, "find_supported_resolut": 123, "get_canvas_best_fit": 124, "resize_with_pad": 125, "tile_crop": 126, "fsdppolicytyp": 127, "fullmodelhfcheckpoint": 128, "fullmodelmetacheckpoint": 129, "fullmodeltorchtunecheckpoint": 130, "modeltyp": 131, "optimizerinbackwardwrapp": 132, "tunerecipeargumentpars": 133, "create_optim_in_bwd_wrapp": 134, "get_devic": 136, "get_dtyp": 137, "get_full_finetune_fsdp_wrap_polici": 138, "get_logg": 139, "get_memory_stat": 140, "get_quantizer_mod": 141, "get_world_size_and_rank": 142, "init_distribut": 143, "is_distribut": 144, "log_memory_stat": 145, "lora_fsdp_wrap_polici": 146, "disklogg": 147, "stdoutlogg": 148, "tensorboardlogg": 149, "wandblogg": 150, "padded_col": 151, "padded_collate_dpo": 152, "register_optim_in_bwd_hook": 153, "set_activation_checkpoint": 154, "set_default_dtyp": 155, "set_se": 156, "setup_torch_profil": 157, "torch_version_g": 158, "validate_expected_param_dtyp": 159, "comput": [161, 165], "time": [161, 165], "welcom": 162, "document": 162, "get": [162, 166, 171], "start": [162, 166], "tutori": 162, "instal": 163, "instruct": [163, 168, 171], "via": [163, 171], "pypi": 163, "git": 163, "clone": 163, "nightli": 163, "kei": 164, "concept": 164, "design": 164, "principl": 164, "download": [166, 169, 170], "list": 166, "built": [166, 168], "copi": 166, "fine": [167, 168, 170, 171], "tune": [167, 168, 170, 171], "chat": [167, 168], "chang": 167, "from": [167, 173], "prompt": 167, "special": 167, "when": 167, "should": 167, "i": 167, "custom": [167, 168], "hug": [168, 169], "face": [168, 169], "set": 168, "max": 168, "sequenc": 168, "length": 168, "sampl": 168, "pack": 168, "unstructur": 168, "corpu": 168, "multipl": 168, "local": 168, "remot": 168, "fulli": 168, "end": 169, "workflow": 169, "7b": 169, "finetun": [169, 172, 173], "evalu": [169, 171], "eleutherai": [169, 171], "s": [169, 171], "eval": [169, 171], "har": [169, 171], "speed": 169, "up": 169, "quantiz": [169, 171], "librari": 169, "upload": 169, "hub": 169, "first": 170, "llm": 170, "select": 170, "modifi": 170, "train": 170, "next": 170, "step": 170, "meta": 171, "8b": 171, "access": 171, "our": 171, "faster": 171, "how": 172, "doe": 172, "work": 172, "appli": 172, "trade": 172, "off": 172, "qlora": 173, "save": 173, "deep": 173, "dive": 173}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})