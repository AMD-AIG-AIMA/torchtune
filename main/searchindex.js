Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_generation", "api_ref_models", "api_ref_modules", "api_ref_rlhf", "api_ref_training", "api_ref_utilities", "basics/chat_datasets", "basics/datasets_overview", "basics/instruct_datasets", "basics/message_transforms", "basics/messages", "basics/model_transforms", "basics/multimodal_datasets", "basics/preference_datasets", "basics/prompt_templates", "basics/tokenizers", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Message", "generated/torchtune.data.OpenAIToMessages", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.format_content_with_images", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.left_pad_sequence", "generated/torchtune.data.load_image", "generated/torchtune.data.padded_collate", "generated/torchtune.data.padded_collate_dpo", "generated/torchtune.data.padded_collate_sft", "generated/torchtune.data.padded_collate_tiled_images_and_mask", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.hh_rlhf_helpful_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.multimodal.llava_instruct_dataset", "generated/torchtune.datasets.multimodal.the_cauldron_dataset", "generated/torchtune.datasets.preference_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.generation.generate", "generated/torchtune.generation.generate_next_token", "generated/torchtune.generation.get_causal_mask_from_padding_mask", "generated/torchtune.generation.get_position_ids_from_padding_mask", "generated/torchtune.generation.sample", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_405b", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.llama3_2.llama3_2_1b", "generated/torchtune.models.llama3_2.llama3_2_3b", "generated/torchtune.models.llama3_2.lora_llama3_2_1b", "generated/torchtune.models.llama3_2.lora_llama3_2_3b", "generated/torchtune.models.llama3_2.qlora_llama3_2_1b", "generated/torchtune.models.llama3_2.qlora_llama3_2_3b", "generated/torchtune.models.llama3_2_vision.Llama3VisionEncoder", "generated/torchtune.models.llama3_2_vision.Llama3VisionProjectionHead", "generated/torchtune.models.llama3_2_vision.Llama3VisionTransform", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_11b", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_decoder", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_encoder", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_transform", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_decoder", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_encoder", "generated/torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.MultiHeadAttention", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TanhGate", "generated/torchtune.modules.TiedLinear", "generated/torchtune.modules.TransformerCrossAttentionLayer", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerSelfAttentionLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss", "generated/torchtune.modules.model_fusion.DeepFusionModel", "generated/torchtune.modules.model_fusion.FusionEmbedding", "generated/torchtune.modules.model_fusion.FusionLayer", "generated/torchtune.modules.model_fusion.get_fusion_params", "generated/torchtune.modules.model_fusion.register_fusion_module", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.rlhf.estimate_advantages", "generated/torchtune.rlhf.get_rewards_ppo", "generated/torchtune.rlhf.loss.DPOLoss", "generated/torchtune.rlhf.loss.PPOLoss", "generated/torchtune.rlhf.loss.RSOLoss", "generated/torchtune.rlhf.loss.SimPOLoss", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.training.FSDPPolicyType", "generated/torchtune.training.FormattedCheckpointFiles", "generated/torchtune.training.FullModelHFCheckpointer", "generated/torchtune.training.FullModelMetaCheckpointer", "generated/torchtune.training.FullModelTorchTuneCheckpointer", "generated/torchtune.training.ModelType", "generated/torchtune.training.OptimizerInBackwardWrapper", "generated/torchtune.training.apply_selective_activation_checkpointing", "generated/torchtune.training.create_optim_in_bwd_wrapper", "generated/torchtune.training.get_dtype", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.training.get_memory_stats", "generated/torchtune.training.get_quantizer_mode", "generated/torchtune.training.get_unmasked_sequence_lengths", "generated/torchtune.training.get_world_size_and_rank", "generated/torchtune.training.init_distributed", "generated/torchtune.training.is_distributed", "generated/torchtune.training.log_memory_stats", "generated/torchtune.training.lora_fsdp_wrap_policy", "generated/torchtune.training.metric_logging.CometLogger", "generated/torchtune.training.metric_logging.DiskLogger", "generated/torchtune.training.metric_logging.StdoutLogger", "generated/torchtune.training.metric_logging.TensorBoardLogger", "generated/torchtune.training.metric_logging.WandBLogger", "generated/torchtune.training.register_optim_in_bwd_hooks", "generated/torchtune.training.set_activation_checkpointing", "generated/torchtune.training.set_default_dtype", "generated/torchtune.training.set_seed", "generated/torchtune.training.setup_torch_profiler", "generated/torchtune.training.update_state_dict_for_classifier", "generated/torchtune.training.validate_expected_param_dtype", "generated/torchtune.utils.batch_to_device", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.torch_version_ge", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "recipes/lora_finetune_single_device", "recipes/qat_distributed", "recipes/recipes_overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/memory_optimizations", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_generation.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_rlhf.rst", "api_ref_training.rst", "api_ref_utilities.rst", "basics/chat_datasets.rst", "basics/datasets_overview.rst", "basics/instruct_datasets.rst", "basics/message_transforms.rst", "basics/messages.rst", "basics/model_transforms.rst", "basics/multimodal_datasets.rst", "basics/preference_datasets.rst", "basics/prompt_templates.rst", "basics/tokenizers.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.OpenAIToMessages.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.format_content_with_images.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.left_pad_sequence.rst", "generated/torchtune.data.load_image.rst", "generated/torchtune.data.padded_collate.rst", "generated/torchtune.data.padded_collate_dpo.rst", "generated/torchtune.data.padded_collate_sft.rst", "generated/torchtune.data.padded_collate_tiled_images_and_mask.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.hh_rlhf_helpful_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.multimodal.llava_instruct_dataset.rst", "generated/torchtune.datasets.multimodal.the_cauldron_dataset.rst", "generated/torchtune.datasets.preference_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.generation.generate.rst", "generated/torchtune.generation.generate_next_token.rst", "generated/torchtune.generation.get_causal_mask_from_padding_mask.rst", "generated/torchtune.generation.get_position_ids_from_padding_mask.rst", "generated/torchtune.generation.sample.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_405b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.llama3_2.llama3_2_1b.rst", "generated/torchtune.models.llama3_2.llama3_2_3b.rst", "generated/torchtune.models.llama3_2.lora_llama3_2_1b.rst", "generated/torchtune.models.llama3_2.lora_llama3_2_3b.rst", "generated/torchtune.models.llama3_2.qlora_llama3_2_1b.rst", "generated/torchtune.models.llama3_2.qlora_llama3_2_3b.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionEncoder.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionProjectionHead.rst", "generated/torchtune.models.llama3_2_vision.Llama3VisionTransform.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_11b.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_decoder.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_encoder.rst", "generated/torchtune.models.llama3_2_vision.llama3_2_vision_transform.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_11b.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_decoder.rst", "generated/torchtune.models.llama3_2_vision.lora_llama3_2_vision_encoder.rst", "generated/torchtune.models.llama3_2_vision.qlora_llama3_2_vision_11b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.MultiHeadAttention.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TanhGate.rst", "generated/torchtune.modules.TiedLinear.rst", "generated/torchtune.modules.TransformerCrossAttentionLayer.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerSelfAttentionLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss.rst", "generated/torchtune.modules.model_fusion.DeepFusionModel.rst", "generated/torchtune.modules.model_fusion.FusionEmbedding.rst", "generated/torchtune.modules.model_fusion.FusionLayer.rst", "generated/torchtune.modules.model_fusion.get_fusion_params.rst", "generated/torchtune.modules.model_fusion.register_fusion_module.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.rlhf.estimate_advantages.rst", "generated/torchtune.rlhf.get_rewards_ppo.rst", "generated/torchtune.rlhf.loss.DPOLoss.rst", "generated/torchtune.rlhf.loss.PPOLoss.rst", "generated/torchtune.rlhf.loss.RSOLoss.rst", "generated/torchtune.rlhf.loss.SimPOLoss.rst", "generated/torchtune.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.training.FSDPPolicyType.rst", "generated/torchtune.training.FormattedCheckpointFiles.rst", "generated/torchtune.training.FullModelHFCheckpointer.rst", "generated/torchtune.training.FullModelMetaCheckpointer.rst", "generated/torchtune.training.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.training.ModelType.rst", "generated/torchtune.training.OptimizerInBackwardWrapper.rst", "generated/torchtune.training.apply_selective_activation_checkpointing.rst", "generated/torchtune.training.create_optim_in_bwd_wrapper.rst", "generated/torchtune.training.get_dtype.rst", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.training.get_memory_stats.rst", "generated/torchtune.training.get_quantizer_mode.rst", "generated/torchtune.training.get_unmasked_sequence_lengths.rst", "generated/torchtune.training.get_world_size_and_rank.rst", "generated/torchtune.training.init_distributed.rst", "generated/torchtune.training.is_distributed.rst", "generated/torchtune.training.log_memory_stats.rst", "generated/torchtune.training.lora_fsdp_wrap_policy.rst", "generated/torchtune.training.metric_logging.CometLogger.rst", "generated/torchtune.training.metric_logging.DiskLogger.rst", "generated/torchtune.training.metric_logging.StdoutLogger.rst", "generated/torchtune.training.metric_logging.TensorBoardLogger.rst", "generated/torchtune.training.metric_logging.WandBLogger.rst", "generated/torchtune.training.register_optim_in_bwd_hooks.rst", "generated/torchtune.training.set_activation_checkpointing.rst", "generated/torchtune.training.set_default_dtype.rst", "generated/torchtune.training.set_seed.rst", "generated/torchtune.training.setup_torch_profiler.rst", "generated/torchtune.training.update_state_dict_for_classifier.rst", "generated/torchtune.training.validate_expected_param_dtype.rst", "generated/torchtune.utils.batch_to_device.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "recipes/lora_finetune_single_device.rst", "recipes/qat_distributed.rst", "recipes/recipes_overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/memory_optimizations.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.generation", "torchtune.models", "torchtune.modules", "torchtune.rlhf", "torchtune.training", "torchtune.utils", "Chat Datasets", "Datasets Overview", "Instruct Datasets", "Message Transforms", "Messages", "Multimodal Transforms", "Multimodal Datasets", "Preference Datasets", "Prompt Templates", "Tokenizers", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "ChatFormat", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "Message", "OpenAIToMessages", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "format_content_with_images", "get_openai_messages", "get_sharegpt_messages", "left_pad_sequence", "load_image", "padded_collate", "padded_collate_dpo", "padded_collate_sft", "padded_collate_tiled_images_and_mask", "truncate", "validate_messages", "torchtune.datasets.ChatDataset", "ConcatDataset", "torchtune.datasets.InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "hh_rlhf_helpful_dataset", "instruct_dataset", "llava_instruct_dataset", "the_cauldron_dataset", "preference_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "wikitext_dataset", "generate", "generate_next_token", "get_causal_mask_from_padding_mask", "get_position_ids_from_padding_mask", "sample", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_405b", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_405b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "llama3_2_1b", "llama3_2_3b", "lora_llama3_2_1b", "lora_llama3_2_3b", "qlora_llama3_2_1b", "qlora_llama3_2_3b", "Llama3VisionEncoder", "Llama3VisionProjectionHead", "Llama3VisionTransform", "llama3_2_vision_11b", "llama3_2_vision_decoder", "llama3_2_vision_encoder", "llama3_2_vision_transform", "lora_llama3_2_vision_11b", "lora_llama3_2_vision_decoder", "lora_llama3_2_vision_encoder", "qlora_llama3_2_vision_11b", "MistralChatTemplate", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "FeedForward", "Fp32LayerNorm", "KVCache", "MultiHeadAttention", "RMSNorm", "RotaryPositionalEmbeddings", "TanhGate", "TiedLinear", "TransformerCrossAttentionLayer", "TransformerDecoder", "TransformerSelfAttentionLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "CEWithChunkedOutputLoss", "DeepFusionModel", "FusionEmbedding", "FusionLayer", "get_fusion_params", "register_fusion_module", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "estimate_advantages", "get_rewards_ppo", "DPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "truncate_sequence_at_first_stop_token", "torchtune.training.FSDPPolicyType", "FormattedCheckpointFiles", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "apply_selective_activation_checkpointing", "create_optim_in_bwd_wrapper", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_memory_stats", "get_quantizer_mode", "get_unmasked_sequence_lengths", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "update_state_dict_for_classifier", "validate_expected_param_dtype", "batch_to_device", "get_device", "get_logger", "torch_version_ge", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "LoRA Single Device Finetuning", "Distributed Quantization-Aware Training (QAT)", "Recipes Overview", "Computation times", "torchtune CLI", "Fine-Tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Fine-Tuning Llama2 with LoRA", "Memory Optimization Overview", "Fine-Tuning Llama3 with QAT", "Fine-Tuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 4, 9, 10, 12, 14, 15, 16, 17, 18, 29, 30, 32, 33, 35, 40, 55, 56, 58, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 143, 146, 147, 151, 159, 165, 166, 167, 174, 175, 176, 257, 260, 261, 264, 265, 268, 270, 272, 273], "prompt": [1, 9, 10, 11, 12, 16, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 43, 44, 53, 55, 57, 58, 61, 62, 64, 65, 66, 69, 70, 71, 72, 75, 76, 92, 98, 104, 117, 142, 146, 151, 160, 167, 177, 187, 193, 210, 266, 267, 269], "chat": [1, 2, 10, 12, 15, 16, 28, 29, 35, 40, 43, 44, 53, 58, 62, 98, 167, 260], "includ": [1, 9, 10, 11, 15, 16, 17, 18, 19, 21, 22, 28, 33, 36, 37, 58, 79, 89, 99, 114, 123, 144, 145, 146, 148, 149, 156, 167, 173, 187, 199, 205, 222, 223, 259, 262, 264, 265, 266, 267, 268, 269, 270, 273], "some": [1, 16, 18, 19, 21, 29, 154, 194, 196, 201, 202, 257, 259, 260, 261, 264, 265, 266, 267, 268, 270, 271, 272, 273], "specif": [1, 5, 10, 11, 14, 17, 18, 21, 22, 24, 57, 58, 67, 68, 142, 206, 230, 261, 265, 266, 267, 271, 272, 273], "format": [1, 2, 7, 10, 17, 18, 28, 33, 34, 43, 45, 46, 53, 55, 57, 58, 61, 62, 65, 66, 69, 98, 142, 151, 206, 221, 222, 223, 224, 225, 264, 265, 267, 268, 269, 270, 271], "differ": [1, 9, 16, 17, 18, 21, 23, 48, 54, 55, 62, 66, 142, 189, 207, 215, 225, 250, 259, 260, 261, 264, 265, 267, 269, 270, 271, 272, 273], "dataset": [1, 12, 13, 14, 17, 21, 30, 32, 33, 34, 35, 40, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 215, 259, 268, 269, 272], "model": [1, 2, 9, 10, 11, 12, 13, 15, 16, 17, 19, 20, 21, 22, 24, 29, 30, 32, 34, 35, 40, 53, 54, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 180, 181, 182, 183, 185, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 209, 210, 211, 213, 214, 215, 216, 217, 218, 222, 223, 224, 225, 227, 228, 230, 231, 238, 239, 244, 245, 249, 257, 259, 260, 261, 265, 266, 273], "from": [1, 2, 4, 10, 12, 13, 14, 17, 19, 20, 21, 22, 23, 24, 30, 33, 34, 35, 40, 44, 45, 46, 47, 50, 53, 54, 55, 56, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 90, 91, 98, 100, 101, 102, 103, 117, 141, 142, 146, 157, 159, 167, 174, 175, 176, 177, 178, 181, 186, 187, 188, 189, 191, 192, 195, 196, 197, 198, 201, 204, 207, 209, 212, 215, 217, 218, 221, 222, 223, 224, 226, 228, 239, 242, 243, 244, 249, 256, 258, 261, 263, 264, 266, 267, 268, 269, 270, 271, 272], "common": [1, 2, 5, 9, 13, 14, 21, 210, 264, 265, 266, 269, 270, 271, 272], "json": [1, 9, 11, 12, 15, 16, 18, 19, 35, 40, 43, 44, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 117, 146, 167, 177, 209, 222, 264, 265, 266, 267, 272], "schema": [1, 9, 10, 11, 15], "convers": [1, 12, 15, 16, 17, 18, 19, 28, 30, 40, 43, 44, 52, 53, 57, 58, 62, 67, 69, 71, 222, 224, 225, 259, 265, 266, 267, 270, 271, 273], "list": [1, 9, 10, 13, 14, 16, 17, 18, 19, 21, 28, 30, 34, 36, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 62, 63, 67, 68, 69, 74, 75, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 96, 97, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 117, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 142, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 187, 189, 192, 193, 194, 195, 198, 199, 203, 204, 205, 206, 207, 208, 210, 212, 221, 222, 223, 224, 239, 253, 262, 265, 266, 267, 268, 269, 271, 272], "us": [1, 2, 4, 5, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 23, 24, 26, 28, 29, 32, 33, 34, 36, 42, 43, 44, 47, 50, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 98, 99, 105, 114, 117, 118, 123, 127, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 163, 167, 169, 173, 177, 178, 180, 181, 182, 183, 185, 187, 188, 189, 190, 192, 193, 194, 197, 200, 203, 207, 208, 212, 213, 214, 215, 216, 218, 220, 222, 223, 225, 226, 229, 230, 231, 238, 239, 240, 241, 242, 243, 247, 249, 251, 252, 257, 258, 259, 260, 261, 262, 264, 266, 268, 269, 270, 271, 272], "collect": [1, 21, 268], "sampl": [1, 9, 10, 11, 12, 13, 14, 15, 17, 18, 20, 23, 28, 30, 32, 33, 34, 35, 40, 42, 43, 44, 50, 53, 55, 56, 57, 58, 59, 64, 65, 67, 68, 69, 70, 71, 73, 75, 76, 181, 183, 187, 188, 189, 193, 211, 212, 217, 265, 267, 271], "batch": [1, 10, 22, 47, 48, 49, 50, 56, 61, 64, 67, 68, 70, 140, 141, 180, 181, 183, 186, 187, 188, 189, 193, 195, 213, 214, 215, 217, 218, 233, 248, 251, 259, 266, 268, 269, 270, 271], "handl": [1, 12, 15, 21, 26, 54, 58, 142, 207, 208, 265, 267, 270, 271, 273], "ani": [1, 5, 10, 12, 13, 14, 15, 18, 19, 21, 22, 24, 26, 27, 30, 33, 34, 35, 36, 40, 42, 43, 44, 47, 50, 51, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 78, 179, 187, 190, 193, 195, 196, 201, 202, 203, 204, 205, 206, 207, 210, 222, 223, 224, 226, 235, 238, 239, 247, 250, 264, 265, 266, 268, 270, 271, 272], "pad": [1, 45, 47, 48, 49, 50, 56, 75, 77, 78, 187, 189, 212, 214, 216, 219, 233, 266], "miscellan": 1, "modifi": [1, 18, 21, 22, 23, 190, 259, 267, 269, 270, 271, 272, 273], "For": [2, 7, 9, 11, 13, 15, 16, 17, 18, 19, 21, 22, 30, 34, 35, 36, 40, 50, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 89, 93, 99, 105, 114, 118, 123, 127, 141, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 181, 187, 189, 192, 193, 194, 197, 200, 211, 222, 228, 232, 239, 243, 245, 247, 258, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "detail": [2, 9, 11, 12, 15, 18, 19, 53, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 158, 180, 189, 192, 216, 220, 230, 238, 247, 260, 261, 264, 266, 267, 268, 269, 270, 271, 272, 273], "usag": [2, 18, 190, 192, 221, 225, 226, 248, 258, 264, 266, 267, 268, 269, 271, 272, 273], "guid": [2, 20, 21, 23, 30, 32, 35, 40, 62, 64, 65, 66, 67, 68, 69, 70, 71, 218, 239, 259, 265, 266, 268, 270], "pleas": [2, 7, 28, 31, 33, 38, 41, 43, 44, 53, 55, 86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 189, 192, 220, 230, 238, 245, 258, 261, 262, 267, 269, 273], "see": [2, 7, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 23, 31, 38, 41, 47, 53, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 86, 87, 88, 96, 97, 98, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 151, 158, 161, 162, 168, 180, 189, 198, 205, 206, 211, 220, 225, 230, 238, 239, 243, 245, 247, 253, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "our": [2, 11, 12, 19, 22, 259, 260, 261, 262, 265, 266, 267, 268, 270, 271, 272, 273], "tutori": [2, 19, 245, 259, 260, 261, 265, 266, 267, 268, 269, 270, 271, 272, 273], "support": [2, 10, 14, 15, 16, 18, 19, 20, 22, 23, 24, 34, 35, 53, 55, 56, 57, 58, 61, 62, 63, 64, 67, 68, 69, 70, 71, 74, 79, 93, 105, 118, 127, 140, 147, 148, 149, 151, 152, 154, 163, 166, 167, 169, 179, 181, 189, 194, 195, 199, 217, 223, 224, 226, 229, 231, 232, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "sever": [2, 271], "wide": [2, 9, 181], "onli": [2, 4, 15, 16, 19, 20, 23, 34, 40, 56, 57, 58, 63, 69, 75, 79, 93, 105, 118, 127, 142, 147, 148, 149, 151, 152, 154, 163, 169, 181, 185, 187, 189, 192, 196, 199, 201, 203, 207, 222, 223, 224, 226, 229, 230, 231, 232, 238, 264, 266, 267, 268, 270, 271, 272, 273], "help": [2, 10, 16, 17, 19, 65, 98, 187, 189, 193, 222, 239, 257, 258, 259, 264, 265, 266, 267, 268, 271, 272, 273], "quickli": [2, 10, 21, 36, 59, 260, 265, 266, 271], "bootstrap": [2, 10], "your": [2, 7, 9, 10, 11, 12, 13, 15, 16, 18, 20, 23, 24, 36, 53, 59, 62, 66, 69, 145, 149, 189, 194, 239, 242, 243, 249, 257, 258, 259, 260, 261, 264, 265, 266, 269, 270, 271, 272, 273], "fine": [2, 9, 10, 11, 15, 16, 17, 19, 20, 22, 23, 34, 56, 57, 58, 73, 249, 257, 259, 260, 261, 262, 267], "tune": [2, 4, 9, 10, 11, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 34, 56, 57, 58, 73, 249, 257, 258, 259, 260, 261, 262, 264, 267], "also": [2, 9, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 54, 62, 66, 69, 73, 75, 76, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 167, 169, 173, 181, 187, 199, 218, 230, 231, 238, 239, 243, 249, 252, 258, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "like": [2, 6, 11, 19, 20, 21, 22, 23, 53, 167, 189, 192, 194, 224, 258, 264, 265, 266, 267, 268, 270, 271, 272], "These": [2, 5, 12, 14, 16, 17, 18, 19, 21, 22, 24, 56, 57, 69, 189, 212, 260, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273], "ar": [2, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 28, 33, 36, 37, 40, 43, 44, 45, 47, 48, 52, 55, 56, 57, 58, 61, 62, 66, 67, 68, 69, 75, 77, 78, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 98, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 142, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 180, 186, 187, 188, 189, 193, 194, 195, 199, 200, 203, 204, 212, 214, 220, 222, 223, 225, 226, 228, 229, 231, 236, 238, 248, 249, 258, 259, 260, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "especi": [2, 259, 264, 267, 271], "specifi": [2, 11, 15, 16, 19, 21, 22, 24, 30, 32, 35, 40, 42, 62, 64, 65, 66, 67, 68, 69, 70, 71, 75, 77, 79, 92, 99, 104, 105, 114, 117, 118, 123, 127, 144, 146, 148, 160, 167, 169, 173, 177, 181, 187, 188, 193, 220, 230, 232, 238, 243, 245, 248, 261, 262, 264, 265, 266, 267, 268, 269, 271, 272, 273], "yaml": [2, 16, 21, 22, 24, 25, 26, 54, 62, 66, 69, 73, 243, 259, 262, 264, 265, 266, 267, 268, 269, 270, 272, 273], "config": [2, 9, 11, 12, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 54, 62, 66, 69, 73, 181, 203, 222, 226, 239, 243, 248, 259, 260, 261, 262, 265, 266, 267, 269, 270, 271, 272, 273], "represent": [2, 221, 270, 272, 273], "abov": [2, 4, 9, 15, 16, 19, 57, 190, 236, 258, 261, 267, 269, 270, 271, 272, 273], "text": [4, 5, 9, 10, 11, 14, 16, 17, 18, 34, 35, 36, 37, 40, 42, 50, 53, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 142, 194, 195, 205, 207, 208, 210, 212, 265, 267, 272], "version": [4, 60, 75, 93, 105, 118, 127, 147, 152, 154, 163, 169, 181, 254, 258, 269, 271, 272, 273], "famili": [4, 19, 22, 61, 63, 67, 68, 71, 72, 74, 225, 259, 264, 269], "import": [4, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 24, 62, 66, 67, 68, 69, 73, 79, 189, 215, 239, 242, 243, 265, 266, 267, 268, 269, 270, 272, 273], "you": [4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 33, 34, 36, 53, 55, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 98, 180, 187, 189, 192, 195, 197, 225, 239, 242, 243, 249, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "need": [4, 9, 11, 13, 15, 16, 17, 19, 20, 21, 22, 23, 33, 36, 53, 56, 58, 181, 187, 189, 193, 194, 218, 238, 239, 242, 243, 244, 258, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 273], "request": [4, 229, 266, 267], "access": [4, 19, 21, 22, 54, 222, 228, 260, 261, 264, 266, 267, 268], "hug": [4, 10, 19, 29, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 117, 146, 167, 177, 191, 209, 259, 264, 268, 269], "face": [4, 10, 19, 29, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 117, 146, 167, 177, 191, 209, 259, 264, 268, 269], "befor": [4, 17, 19, 36, 52, 55, 56, 67, 89, 93, 145, 149, 181, 186, 187, 188, 189, 192, 193, 195, 199, 208, 222, 239, 261, 264, 267, 271, 272], "download": [4, 10, 15, 19, 67, 255, 258, 260, 261, 265, 266, 269, 270, 272, 273], "To": [4, 9, 11, 12, 13, 15, 16, 17, 19, 21, 22, 23, 56, 67, 187, 189, 195, 222, 249, 258, 259, 261, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273], "1b": [4, 134, 136, 138], "meta": [4, 14, 15, 18, 19, 98, 183, 222, 223, 260, 261, 264, 265, 267, 268], "output": [4, 11, 12, 13, 18, 19, 32, 33, 45, 54, 55, 57, 58, 61, 64, 66, 70, 71, 75, 83, 84, 85, 89, 93, 99, 103, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 140, 141, 144, 145, 147, 148, 149, 152, 153, 154, 155, 156, 159, 163, 164, 169, 172, 173, 178, 179, 181, 183, 184, 186, 187, 188, 189, 192, 193, 194, 195, 199, 202, 203, 204, 212, 224, 230, 241, 248, 249, 258, 260, 261, 264, 266, 267, 268, 269, 270, 271, 273], "dir": [4, 18, 19, 243, 258, 260, 261, 264, 267, 268, 269, 272], "tmp": [4, 9, 11, 13, 14, 15, 16, 17, 18, 21, 226, 260, 261, 265, 268], "ignor": [4, 9, 11, 19, 40, 73, 181, 185, 186, 188, 227, 249, 260, 261, 264], "pattern": [4, 17, 208, 260, 261, 264], "origin": [4, 14, 15, 18, 19, 60, 61, 65, 190, 194, 195, 199, 260, 261, 265, 267, 269, 270, 271, 272, 273], "consolid": [4, 19, 260, 261], "00": [4, 15, 19, 62, 66, 256, 260, 261, 263, 268], "pth": [4, 19, 221, 260, 261, 267], "hf": [4, 9, 16, 18, 19, 215, 217, 222, 264, 265, 267, 268, 269], "token": [4, 9, 10, 11, 12, 14, 15, 16, 17, 19, 21, 22, 34, 40, 47, 49, 50, 51, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 89, 92, 93, 99, 104, 105, 114, 117, 118, 123, 127, 142, 144, 145, 146, 148, 149, 152, 154, 156, 158, 160, 163, 165, 167, 169, 173, 177, 181, 183, 186, 187, 188, 189, 192, 193, 194, 195, 205, 206, 207, 208, 209, 210, 212, 214, 216, 219, 230, 233, 260, 264, 266, 267, 268, 269, 270, 271, 272, 273], "hf_token": [4, 18, 261], "3b": [4, 135, 137, 139], "The": [4, 9, 10, 11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 33, 34, 43, 44, 46, 47, 52, 53, 54, 55, 56, 57, 58, 62, 65, 66, 67, 68, 69, 72, 83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 140, 142, 145, 147, 148, 149, 152, 154, 163, 164, 169, 170, 171, 172, 179, 182, 183, 184, 185, 189, 190, 191, 192, 193, 194, 195, 200, 205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 217, 218, 220, 222, 224, 229, 232, 239, 243, 246, 248, 252, 253, 254, 258, 259, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "reus": [4, 259], "llama3_token": [4, 14, 18, 67, 68, 75, 265, 269], "class": [4, 12, 13, 14, 18, 21, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 43, 44, 53, 54, 55, 56, 57, 58, 59, 67, 68, 92, 98, 103, 104, 117, 140, 141, 142, 146, 151, 154, 158, 159, 160, 167, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 193, 194, 195, 196, 198, 199, 201, 202, 205, 206, 207, 208, 211, 212, 215, 216, 217, 218, 221, 222, 223, 224, 225, 226, 239, 240, 241, 242, 243, 262, 265, 266, 268, 270, 271, 273], "languag": [4, 15, 29, 75, 148, 194, 195, 199, 215, 249, 270, 271], "11b": [4, 143, 150], "8b": [4, 14, 15, 18, 116, 120, 122, 126, 128, 130, 133, 164, 260, 261, 264, 265, 272], "70b": [4, 81, 84, 87, 101, 107, 111, 115, 119, 121, 125, 129, 132, 269], "405b": [4, 124, 128, 131], "weight": [4, 18, 19, 22, 50, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 143, 146, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 185, 190, 198, 199, 203, 207, 215, 222, 223, 224, 225, 232, 243, 249, 257, 261, 264, 265, 267, 268, 269, 270, 271, 272, 273], "can": [4, 5, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 27, 30, 32, 34, 35, 36, 37, 40, 50, 54, 55, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 142, 145, 149, 182, 183, 185, 186, 187, 189, 192, 193, 195, 197, 200, 207, 208, 220, 222, 225, 227, 230, 238, 239, 242, 243, 245, 248, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "instead": [4, 9, 11, 13, 15, 19, 22, 43, 44, 45, 56, 57, 73, 127, 148, 149, 180, 185, 189, 199, 218, 264, 269, 270, 271, 272], "builder": [4, 9, 10, 11, 12, 14, 15, 19, 60, 62, 63, 66, 69, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 94, 95, 96, 97, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 119, 120, 121, 122, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 150, 153, 155, 157, 159, 161, 162, 164, 166, 168, 170, 171, 172, 174, 175, 176, 265, 266, 271, 273], "all": [4, 5, 10, 13, 14, 17, 18, 22, 27, 34, 36, 40, 45, 47, 50, 54, 56, 57, 58, 117, 140, 146, 167, 177, 181, 185, 187, 189, 190, 193, 194, 195, 197, 200, 211, 222, 226, 228, 236, 244, 250, 251, 255, 257, 259, 260, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272], "7b": [4, 9, 11, 13, 16, 17, 18, 19, 55, 63, 74, 82, 85, 88, 91, 95, 102, 103, 108, 109, 112, 113, 153, 155, 157, 159, 162, 172, 176, 222, 223, 265, 268, 269, 270, 273], "13b": [4, 19, 80, 83, 86, 100, 106, 110], "codellama": 4, "size": [4, 13, 14, 15, 19, 22, 24, 45, 50, 61, 64, 67, 68, 70, 140, 141, 142, 143, 145, 146, 147, 149, 180, 181, 182, 183, 186, 187, 188, 189, 192, 193, 194, 195, 212, 213, 214, 233, 234, 236, 259, 261, 264, 266, 267, 268, 269, 270, 271, 272], "0": [4, 9, 11, 13, 14, 15, 16, 19, 22, 45, 47, 48, 49, 50, 56, 62, 66, 69, 75, 76, 78, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 142, 144, 145, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 174, 175, 180, 181, 187, 189, 191, 194, 199, 210, 215, 216, 217, 218, 219, 232, 233, 239, 242, 243, 247, 252, 254, 256, 261, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273], "5b": [4, 170, 171, 174, 175, 271], "qwen2": [4, 169, 170, 171, 172, 174, 175, 176, 177, 225, 271], "exampl": [4, 17, 18, 19, 20, 21, 22, 23, 24, 26, 30, 35, 36, 40, 42, 45, 46, 47, 48, 49, 50, 54, 55, 56, 58, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 73, 74, 75, 77, 78, 79, 141, 142, 145, 149, 180, 181, 189, 192, 193, 194, 195, 197, 198, 200, 205, 206, 207, 208, 210, 211, 215, 217, 218, 219, 220, 221, 222, 223, 225, 226, 232, 233, 239, 242, 243, 246, 249, 252, 253, 254, 255, 256, 258, 260, 261, 263, 264, 265, 266, 267, 269, 270, 271, 272, 273], "none": [4, 9, 15, 22, 23, 25, 27, 30, 32, 33, 35, 40, 50, 51, 52, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 92, 99, 104, 105, 114, 117, 118, 123, 127, 140, 141, 142, 144, 146, 148, 160, 167, 177, 178, 180, 181, 183, 186, 187, 188, 189, 193, 195, 200, 202, 203, 204, 207, 210, 212, 213, 214, 216, 222, 223, 224, 225, 227, 229, 232, 237, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 250, 251, 252, 253, 264, 266, 267, 272], "mini": [4, 18, 164, 165, 166, 167, 168], "4k": [4, 18, 165, 166, 167], "microsoft": [4, 166, 167], "ai": [4, 11, 13, 17, 57, 58, 157, 243, 265, 269], "thi": [4, 9, 11, 12, 13, 15, 16, 17, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 40, 41, 42, 43, 44, 45, 47, 48, 50, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 77, 89, 93, 99, 105, 114, 118, 123, 127, 140, 142, 144, 145, 148, 149, 151, 152, 154, 156, 158, 163, 165, 166, 167, 169, 173, 178, 180, 181, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 200, 203, 204, 205, 206, 207, 208, 210, 211, 212, 214, 215, 216, 218, 220, 221, 222, 223, 224, 226, 229, 231, 233, 236, 238, 239, 240, 242, 243, 244, 245, 247, 249, 251, 252, 257, 258, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "v0": [4, 9, 13, 16, 17, 18, 151], "mistralai": [4, 18, 264], "2b": [4, 90, 94], "gemma2": 4, "googl": [4, 90, 91], "gguf": 4, "perform": [5, 11, 12, 17, 18, 19, 56, 75, 189, 192, 200, 211, 218, 259, 260, 261, 265, 267, 269, 272, 273], "direct": [5, 16, 22, 48, 83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164, 215, 258, 262], "encod": [5, 14, 18, 50, 58, 75, 76, 140, 141, 143, 144, 145, 147, 148, 149, 181, 186, 187, 188, 193, 194, 195, 197, 205, 207, 208, 210, 212, 215, 218, 265], "id": [5, 13, 18, 19, 47, 48, 49, 50, 53, 55, 56, 63, 67, 68, 74, 75, 76, 78, 79, 142, 181, 183, 187, 188, 193, 205, 206, 207, 208, 209, 210, 212, 222, 224, 239, 265, 266, 267], "decod": [5, 9, 11, 13, 14, 15, 16, 18, 62, 66, 69, 75, 89, 93, 99, 105, 114, 118, 123, 127, 141, 142, 143, 144, 145, 147, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 181, 186, 187, 188, 193, 195, 197, 205, 207, 208, 265], "typic": [5, 9, 11, 21, 30, 35, 40, 50, 56, 57, 58, 59, 73, 167, 197, 215, 218, 266, 271, 272, 273], "byte": [5, 18, 208, 271, 273], "pair": [5, 16, 18, 21, 48, 49, 65, 69, 72, 208, 266], "underli": [5, 12, 16, 18, 207, 271, 273], "helper": 5, "method": [5, 12, 13, 14, 17, 18, 19, 21, 22, 23, 26, 46, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 142, 187, 190, 192, 193, 196, 197, 198, 201, 203, 205, 206, 226, 232, 258, 259, 266, 270, 273], "two": [5, 14, 16, 17, 19, 21, 32, 50, 52, 67, 68, 75, 76, 189, 194, 197, 212, 219, 221, 259, 261, 267, 268, 269, 270, 271, 272, 273], "pre": [5, 9, 10, 11, 16, 17, 56, 57, 58, 59, 67, 68, 73, 98, 143, 146, 147, 189, 193, 195, 197, 261, 265, 266], "train": [5, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23, 30, 32, 50, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 98, 142, 143, 146, 147, 179, 181, 183, 187, 188, 190, 191, 192, 193, 194, 195, 197, 215, 218, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 257, 259, 260, 262, 264, 265, 266, 267, 269, 270, 271, 272, 273], "function": [5, 19, 21, 22, 24, 26, 45, 46, 47, 48, 53, 55, 62, 66, 69, 75, 145, 149, 178, 181, 189, 190, 200, 203, 204, 215, 216, 220, 222, 234, 247, 249, 251, 252, 259, 266, 273], "preprocess": [5, 56, 189], "imag": [5, 10, 14, 34, 35, 40, 42, 46, 50, 58, 67, 68, 140, 141, 142, 143, 145, 146, 147, 149, 189, 194, 212, 270], "compon": [6, 13, 18, 19, 22, 27, 48, 57, 58, 67, 68, 259, 262, 266, 268, 270, 273], "loss": [6, 9, 11, 13, 21, 22, 34, 36, 55, 57, 58, 61, 62, 64, 66, 69, 70, 71, 192, 215, 216, 217, 218, 268, 270, 273], "algorithm": [6, 18, 213, 218, 247], "ppo": [6, 213, 214, 215, 216, 262], "dpo": [6, 16, 48, 57, 200, 215, 217, 218, 262], "offer": 7, "allow": [7, 54, 195, 203, 242, 261, 264, 271, 272, 273], "seamless": 7, "transit": 7, "between": [7, 9, 16, 17, 18, 19, 57, 62, 69, 144, 148, 186, 187, 193, 214, 216, 218, 222, 225, 239, 266, 267, 269, 270, 272, 273], "interoper": [7, 19, 22, 259, 267, 273], "rest": [7, 265, 271, 273], "ecosystem": [7, 19, 22, 259, 267, 269, 273], "comprehens": [7, 271], "overview": [7, 21, 23, 193, 257, 260, 261, 268, 270, 273], "deep": [7, 19, 20, 21, 22, 23, 195, 197, 259, 262, 268, 269, 271], "dive": [7, 19, 20, 21, 22, 23, 259, 261, 262, 268, 269, 271], "util": [7, 13, 15, 19, 21, 22, 24, 45, 47, 50, 140, 227, 242, 244, 245, 251, 252, 253, 254, 259, 267, 268, 271, 273], "work": [7, 19, 22, 40, 185, 194, 195, 259, 261, 264, 267, 269, 271, 273], "set": [7, 9, 11, 16, 19, 20, 21, 22, 23, 30, 34, 35, 40, 55, 56, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 99, 105, 114, 118, 123, 127, 142, 144, 147, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181, 183, 186, 187, 193, 200, 202, 220, 230, 236, 238, 239, 245, 246, 247, 248, 251, 252, 259, 262, 264, 265, 267, 268, 269, 270, 271, 272], "enabl": [7, 10, 18, 20, 21, 22, 23, 54, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 150, 153, 155, 161, 162, 164, 168, 170, 171, 172, 174, 175, 181, 195, 199, 247, 248, 261, 269, 270, 271, 273], "consumpt": [7, 54, 77, 260, 271], "dure": [7, 10, 19, 55, 56, 61, 62, 64, 66, 69, 70, 71, 180, 181, 183, 187, 188, 189, 190, 193, 194, 212, 218, 231, 260, 261, 265, 267, 269, 270, 271, 272, 273], "variou": [7, 33], "provid": [7, 10, 11, 13, 19, 21, 22, 24, 29, 30, 32, 35, 40, 46, 47, 51, 54, 55, 56, 75, 77, 181, 185, 187, 189, 193, 200, 210, 215, 224, 230, 239, 243, 248, 252, 259, 260, 261, 264, 265, 266, 267, 268, 269, 271], "debug": [7, 19, 21, 22, 239, 264], "finetun": [7, 19, 21, 22, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 193, 257, 259, 261, 268, 269, 271], "job": [7, 23, 247, 268], "involv": [9, 11, 58, 272], "multi": [9, 16, 22, 53, 181, 269], "turn": [9, 16, 22, 30, 34, 35, 40, 52, 53, 57, 69, 265, 271], "multipl": [9, 15, 16, 19, 21, 22, 30, 34, 35, 40, 48, 53, 54, 58, 69, 140, 141, 181, 187, 188, 189, 193, 199, 239, 240, 241, 242, 243, 248, 268, 269, 271], "back": [9, 18, 19, 52, 200, 222, 266, 270, 271, 273], "forth": [9, 52, 266], "user": [9, 11, 12, 13, 14, 15, 16, 17, 18, 22, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 52, 53, 57, 58, 62, 66, 69, 92, 99, 104, 105, 114, 117, 118, 123, 127, 144, 146, 148, 152, 154, 156, 158, 160, 163, 165, 167, 169, 173, 177, 181, 210, 262, 265, 266, 268, 272], "assist": [9, 11, 12, 13, 14, 15, 16, 17, 18, 28, 29, 30, 32, 34, 35, 36, 37, 39, 40, 42, 43, 44, 52, 53, 57, 58, 62, 69, 75, 92, 98, 104, 117, 146, 160, 167, 177, 210, 265, 266], "role": [9, 12, 13, 14, 15, 16, 17, 18, 28, 30, 34, 35, 36, 37, 40, 42, 43, 44, 53, 57, 58, 62, 69, 92, 104, 117, 142, 146, 160, 167, 177, 210, 265, 266], "content": [9, 12, 14, 15, 16, 17, 18, 19, 28, 30, 34, 35, 36, 37, 40, 42, 43, 44, 53, 57, 58, 62, 69, 210, 265, 266], "what": [9, 13, 14, 15, 16, 19, 20, 21, 23, 34, 35, 57, 58, 62, 66, 69, 98, 151, 189, 257, 262, 265, 266, 267, 268, 269, 271], "answer": [9, 14, 15, 17, 38, 66, 267, 269], "ultim": [9, 272], "question": [9, 14, 15, 17, 38, 66, 266, 267, 269], "life": 9, "42": [9, 75, 189], "That": [9, 265], "s": [9, 11, 12, 13, 15, 16, 17, 19, 21, 22, 23, 24, 26, 28, 29, 35, 40, 43, 44, 52, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 83, 84, 85, 98, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 140, 141, 142, 147, 148, 149, 151, 152, 153, 154, 155, 163, 164, 167, 169, 172, 173, 180, 181, 183, 187, 188, 189, 190, 193, 196, 197, 198, 201, 203, 204, 208, 215, 217, 218, 219, 220, 222, 223, 226, 230, 231, 233, 238, 239, 242, 245, 246, 249, 251, 252, 259, 264, 265, 266, 268, 270, 271, 272, 273], "ridicul": 9, "oh": 9, "i": [9, 11, 13, 16, 17, 22, 34, 69, 75, 98, 140, 141, 151, 181, 186, 187, 188, 189, 190, 193, 202, 221, 226, 266, 267, 269, 271, 272, 273], "know": [9, 265, 266, 267, 270], "more": [9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 36, 53, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 180, 189, 192, 197, 203, 220, 221, 224, 239, 243, 245, 247, 251, 259, 260, 261, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273], "structur": [9, 11, 12, 13, 17, 22, 28, 35, 37, 40, 43, 44, 53, 62, 117, 142, 146, 167, 177, 212, 265, 266, 267, 272], "than": [9, 11, 15, 16, 21, 52, 75, 77, 180, 181, 189, 215, 220, 224, 225, 250, 251, 254, 265, 266, 267, 268, 269, 270, 271, 273], "freeform": [9, 11, 59, 73], "associ": [9, 10, 11, 19, 21, 22, 75, 76, 89, 99, 114, 123, 144, 148, 156, 173, 239, 267, 270], "where": [9, 11, 13, 15, 16, 17, 34, 36, 45, 48, 53, 61, 75, 77, 78, 103, 140, 141, 159, 178, 181, 187, 189, 192, 193, 199, 207, 212, 213, 215, 216, 219, 230, 233, 238, 266, 271], "thei": [9, 10, 11, 17, 18, 21, 22, 54, 67, 68, 140, 145, 149, 187, 189, 195, 204, 230, 264, 265, 266, 270, 271, 272], "learn": [9, 11, 22, 54, 191, 194, 195, 197, 259, 260, 261, 262, 265, 266, 268, 269, 270, 271, 272, 273], "simpli": [9, 11, 12, 13, 15, 19, 21, 56, 58, 215, 264, 265, 266, 267, 269, 271, 273], "predict": [9, 11, 75, 76, 79, 213, 214, 216, 260], "next": [9, 11, 19, 56, 73, 75, 76, 189, 212, 260, 269, 273], "respond": [9, 266], "accur": 9, "primari": [9, 11, 15, 16, 19, 21, 22, 57, 58, 262, 268], "entri": [9, 11, 15, 16, 21, 22, 47, 50, 262, 268, 271], "point": [9, 11, 15, 16, 18, 21, 22, 43, 44, 46, 62, 210, 262, 266, 267, 268, 269, 270, 272, 273], "torchtun": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 258, 260, 261, 262, 265, 266, 268, 271], "chat_dataset": [9, 11, 12, 16, 53, 265, 266], "let": [9, 10, 11, 15, 16, 19, 21, 23, 264, 265, 266, 267, 268, 269, 270, 271, 273], "follow": [9, 10, 11, 14, 15, 17, 19, 22, 34, 35, 36, 40, 43, 44, 50, 53, 56, 57, 58, 66, 69, 142, 181, 186, 191, 212, 216, 224, 225, 226, 236, 243, 248, 257, 258, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "data": [9, 11, 12, 13, 14, 15, 17, 18, 20, 28, 29, 30, 32, 33, 34, 35, 36, 37, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 98, 146, 177, 189, 211, 215, 217, 231, 239, 240, 241, 242, 243, 251, 260, 261, 266, 267, 271, 272, 273], "directli": [9, 11, 12, 13, 15, 19, 21, 22, 24, 57, 58, 62, 66, 67, 69, 73, 215, 220, 222, 264, 267, 268, 269, 270, 271, 272, 273], "llm": [9, 10, 11, 18, 22, 193, 195, 257, 258, 259, 260, 262, 266, 267, 269, 270], "my_data": [9, 11, 12, 15, 265, 266], "human": [9, 15, 16, 34, 40, 44, 62, 98, 215, 216, 217, 265], "valu": [9, 15, 19, 21, 30, 32, 35, 40, 44, 45, 47, 48, 61, 62, 64, 65, 66, 69, 70, 71, 72, 75, 76, 78, 79, 80, 81, 82, 89, 90, 91, 93, 99, 100, 101, 102, 103, 105, 114, 115, 116, 118, 123, 124, 125, 126, 127, 134, 135, 142, 144, 148, 152, 154, 156, 157, 158, 159, 163, 165, 169, 173, 174, 175, 176, 180, 181, 182, 186, 187, 188, 191, 193, 195, 203, 213, 214, 216, 219, 222, 225, 226, 233, 239, 240, 241, 242, 243, 247, 261, 264, 265, 266, 268, 269, 270, 271, 272], "gpt": [9, 15, 40, 44, 62, 76, 265, 267], "mistral": [9, 13, 16, 17, 18, 53, 142, 151, 152, 153, 154, 155, 157, 158, 159, 160, 161, 162, 225, 264, 265, 267, 268], "mistral_token": [9, 13, 16, 17, 18], "m_token": [9, 13, 16, 17, 18], "path": [9, 11, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 40, 46, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 92, 104, 117, 142, 146, 160, 167, 177, 207, 208, 209, 222, 223, 224, 248, 264, 265, 266, 267, 269, 270], "1": [9, 13, 15, 16, 17, 18, 19, 22, 40, 45, 47, 48, 49, 50, 56, 71, 75, 76, 78, 79, 99, 105, 114, 118, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 141, 142, 144, 148, 151, 152, 154, 156, 158, 163, 165, 169, 170, 171, 173, 174, 175, 180, 181, 187, 189, 191, 192, 207, 208, 210, 215, 216, 217, 218, 223, 225, 233, 236, 239, 242, 243, 246, 247, 259, 260, 264, 265, 267, 268, 270, 271, 272, 273], "prompt_templ": [9, 11, 13, 15, 16, 17, 92, 104, 117, 142, 146, 160, 167, 177], "mistralchattempl": [9, 13, 16, 17, 160, 265], "max_seq_len": [9, 11, 13, 15, 16, 18, 21, 24, 47, 50, 51, 53, 55, 56, 61, 62, 63, 64, 66, 67, 68, 70, 71, 73, 74, 89, 92, 93, 99, 104, 105, 114, 117, 118, 123, 127, 142, 144, 146, 148, 152, 154, 156, 158, 160, 163, 165, 167, 169, 173, 177, 180, 181, 183, 187, 266, 272], "8192": [9, 11, 13, 15, 16, 18, 146, 270, 272], "ds": [9, 11, 14, 15, 16, 56, 71, 265], "sourc": [9, 11, 12, 15, 16, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 265, 266, 267, 272], "data_fil": [9, 11, 12, 15, 16, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 265, 266], "split": [9, 11, 12, 13, 15, 16, 19, 42, 53, 54, 55, 56, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 208, 265, 266, 267, 272], "conversation_column": [9, 62, 265], "conversation_styl": [9, 62, 265, 266], "By": [9, 11, 19, 261, 264, 270, 271, 272, 273], "default": [9, 11, 15, 19, 21, 29, 30, 32, 34, 35, 40, 43, 44, 45, 48, 49, 50, 51, 53, 55, 56, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 142, 143, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 182, 183, 186, 187, 188, 190, 191, 193, 199, 203, 207, 208, 210, 212, 213, 214, 215, 218, 222, 223, 224, 229, 234, 238, 239, 240, 243, 246, 247, 248, 258, 261, 264, 265, 266, 267, 269, 270, 271, 272, 273], "true": [9, 11, 12, 13, 14, 15, 19, 21, 34, 43, 44, 45, 54, 55, 56, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 73, 74, 77, 78, 86, 87, 88, 89, 93, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 142, 143, 150, 161, 162, 168, 181, 186, 187, 188, 190, 193, 195, 200, 207, 208, 210, 212, 213, 216, 219, 220, 222, 223, 224, 230, 231, 233, 235, 236, 239, 242, 248, 254, 260, 264, 265, 266, 267, 269, 270, 271, 272, 273], "train_on_input": [9, 11, 12, 16, 21, 30, 32, 35, 40, 43, 44, 53, 54, 55, 60, 61, 62, 64, 65, 66, 69, 70, 71, 72, 266], "new_system_prompt": [9, 11, 12, 30, 32, 35, 40, 62, 64, 65, 66, 67, 68, 69, 70, 71], "tokenized_dict": [9, 11, 14, 15, 16], "label": [9, 11, 22, 47, 48, 49, 50, 53, 55, 56, 63, 71, 74, 192, 215, 218], "print": [9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 42, 50, 54, 61, 64, 67, 68, 70, 71, 75, 142, 189, 207, 208, 210, 254, 265, 266, 268, 270, 272, 273], "inst": [9, 13, 17, 18, 53, 98, 142, 151, 265, 266], "733": [9, 13, 18], "16289": [9, 13, 18], "28793": [9, 13, 18], "1824": 9, "349": 9, "272": 9, "4372": 9, "In": [9, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 53, 57, 145, 149, 183, 187, 189, 199, 220, 238, 242, 243, 261, 265, 267, 269, 270, 271, 272, 273], "_component_": [9, 11, 12, 15, 16, 17, 18, 19, 20, 21, 23, 24, 54, 62, 66, 69, 73, 248, 261, 265, 266, 267, 269, 270, 271, 272], "null": [9, 19, 21, 272], "have": [9, 12, 13, 16, 18, 19, 21, 24, 32, 34, 57, 62, 69, 77, 140, 145, 149, 179, 180, 181, 182, 185, 187, 189, 192, 198, 204, 212, 218, 221, 224, 226, 230, 242, 250, 258, 265, 266, 267, 268, 269, 270, 271, 272, 273], "singl": [9, 15, 16, 17, 19, 21, 24, 28, 30, 32, 33, 35, 40, 43, 44, 47, 54, 56, 57, 58, 59, 62, 69, 73, 92, 103, 104, 117, 140, 141, 142, 145, 146, 149, 159, 160, 167, 181, 187, 189, 193, 222, 223, 224, 225, 226, 228, 262, 264, 265, 266, 267, 268, 269, 270, 271, 273], "name": [9, 11, 12, 13, 15, 16, 19, 20, 21, 23, 25, 30, 32, 33, 35, 40, 55, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 198, 202, 204, 208, 222, 223, 224, 225, 226, 228, 239, 240, 241, 242, 243, 249, 250, 252, 264, 265, 267, 269, 271, 272], "messag": [9, 10, 11, 14, 15, 16, 17, 18, 28, 29, 30, 32, 35, 36, 37, 40, 42, 43, 44, 52, 53, 57, 58, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 92, 104, 117, 142, 146, 160, 167, 206, 210, 258, 264, 265, 266], "contain": [9, 10, 12, 13, 14, 15, 16, 19, 30, 32, 34, 40, 43, 47, 48, 49, 50, 56, 57, 58, 59, 62, 67, 73, 117, 142, 146, 167, 177, 180, 181, 183, 187, 188, 193, 196, 198, 201, 202, 203, 208, 210, 213, 219, 222, 223, 224, 226, 228, 231, 237, 242, 248, 249, 251, 265, 267, 269, 270], "topic": [9, 257], "per": [9, 15, 47, 86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 141, 142, 150, 161, 162, 168, 180, 189, 190, 212, 214, 215, 264, 271, 272, 273], "could": [9, 16, 17, 43, 270], "system": [9, 11, 12, 16, 17, 28, 29, 30, 32, 34, 35, 36, 37, 39, 40, 42, 43, 44, 52, 53, 57, 58, 62, 64, 65, 66, 67, 68, 69, 70, 71, 92, 98, 104, 117, 146, 151, 160, 167, 177, 210, 265, 266], "tool": [9, 16, 17, 19, 34, 36, 58, 151, 239, 266, 267, 268], "call": [9, 13, 16, 18, 19, 24, 34, 36, 58, 67, 68, 151, 181, 187, 189, 190, 193, 203, 239, 240, 241, 242, 243, 244, 248, 249, 265, 266, 270, 273], "return": [9, 12, 14, 16, 17, 18, 24, 26, 28, 33, 34, 36, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 246, 247, 248, 252, 253, 254, 266, 270, 273], "dai": 9, "todai": 9, "It": [9, 13, 15, 29, 34, 36, 57, 58, 62, 64, 66, 67, 68, 70, 72, 142, 145, 149, 151, 185, 187, 189, 193, 215, 218, 239, 264, 265, 266, 273], "tuesdai": 9, "about": [9, 12, 13, 16, 19, 22, 67, 68, 189, 215, 218, 239, 243, 259, 260, 261, 262, 264, 265, 267, 268, 269, 270, 271, 272, 273], "tomorrow": 9, "wednesdai": 9, "As": [9, 11, 15, 19, 21, 22, 23, 199, 259, 267, 271, 273], "an": [9, 11, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 46, 52, 54, 55, 59, 62, 64, 66, 67, 68, 69, 70, 73, 74, 105, 118, 127, 142, 145, 147, 149, 152, 154, 158, 163, 169, 170, 171, 174, 175, 181, 185, 187, 189, 193, 194, 195, 197, 198, 200, 201, 202, 206, 211, 212, 215, 220, 221, 222, 223, 224, 226, 227, 230, 239, 243, 248, 252, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "slimorca": [9, 71], "pass": [9, 10, 11, 13, 14, 15, 16, 17, 18, 21, 24, 34, 36, 53, 54, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 89, 93, 99, 105, 114, 118, 123, 127, 147, 152, 154, 156, 158, 163, 165, 169, 173, 181, 187, 190, 193, 200, 204, 208, 216, 220, 224, 229, 230, 231, 235, 238, 239, 242, 243, 245, 248, 264, 265, 266, 270, 272, 273], "repo": [9, 11, 15, 16, 19, 67, 222, 223, 225, 264, 267], "select": [9, 227], "one": [9, 10, 11, 12, 15, 16, 19, 22, 30, 32, 35, 40, 47, 50, 52, 62, 68, 69, 189, 192, 210, 224, 239, 266, 267, 268, 269, 271, 273], "most": [9, 11, 12, 15, 16, 19, 21, 34, 36, 265, 268, 270, 271, 273], "gemma": [9, 11, 16, 90, 91, 92, 93, 94, 95, 96, 97, 185, 225, 271], "gemma_token": [9, 11, 16], "g_token": [9, 11, 16], "open": [9, 46, 71, 90, 91, 266, 267], "orca": [9, 71], "dedup": [9, 71], "recip": [9, 10, 11, 15, 16, 19, 20, 21, 23, 24, 25, 26, 142, 187, 193, 222, 223, 224, 259, 260, 261, 265, 266, 267, 269, 271, 273], "via": [9, 11, 13, 15, 16, 20, 21, 23, 57, 62, 66, 69, 73, 181, 187, 188, 199, 222, 270, 273], "http": [9, 11, 15, 24, 46, 53, 55, 59, 63, 65, 67, 73, 74, 76, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 150, 152, 153, 154, 155, 157, 159, 161, 162, 163, 164, 166, 167, 168, 170, 171, 172, 174, 175, 176, 181, 182, 183, 189, 191, 192, 212, 213, 215, 216, 217, 218, 220, 222, 223, 236, 239, 242, 243, 245, 247, 253, 258, 266, 267, 269], "ha": [9, 16, 19, 66, 75, 141, 184, 186, 187, 189, 192, 193, 196, 198, 200, 201, 204, 219, 224, 226, 249, 250, 265, 266, 267, 268, 269, 270, 271, 273], "addition": [9, 19, 207, 208, 218, 247, 265, 266, 270], "argument": [9, 11, 15, 19, 21, 24, 31, 33, 38, 41, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 181, 220, 230, 235, 239, 240, 242, 243, 245, 264, 265, 266, 270, 272], "load_dataset": [9, 11, 15, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 265, 266], "document": [9, 11, 15, 181, 187, 188, 220, 230, 238, 260, 262, 264, 266, 271], "file": [9, 10, 11, 15, 19, 20, 21, 22, 23, 24, 25, 26, 46, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 117, 142, 146, 167, 177, 207, 208, 209, 222, 223, 224, 240, 243, 248, 256, 259, 261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "raw": [9, 10, 12, 13, 15, 18, 42], "vari": [9, 50, 56, 187], "field": [9, 13, 14, 24, 32, 33, 34, 40, 42, 43, 44, 53, 56, 57, 58, 61, 67, 68, 237, 266], "indic": [9, 13, 15, 16, 17, 50, 54, 55, 56, 77, 78, 145, 149, 181, 183, 187, 188, 189, 193, 194, 212, 213, 216, 219, 220, 233, 236, 265], "There": [9, 21, 52, 265, 268, 269, 270, 271], "few": [9, 195, 266, 269, 270, 273], "standard": [9, 11, 13, 14, 17, 19, 31, 43, 57, 58, 62, 65, 99, 105, 114, 118, 123, 127, 142, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181, 241, 259, 265, 267, 269], "across": [9, 19, 22, 50, 54, 222, 242, 247, 267, 269, 272], "mani": [9, 13, 15, 17, 21, 56, 260, 261, 266, 267], "we": [9, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 47, 50, 55, 56, 57, 58, 62, 63, 69, 74, 75, 79, 180, 181, 183, 187, 188, 189, 192, 193, 199, 215, 218, 222, 223, 224, 229, 232, 238, 244, 249, 259, 260, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "convert": [9, 10, 13, 18, 19, 30, 32, 35, 40, 43, 44, 49, 53, 57, 58, 62, 67, 68, 69, 77, 140, 222, 267, 272, 273], "ipython": [9, 13, 17, 34, 36, 39, 57, 58, 92, 104, 117, 146, 160, 167, 177], "transform": [9, 10, 15, 19, 22, 30, 32, 53, 55, 57, 58, 61, 62, 64, 65, 67, 68, 69, 70, 71, 83, 84, 85, 89, 93, 94, 95, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 141, 142, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 158, 163, 164, 165, 169, 170, 171, 172, 173, 186, 187, 188, 189, 191, 195, 212, 245, 270, 271, 272], "sharegpttomessag": [9, 12, 44, 62, 71], "expect": [9, 11, 12, 14, 15, 16, 17, 19, 21, 24, 30, 32, 33, 34, 35, 40, 46, 50, 53, 55, 57, 58, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 141, 142, 183, 193, 204, 226, 239, 243, 250, 265, 266, 270, 272], "code": [9, 11, 12, 15, 17, 18, 19, 22, 80, 81, 82, 83, 84, 85, 86, 87, 88, 187, 239, 255, 259, 266, 268], "openaitomessag": [9, 12, 43, 62, 69], "If": [9, 12, 13, 15, 17, 18, 19, 21, 27, 30, 32, 33, 34, 35, 40, 42, 43, 46, 47, 50, 51, 52, 53, 55, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 73, 75, 77, 79, 92, 99, 104, 105, 114, 117, 118, 123, 127, 140, 142, 144, 146, 148, 160, 167, 169, 173, 177, 180, 181, 183, 185, 187, 188, 189, 190, 192, 193, 199, 204, 210, 222, 223, 224, 225, 226, 227, 229, 230, 231, 232, 235, 239, 242, 243, 247, 248, 250, 252, 258, 264, 265, 266, 267, 268, 269, 270, 271, 272], "doe": [9, 19, 42, 43, 50, 53, 56, 69, 73, 89, 151, 156, 166, 181, 185, 187, 188, 193, 198, 210, 222, 224, 226, 249, 264, 267, 272], "fit": [9, 22, 53, 55, 56, 63, 73, 74, 189, 215, 265, 266], "creat": [9, 12, 15, 17, 19, 21, 24, 36, 56, 58, 62, 69, 77, 80, 81, 82, 83, 84, 85, 86, 87, 88, 90, 91, 94, 95, 96, 97, 100, 101, 102, 103, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 119, 120, 121, 122, 124, 125, 126, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 150, 153, 155, 157, 159, 161, 162, 164, 166, 168, 170, 171, 172, 174, 175, 176, 180, 181, 187, 188, 189, 191, 220, 222, 223, 224, 228, 239, 240, 242, 264, 266, 267, 273], "custom": [9, 14, 15, 18, 21, 22, 28, 33, 36, 53, 55, 57, 58, 62, 66, 67, 68, 69, 73, 92, 104, 117, 146, 160, 167, 177, 245, 259, 260, 261, 264, 268, 269, 270, 271], "dialogu": [9, 15, 41, 70, 265], "defin": [9, 19, 21, 22, 36, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 181, 186, 187, 193, 196, 198, 199, 201, 214, 266, 268, 270], "same": [9, 10, 14, 16, 19, 21, 36, 45, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 141, 164, 170, 171, 172, 179, 180, 182, 184, 185, 186, 188, 189, 193, 195, 210, 216, 218, 219, 226, 230, 243, 249, 251, 261, 264, 265, 267, 269, 270, 271, 272, 273], "wai": [9, 13, 17, 19, 21, 53, 57, 58, 203, 221, 264, 266, 267, 268, 269], "instruct_dataset": [9, 11, 12, 54, 55, 266], "info": [9, 253, 268], "slimorca_dataset": [9, 21], "vlm": [10, 15], "found": [10, 19, 20, 21, 23, 182, 183, 222, 223, 224, 261, 264, 270, 273], "hub": [10, 19, 57, 58, 264, 266, 268], "local": [10, 13, 46, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 117, 146, 167, 177, 239, 243, 247, 258, 264, 265, 267, 268], "remot": [10, 13, 40, 46, 57, 58], "url": [10, 15, 35, 40, 46, 258], "built": [10, 12, 20, 21, 23, 65, 69, 72, 258, 265, 268, 273], "project": [10, 20, 23, 83, 84, 85, 89, 93, 99, 103, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 140, 141, 144, 145, 147, 148, 149, 152, 153, 154, 155, 156, 159, 163, 164, 169, 172, 173, 178, 181, 187, 189, 193, 197, 203, 204, 225, 230, 239, 243, 257, 270, 271, 273], "workflow": [10, 257, 260, 266, 268, 270], "prefer": [10, 12, 22, 48, 57, 65, 69, 72, 215, 216, 217, 218, 259, 262, 264, 266, 271], "align": [10, 67, 68, 215, 265], "continu": [10, 56, 189, 239, 266], "pretrain": [10, 140, 141, 142, 193, 195, 197, 207, 208, 264, 265, 268, 270, 273], "beyond": [10, 267, 271, 273], "those": [10, 19, 225, 267, 269, 270], "full": [10, 12, 15, 19, 21, 22, 31, 38, 41, 57, 74, 86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 147, 150, 161, 162, 168, 193, 203, 204, 210, 227, 258, 259, 262, 264, 266, 267, 269, 270, 271, 272], "customiz": 10, "task": [10, 11, 15, 16, 17, 31, 38, 41, 54, 63, 142, 260, 265, 266, 267, 269, 270, 271, 272, 273], "supervis": [10, 58], "multimod": [10, 13, 34, 40, 58, 67, 68, 193, 258], "rlhf": [10, 57, 65, 213, 214, 215, 216, 217, 218, 219, 266], "complet": [10, 11, 16, 19, 22, 35, 56, 63, 73, 167, 265, 266, 267, 268, 269, 271], "input": [10, 11, 12, 13, 14, 18, 19, 32, 33, 47, 48, 49, 50, 53, 55, 56, 57, 58, 61, 63, 64, 66, 67, 68, 70, 71, 74, 92, 104, 117, 140, 141, 142, 145, 146, 149, 160, 167, 169, 173, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 194, 195, 199, 207, 208, 212, 222, 224, 247, 250, 265, 266, 270, 273], "queri": [10, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 187, 188, 193, 269, 271], "time": [10, 15, 19, 62, 66, 89, 156, 192, 210, 213, 240, 242, 248, 261, 264, 265, 266, 267, 269, 273], "which": [10, 11, 13, 15, 16, 17, 18, 19, 21, 22, 46, 47, 54, 55, 56, 59, 61, 62, 64, 66, 69, 70, 71, 73, 78, 79, 83, 84, 85, 92, 93, 94, 95, 104, 105, 106, 107, 108, 109, 117, 118, 119, 120, 127, 128, 129, 130, 136, 137, 142, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 160, 163, 164, 167, 169, 170, 171, 172, 180, 181, 183, 187, 188, 189, 191, 193, 195, 203, 204, 207, 222, 223, 224, 226, 229, 240, 243, 245, 249, 259, 260, 261, 262, 264, 265, 266, 267, 268, 270, 271, 272, 273], "take": [10, 11, 12, 15, 16, 19, 21, 22, 24, 48, 57, 58, 67, 68, 69, 140, 180, 189, 190, 195, 222, 224, 251, 252, 261, 265, 266, 267, 268, 269, 270, 271, 273], "object": [10, 12, 13, 14, 17, 18, 21, 24, 25, 28, 181, 215, 218, 220, 232], "appli": [10, 11, 14, 17, 19, 22, 47, 53, 55, 57, 58, 61, 67, 68, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 144, 147, 148, 149, 150, 152, 153, 154, 155, 156, 161, 162, 163, 164, 168, 169, 170, 171, 172, 173, 181, 185, 186, 187, 188, 193, 203, 204, 245, 259, 260, 271, 273], "templat": [10, 29, 31, 33, 36, 37, 38, 41, 53, 54, 55, 57, 58, 61, 64, 70, 92, 98, 104, 117, 142, 146, 151, 160, 167, 177], "anyth": [10, 63, 251], "els": [10, 11, 17, 22, 243, 259, 273], "requir": [10, 14, 17, 18, 19, 21, 47, 48, 54, 57, 58, 59, 67, 68, 69, 73, 142, 185, 187, 194, 222, 224, 226, 232, 235, 236, 238, 239, 242, 243, 247, 248, 258, 261, 264, 265, 266, 268, 271, 272, 273], "particular": [10, 12, 17, 18, 21, 53, 54, 142, 220, 266, 270, 273], "collat": [10, 47, 49, 50, 56, 266], "packag": [10, 20, 23, 239, 242, 243, 258, 266], "process": [10, 13, 14, 22, 23, 57, 58, 59, 65, 67, 68, 73, 145, 149, 189, 190, 234, 235, 247, 266, 268, 272, 273], "togeth": [10, 22, 56, 192, 243, 262, 268, 270, 271, 272], "form": [11, 16, 19, 21, 22, 42, 52, 57, 58, 264], "command": [11, 18, 20, 22, 23, 258, 261, 262, 264, 265, 266, 267, 268, 269, 270, 272, 273], "respons": [11, 12, 16, 17, 18, 29, 30, 32, 34, 35, 40, 57, 58, 62, 64, 65, 66, 67, 68, 69, 70, 71, 210, 213, 214, 215, 217, 218, 266, 267, 268, 269], "along": [11, 19, 270], "option": [11, 16, 18, 19, 21, 22, 30, 32, 33, 35, 40, 50, 51, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 83, 84, 85, 92, 93, 94, 95, 99, 104, 105, 106, 107, 108, 109, 114, 117, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 140, 141, 142, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 160, 163, 164, 167, 169, 170, 171, 172, 173, 177, 178, 181, 183, 186, 187, 188, 189, 190, 193, 203, 204, 205, 207, 210, 212, 213, 214, 216, 222, 223, 224, 227, 229, 232, 239, 240, 243, 247, 248, 252, 253, 258, 259, 264, 265, 266, 267, 271], "describ": [11, 245, 266], "hand": [11, 34], "here": [11, 13, 14, 15, 16, 18, 19, 20, 21, 23, 29, 64, 67, 68, 182, 183, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273], "grammar": [11, 17, 31, 64, 266], "correct": [11, 13, 17, 22, 31, 64, 182, 183, 187, 252, 259, 265, 266], "head": [11, 89, 93, 99, 105, 114, 118, 123, 127, 140, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 183, 187, 193, 197, 225, 269], "csv": [11, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 266], "incorrect": [11, 17], "cat": [11, 15, 17, 212], "grammarerrorcorrectiontempl": [11, 17, 64], "prepend": [11, 13, 15, 17, 28, 30, 32, 35, 36, 37, 40, 62, 64, 65, 66, 67, 68, 69, 70, 71, 92, 104, 117, 142, 146, 160, 167, 177, 207], "everi": [11, 19, 22, 64, 65, 69, 70, 71, 144, 148, 189, 242, 248, 258, 264, 271, 273], "column_map": [11, 12, 15, 16, 30, 32, 33, 35, 40, 54, 55, 60, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 266], "english": [11, 17, 31], "ncorrect": [11, 31], "mask": [11, 12, 13, 14, 17, 18, 34, 36, 50, 55, 56, 58, 61, 62, 64, 66, 67, 68, 69, 70, 71, 76, 77, 78, 142, 181, 186, 187, 188, 193, 206, 210, 212, 213, 216, 233, 265, 266], "out": [11, 14, 16, 19, 21, 22, 55, 61, 62, 64, 66, 69, 70, 71, 77, 78, 212, 222, 223, 233, 257, 259, 260, 261, 262, 264, 265, 267, 268, 269, 270, 271, 273], "100": [11, 16, 22, 48, 49, 50, 55, 61, 62, 64, 66, 69, 70, 71, 75, 192, 194, 270, 273], "27957": 11, "736": 11, "577": 11, "anoth": [11, 12, 15, 21, 58, 185, 239, 267, 271], "c4": [11, 73, 266, 272], "200m": 11, "liweili": [11, 64], "c4_200m": [11, 64], "chang": [11, 12, 15, 18, 19, 20, 21, 23, 30, 32, 66, 68, 72, 224, 258, 264, 267, 268, 269, 270, 271, 272, 273], "remap": 11, "each": [11, 14, 16, 17, 19, 22, 28, 33, 36, 37, 40, 47, 48, 50, 54, 56, 57, 58, 83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 141, 142, 145, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 181, 183, 187, 188, 189, 192, 193, 195, 203, 204, 210, 212, 213, 214, 215, 217, 218, 233, 247, 248, 259, 261, 262, 264, 266, 267, 268, 270, 271, 272], "them": [11, 14, 16, 17, 19, 21, 54, 55, 69, 189, 190, 195, 210, 251, 261, 264, 265, 266, 267, 270, 271, 272, 273], "someth": [11, 19, 22, 23, 265, 267, 272], "hello": [11, 12, 13, 17, 18, 42, 207, 208, 253, 265, 267, 269], "world": [11, 12, 13, 17, 18, 42, 207, 208, 234, 236, 253, 267], "bye": [11, 12], "robot": [11, 14], "am": [11, 13, 15, 62, 66, 98, 151, 265, 266, 267, 269], "want": [11, 17, 19, 21, 22, 23, 24, 50, 53, 57, 58, 75, 197, 258, 264, 265, 266, 267, 268, 269, 270, 271], "add": [11, 12, 13, 15, 17, 18, 20, 21, 23, 50, 53, 56, 59, 73, 142, 151, 189, 197, 208, 210, 224, 225, 265, 266, 267, 269, 270, 273], "prompttempl": [11, 28, 31, 33, 38, 41, 142], "relev": [11, 13, 22, 186, 188, 264, 267, 270, 271], "inform": [11, 13, 19, 239, 243, 245, 259, 264, 267, 268], "mai": [11, 15, 21, 23, 62, 75, 189, 194, 230, 249, 260, 261, 265, 266, 268, 270, 271], "automat": [11, 15, 17, 18, 20, 21, 23, 24, 61, 62, 264, 267, 273], "alpaca_dataset": [11, 21, 60, 266], "grammar_dataset": 11, "samsum_dataset": 11, "dictionari": [12, 13, 14, 34, 36, 42, 47, 48, 49, 56, 57, 58, 92, 104, 117, 146, 160, 167, 177, 231, 237, 239, 240, 241, 242, 243, 251, 267], "onc": [12, 18, 21, 36, 267, 268, 269, 270, 271, 273], "repres": [12, 34, 48, 189, 221, 227, 265, 271, 272], "prepar": [12, 14, 53, 265, 272], "paramet": [12, 13, 14, 15, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 230, 231, 232, 233, 235, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 257, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 272, 273], "control": [12, 16, 18, 22, 34, 55, 61, 62, 64, 66, 69, 70, 71, 195, 200, 239, 247, 261, 267, 271], "ad": [12, 15, 17, 18, 22, 36, 50, 144, 148, 158, 189, 193, 194, 197, 207, 210, 224, 225, 265, 266, 270, 271, 272, 273], "column": [12, 15, 16, 30, 32, 33, 35, 40, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 181, 187, 188, 193, 266, 272], "expos": [12, 21, 22, 224, 262, 268], "so": [12, 16, 17, 19, 21, 56, 67, 185, 189, 222, 258, 259, 265, 267, 268, 269, 270, 271, 272, 273], "don": [12, 13, 16, 17, 19, 21, 22, 243, 247, 264, 265, 266, 267, 268, 271, 273], "t": [12, 13, 16, 17, 19, 21, 22, 45, 140, 141, 192, 195, 229, 243, 247, 264, 265, 266, 267, 268, 271, 273], "worri": [12, 19, 265, 268], "itself": [12, 21], "do": [12, 14, 16, 18, 19, 20, 22, 34, 47, 55, 67, 69, 203, 210, 239, 243, 249, 264, 266, 267, 268, 269, 270, 271, 272], "well": [12, 16, 19, 21, 22, 259, 264, 266, 267, 269, 271, 273], "own": [12, 16, 18, 19, 36, 238, 247, 264, 265, 266, 267, 269, 270], "flexibl": [12, 21, 54, 266, 271], "inherit": [12, 13, 17, 22, 259, 266], "__call__": [12, 14, 17, 67, 68, 142], "A": [12, 14, 17, 22, 23, 30, 31, 35, 38, 40, 41, 43, 44, 47, 48, 49, 50, 54, 56, 69, 177, 181, 185, 186, 187, 188, 189, 190, 193, 199, 203, 207, 208, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 225, 226, 231, 232, 237, 238, 256, 257, 263, 264, 265, 270, 271, 272, 273], "simpl": [12, 19, 22, 189, 218, 257, 266, 268, 270, 272, 273], "contriv": [12, 17], "would": [12, 14, 17, 19, 21, 23, 36, 56, 189, 193, 258, 265, 266, 267, 270, 271, 273], "inde": [12, 229, 267], "quit": [12, 266, 271, 273], "similar": [12, 15, 62, 63, 65, 67, 68, 69, 72, 73, 74, 203, 215, 266, 267, 269, 270, 271, 273], "inputoutputtomessag": [12, 13, 64, 70], "modul": [12, 14, 18, 21, 24, 67, 68, 140, 141, 142, 145, 149, 154, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 215, 216, 217, 220, 225, 227, 228, 230, 238, 244, 245, 247, 266, 268, 270, 273], "type": [12, 13, 14, 15, 18, 23, 24, 26, 34, 35, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 192, 193, 194, 195, 196, 199, 201, 205, 206, 207, 208, 209, 210, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 245, 246, 247, 248, 250, 252, 253, 254, 261, 266, 267, 270, 271, 272, 273], "map": [12, 14, 17, 18, 19, 30, 32, 33, 35, 36, 40, 43, 44, 47, 53, 54, 55, 56, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 92, 104, 117, 142, 146, 160, 167, 177, 202, 208, 209, 222, 226, 228, 239, 240, 241, 242, 243, 244, 248, 266, 267, 270], "messagetransform": 12, "def": [12, 14, 17, 18, 21, 22, 23, 26, 67, 68, 220, 225, 266, 270, 273], "self": [12, 14, 16, 17, 18, 22, 23, 56, 67, 68, 83, 84, 85, 89, 93, 94, 95, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 144, 147, 148, 149, 152, 153, 154, 155, 156, 158, 163, 164, 165, 169, 170, 171, 172, 173, 181, 186, 187, 188, 192, 193, 195, 198, 203, 204, 222, 225, 226, 266, 270, 273], "str": [12, 14, 18, 21, 24, 25, 30, 32, 33, 34, 35, 36, 40, 42, 43, 44, 46, 47, 48, 49, 50, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 92, 104, 117, 142, 146, 147, 150, 160, 167, 177, 190, 194, 195, 196, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 221, 222, 223, 224, 225, 226, 227, 229, 231, 232, 235, 237, 239, 240, 241, 242, 243, 247, 248, 249, 250, 252, 253, 254, 266, 271], "eot": [12, 13, 17, 34, 142], "fals": [12, 13, 14, 15, 16, 17, 19, 21, 30, 32, 34, 35, 40, 43, 44, 45, 53, 54, 55, 56, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 143, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 173, 181, 187, 188, 193, 194, 195, 199, 200, 203, 207, 219, 222, 223, 224, 233, 236, 248, 249, 264, 265, 266, 267, 269, 270, 272, 273], "_messag": 12, "0x7fb0a10094e0": 12, "0x7fb0a100a290": 12, "msg": [12, 13, 15, 17, 18, 265], "text_cont": [12, 13, 15, 17, 34, 265], "how": [12, 13, 16, 19, 20, 21, 22, 23, 189, 220, 239, 245, 257, 260, 261, 264, 265, 266, 267, 268, 269, 271, 272, 273], "manipul": 12, "must": [12, 24, 36, 54, 67, 68, 181, 198, 221, 239, 273], "sftdataset": [12, 53, 55, 57, 60, 61, 62, 64, 66, 67, 68, 70, 71], "py": [12, 21, 24, 76, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 180, 182, 183, 191, 215, 216, 217, 218, 264, 267, 269], "custom_dataset": 12, "load_dataset_kwarg": [12, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74], "message_transform": [12, 57, 58], "mymessagetransform": 12, "model_transform": [12, 14, 15, 57, 58, 64, 67, 68, 70, 71, 142], "chosenrejectedtomessag": [12, 65, 69], "core": [13, 22, 57, 58, 259, 262, 266, 268, 273], "govern": [13, 265], "serv": [13, 17, 21, 30, 32, 35, 40, 62, 64, 65, 66, 67, 68, 69, 70, 71, 210, 220, 266, 270], "interfac": [13, 22, 28, 33, 36, 37, 54, 198, 211, 266], "api": [13, 22, 23, 31, 38, 41, 43, 57, 58, 61, 67, 68, 86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 203, 239, 258, 262, 264, 265, 268, 269, 273], "oper": [13, 22, 189, 200, 211, 247, 272], "send": 13, "other": [13, 14, 16, 19, 22, 24, 32, 36, 54, 224, 230, 248, 251, 260, 261, 265, 266, 268, 269, 270, 271, 272], "special": [13, 15, 17, 34, 40, 53, 117, 142, 144, 146, 148, 167, 177, 189, 194, 205, 206, 208, 209, 210, 212, 226, 266], "individu": [13, 34, 56, 193, 231, 243, 245, 265, 266], "ref": [13, 57, 58, 61, 67, 68, 166, 167, 243], "constructor": [13, 18], "ident": [13, 16, 33, 45, 47, 55, 56, 67, 69, 151, 187, 267, 272], "from_dict": [13, 34, 265], "becaus": [13, 18, 57, 58, 93, 180, 187, 189, 193, 224, 264, 265, 272], "correspond": [13, 16, 18, 34, 48, 76, 77, 78, 196, 198, 201, 213, 216, 229, 261, 268, 269, 271, 272], "begin": [13, 19, 56, 73, 189, 208, 210, 265, 269, 273], "pil": [13, 14, 15, 34, 35, 42, 46], "img_msg": 13, "place": [13, 15, 249, 265, 266, 271], "new": [13, 14, 15, 17, 18, 22, 35, 40, 43, 44, 61, 63, 64, 65, 67, 69, 70, 71, 157, 180, 194, 195, 225, 239, 240, 242, 265, 267, 268, 269, 270, 273], "mode": [13, 14, 15, 227, 232, 239, 267], "rgb": [13, 14, 15, 140], "4": [13, 14, 15, 19, 21, 45, 47, 48, 49, 50, 78, 142, 145, 149, 180, 181, 189, 232, 233, 254, 259, 261, 264, 266, 267, 269, 270, 271, 272, 273], "appropri": [13, 34, 54, 78, 98, 191, 194, 222, 266, 273], "case": [13, 15, 19, 22, 23, 34, 36, 57, 145, 149, 189, 222, 226, 229, 232, 238, 240, 245, 259, 264, 265, 266, 267, 269, 270, 271, 273], "load_imag": [13, 15], "both": [13, 14, 18, 19, 35, 50, 54, 65, 69, 178, 187, 193, 195, 197, 204, 264, 267, 270, 272, 273], "image_path": [13, 15], "jpg": [13, 15, 40, 46], "tag": [13, 15, 17, 18, 28, 36, 40, 42, 53, 92, 98, 104, 117, 142, 146, 151, 160, 167, 177, 239, 240, 241, 242, 243, 265], "placehold": [13, 15, 33, 40, 55, 221, 266], "should": [13, 14, 15, 16, 19, 21, 22, 28, 30, 32, 33, 34, 35, 36, 40, 43, 44, 47, 56, 61, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 78, 83, 84, 85, 93, 94, 95, 98, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 140, 144, 147, 148, 149, 151, 152, 153, 154, 155, 156, 158, 163, 164, 165, 169, 170, 171, 172, 173, 180, 181, 187, 189, 193, 198, 203, 204, 213, 216, 220, 221, 237, 239, 240, 241, 242, 243, 258, 259, 266, 267, 268, 269, 270, 271, 272, 273], "insert": [13, 195, 272], "format_content_with_imag": [13, 15], "image_tag": [13, 15, 40, 42], "conveni": [13, 21, 22, 46, 264], "discuss": [13, 17, 18, 21, 267, 268, 269, 270], "prompttemplateinterfac": [13, 17, 92, 104, 117, 146, 160, 167, 177], "templated_msg": [13, 17], "contains_media": [13, 15, 34], "get_media": [13, 14, 15, 34], "4x4": 13, "0x7f8d27e72740": 13, "tokenize_messsag": 13, "hi": [13, 75, 265], "tokenize_messag": [13, 14, 18, 34, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 142, 206, 210, 265, 266], "22557": 13, "1526": [13, 18], "28808": 13, "28705": [13, 18], "28748": [13, 18], "15359": 13, "28725": 13, "315": 13, "837": 13, "396": 13, "16107": 13, "13892": 13, "28723": 13, "2": [13, 14, 18, 19, 23, 45, 47, 48, 49, 50, 52, 56, 71, 78, 79, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 180, 181, 189, 207, 208, 210, 216, 218, 219, 222, 223, 232, 233, 246, 247, 248, 254, 261, 265, 267, 268, 269, 270, 271, 272], "modal": [14, 15, 58, 142, 195], "current": [14, 15, 16, 19, 40, 56, 69, 77, 89, 93, 105, 118, 127, 147, 148, 149, 152, 154, 156, 163, 166, 169, 180, 181, 183, 187, 188, 193, 216, 223, 224, 230, 232, 234, 240, 242, 244, 247, 261, 262, 266, 268, 269, 271, 272], "intend": [14, 251, 265], "drop": [14, 142, 194, 272], "replac": [14, 15, 40, 51, 55, 61, 62, 64, 66, 69, 70, 71, 142, 190, 194, 249, 270], "llama3_2_vis": [14, 15, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150], "llama3visiontransform": [14, 15, 146], "modeltoken": [14, 18, 21, 34, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 210, 266], "__init__": [14, 21, 22, 67, 68, 270, 273], "transform_imag": 14, "clipimagetransform": [14, 67, 68, 142, 189], "xattn_mask": 14, "visioncrossattentionmask": [14, 142, 211], "224": [14, 15, 142], "llama": [14, 15, 18, 19, 53, 98, 140, 142, 143, 144, 145, 146, 148, 149, 182, 183, 222, 223, 260, 261, 264, 265, 267, 268, 269, 270], "3": [14, 15, 18, 19, 45, 47, 48, 49, 50, 56, 78, 79, 140, 142, 143, 144, 145, 146, 148, 149, 151, 164, 166, 167, 189, 225, 232, 233, 246, 253, 260, 261, 264, 265, 267, 268, 269, 272, 273], "tile_s": [14, 142, 145, 149, 189, 212], "patch_siz": [14, 142, 145, 149, 189, 212], "14": [14, 48, 142, 189, 272, 273], "skip_special_token": [14, 15, 69, 142, 208], "begin_of_text": [14, 15, 18, 265], "start_header_id": [14, 15, 265], "end_header_id": [14, 15, 265], "n": [14, 15, 16, 17, 18, 31, 36, 38, 41, 181, 189, 210, 256, 263, 264, 265, 266, 272], "eot_id": [14, 15, 18, 265], "na": [14, 265], "encoder_input": [14, 15, 50, 186, 187, 193], "shape": [14, 15, 19, 47, 50, 75, 76, 77, 78, 140, 141, 142, 145, 149, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 192, 193, 194, 195, 199, 212, 213, 214, 215, 216, 217, 218, 219, 233, 248, 249], "num_til": [14, 15, 140, 141, 189], "num_channel": [14, 15, 189], "tile_height": [14, 15], "tile_width": [14, 15], "torch": [14, 15, 19, 21, 45, 47, 48, 49, 50, 75, 76, 77, 78, 79, 140, 141, 142, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 199, 213, 214, 215, 216, 217, 218, 219, 224, 226, 228, 229, 231, 232, 233, 235, 236, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 254, 258, 261, 267, 268, 269, 270, 271, 273], "just": [14, 17, 19, 259, 261, 264, 265, 266, 268, 269, 270, 271, 272], "the_cauldron_dataset": [14, 15], "subset": [14, 15, 47, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 93, 105, 118, 127, 147, 152, 154, 163, 169, 196, 201], "ai2d": [14, 68], "respir": 14, "combust": 14, "give": [14, 18, 21, 221, 266, 270, 271], "choic": [14, 16], "oxygen": 14, "b": [14, 22, 45, 47, 140, 141, 180, 181, 183, 187, 188, 193, 199, 213, 214, 218, 233, 243, 270, 273], "carbon": 14, "dioxid": 14, "c": [14, 45, 47, 50, 67, 140, 265], "nitrogen": 14, "d": [14, 21, 34, 67, 140, 141, 180, 181, 187, 193, 264, 265, 270, 272], "heat": 14, "letter": 14, "mymultimodaltransform": 14, "my_tokenizer_build": 14, "myimagetransform": 14, "add_eo": [14, 59, 73, 142, 207, 208, 265], "bool": [14, 17, 18, 21, 30, 32, 34, 35, 40, 43, 44, 45, 53, 55, 56, 59, 60, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 142, 143, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 173, 181, 186, 187, 188, 190, 193, 195, 199, 203, 204, 206, 207, 208, 210, 213, 219, 220, 222, 223, 224, 230, 231, 235, 236, 238, 239, 242, 245, 248, 249, 254, 271, 273], "tupl": [14, 17, 18, 21, 24, 36, 48, 75, 76, 92, 104, 117, 142, 146, 160, 167, 177, 180, 189, 190, 206, 210, 213, 214, 215, 216, 217, 218, 219, 220, 234, 248, 249, 250], "int": [14, 18, 21, 23, 47, 48, 49, 50, 51, 53, 55, 56, 63, 67, 68, 74, 75, 76, 77, 79, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 99, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 160, 161, 162, 163, 164, 165, 167, 168, 169, 170, 171, 172, 173, 177, 180, 181, 182, 183, 186, 187, 188, 189, 191, 192, 193, 194, 195, 199, 205, 206, 207, 208, 209, 210, 212, 219, 220, 222, 223, 224, 227, 230, 234, 238, 239, 240, 241, 242, 243, 245, 247, 248, 264, 266, 270, 271, 273], "logic": [14, 22, 58, 206, 225, 259, 262, 268, 270], "infer": [14, 17, 19, 50, 53, 58, 89, 98, 156, 180, 181, 183, 187, 188, 193, 212, 252, 257, 261, 262, 265, 267, 268, 269, 272, 273], "vision": [14, 15, 58, 140, 142, 143, 144, 145, 146, 147, 148, 149, 150, 194, 225], "aspect_ratio": [14, 50, 140, 189], "append": [14, 17, 28, 36, 37, 92, 104, 117, 142, 146, 160, 167, 177, 187, 193, 207, 239, 258, 266], "addit": [14, 18, 19, 21, 22, 24, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 142, 144, 145, 148, 149, 151, 194, 195, 203, 215, 220, 222, 223, 224, 229, 230, 235, 238, 239, 240, 242, 243, 245, 259, 265, 268, 270, 271], "kei": [14, 18, 19, 21, 23, 30, 32, 35, 40, 43, 47, 48, 53, 55, 57, 58, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 186, 187, 188, 193, 195, 202, 203, 204, 218, 222, 224, 226, 239, 248, 264, 267, 268, 270, 271, 273], "e": [15, 16, 17, 34, 46, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 140, 141, 142, 145, 149, 181, 189, 190, 193, 198, 202, 212, 221, 222, 226, 231, 248, 252, 258, 261, 267, 269, 270, 271, 272, 273], "g": [15, 16, 46, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 140, 141, 142, 145, 149, 181, 189, 193, 198, 212, 221, 222, 231, 248, 252, 261, 269, 270, 271, 272, 273], "base": [15, 16, 19, 24, 34, 36, 55, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 142, 143, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 183, 191, 199, 200, 202, 203, 204, 214, 215, 217, 218, 222, 230, 238, 240, 249, 252, 257, 265, 267, 268, 269, 270, 271, 273], "multimodal_chat_dataset": 15, "visual": [15, 195], "note": [15, 18, 19, 21, 33, 93, 193, 198, 226, 244, 247, 249, 261, 265, 266, 267, 270, 271, 272, 273], "get": [15, 19, 20, 21, 22, 23, 50, 53, 142, 229, 231, 234, 239, 253, 258, 259, 260, 261, 265, 266, 267, 268, 270, 271, 272], "below": [15, 20, 23, 47, 220, 266, 269, 270, 273], "clock": 15, "10": [15, 45, 47, 48, 49, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 189, 194, 267, 269, 271, 272, 273], "llama3_2_vision_transform": 15, "questionanswertempl": [15, 17, 72], "image_s": [15, 143, 146, 147, 150, 189], "560": [15, 143, 146, 147, 150], "image_dir": [15, 40, 67], "home": [15, 21, 40, 46], "nquestion": 15, "nit": 15, "00am": 15, "sharegpt": [15, 40, 44, 62, 265], "q1": [15, 30, 57, 62, 69], "a1": [15, 30, 57, 62], "sharegpt4v": 15, "lin": 15, "chen": 15, "renam": 15, "themselv": [15, 273], "write": [15, 19, 22, 222, 223, 224, 240, 266, 268], "pathlib": 15, "pil_imag": 15, "Then": [15, 23, 200, 268, 271], "relat": [15, 186, 187, 193, 270], "user_messag": [15, 31, 38, 41, 142, 265], "rel": [15, 56, 181, 183, 187, 188, 193, 215, 231, 270], "locat": [15, 18, 21, 40, 264, 266, 269, 270, 272, 273], "long": [15, 56, 208, 265, 266, 270], "image_dog": 15, "image_cat": 15, "image_bird": 15, "dog": [15, 212], "bird": [15, 46], "best": [15, 16, 22, 261, 265, 271], "pet": 15, "three": [15, 19, 22, 50, 142, 215, 217, 218, 262, 268], "referenc": 15, "huggingfac": [15, 53, 55, 59, 63, 65, 73, 74, 159, 166, 167, 174, 175, 176, 191, 215, 217, 218, 222, 223, 264, 267], "co": [15, 53, 55, 59, 63, 65, 73, 74, 159, 166, 167, 174, 175, 176, 222, 223, 267], "easili": [15, 19, 21, 259, 266, 270, 272, 273], "img": 15, "when": [15, 16, 18, 19, 21, 22, 26, 54, 56, 57, 58, 59, 69, 73, 75, 77, 180, 181, 183, 187, 188, 189, 190, 191, 192, 193, 194, 200, 203, 214, 230, 242, 244, 249, 260, 264, 267, 269, 270, 271, 272, 273], "llava_instruct_dataset": 15, "reward": [16, 103, 109, 113, 155, 159, 162, 213, 214, 215, 217, 218, 225], "downstream": 16, "captur": 16, "optim": [16, 17, 19, 21, 22, 48, 54, 57, 89, 156, 166, 191, 215, 216, 217, 218, 224, 226, 228, 231, 244, 248, 260, 261, 262, 265, 267, 268, 269, 270, 273], "ground": [16, 192, 271], "truth": [16, 21, 192, 267, 269], "usual": [16, 18, 19, 183, 187, 219, 222, 233, 243, 264, 267, 270, 271], "outcom": 16, "binari": 16, "comparison": [16, 22, 270, 273], "annot": 16, "accord": [16, 17, 28, 67, 68, 78, 151, 265], "criterion": 16, "style": [16, 56, 60, 61, 62, 71, 195, 273], "interact": [16, 22, 57, 69, 257, 262, 268], "free": [16, 218, 262, 270], "preference_dataset": 16, "my_preference_dataset": [16, 69], "chosen_convers": [16, 69], "hole": [16, 69], "my": [16, 17, 20, 69, 75, 264, 265, 266, 267, 269], "trouser": [16, 69], "fix": [16, 69, 266, 272], "rejected_convers": [16, 69], "off": [16, 22, 36, 69, 260, 261, 267, 272], "chosen": [16, 30, 57, 65, 69, 72, 215, 217, 218, 248, 266], "reject": [16, 30, 57, 65, 69, 72, 215, 217, 218, 266], "rejected_input_id": [16, 48, 69, 266], "nwhat": 16, "ntake": 16, "rejected_label": [16, 48, 266], "128006": 16, "78191": 16, "128007": 16, "271": 16, "18293": 16, "1124": 16, "1022": 16, "13": [16, 18, 48, 189, 210, 219, 273], "128009": [16, 265], "accomplish": [16, 54, 62, 66, 69, 73], "ve": [16, 18, 21, 180, 261, 264, 265, 266, 267, 269, 270, 271], "shown": [16, 267, 272], "di": 16, "look": [16, 17, 19, 21, 22, 228, 242, 258, 265, 266, 267, 268, 269, 270, 272], "anthrop": [16, 65], "harmless": [16, 65], "granni": 16, "her": 16, "mobil": [16, 267], "phone": [16, 267], "issu": [16, 262, 272], "grandmoth": 16, "manag": [16, 19, 54, 200, 239, 246, 265], "behavior": [16, 19, 238, 265, 266], "thing": [16, 271, 273], "grandma": 16, "feel": [16, 262, 270], "box": [16, 259, 261, 273], "through": [16, 19, 20, 21, 22, 23, 57, 145, 149, 178, 180, 189, 195, 200, 259, 260, 261, 262, 264, 265, 266, 267, 268, 271, 272, 273], "hh_rlhf_helpful_dataset": 16, "ll": [16, 19, 21, 22, 75, 232, 259, 261, 265, 266, 267, 268, 269, 271, 272, 273], "hendrydong": 16, "preference_700k": 16, "stack_exchange_paired_dataset": 16, "purpos": [17, 67, 68, 268, 269], "whenev": [17, 142, 192, 270], "llama2": [17, 19, 21, 22, 24, 53, 55, 63, 74, 80, 81, 82, 83, 84, 85, 86, 87, 88, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 142, 178, 187, 188, 225, 257, 260, 264, 268, 269, 271, 272], "were": [17, 18, 54, 140, 189, 200, 216, 268, 272], "ensur": [17, 18, 19, 21, 27, 52, 57, 58, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181, 222, 224, 229, 259, 266, 268], "gear": [17, 142], "after": [17, 20, 22, 36, 58, 67, 68, 92, 104, 117, 142, 146, 160, 167, 180, 181, 184, 185, 187, 188, 193, 195, 219, 238, 239, 240, 241, 242, 243, 261, 265, 267, 269, 272, 273], "summar": [17, 41, 70, 265, 266, 271], "summarizetempl": [17, 54, 70, 265, 266], "commun": [17, 142, 266, 267, 271], "chatmltempl": [17, 142, 177], "gec_templ": 17, "extend": [17, 18, 19, 22, 259], "customprompttempl": 17, "achiev": [17, 36, 244, 261, 267, 269, 270, 272, 273], "prepend_tag": [17, 36], "append_tag": [17, 36], "thu": [17, 36, 57, 58, 187, 271, 272], "now": [17, 19, 180, 226, 228, 261, 265, 266, 267, 268, 269, 270, 272, 273], "empti": [17, 47, 50, 52, 79, 264], "standalon": [17, 180], "my_custom_templ": 17, "Is": 17, "overhyp": 17, "advanc": [17, 145, 149, 189, 266], "configur": [17, 18, 22, 55, 57, 58, 61, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 93, 105, 118, 127, 147, 152, 163, 169, 239, 259, 261, 262, 265, 268, 269, 270, 271, 272, 273], "doesn": [17, 267], "neatli": 17, "fall": 17, "implement": [17, 18, 19, 22, 53, 55, 57, 59, 61, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 178, 182, 183, 184, 189, 191, 198, 199, 205, 206, 211, 215, 216, 217, 218, 222, 232, 242, 259, 261, 266, 270, 271, 272, 273], "protocol": [17, 18, 198, 205, 206, 211], "arg": [17, 18, 21, 24, 37, 179, 187, 190, 195, 198, 205, 206, 211, 241, 248, 261, 272], "whether": [17, 30, 32, 34, 35, 40, 43, 44, 47, 50, 53, 55, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 83, 84, 85, 89, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 142, 143, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 173, 190, 193, 199, 203, 204, 207, 208, 220, 229, 231, 239, 249, 265, 266], "being": [17, 19, 58, 222, 223, 224, 228, 252, 271, 272, 273], "make": [17, 19, 20, 21, 22, 23, 143, 189, 259, 264, 267, 268, 269, 270, 271, 272, 273], "sai": [17, 264, 265, 268], "eureka": 17, "eurekatempl": 17, "formatted_dialogu": 17, "llama2chattempl": [17, 104, 151, 177, 265], "index": [18, 48, 49, 50, 54, 56, 181, 183, 188, 191, 193, 214, 252, 258, 265, 267], "embed": [18, 19, 89, 93, 99, 105, 114, 118, 123, 127, 140, 141, 142, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 182, 183, 186, 187, 189, 193, 194, 195, 197, 230, 265, 269, 271, 272], "vector": [18, 217, 265], "understood": 18, "plai": [18, 267, 271], "necessari": [18, 19, 57, 58, 239, 240, 241, 242, 243, 265, 270], "phi3": [18, 19, 163, 164, 166, 167, 168, 225, 264], "phi3_mini_token": 18, "p_token": 18, "phi": [18, 166, 167, 225], "32010": 18, "29871": 18, "1792": [18, 210], "9508": [18, 210], "32007": 18, "32001": 18, "4299": 18, "2933": [18, 210], "nuser": 18, "nmodel": 18, "sentencepiec": [18, 207, 269], "tiktoken": [18, 142, 208, 269], "host": [18, 258, 264, 268, 271], "distribut": [18, 79, 226, 235, 236, 245, 247, 252, 259, 262, 264, 268, 269, 271], "alongsid": [18, 230], "cd": [18, 258, 267], "ls": [18, 258, 262, 264, 267, 268, 269], "alreadi": [18, 21, 30, 35, 40, 64, 65, 67, 68, 69, 70, 71, 180, 181, 193, 225, 235, 258, 264, 266, 267, 270], "_token": [18, 22, 266], "mistraltoken": [18, 160, 265], "over": [18, 22, 34, 58, 191, 215, 259, 261, 264, 267, 270, 271, 273], "memori": [18, 22, 54, 55, 56, 59, 63, 73, 74, 185, 187, 190, 192, 193, 203, 230, 231, 237, 238, 248, 257, 259, 260, 261, 267, 268, 269, 272], "adher": [18, 35, 40, 43, 44], "arbitrarili": 18, "small": [18, 182, 267], "seq": [18, 187, 193], "len": [18, 19, 50, 54, 61, 64, 67, 68, 70, 187, 189, 193], "demonstr": [18, 271, 272], "7": [18, 19, 45, 47, 48, 49, 50, 180, 189, 212, 216], "6312": 18, "28709": 18, "assign": [18, 21, 57, 58], "uniqu": [18, 57, 58, 225], "abil": 18, "experiment": [18, 21], "NOT": [18, 19, 89, 142, 156], "correctli": [18, 19, 22, 27, 203, 222, 258, 262, 265, 268, 273], "presenc": 18, "certain": [18, 19, 21, 248, 265], "proper": [18, 258, 268], "llama3": [18, 21, 53, 67, 68, 75, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 144, 146, 147, 148, 150, 163, 192, 225, 230, 257, 259, 260, 261, 264, 266, 267, 271], "end_of_text": 18, "special_token": [18, 142, 208, 265], "added_token": 18, "128257": 18, "128258": 18, "remain": [18, 35, 40, 43, 44, 191, 270, 271], "special_tokens_path": [18, 117, 146, 167, 177], "basetoken": 18, "actual": [18, 20, 21, 23, 30, 32, 53, 57, 58, 61, 64, 65, 66, 68, 69, 70, 72, 142, 261, 265, 272], "string": [18, 19, 32, 34, 36, 42, 62, 63, 92, 104, 117, 142, 146, 160, 167, 177, 198, 205, 207, 208, 210, 221, 227, 229, 232, 239, 252, 264, 266, 271], "kwarg": [18, 21, 24, 37, 177, 179, 186, 188, 190, 195, 198, 205, 206, 211, 235, 239, 240, 241, 242, 243, 245, 248, 266], "dict": [18, 19, 21, 22, 23, 24, 30, 32, 33, 34, 35, 36, 40, 42, 43, 44, 47, 48, 49, 50, 53, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 92, 104, 117, 142, 146, 160, 167, 177, 186, 188, 190, 193, 195, 196, 201, 202, 203, 204, 205, 206, 208, 209, 211, 222, 223, 224, 226, 228, 231, 235, 237, 239, 244, 249, 251, 266], "given": [18, 22, 24, 33, 42, 47, 52, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 78, 142, 199, 200, 205, 206, 214, 229, 232, 238, 244, 252, 254, 259, 270], "token_id": [18, 142, 205, 208], "its": [18, 56, 98, 151, 154, 181, 183, 187, 188, 193, 195, 244, 247, 264, 265, 266, 267, 269, 270, 271], "sentencepiecebasetoken": [18, 205], "bpe": 18, "sp_token": 18, "concaten": [18, 48, 54, 145, 149, 206, 210], "reason": [18, 22, 75, 267, 271, 272], "walk": [19, 22, 242, 259, 265, 266, 267, 268, 272, 273], "design": [19, 22, 218], "cover": [19, 20, 21, 22, 23, 265, 267, 273], "scenario": [19, 54, 142], "compos": [19, 189], "plug": [19, 271], "evalu": [19, 22, 257, 259, 261, 262, 268, 270, 273], "gener": [19, 22, 47, 53, 55, 56, 63, 73, 76, 77, 78, 79, 142, 200, 213, 239, 246, 247, 248, 255, 257, 261, 265, 266, 270, 271, 272, 273], "easi": [19, 22, 259, 266, 270, 271], "understand": [19, 21, 22, 195, 257, 259, 260, 265, 266, 270, 271, 273], "concept": [19, 262, 267, 268, 271], "talk": 19, "close": [19, 22, 239, 240, 241, 242, 243, 270], "veri": [19, 54, 187, 193, 264, 267, 271], "dictat": 19, "state_dict": [19, 190, 194, 195, 203, 222, 223, 224, 225, 226, 249, 270, 273], "store": [19, 57, 58, 239, 240, 243, 270, 271, 273], "disk": [19, 59, 240], "identifi": [19, 239], "state": [19, 22, 141, 187, 189, 190, 196, 201, 202, 203, 204, 213, 215, 222, 223, 224, 226, 228, 249, 267, 269, 270, 271, 273], "match": [19, 42, 55, 204, 239, 249, 258, 264, 266, 267, 269, 270], "up": [19, 20, 22, 23, 50, 55, 56, 63, 74, 142, 187, 193, 208, 212, 228, 239, 248, 260, 261, 262, 264, 265, 266, 268, 269, 270, 271, 273], "exactli": [19, 204, 221, 272], "definit": [19, 270], "either": [19, 47, 57, 58, 75, 181, 187, 188, 204, 222, 239, 245, 258, 264, 270, 271, 272, 273], "run": [19, 20, 21, 23, 26, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 187, 190, 192, 222, 223, 224, 226, 227, 228, 236, 239, 242, 243, 244, 258, 259, 260, 261, 262, 265, 266, 268, 269, 270, 271, 272, 273], "explicit": 19, "error": [19, 21, 31, 52, 222, 247, 264], "load": [19, 22, 40, 46, 53, 54, 55, 56, 57, 58, 59, 61, 63, 64, 65, 67, 68, 69, 70, 71, 72, 73, 74, 193, 203, 222, 223, 224, 226, 242, 249, 266, 267, 269, 270], "rais": [19, 24, 27, 30, 32, 35, 40, 42, 43, 46, 47, 50, 52, 55, 61, 62, 64, 66, 67, 68, 70, 71, 73, 77, 169, 180, 181, 185, 186, 187, 189, 203, 204, 210, 222, 223, 224, 226, 229, 231, 235, 239, 243, 247, 249, 250, 251], "except": [19, 34, 151, 210, 266], "wors": [19, 271], "silent": 19, "succe": 19, "line": [19, 20, 22, 262, 264, 266, 268, 269], "popular": [19, 193, 259, 266, 267], "offici": [19, 98, 265, 268, 269], "websit": 19, "inspect": [19, 267, 270, 273], "mmap": [19, 267], "weights_onli": [19, 224], "map_loc": [19, 267], "cpu": [19, 22, 190, 229, 248, 252, 258, 264, 267, 271, 273], "tensor": [19, 45, 47, 48, 49, 50, 75, 76, 77, 78, 79, 140, 141, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 189, 190, 192, 193, 194, 195, 199, 213, 214, 215, 216, 217, 218, 219, 222, 233, 239, 240, 241, 242, 243, 246, 249, 251, 270, 271, 273], "item": 19, "f": [19, 23, 61, 64, 67, 68, 70, 221, 265, 267, 270, 273], "tok_embed": [19, 187, 193, 194], "32000": [19, 24, 270], "4096": [19, 24, 55, 63, 74, 181, 183, 266, 270, 272], "292": 19, "tabl": [19, 194, 265, 267, 269, 271, 273], "layer": [19, 22, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 99, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 144, 145, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 159, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 181, 184, 185, 186, 187, 188, 189, 193, 195, 197, 199, 203, 204, 220, 227, 230, 259, 260, 269, 270, 271, 272, 273], "dim": [19, 50, 140, 141, 178, 181, 182, 183, 187, 192, 193], "within": [19, 21, 24, 53, 56, 75, 79, 93, 105, 118, 127, 147, 148, 149, 152, 154, 163, 169, 189, 242, 247, 248, 264, 266, 270, 273], "first": [19, 21, 24, 40, 52, 56, 67, 78, 145, 149, 187, 189, 193, 219, 222, 257, 259, 260, 265, 266, 267, 269, 270, 272, 273], "big": 19, "bin": [19, 264, 267], "piec": 19, "pytorch_model": [19, 267], "00001": [19, 264], "00002": [19, 264], "embed_token": 19, "241": 19, "Not": 19, "fewer": [19, 181], "sinc": [19, 21, 24, 57, 58, 222, 224, 265, 267, 269, 271, 272], "mismatch": 19, "caus": [19, 207], "try": [19, 21, 265, 267, 268, 269, 273], "re": [19, 21, 195, 218, 224, 259, 260, 261, 265, 267, 268, 270, 271], "care": [19, 222, 224, 267, 269, 270], "end": [19, 22, 34, 59, 73, 142, 208, 210, 257, 259, 260, 265, 269, 270, 272], "number": [19, 22, 42, 50, 53, 55, 56, 63, 74, 75, 89, 93, 99, 105, 114, 118, 123, 127, 140, 141, 142, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 187, 189, 191, 212, 222, 223, 224, 227, 234, 247, 248, 264, 268, 270, 271], "save": [19, 22, 23, 187, 190, 192, 193, 222, 223, 224, 226, 230, 238, 243, 257, 261, 264, 265, 266, 267, 269, 270, 271, 272], "less": [19, 75, 267, 268, 269, 271, 273], "prone": 19, "invari": 19, "accept": [19, 21, 220, 266, 268, 271, 273], "explicitli": [19, 198, 259, 270], "produc": [19, 226, 261, 272, 273], "One": [19, 50, 272], "advantag": [19, 213, 216, 261, 270], "abl": [19, 22, 267, 268, 272], "post": [19, 189, 244, 248, 261, 267, 269, 272, 273], "quantiz": [19, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 199, 224, 232, 257, 258, 260, 262, 268, 273], "eval": [19, 257, 259, 272], "without": [19, 21, 23, 181, 185, 203, 258, 259, 261, 265, 267, 270, 271, 272], "OR": [19, 43], "script": [19, 23, 262, 264, 266, 267, 268, 269], "surround": [19, 22, 259], "load_checkpoint": [19, 22, 222, 223, 224, 225], "save_checkpoint": [19, 22, 23, 222, 223, 224], "permut": 19, "behav": 19, "further": [19, 189, 218, 264, 266, 270, 271, 272, 273], "illustr": [19, 67, 68, 269], "whilst": [19, 260, 271], "folder": 19, "read": [19, 222, 223, 224, 259, 271], "compat": [19, 222, 224, 271, 272], "framework": [19, 22, 259], "mention": [19, 267, 273], "assum": [19, 33, 40, 45, 47, 55, 67, 92, 104, 117, 146, 160, 167, 177, 180, 181, 183, 188, 191, 193, 194, 196, 201, 208, 226, 228, 229, 265, 267, 270], "checkpoint_dir": [19, 21, 222, 223, 224, 267, 269, 272], "easiest": [19, 267, 268], "sure": [19, 21, 267, 268, 269, 270, 271, 272, 273], "everyth": [19, 22, 259, 262, 268], "flow": [19, 53, 55, 56, 272, 273], "safetensor": [19, 221, 222, 264], "output_dir": [19, 21, 222, 223, 224, 248, 267, 269, 270, 272, 273], "snippet": 19, "explain": [19, 271], "setup": [19, 21, 22, 77, 180, 181, 186, 187, 188, 193, 195, 227, 248, 264, 266, 267, 270, 273], "fullmodelhfcheckpoint": [19, 267], "directori": [19, 21, 40, 67, 222, 223, 224, 240, 242, 243, 248, 264, 266, 267, 268, 269], "sort": [19, 222, 224], "order": [19, 20, 22, 222, 224, 242, 243, 268, 271], "matter": [19, 222, 224, 264, 270], "checkpoint_fil": [19, 21, 23, 222, 223, 224, 267, 269, 270, 272, 273], "restart": [19, 264], "previou": [19, 56, 222, 223, 224], "section": [19, 22, 231, 257, 267, 269, 271, 273], "recipe_checkpoint": [19, 222, 223, 224, 272], "model_typ": [19, 222, 223, 224, 267, 269, 272], "resume_from_checkpoint": [19, 222, 223, 224], "param": [19, 22, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 143, 147, 164, 170, 171, 172, 196, 197, 199, 201, 202, 204, 222, 270, 272, 273], "discrep": [19, 222], "github": [19, 24, 76, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 182, 183, 191, 192, 215, 216, 217, 218, 258, 266, 267, 269], "repositori": [19, 53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 98, 260, 261, 267, 268], "fullmodelmetacheckpoint": [19, 269, 272], "test": [19, 21, 22, 75, 259, 261, 265, 271], "written": [19, 21, 22, 222, 223, 239, 240, 241, 242, 243, 259], "partit": [19, 222, 273], "key_1": [19, 224], "weight_1": 19, "key_2": 19, "weight_2": 19, "mid": 19, "chekpoint": 19, "middl": [19, 195, 267, 271], "subsequ": [19, 22, 180, 187, 189, 212], "recipe_st": [19, 222, 223, 224], "pt": [19, 23, 222, 223, 224, 267, 269, 272], "epoch": [19, 22, 23, 191, 222, 223, 224, 264, 265, 267, 268, 269, 272], "etc": [19, 22, 141, 222, 231, 268], "prevent": [19, 56, 215, 264, 271], "flood": 19, "overwritten": 19, "updat": [19, 21, 22, 36, 180, 181, 211, 215, 216, 222, 226, 248, 251, 258, 265, 267, 268, 269, 270, 271, 272, 273], "hf_model_0001_0": [19, 267], "hf_model_0002_0": [19, 267], "adapt": [19, 83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 141, 152, 153, 154, 155, 163, 164, 193, 195, 196, 198, 199, 200, 201, 202, 222, 223, 224, 238, 260, 265, 267, 270, 273], "merg": [19, 24, 25, 177, 222, 267, 269, 273], "save_adapter_weights_onli": 19, "choos": [19, 62, 270], "resum": [19, 22, 191, 222, 223, 224, 273], "initi": [19, 22, 26, 54, 56, 80, 81, 82, 90, 91, 100, 101, 102, 103, 115, 116, 124, 125, 126, 134, 135, 157, 159, 174, 175, 176, 215, 226, 235, 236, 249, 261, 268, 270, 273], "frozen": [19, 141, 147, 150, 194, 215, 270, 271, 273], "learnt": [19, 265, 267], "refer": [19, 21, 22, 182, 183, 185, 189, 192, 200, 214, 215, 216, 217, 218, 239, 259, 270, 271, 272], "adapter_checkpoint": [19, 222, 223, 224], "adapter_0": [19, 267], "knowledg": 19, "forward": [19, 22, 140, 141, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 192, 193, 194, 195, 199, 215, 216, 217, 218, 231, 248, 269, 270, 271, 273], "modeltyp": [19, 222, 223, 224], "llama2_13b": [19, 106], "right": [19, 47, 50, 78, 187, 222, 267, 269, 270], "pytorch_fil": 19, "00003": [19, 221], "torchtune_sd": 19, "load_state_dict": [19, 193, 194, 195, 203, 226, 249, 270], "successfulli": [19, 264, 268], "vocab": [19, 24, 177, 187, 193, 194, 269], "70": [19, 115], "x": [19, 45, 75, 76, 77, 140, 141, 178, 179, 181, 182, 183, 184, 186, 187, 188, 189, 193, 194, 195, 199, 233, 246, 270, 272, 273], "randint": 19, "no_grad": 19, "6": [19, 45, 47, 48, 49, 50, 56, 89, 93, 182, 189, 233, 261, 272, 273], "3989": 19, "9": [19, 45, 47, 48, 50, 180, 189, 233, 267, 272, 273], "0531": 19, "2375": 19, "5": [19, 21, 45, 47, 48, 49, 50, 77, 189, 191, 215, 218, 219, 267, 268, 269, 271], "2822": 19, "4872": 19, "7469": 19, "8": [19, 45, 47, 48, 50, 61, 64, 67, 68, 70, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 153, 155, 161, 162, 164, 168, 170, 171, 172, 180, 189, 192, 267, 270, 271, 272, 273], "6737": 19, "11": [19, 45, 47, 48, 189, 267, 272, 273], "0023": 19, "8235": 19, "6819": 19, "2424": 19, "0109": 19, "6915": 19, "3618": 19, "1628": 19, "8594": 19, "5857": 19, "1151": 19, "7808": 19, "2322": 19, "8850": 19, "9604": 19, "7624": 19, "6040": 19, "3159": 19, "5849": 19, "8039": 19, "9322": 19, "2010": [19, 189], "6824": 19, "8929": 19, "8465": 19, "3794": 19, "3500": 19, "6145": 19, "5931": 19, "find": [19, 20, 22, 23, 215, 264, 267, 268, 270, 271], "hope": 19, "deeper": [19, 260, 261, 268, 271], "insight": [19, 267], "happi": [19, 267], "start": [20, 22, 23, 46, 78, 210, 225, 239, 258, 259, 265, 266, 267, 268, 271, 272], "cometlogg": 20, "checkpoint": [20, 21, 22, 190, 193, 195, 208, 221, 222, 223, 224, 225, 226, 227, 243, 245, 249, 259, 261, 264, 269, 270, 272, 273], "workspac": [20, 23, 239], "seen": [20, 23, 270, 273], "screenshot": [20, 23], "instal": [20, 21, 23, 236, 239, 242, 243, 257, 264, 266, 267, 268, 269, 270, 271, 272, 273], "comet_ml": [20, 239], "featur": [20, 22, 23, 258, 259, 260, 261, 267, 268, 271], "pip": [20, 23, 239, 242, 243, 258, 267, 269, 271], "login": [20, 23, 239, 243, 264, 267], "metric_logg": [20, 21, 22, 23], "metric_log": [20, 21, 23, 239, 240, 241, 242, 243], "experiment_nam": [20, 239], "experi": [20, 21, 239, 243, 257, 259, 269, 270], "grab": [20, 23, 269], "hyperparamet": [20, 218, 226, 259, 268, 270, 273], "tab": [20, 23], "asset": 20, "artifact": [20, 23, 248], "click": [20, 23], "pars": [21, 24, 25, 209, 262, 268], "effect": [21, 218, 271, 272], "cli": [21, 23, 25, 26, 258, 260, 267, 268, 271], "prerequisit": [21, 265, 266, 267, 268, 269, 270, 272, 273], "Be": [21, 265, 267, 268, 269, 270, 271, 272, 273], "familiar": [21, 265, 267, 268, 269, 270, 272, 273], "fundament": [21, 272], "reproduc": [21, 239], "overridden": [21, 248], "quick": 21, "seed": [21, 22, 23, 247, 268, 272], "shuffl": [21, 56, 272], "devic": [21, 22, 226, 229, 231, 251, 252, 262, 264, 265, 267, 268, 269, 270, 271, 273], "cuda": [21, 229, 231, 248, 252, 258, 267, 271, 273], "dtype": [21, 22, 79, 180, 181, 186, 187, 188, 190, 193, 195, 229, 246, 250, 267, 271, 272, 273], "fp32": [21, 187, 192, 271, 272, 273], "enable_fsdp": 21, "keyword": [21, 24, 53, 55, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 190, 265, 266], "subfield": 21, "dotpath": [21, 92, 104, 117, 146, 160, 167, 177, 266], "wish": [21, 180, 249, 266], "exact": [21, 24, 267], "normal": [21, 53, 56, 142, 179, 181, 182, 186, 187, 188, 192, 207, 265, 266, 270, 272, 273], "python": [21, 239, 243, 247, 253, 255, 264, 266, 267, 272], "instanc": [21, 24, 54, 55, 105, 118, 127, 147, 152, 154, 163, 169, 170, 171, 174, 175, 190, 196, 201, 202, 270], "cfg": [21, 22, 25, 26, 27], "under": [21, 248, 266, 271, 273], "preced": [21, 24, 264, 269, 270], "throw": 21, "notic": [21, 189, 265, 266, 270], "miss": [21, 203, 204, 248, 270], "posit": [21, 24, 56, 76, 78, 89, 93, 123, 127, 140, 145, 149, 152, 154, 156, 158, 163, 165, 180, 181, 183, 186, 187, 188, 189, 193, 194, 269], "dictconfig": [21, 22, 24, 25, 26, 27, 239, 243, 248], "mean": [21, 142, 181, 182, 186, 187, 188, 193, 213, 238, 264, 265, 266, 268, 270, 272], "llama2_token": [21, 265, 267], "llama2token": [21, 104], "512": [21, 266, 273], "instructdataset": [21, 266], "overwrit": [21, 224, 249, 258, 264], "duplic": [21, 22, 259, 264], "sometim": 21, "resolv": [21, 25, 268], "alpaca": [21, 54, 60, 61, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 266], "disklogg": 21, "log_dir": [21, 240, 242, 243], "verifi": [21, 229, 230, 252, 265, 268, 270], "properli": [21, 203, 236, 264], "wa": [21, 40, 50, 77, 145, 149, 189, 203, 265, 270, 272, 273], "cp": [21, 258, 264, 265, 267, 268, 269, 272], "7b_lora_single_devic": [21, 267, 268, 270, 273], "my_config": [21, 264], "guidelin": 21, "while": [21, 22, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 194, 259, 261, 267, 272, 273], "tempt": 21, "put": [21, 22, 262, 268, 270, 272], "much": [21, 194, 218, 267, 269, 270, 272, 273], "maximum": [21, 47, 50, 51, 53, 55, 56, 63, 74, 77, 89, 92, 93, 99, 104, 105, 114, 117, 118, 123, 127, 142, 144, 145, 146, 148, 149, 152, 154, 156, 158, 160, 163, 165, 167, 169, 173, 180, 181, 183, 186, 187, 188, 193, 195, 212, 221, 264], "switch": 21, "encourag": [21, 218, 270, 271], "clariti": 21, "significantli": [21, 215, 260, 261, 271], "easier": [21, 267, 268], "dont": 21, "privat": 21, "parent": [21, 264], "guarante": 21, "stabil": [21, 192, 259, 261, 271, 272, 273], "underscor": 21, "_alpaca": 21, "k1": [21, 22], "v1": [21, 22, 74], "k2": [21, 22], "v2": [21, 22, 239, 266], "lora": [21, 83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 199, 200, 203, 204, 222, 238, 257, 259, 262, 265, 268, 269], "lora_finetune_single_devic": [21, 260, 264, 265, 267, 268, 269, 270, 271, 273], "my_model_checkpoint": 21, "file_1": 21, "file_2": 21, "my_tokenizer_path": 21, "nest": [21, 251], "dot": 21, "notat": [21, 50, 140, 141, 181, 183, 187, 193, 213, 214, 233], "flag": [21, 22, 34, 55, 61, 62, 64, 66, 69, 70, 71, 220, 224, 230, 264, 271, 273], "bitsandbyt": [21, 271], "pagedadamw8bit": [21, 271], "delet": 21, "foreach": [21, 53], "pytorch": [21, 22, 76, 187, 190, 192, 220, 236, 242, 245, 247, 248, 257, 258, 259, 261, 267, 269, 270, 271, 272, 273], "8b_full": [21, 264, 266], "adamw": [21, 270, 271], "lr": [21, 191, 271], "2e": [21, 271], "fuse": [21, 144, 148, 193, 194, 195, 196, 244, 272], "nproc_per_nod": [21, 261, 266, 269, 270, 272], "full_finetune_distribut": [21, 264, 266, 267, 268], "thought": [22, 259, 262, 268, 273], "target": [22, 77, 218, 259], "pipelin": [22, 259, 261], "eg": [22, 187, 193, 222, 259], "meaning": [22, 259, 267], "fsdp": [22, 185, 220, 226, 230, 238, 268, 269], "activ": [22, 178, 227, 231, 237, 245, 248, 259, 261, 272, 273], "gradient": [22, 238, 244, 248, 259, 261, 267, 269, 270, 273], "accumul": [22, 244, 248, 259, 261], "mix": [22, 179, 264, 266, 267, 271], "precis": [22, 179, 190, 229, 259, 261, 268, 273], "complex": 22, "becom": [22, 189, 258, 266], "harder": 22, "anticip": 22, "architectur": [22, 98, 151, 187, 189, 193, 195, 225, 264, 266], "methodolog": 22, "possibl": [22, 56, 221, 264, 266, 271], "trade": [22, 271], "vs": [22, 268], "qualiti": [22, 267, 270, 272], "believ": 22, "suit": [22, 268, 271], "solut": 22, "result": [22, 67, 145, 149, 189, 210, 212, 248, 261, 267, 269, 270, 271, 272, 273], "meant": [22, 190, 226], "depend": [22, 23, 222, 248, 264, 266, 267, 270, 271, 273], "level": [22, 57, 58, 192, 211, 228, 238, 253, 259, 273], "expertis": 22, "routin": 22, "yourself": [22, 264, 269, 270], "exist": [22, 195, 239, 258, 264, 267, 268, 269, 273], "ones": [22, 50, 180], "modular": [22, 259], "build": [22, 73, 89, 99, 114, 123, 144, 145, 148, 149, 156, 158, 173, 221, 259, 269, 270], "block": [22, 56, 83, 84, 85, 89, 93, 94, 95, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 144, 147, 148, 149, 152, 153, 154, 155, 156, 163, 164, 169, 170, 171, 172, 173, 181, 187, 188, 203, 204, 259], "wandb": [22, 23, 243, 268], "log": [22, 25, 215, 216, 217, 218, 231, 237, 239, 240, 241, 242, 243, 253, 267, 268, 269, 270, 271, 273], "fulli": [22, 54, 147], "nativ": [22, 257, 259, 270, 272, 273], "numer": [22, 68, 259, 261, 272], "pariti": [22, 259], "verif": [22, 182], "extens": [22, 224, 259], "benchmark": [22, 247, 259, 267, 269, 270, 272], "limit": [22, 226, 266, 271, 272], "hidden": [22, 141, 145, 149, 178, 187, 189], "behind": 22, "unnecessari": 22, "abstract": [22, 28, 33, 205, 206, 259, 268, 273], "No": [22, 224, 259], "go": [22, 98, 145, 149, 151, 189, 210, 259, 266, 267, 268, 271, 273], "upon": [22, 54, 269], "figur": [22, 270, 273], "spectrum": 22, "decid": 22, "avail": [22, 40, 74, 193, 195, 229, 236, 252, 259, 264, 267, 269, 270], "paradigm": [22, 260, 271], "consist": [22, 30, 35, 40, 67, 68, 74, 262, 268], "overrid": [22, 25, 26, 30, 35, 40, 64, 65, 67, 68, 69, 70, 71, 249, 262, 264, 267, 268, 269, 273], "valid": [22, 52, 78, 203, 204, 214, 249, 250, 258, 262, 267, 268], "environ": [22, 236, 239, 252, 258, 262, 264, 266, 267, 268, 272], "closer": [22, 270], "monolith": [22, 259], "trainer": [22, 215, 217, 218], "wrapper": [22, 179, 207, 208, 226, 228, 264, 270], "around": [22, 53, 142, 179, 207, 208, 231, 264, 265, 267, 270, 271, 272, 273], "extern": [22, 266], "primarili": [22, 54, 270], "eleutherai": [22, 74, 259, 270, 272], "har": [22, 259, 270, 272], "stage": [22, 189], "distil": 22, "dataload": [22, 56, 61, 64, 67, 68, 70], "applic": [22, 222, 223, 243], "clean": [22, 23, 60], "group": [22, 181, 234, 235, 239, 240, 241, 242, 243, 264, 269, 272], "init_process_group": [22, 235], "backend": [22, 264, 272], "gloo": 22, "nccl": 22, "fullfinetunerecipedistribut": 22, "cleanup": 22, "stuff": 22, "carri": [22, 58], "metric": [22, 268, 271, 272], "logger": [22, 237, 239, 240, 241, 242, 243, 253, 268], "_devic": 22, "get_devic": 22, "_dtype": 22, "get_dtyp": 22, "ckpt_dict": 22, "wrap": [22, 195, 220, 227, 230, 238, 245, 265], "_model": [22, 226], "_setup_model": 22, "_setup_token": 22, "_optim": 22, "_setup_optim": 22, "_loss_fn": 22, "_setup_loss": 22, "_sampler": 22, "_dataload": 22, "_setup_data": 22, "backward": [22, 226, 228, 244, 248, 273], "zero_grad": 22, "curr_epoch": 22, "rang": [22, 194, 215, 216, 218, 247, 264, 269, 272], "epochs_run": [22, 23], "total_epoch": [22, 23], "idx": [22, 56], "enumer": 22, "_autocast": 22, "logit": [22, 75, 76, 79, 192, 233], "global_step": 22, "_log_every_n_step": 22, "_metric_logg": 22, "log_dict": [22, 239, 240, 241, 242, 243], "step": [22, 56, 57, 58, 67, 68, 187, 191, 193, 213, 228, 239, 240, 241, 242, 243, 244, 248, 257, 261, 267, 270, 272, 273], "decor": [22, 26], "recipe_main": [22, 26], "fullfinetunerecip": 22, "wandblogg": [23, 270, 273], "tip": 23, "straggler": 23, "background": 23, "crash": 23, "otherwis": [23, 45, 47, 50, 145, 149, 185, 187, 189, 236, 239, 265, 272], "exit": [23, 200, 258, 264], "resourc": [23, 239, 240, 241, 242, 243, 271, 272], "kill": 23, "ps": 23, "aux": 23, "grep": 23, "awk": 23, "xarg": 23, "desir": [23, 53, 57, 58, 246, 265, 271], "suggest": 23, "approach": [23, 54, 266], "full_finetun": 23, "joinpath": 23, "_checkpoint": [23, 267], "_output_dir": [23, 222, 223, 224], "torchtune_model_": 23, "with_suffix": 23, "wandb_at": 23, "descript": [23, 264], "whatev": 23, "metadata": [23, 272], "seed_kei": 23, "epochs_kei": 23, "total_epochs_kei": 23, "max_steps_kei": 23, "max_steps_per_epoch": [23, 272], "add_fil": 23, "log_artifact": 23, "hydra": 24, "facebook": 24, "research": 24, "com": [24, 76, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172, 182, 183, 191, 192, 215, 216, 217, 218, 239, 258, 267, 269], "facebookresearch": [24, 182], "blob": [24, 76, 83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 167, 170, 171, 172, 182, 183, 191, 215, 216, 217, 218], "main": [24, 26, 167, 182, 183, 258, 261, 267, 269], "_intern": 24, "_instantiate2": 24, "l148": 24, "omegaconf": 24, "num_lay": [24, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 187, 189, 193, 195], "32": [24, 180, 189, 193, 195, 239, 269, 270, 271, 272, 273], "num_head": [24, 89, 93, 99, 105, 114, 118, 123, 127, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181, 183, 187], "num_kv_head": [24, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 180, 181], "vocab_s": [24, 75, 76, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 192, 194], "nn": [24, 45, 47, 50, 140, 141, 178, 180, 181, 185, 186, 187, 188, 189, 190, 193, 194, 195, 196, 197, 198, 200, 201, 202, 220, 227, 228, 238, 244, 245, 249, 250, 270, 273], "parsed_yaml": 24, "embed_dim": [24, 89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181, 183, 186, 187, 188, 189, 194, 195, 249, 270], "valueerror": [24, 30, 32, 35, 40, 42, 43, 46, 47, 50, 52, 55, 61, 62, 64, 66, 67, 68, 70, 71, 73, 169, 180, 181, 189, 222, 223, 224, 229, 231, 247, 250], "recipe_nam": 25, "rank": [25, 83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 199, 234, 236, 247, 260, 268, 270, 273], "zero": [25, 50, 180, 182, 221, 267, 269, 272], "displai": 25, "callabl": [26, 53, 55, 57, 58, 59, 67, 68, 73, 75, 187, 200, 220, 230, 232, 238, 245], "With": [26, 267, 270, 272, 273], "my_recip": 26, "foo": 26, "bar": [26, 259, 268, 271], "instanti": [27, 36, 80, 81, 82, 83, 84, 85, 89, 90, 91, 92, 93, 94, 95, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 119, 120, 123, 124, 125, 126, 127, 128, 129, 130, 134, 135, 136, 137, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 226], "configerror": 27, "cannot": [27, 46, 224, 269], "deprec": [28, 33, 43, 44, 53, 55], "remov": [28, 33, 43, 44, 53, 55, 271], "futur": [28, 33, 43, 44, 53, 55, 261, 272], "releas": [28, 33, 43, 44, 53, 55, 264, 269], "classmethod": [28, 33, 34, 266], "openai": [29, 35, 43, 62, 216, 266], "markup": 29, "im_start": 29, "context": [29, 166, 200, 246, 248, 266, 271], "im_end": 29, "goe": [29, 200], "a2": [30, 57], "keep": [30, 32, 35, 40, 65, 66, 68, 69, 72, 194, 267, 270, 271], "present": [30, 35, 40, 64, 65, 67, 68, 69, 70, 71, 208, 224, 249], "functool": [31, 38, 41, 220], "partial": [31, 38, 41, 220], "_prompt_templ": [31, 38, 41], "assistant_messag": [31, 38, 41], "equival": [32, 43, 44, 217, 218], "respect": [32, 54, 98, 180, 202, 248, 265, 266], "alwai": [33, 40, 239, 249, 265, 271], "liter": [34, 36, 39, 83, 84, 85, 86, 87, 88, 92, 93, 94, 95, 96, 97, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 117, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 146, 147, 148, 149, 150, 152, 153, 154, 155, 160, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 177, 203, 204], "union": [34, 46, 47, 60, 61, 62, 64, 66, 70, 71, 73, 74, 92, 104, 117, 146, 160, 167, 177, 187, 193, 204, 222, 227, 239, 240, 241, 242, 243, 245, 247], "interleav": [34, 212], "attach": 34, "writer": 34, "calcul": [34, 36, 78, 140, 142, 181, 186, 188, 189, 213, 214, 216, 269], "consecut": [34, 52, 180, 212], "last": [34, 51, 56, 73, 187, 191, 214, 266], "properti": [34, 186, 188, 195, 270, 271], "media": [34, 58], "construct": [34, 65, 212, 262, 270], "image_url": 35, "unmask": [35, 40, 43, 44], "consid": [36, 54, 57, 58, 145, 149, 189, 271], "come": [36, 52, 198, 270, 271], "nanswer": 38, "alia": [39, 220], "final": [40, 57, 58, 83, 84, 85, 89, 93, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 144, 145, 147, 148, 149, 152, 153, 154, 155, 156, 163, 164, 169, 172, 173, 178, 187, 193, 203, 204, 267, 269, 270, 271, 273], "leav": [40, 271], "nsummari": [41, 265], "summari": [41, 54, 70, 189, 231, 266], "transformed_sampl": [43, 44, 266], "sequenc": [45, 47, 48, 49, 50, 55, 56, 59, 63, 67, 68, 73, 74, 77, 78, 89, 92, 93, 99, 104, 105, 114, 117, 118, 123, 127, 140, 141, 142, 144, 146, 148, 152, 154, 156, 158, 160, 163, 165, 167, 169, 173, 177, 180, 181, 183, 186, 187, 188, 189, 193, 195, 208, 210, 212, 214, 218, 219, 233, 265], "batch_first": 45, "padding_valu": 45, "float": [45, 75, 76, 79, 83, 84, 85, 86, 87, 88, 89, 93, 94, 95, 96, 97, 99, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 142, 147, 148, 149, 150, 152, 153, 154, 155, 156, 158, 161, 162, 163, 164, 165, 168, 169, 170, 171, 172, 173, 181, 182, 191, 199, 213, 214, 215, 216, 217, 218, 231, 237, 239, 240, 241, 242, 243, 270, 271, 272, 273], "rnn": [45, 47, 50], "pad_sequ": [45, 47, 50], "variabl": [45, 55, 225, 236, 239, 266, 271, 273], "length": [45, 47, 49, 50, 51, 52, 54, 55, 56, 63, 74, 77, 89, 92, 93, 99, 104, 105, 114, 117, 118, 123, 127, 140, 141, 142, 144, 146, 148, 152, 154, 156, 158, 160, 163, 165, 166, 167, 169, 173, 177, 180, 181, 183, 186, 187, 188, 192, 193, 195, 208, 212, 213, 214, 223, 233, 239, 271], "left": [45, 47, 50, 142, 187, 270], "longest": [45, 49, 50], "trail": 45, "dimens": [45, 50, 89, 93, 99, 105, 114, 118, 123, 127, 140, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 178, 180, 181, 183, 187, 189, 194, 199, 269, 270, 271, 273], "element": [45, 47, 50, 54, 233, 267], "12": [45, 47, 48, 71, 189, 258, 272], "image_loc": 46, "www": [46, 239], "wikipedia": [46, 74], "org": [46, 67, 80, 81, 82, 83, 84, 86, 87, 88, 93, 94, 95, 96, 97, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113, 118, 119, 120, 121, 122, 129, 130, 131, 132, 133, 136, 137, 138, 139, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 181, 182, 183, 189, 212, 213, 215, 216, 217, 218, 220, 236, 242, 245, 247, 253, 258], "en": [46, 53, 55, 59, 63, 65, 73, 74, 272], "pad_direct": [47, 50], "keys_to_pad": 47, "padding_idx": [47, 48, 49, 50, 56], "left_pad_sequ": [47, 50], "integ": [47, 49, 194, 220, 221, 227, 247], "batch_siz": [47, 61, 64, 67, 68, 70, 180, 181, 186, 187, 188, 192, 193, 194, 195, 215, 217, 219, 267, 271, 272], "ignore_idx": [48, 49, 50], "input_id": [48, 233], "chosen_input_id": [48, 69, 266], "chosen_label": [48, 266], "15": [48, 189, 230, 265, 267, 270, 273], "16": [48, 83, 84, 85, 86, 87, 88, 94, 95, 96, 97, 106, 107, 108, 109, 110, 111, 112, 113, 119, 120, 121, 122, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 153, 155, 161, 162, 164, 168, 170, 171, 172, 180, 189, 270, 273], "17": [48, 189, 270], "18": [48, 189, 269], "19": [48, 189, 273], "20": [48, 189, 219, 272], "token_pair": 49, "padded_col": [49, 266], "pad_max_imag": 50, "tile": [50, 140, 141, 142, 143, 145, 146, 147, 149, 189, 212], "aspect": [50, 259], "ratio": [50, 215, 216], "cross": [50, 56, 144, 148, 186, 192, 193, 195, 212], "attent": [50, 56, 76, 77, 78, 83, 84, 85, 89, 93, 94, 95, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 144, 145, 147, 148, 149, 152, 153, 154, 155, 156, 158, 163, 164, 165, 166, 169, 170, 171, 172, 173, 180, 181, 183, 186, 187, 188, 193, 195, 203, 204, 212, 269, 270, 271, 273], "text_seq_len": [50, 212], "n_tile": [50, 189], "h": [50, 140, 180, 189, 192, 258, 264], "w": [50, 80, 81, 82, 90, 91, 100, 101, 102, 103, 115, 116, 124, 125, 126, 134, 135, 140, 157, 159, 174, 175, 176, 189, 239, 242, 243, 265, 267, 270, 273], "h_ratio": 50, "w_ratio": 50, "encoder_mask": [50, 186, 187, 193], "image_seq_len": [50, 212], "channel": [50, 140, 142, 145, 149, 189, 272], "height": [50, 140], "largest": 50, "max": [50, 56, 177, 187, 189, 191, 193, 208, 221, 264, 270], "bsz": [50, 75, 76, 77, 78, 189, 192], "max_num_imag": 50, "max_num_til": [50, 142, 145, 149, 189, 212], "tokens_per_til": 50, "image_id": 50, "four": [50, 270], "model_input": 50, "max_text_seq_len": 50, "40": [50, 145, 149, 189, 212, 271, 273], "got": 50, "did": [50, 269, 273], "extra": [50, 53, 142, 193, 258, 265, 270, 271, 272, 273], "second": [50, 181, 194, 267, 270, 271, 273], "eos_id": [51, 142, 208, 210], "shorter": [52, 187], "min": [52, 270], "invalid": 52, "multiturn": [53, 265], "convert_to_messag": 53, "truncat": [53, 55, 56, 63, 73, 74, 92, 104, 117, 142, 146, 160, 167, 177, 208, 219, 266], "filepath": [53, 55, 57, 58, 59, 61, 62, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74], "doc": [53, 55, 58, 59, 63, 65, 73, 74, 220, 236, 239, 242, 243, 247, 253, 264, 266, 267], "package_refer": [53, 55, 59, 63, 65, 73, 74], "loading_method": [53, 55, 59, 63, 65, 73, 74], "chat_format": [53, 266], "chatformat": [53, 266], "still": [53, 142, 192, 194, 195, 260, 270, 272, 273], "unless": 53, "sub": [54, 242], "unifi": [54, 159], "simplifi": [54, 215, 264, 270], "simultan": 54, "intern": 54, "aggreg": 54, "transpar": 54, "howev": [54, 167, 258, 271], "constitu": 54, "might": [54, 194, 197, 264, 267, 271], "larg": [54, 192, 199, 248, 264, 271, 273], "comput": [54, 57, 58, 99, 105, 114, 118, 123, 127, 140, 141, 144, 148, 169, 173, 181, 183, 187, 188, 192, 193, 212, 215, 217, 218, 231, 247, 261, 267, 271, 272, 273], "cumul": 54, "maintain": [54, 195, 260, 271, 273], "deleg": 54, "retriev": [54, 57, 58, 187, 230], "lead": [54, 207, 221, 261], "high": [54, 57, 58, 259, 270, 271], "scale": [54, 75, 76, 79, 83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 182, 184, 186, 188, 199, 214, 218, 270, 271, 272, 273], "strategi": [54, 261], "stream": [54, 253, 271], "demand": 54, "deriv": [54, 178, 187, 188], "dataset1": 54, "mycustomdataset": 54, "params1": 54, "dataset2": 54, "params2": 54, "concat_dataset": 54, "total": [54, 191, 214, 216, 234, 256, 263, 267, 269, 270, 271], "data_point": 54, "1500": 54, "vicgal": [54, 266], "gpt4": [54, 266], "alpacainstructtempl": [54, 266], "samsum": [54, 70, 266], "focus": [54, 262, 268, 271], "enhanc": [54, 189, 218, 271, 273], "divers": 54, "machin": [54, 217, 252, 264, 267], "instructtempl": [55, 266], "contribut": [55, 61, 62, 64, 66, 69, 70, 71, 214, 216], "disabl": [55, 63, 74, 200, 247, 272], "recommend": [55, 62, 63, 64, 69, 70, 72, 74, 151, 187, 192, 239, 242, 265, 267, 271, 273], "highest": [55, 63, 74], "max_pack": 56, "split_across_pack": [56, 73], "greedi": 56, "pack": [56, 60, 61, 62, 64, 66, 67, 68, 70, 71, 73, 74, 181, 183, 187, 188, 193, 272], "done": [56, 187, 203, 229, 238, 249, 270, 272, 273], "outsid": [56, 247, 248, 270], "sampler": [56, 268], "part": [56, 194, 217, 265, 273], "buffer": [56, 271], "enough": [56, 265], "lower": [56, 261, 270], "triangular": 56, "attend": [56, 181, 186, 187, 188, 193, 212], "wise": 56, "made": [56, 62, 66, 69, 73, 142, 267], "smaller": [56, 194, 267, 269, 270, 271, 272, 273], "jam": 56, "s1": [56, 207], "s2": [56, 207], "s3": 56, "s4": 56, "contamin": 56, "input_po": [56, 76, 181, 183, 187, 188, 193], "matrix": [56, 186, 187, 193], "causal": [56, 77, 181, 187, 188, 193], "increment": 56, "move": [56, 73, 187, 251, 271], "entir": [56, 73, 192, 197, 238, 265, 273], "avoid": [56, 73, 182, 189, 190, 247, 264, 272, 273], "sentenc": [56, 73], "techniqu": [57, 259, 260, 261, 267, 268, 269, 270, 271, 272], "separ": [57, 195, 210, 222, 265, 268, 269, 270, 273], "repons": 57, "At": [57, 58, 187, 193], "extract": [57, 58, 63, 209], "against": [57, 58, 218, 254, 272, 273], "unit": [57, 58, 238, 259], "row": [57, 58, 181, 187, 188, 193], "filter_fn": [58, 59, 73], "round": [58, 272], "incorpor": [58, 215, 266], "happen": [58, 192], "ti": [58, 93, 169, 173, 185, 271], "agnost": [58, 266], "treat": [58, 189, 200, 265], "minimum": [58, 67, 68], "filter": [58, 59, 73, 272], "prior": [58, 59, 61, 62, 64, 66, 67, 68, 70, 71, 73, 74, 249], "unstructur": [59, 73, 74], "corpu": [59, 63, 73, 74], "tabular": [59, 73], "txt": [59, 73, 177, 240, 266, 268], "eo": [59, 73, 167, 207, 210, 265, 266], "yahma": 60, "packeddataset": [60, 61, 62, 64, 66, 70, 71, 73, 74, 266], "variant": [60, 64, 70], "page": [60, 74, 258, 259, 264, 268, 269, 271], "tatsu": 61, "lab": [61, 76], "codebas": [61, 267], "independ": 61, "alpacatomessag": 61, "alpaca_d": 61, "altern": [62, 66, 69, 268, 271], "friendli": [62, 66, 69, 73, 75, 265], "toward": [62, 218], "my_dataset": [62, 66], "london": [62, 66], "ccdv": 63, "cnn_dailymail": 63, "textcompletiondataset": [63, 73, 74, 266], "cnn": 63, "dailymail": 63, "articl": [63, 74], "highlight": [63, 273], "conjunct": [64, 70, 72, 187, 271], "grammar_d": 64, "rlhflow": 65, "hh": 65, "preferencedataset": [65, 69, 72, 266], "liuhaotian": 67, "llava": 67, "150k": 67, "coco": 67, "train2017": 67, "llava_instruct_150k": 67, "2017": 67, "visit": [67, 267], "cocodataset": 67, "wget": 67, "zip": [67, 255], "unzip": 67, "minim": [67, 68, 266, 268, 270, 272, 273], "clip": [67, 68, 140, 141, 142, 145, 149, 189, 216], "mymodeltransform": [67, 68], "tokenizer_path": [67, 68], "image_transform": [67, 68], "yet": [67, 68, 151, 265, 267], "llava_instruct_d": 67, "huggingfacem4": 68, "the_cauldron": 68, "cauldron": 68, "card": 68, "cauldron_d": 68, "compris": 69, "share": [69, 181, 185, 266, 267], "c1": 69, "r1": 69, "chosen_messag": [69, 266], "rejected_messag": [69, 266], "samsung": 70, "samsum_d": 70, "351": 71, "82": 71, "391": 71, "221": 71, "220": 71, "193": 71, "471": 71, "lvwerra": [72, 266], "stack": [72, 189, 248, 266], "exchang": [72, 266], "allenai": [73, 266, 272], "data_dir": [73, 266], "realnewslik": [73, 266], "wikitext_document_level": 74, "wikitext": [74, 272], "103": [74, 267], "transformerdecod": [75, 76, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 144, 148, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 168, 169, 170, 171, 172, 173, 174, 175, 176, 193, 194, 195, 270], "max_generated_token": 75, "pad_id": [75, 219], "temperatur": [75, 76, 79, 215, 217, 218, 267], "top_k": [75, 76, 79, 267], "stop_token": [75, 219], "rng": 75, "custom_generate_next_token": 75, "condit": [75, 236, 264, 266], "seq_length": [75, 76, 77, 186, 188, 194, 195], "prune": [75, 79, 273], "probabl": [75, 79, 83, 84, 85, 93, 94, 95, 105, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 199, 215, 216, 217, 218, 267], "stop": [75, 219], "random": [75, 189, 247, 268], "compil": [75, 192, 267, 269, 271, 273], "generate_next_token": 75, "been": [75, 77, 180, 187, 219, 230, 265, 272], "llama3_8b": [75, 120, 128, 193, 269, 271, 272], "manual_se": 75, "tolist": 75, "jeremi": 75, "m": [75, 190, 265, 272], "seq_len": [75, 77, 78, 187], "num_generated_token": 75, "q": [76, 79, 181, 270], "randomli": [76, 79, 249], "softmax": [76, 79, 181, 187, 188, 193], "trick": [76, 79], "fast": [76, 267], "32971d3129541c5bfb4f715abc33d1c5f408d204": 76, "l40": 76, "top": [76, 79, 145, 149, 228, 271, 273], "k": [76, 79, 181, 270], "padding_mask": [77, 78, 216, 219], "target_seq_len": 77, "suitabl": 77, "scaled_dot_product_attent": [77, 89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173, 181], "static": 77, "kv": [77, 180, 181, 187, 188, 193, 272], "cach": [77, 180, 181, 183, 186, 187, 188, 193, 195, 258, 264], "longer": [77, 180, 266, 271], "boolean": [77, 78, 181, 186, 187, 188, 193, 195, 220, 233], "assertionerror": [77, 186, 187, 203, 204, 249], "shift": [78, 187], "uniform_": 79, "int32": 79, "code_llama2": [80, 81, 82, 83, 84, 85, 86, 87, 88, 264], "arxiv": [80, 81, 82, 83, 84, 86, 87, 88, 93, 94, 95, 96, 97, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113, 118, 119, 120, 121, 122, 129, 130, 131, 132, 133, 136, 137, 138, 139, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 181, 182, 183, 189, 212, 213, 215, 216, 217, 218], "pdf": [80, 81, 82, 212, 213], "2308": [80, 81, 82], "12950": [80, 81, 82], "lora_attn_modul": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 203, 204, 260, 270, 271, 273], "q_proj": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 181, 203, 204, 260, 270, 271, 272, 273], "k_proj": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 181, 203, 204, 260, 270, 271, 272, 273], "v_proj": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 181, 203, 204, 260, 270, 271, 272, 273], "output_proj": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 181, 203, 204, 270, 271, 272, 273], "apply_lora_to_mlp": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 203, 204, 260, 270, 271], "apply_lora_to_output": [83, 84, 85, 86, 87, 88, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 172, 203, 204, 270, 271], "lora_rank": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 260, 270, 271], "lora_alpha": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 260, 270, 271], "lora_dropout": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 271], "use_dora": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 129, 130, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172], "quantize_bas": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 199, 273], "code_llama2_13b": 83, "tloen": [83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172], "8bb8579e403dc78e37fe81ffbb253c413007323f": [83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172], "l41": [83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172], "l43": [83, 84, 85, 94, 95, 106, 107, 108, 109, 119, 120, 128, 129, 130, 136, 137, 164, 170, 171, 172], "linear": [83, 84, 85, 86, 87, 88, 93, 94, 95, 96, 97, 105, 106, 107, 108, 109, 110, 111, 112, 113, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 141, 147, 148, 149, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 169, 170, 171, 172, 185, 187, 198, 199, 203, 204, 270, 271, 272, 273], "mlp": [83, 84, 85, 89, 93, 94, 95, 99, 105, 106, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 144, 147, 148, 149, 152, 153, 154, 155, 156, 158, 163, 164, 165, 169, 170, 171, 172, 173, 186, 187, 188, 203, 204, 269, 270, 271], "low": [83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 199, 260, 267, 270, 273], "approxim": [83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 199, 270], "factor": [83, 84, 85, 93, 94, 95, 105, 106, 107, 108, 109, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 163, 164, 169, 170, 171, 172, 199, 213, 267], "dropout": [83, 84, 85, 89, 93, 94, 95, 99, 105, 107, 108, 109, 114, 118, 119, 120, 123, 127, 128, 129, 130, 136, 137, 147, 148, 149, 152, 153, 154, 155, 156, 158, 163, 164, 165, 169, 170, 171, 172, 173, 181, 199, 270, 271, 273], "decompos": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164], "magnitud": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164, 271], "introduc": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164, 181, 182, 195, 199, 218, 261, 265, 266, 270, 271, 272, 273], "dora": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 127, 129, 130, 136, 137, 148, 149, 152, 153, 154, 155, 163, 164], "ab": [83, 84, 86, 87, 88, 93, 94, 95, 96, 97, 100, 101, 102, 103, 105, 106, 107, 108, 110, 111, 112, 113, 118, 119, 120, 121, 122, 129, 130, 131, 132, 133, 136, 137, 138, 139, 150, 152, 153, 154, 155, 161, 162, 163, 164, 168, 181, 182, 183, 189, 215, 216, 217, 218], "2402": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164], "09353": [83, 84, 93, 94, 95, 105, 106, 107, 108, 118, 119, 120, 129, 130, 136, 137, 152, 153, 154, 155, 163, 164], "code_llama2_70b": 84, "code_llama2_7b": 85, "qlora": [86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 190, 257, 259, 260, 269, 270], "paper": [86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 212, 215, 217, 218, 270, 273], "2305": [86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168, 181, 215, 217], "14314": [86, 87, 88, 96, 97, 110, 111, 112, 113, 121, 122, 131, 132, 133, 138, 139, 150, 161, 162, 168], "lora_code_llama2_13b": 86, "lora_code_llama2_70b": 87, "lora_code_llama2_7b": 88, "head_dim": [89, 93, 180, 181, 187], "intermediate_dim": [89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173], "attn_dropout": [89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173, 181, 187], "norm_ep": [89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173], "1e": [89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173, 182], "06": [89, 93, 182, 270], "rope_bas": [89, 93, 99, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173], "10000": [89, 93, 99, 152, 154, 156, 158, 163, 165, 183], "norm_embed": [89, 93], "transformerselfattentionlay": [89, 99, 114, 123, 156, 173, 186, 187, 193, 195], "rm": [89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173], "norm": [89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 187], "space": [89, 99, 114, 123, 144, 148, 156, 173, 187, 197, 271], "slide": [89, 156, 166], "window": [89, 156, 166, 266], "vocabulari": [89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 192, 270, 271], "mha": [89, 93, 99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181, 187], "intermedi": [89, 93, 99, 105, 114, 118, 123, 127, 144, 145, 148, 149, 152, 154, 156, 158, 163, 165, 169, 173, 189, 224, 245, 269, 273], "onto": [89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173, 181, 197], "epsilon": [89, 93, 99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173, 216], "rotari": [89, 93, 99, 123, 127, 152, 154, 156, 158, 163, 165, 183, 269], "10_000": [89, 93, 152, 154, 156, 158, 165], "blog": [90, 91], "technolog": [90, 91], "develop": [90, 91, 258, 273], "gemmatoken": 92, "_templatetyp": [92, 104, 117, 146, 160, 167, 177], "gemma_2b": 94, "gemma_7b": 95, "lora_gemma_2b": 96, "lora_gemma_7b": 97, "taken": [98, 270, 273], "sy": [98, 265, 266], "honest": [98, 265, 266], "pari": [98, 151, 266], "capit": [98, 151, 266], "franc": [98, 151, 266], "known": [98, 151, 232, 266, 272], "stun": [98, 151, 266], "05": [99, 105, 114, 118, 123, 127, 152, 154, 156, 158, 163, 165, 169, 173], "gqa": [99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181], "mqa": [99, 105, 114, 118, 123, 127, 144, 148, 152, 154, 156, 158, 163, 165, 169, 173, 181], "kvcach": [99, 105, 114, 118, 123, 127, 144, 148, 163, 169, 173, 181, 187], "scale_hidden_dim_for_mlp": [99, 105, 114, 118, 123, 127, 144, 148, 169, 173], "2307": [100, 101, 102, 103], "09288": [100, 101, 102, 103], "classif": [103, 154, 158, 159, 225], "llama2_70b": 107, "llama2_7b": [108, 270], "classifi": [109, 154, 158, 159, 249, 266, 271], "llama2_reward_7b": [109, 225], "lora_llama2_13b": 110, "lora_llama2_70b": 111, "lora_llama2_7b": [112, 270], "lora_llama2_reward_7b": 113, "500000": [114, 118, 123, 127, 144, 148], "llama3token": [117, 142, 206], "regist": [117, 142, 146, 167, 177, 190, 244, 273], "similarli": [117, 146, 167, 177, 266, 272], "canon": [117, 142, 146, 167, 177], "llama3_70b": 119, "lora_llama3_70b": 121, "lora_llama3_8b": 122, "scale_factor": [123, 127], "500_000": [123, 127], "rope": [123, 127, 169, 173, 181, 183], "llama3_1": [124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 260], "llama3_1_70b": 129, "llama3_1_8b": 130, "lora_llama3_1_405b": 131, "lora_llama3_1_70b": 132, "lora_llama3_1_8b": 133, "llama3_2": [134, 135, 136, 137, 138, 139], "llama3_2_1b": 136, "llama3_2_3b": 137, "lora_llama3_2_1b": 138, "lora_llama3_2_3b": 139, "projection_head": [140, 193, 197], "combin": [140, 142, 145, 149, 187, 193, 195, 197, 214], "learnabl": [140, 184, 193, 195, 267], "fusion": [140, 143, 144, 145, 147, 148, 149, 193, 194, 195, 196, 197], "encoder_dim": [140, 141], "decoder_dim": [140, 141], "crop": [140, 145, 149, 189], "num_img": [140, 141], "num_emb": [140, 141], "broken": [140, 141, 189, 195], "width": [140, 272], "clip_embeds_per_til": 140, "emb": [140, 141, 181, 186, 187, 193], "num_hidden_input": 141, "sequenti": [141, 193, 197], "num_hidden": 141, "hidden_st": [141, 189], "cl": [141, 189, 266], "patch": [141, 142, 145, 149, 189, 212], "image_mean": 142, "image_std": 142, "divid": [142, 145, 149, 189, 212], "tranform": 142, "possible_resolut": 142, "break": [142, 208], "448": [142, 143, 146, 147], "deviat": 142, "transformed_data": 142, "img1": [142, 212], "img2": [142, 212], "31587": [142, 207, 208], "29644": [142, 207, 208], "102": [142, 207, 208], "truncate_at_eo": [142, 208], "show": [142, 208, 212, 258, 260, 261, 264, 265, 270], "skip": [142, 181, 208], "tokenize_head": 142, "tokenize_end": 142, "header": 142, "eom": 142, "wether": 142, "decoder_train": [143, 147, 150, 193], "encoder_train": [143, 147, 150, 193], "fusion_train": [143, 147, 150, 193], "deepfusionmodel": [143, 147, 150], "trainabl": [143, 147, 195, 199, 202, 238, 270, 271, 273], "resiz": [143, 146, 147], "fusion_interv": [144, 148], "num_special_token": [144, 148], "encoder_max_seq_len": [144, 148, 186, 187, 188, 193, 195], "causalselfattent": [144, 148], "interv": [144, 148, 268], "clip_embed_dim": [145, 149], "clip_num_lay": [145, 149], "clip_hidden_st": [145, 149], "num_layers_project": [145, 149], "decoder_embed_dim": [145, 149], "in_channel": [145, 149, 189], "llama3visionencod": [145, 149], "spatial": [145, 149], "backbon": [145, 149], "400": [145, 149, 189, 212], "10x10": [145, 149, 189, 212], "grid": [145, 149, 189, 212], "dimension": [145, 149, 189], "fourth": [145, 149, 189], "determin": [145, 149, 204], "trainbl": 147, "decoder_lora": 148, "fusion_lora": [148, 149], "encoder_lora": 149, "lora_llama3_2_vision_11b": 150, "num_class": [154, 158, 249], "announc": 157, "ray2333": 159, "feedback": [159, 215], "lora_mistral_7b": 161, "lora_mistral_reward_7b": 162, "phi3_mini": [164, 225], "128k": 166, "nor": 166, "phi3minitoken": 167, "tokenizer_config": 167, "spm": 167, "lm": [167, 216], "bo": [167, 207, 210, 265, 266], "unk": 167, "augment": [167, 273], "endoftext": 167, "phi3minisentencepiecebasetoken": 167, "lora_phi3_mini": 168, "1000000": [169, 173], "tie_word_embed": [169, 170, 171, 173, 174, 175], "qwen2transformerdecod": 169, "period": [169, 173], "word": [169, 173, 272], "qwen2_0_5b": [170, 185], "qwen2_1_5b": [171, 185], "qwen2_7b": 172, "qwen": [174, 175, 176], "merges_fil": 177, "qwen2token": 177, "gate_proj": 178, "down_proj": 178, "up_proj": 178, "silu": 178, "feed": [178, 186, 188], "network": [178, 200, 270, 273], "fed": [178, 265], "multipli": [178, 271], "in_dim": [178, 198, 199, 270, 271, 273], "out_dim": [178, 187, 198, 199, 270, 271, 273], "layernorm": 179, "past": 180, "expand": 180, "dpython": [180, 181, 186, 187, 188, 190, 193, 195, 246, 250], "reset": [180, 181, 186, 187, 188, 193, 195, 231], "k_val": 180, "v_val": 180, "fill": 180, "th": 180, "bfloat16": [180, 190, 246, 267, 268, 269, 270, 271, 272], "greater": [180, 189, 254], "pos_embed": [181, 186, 270, 272], "q_norm": 181, "k_norm": 181, "kv_cach": 181, "is_caus": 181, "13245v1": 181, "multihead": 181, "extrem": 181, "credit": 181, "litgpt": 181, "v": [181, 187, 193, 270], "n_kv_head": 181, "rotarypositionalembed": [181, 270, 272], "rmsnorm": 181, "vice": [181, 264], "versa": [181, 264], "y": 181, "s_x": 181, "s_y": 181, "_masktyp": [181, 187, 188], "score": [181, 187, 188, 214], "encoder_max_cache_seq_len": [181, 187, 188], "j": [181, 186, 187, 188, 193], "blockmask": [181, 187, 188], "create_block_mask": [181, 187, 188], "flex_attent": [181, 187, 188], "n_h": [181, 183], "num": [181, 183], "n_kv": 181, "h_d": [181, 183], "reset_cach": [181, 186, 187, 188, 193, 195], "setup_cach": [181, 186, 187, 188, 193, 195], "ep": 182, "root": [182, 242, 243], "squar": 182, "1910": 182, "07467": 182, "divis": 182, "propos": [183, 271], "2104": 183, "09864": 183, "verfic": 183, "l80": 183, "upto": 183, "init": [183, 231, 243, 273], "exceed": 183, "freq": 183, "recomput": [183, 271], "geometr": 183, "progress": [183, 262, 268, 271], "rotat": 183, "angl": 183, "basic": [184, 269], "gate": [184, 225, 260, 261, 264, 268], "tied_modul": 185, "bia": [185, 198, 199, 249, 270, 272, 273], "lost": 185, "whose": [185, 200, 239, 244], "attributeerror": [185, 251], "attribut": [185, 200, 210, 218, 228], "attn": [186, 188, 270, 272, 273], "multiheadattent": [186, 188, 270, 272], "ca_norm": 186, "mlp_norm": [186, 188], "ca_scal": 186, "mlp_scale": [186, 188], "convent": 186, "ff": [186, 188], "cache_en": [186, 188, 195], "check": [186, 187, 188, 189, 193, 195, 203, 229, 236, 254, 257, 259, 260, 261, 262, 265, 267, 268, 270, 271], "token_sequ": 186, "embed_sequ": 186, "decoder_max_seq_len": [186, 187, 188, 193, 195], "modulelist": 187, "output_hidden_st": [187, 193], "belong": [187, 228], "reduc": [187, 215, 259, 260, 261, 266, 270, 271, 272, 273], "statement": 187, "improv": [187, 208, 217, 230, 261, 269, 270, 271], "readabl": [187, 267], "caches_are_en": [187, 193], "effici": [187, 203, 230, 257, 259, 260, 267, 268, 270, 272], "chunked_output": 187, "last_hidden_st": 187, "chunk": [187, 192, 208], "cewithchunkedoutputloss": [187, 193], "upcast": [187, 192], "set_num_output_chunk": [187, 193], "num_chunk": [187, 192], "s_e": [187, 193], "d_e": [187, 193], "arang": [187, 193], "prompt_length": [187, 193], "correspondingli": 187, "padded_prompt_length": 187, "m_": [187, 193], "num_output_chunk": [187, 192, 193], "transformercrossattentionlay": [187, 193, 195], "fusionlay": [187, 193], "sa_norm": 188, "sa_scal": 188, "token_pos_embed": 189, "pre_tile_pos_emb": 189, "post_tile_pos_emb": 189, "cls_project": 189, "out_indic": 189, "vit": 189, "11929": 189, "convolut": 189, "flatten": 189, "downscal": 189, "800x400": 189, "400x400": 189, "_transform": 189, "down": [189, 224, 266, 270, 271, 273], "whole": 189, "n_token": 189, "101": 189, "pool": 189, "_position_embed": 189, "tiledtokenpositionalembed": 189, "tilepositionalembed": 189, "tile_pos_emb": 189, "tokenpositionalembed": 189, "even": [189, 249, 258, 264, 265, 266, 269, 270, 271, 273], "8x8": 189, "21": 189, "22": 189, "23": [189, 191], "24": [189, 268, 269], "25": [189, 267], "26": 189, "27": [189, 267], "28": [189, 267], "29": [189, 273], "30": [189, 219, 272], "31": [189, 269], "33": 189, "34": 189, "35": [189, 273], "36": 189, "37": 189, "38": [189, 267], "39": 189, "41": 189, "43": 189, "44": 189, "45": 189, "46": 189, "47": 189, "48": [189, 267, 273], "49": 189, "50": [189, 219, 239, 267], "51": 189, "52": [189, 268], "53": 189, "54": 189, "55": [189, 268], "56": 189, "57": [189, 270, 273], "58": 189, "59": [189, 273], "60": 189, "61": [189, 267], "62": 189, "63": 189, "64": [189, 260, 270], "num_patches_per_til": 189, "emb_dim": 189, "cls_output_dim": 189, "n_img": 189, "constain": 189, "anim": [189, 266], "max_n_img": 189, "n_channel": 189, "vision_util": 189, "tile_crop": 189, "800": 189, "patch_grid_s": 189, "rand": 189, "nch": 189, "tile_cropped_imag": 189, "batch_imag": 189, "unsqueez": 189, "batch_aspect_ratio": 189, "clip_vision_encod": 189, "common_util": 190, "offload_to_cpu": 190, "hook": [190, 244, 271, 273], "nf4": [190, 271, 273], "restor": 190, "higher": [190, 269, 271, 272, 273], "offload": [190, 273], "increas": [190, 191, 215, 269, 270, 271, 272], "peak": [190, 231, 237, 267, 269, 270, 273], "gpu": [190, 261, 264, 267, 268, 269, 270, 271, 272, 273], "_register_state_dict_hook": 190, "mymodul": 190, "_after_": 190, "nf4tensor": [190, 273], "unquant": [190, 272, 273], "unus": 190, "num_warmup_step": 191, "num_training_step": 191, "num_cycl": [191, 248], "last_epoch": 191, "lambdalr": 191, "rate": [191, 259, 268, 271], "schedul": [191, 248, 268, 271], "linearli": 191, "decreas": [191, 266, 270, 271, 272, 273], "cosin": 191, "v4": 191, "src": 191, "l104": 191, "warmup": [191, 248], "phase": 191, "wave": 191, "half": [191, 271], "lr_schedul": 191, "ignore_index": 192, "entropi": 192, "bf16": [192, 229, 271, 273], "ce": 192, "better": [192, 218, 259, 265, 266, 267, 271, 272], "accuraci": [192, 261, 267, 269, 270, 271, 272, 273], "doubl": [192, 273], "therefor": [192, 273], "num_token": 192, "consider": 192, "compute_cross_entropi": 192, "gain": [192, 261, 269], "won": [192, 265], "realiz": 192, "pull": [192, 260, 261, 264], "1390": 192, "loss_fn": 192, "chunkedcrossentropyloss": 192, "output_chunk": 192, "model_fus": [193, 194, 195, 196, 197], "deepfus": 193, "evolut": 193, "signatur": 193, "interchang": 193, "fusion_param": [193, 194, 195, 196, 197], "fusionembed": 193, "fusion_lay": [193, 195], "clip_vit_224": [193, 197], "feedforward": [193, 197], "register_fusion_modul": 193, "flamingo": [193, 195, 212], "Or": [193, 258], "strict": [193, 194, 195, 203, 270], "freez": [193, 267, 270], "fusion_vocab_s": 194, "necessit": 194, "rout": 194, "128": [194, 260, 269, 270, 271], "fusion_first": 195, "shot": [195, 267, 269, 272], "infus": 195, "interpret": [195, 266], "enocd": 195, "isn": [195, 229, 264], "fused_lay": 195, "mark": [197, 265], "earli": 197, "peft": [198, 199, 200, 201, 202, 203, 204, 222, 260, 270, 273], "adapter_param": [198, 199, 200, 201, 202], "proj": 198, "loralinear": [198, 270, 273], "alpha": [199, 270, 271, 273], "use_bia": 199, "perturb": 199, "decomposit": [199, 270, 271], "matric": [199, 270, 273], "mapsto": 199, "w_0x": 199, "r": [199, 270], "bax": 199, "lora_a": [199, 270, 273], "lora_b": [199, 270, 273], "temporarili": [200, 271], "polici": [200, 214, 215, 216, 217, 218, 220, 230, 238, 245, 262], "neural": [200, 270, 273], "caller": 200, "yield": 200, "get_adapter_param": [202, 270], "base_miss": 203, "base_unexpect": 203, "lora_miss": 203, "lora_unexpect": 203, "validate_state_dict_for_lora": [203, 270], "unlik": 203, "reli": [203, 210, 267, 269], "unexpect": 203, "nonempti": 203, "full_model_state_dict_kei": 204, "lora_state_dict_kei": 204, "base_model_state_dict_kei": 204, "confirm": [204, 258], "lora_modul": 204, "complement": 204, "disjoint": 204, "non": [204, 214], "overlap": [204, 271], "tiktokenbasetoken": 205, "light": 207, "sentencepieceprocessor": 207, "trim": 207, "whitespac": 207, "spm_model": [207, 265], "tokenized_text": [207, 208], "add_bo": [207, 208, 265], "trim_leading_whitespac": 207, "prefix": [207, 271], "unbatch": 207, "due": [207, 270, 271, 273], "bos_id": [208, 210], "lightweight": [208, 265], "substr": 208, "repetit": 208, "speed": [208, 248, 269, 271, 272, 273], "identif": 208, "regex": 208, "absent": 208, "tt_model": 208, "tokenizer_json_path": 209, "heavili": 210, "concat": 210, "1788": 210, "2643": 210, "465": 210, "22137": 210, "join": 210, "runtimeerror": [210, 226, 229, 235], "satisfi": [210, 267], "loos": 211, "image_token_id": 212, "particip": [212, 213], "laid": 212, "fig": 212, "2204": 212, "14198": 212, "immedi": [212, 271], "until": [212, 271], "img3": 212, "equal": [212, 254], "gamma": [213, 217, 218], "lmbda": 213, "estim": [213, 214], "1506": 213, "02438": 213, "response_len": [213, 214], "receiv": 213, "discount": 213, "gae": 213, "lambda": 213, "logprob": [214, 218], "ref_logprob": 214, "kl_coeff": 214, "valid_score_idx": 214, "kl": 214, "coeffici": [214, 216], "total_reward": 214, "diverg": 214, "kl_reward": 214, "beta": [215, 218], "label_smooth": [215, 218], "18290": 215, "intuit": [215, 217, 218], "dispref": 215, "dynam": [215, 272], "degener": 215, "occur": [215, 261], "naiv": 215, "trl": [215, 217, 218], "librari": [215, 217, 229, 247, 253, 257, 258, 259, 264, 266, 271, 273], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 215, "dpo_train": [215, 217], "l844": 215, "retain": [215, 271, 273], "2009": 215, "01325": 215, "regular": [215, 218, 271, 272, 273], "baselin": [215, 216, 267, 270], "rather": [215, 271], "overhead": [215, 261, 271, 272], "uncertainti": [215, 218], "policy_chosen_logp": [215, 217, 218], "policy_rejected_logp": [215, 217, 218], "reference_chosen_logp": [215, 217], "reference_rejected_logp": [215, 217], "chosen_reward": [215, 217, 218], "rejected_reward": [215, 217, 218], "value_clip_rang": 216, "value_coeff": 216, "proxim": [216, 262], "1707": 216, "06347": 216, "eqn": 216, "vwxyzjn": 216, "ccc19538e817e98a60d3253242ac15e2a562cb49": 216, "lm_human_preference_detail": 216, "train_policy_acceler": 216, "l719": 216, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 216, "ppo2": 216, "l68": 216, "l75": 216, "pi_old_logprob": 216, "pi_logprob": 216, "phi_old_valu": 216, "phi_valu": 216, "value_padding_mask": 216, "old": 216, "participag": 216, "five": 216, "policy_loss": 216, "value_loss": 216, "clipfrac": 216, "fraction": 216, "statist": [217, 271], "rso": 217, "hing": 217, "2309": 217, "06657": 217, "logist": 217, "regress": 217, "slic": 217, "10425": 217, "almost": [217, 270], "svm": 217, "counter": 217, "4dce042a3863db1d375358e8c8092b874b02934b": 217, "l1141": 217, "simpo": 218, "2405": 218, "14734": 218, "averag": 218, "implicit": 218, "margin": 218, "bradlei": 218, "terri": 218, "larger": [218, 224, 267, 269, 271], "win": 218, "lose": 218, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 218, "cpo_train": 218, "l603": 218, "pretti": [218, 267], "identitc": 218, "elimin": 218, "kind": 218, "ipoloss": 218, "fill_valu": 219, "sequence_length": 219, "stop_token_id": 219, "869": 219, "eos_mask": 219, "truncated_sequ": 219, "datatyp": [220, 271, 273], "denot": 220, "auto_wrap_polici": [220, 230, 245], "submodul": [220, 238], "obei": 220, "contract": 220, "get_fsdp_polici": 220, "modules_to_wrap": [220, 230, 238], "min_num_param": 220, "my_fsdp_polici": 220, "recurs": [220, 238, 242], "isinst": [220, 266], "sum": [220, 270], "p": [220, 226, 270, 272, 273], "numel": [220, 270], "1000": [220, 272], "stabl": [220, 236, 242, 247, 258, 271], "html": [220, 236, 242, 245, 247, 253, 257], "filename_format": 221, "max_filenam": 221, "concis": 221, "filenam": [221, 240], "file_": 221, "_of_": 221, "n_file": 221, "build_checkpoint_filenam": 221, "file_00001_of_00003": 221, "file_00002_of_00003": 221, "file_00003_of_00003": 221, "safe_seri": 222, "from_pretrain": 222, "0001_of_0003": 222, "0002_of_0003": 222, "todo": 222, "preserv": [222, 273], "weight_map": [222, 267], "convert_weight": 222, "_model_typ": [222, 225], "intermediate_checkpoint": [222, 223, 224], "adapter_onli": [222, 223, 224], "_weight_map": 222, "shard": [223, 269], "wip": 223, "qualnam": 225, "boundari": 225, "distinguish": 225, "llama3_vis": 225, "llama3_2_vision_decod": 225, "mistral_reward_7b": 225, "my_new_model": 225, "my_custom_state_dict_map": 225, "optim_map": 226, "bare": 226, "bone": 226, "optim_dict": [226, 228, 244], "cfg_optim": 226, "ckpt": 226, "optim_ckpt": 226, "placeholder_optim_dict": 226, "optiminbackwardwrapp": 226, "get_optim_kei": 226, "arbitrari": [226, 270, 271], "optim_ckpt_map": 226, "loadabl": 226, "ac_mod": 227, "ac_opt": 227, "op": [227, 272], "ac": [227, 230], "optimizerinbackwardwrapp": 228, "named_paramet": [228, 249], "float32": 229, "kernel": 229, "hardwar": [229, 259, 266, 267, 270, 271], "memory_efficient_fsdp_wrap": [230, 272], "maxim": [230, 238, 257, 259], "workload": [230, 261, 271, 272], "fullyshardeddataparallel": [230, 238], "fsdppolicytyp": [230, 238], "reset_stat": 231, "track": [231, 239], "alloc": [231, 237, 238, 269, 273], "reserv": [231, 237, 265, 273], "stat": [231, 237, 273], "int4": [232, 272], "4w": 232, "recogn": 232, "int8dynactint4weightquant": [232, 261, 272], "8da4w": [232, 272], "int8dynactint4weightqatquant": [232, 261, 272], "qat": [232, 257, 262], "exclud": 233, "aka": 234, "master": 236, "port": [236, 264], "address": [236, 271], "hold": [236, 268], "peak_memory_act": 237, "peak_memory_alloc": 237, "peak_memory_reserv": 237, "get_memory_stat": 237, "hierarch": 238, "api_kei": 239, "experiment_kei": 239, "onlin": 239, "log_cod": 239, "comet": 239, "site": [239, 266, 267], "ml": 239, "team": 239, "compar": [239, 242, 254, 267, 269, 270, 272, 273], "sdk": 239, "uncategor": 239, "alphanumer": 239, "charact": 239, "get_or_cr": 239, "fresh": 239, "persist": 239, "hpo": 239, "sweep": 239, "server": 239, "offlin": 239, "auto": [239, 264], "creation": 239, "experimentconfig": 239, "project_nam": 239, "my_project": [239, 243], "my_workspac": 239, "my_metr": [239, 242, 243], "importerror": [239, 243], "termin": [239, 242, 243], "comet_api_kei": 239, "flush": [239, 240, 241, 242, 243], "ndarrai": [239, 240, 241, 242, 243], "scalar": [239, 240, 241, 242, 243], "record": [239, 240, 241, 242, 243, 248], "log_config": [239, 243], "payload": [239, 240, 241, 242, 243], "log_": 240, "unixtimestamp": 240, "thread": 240, "safe": 240, "organize_log": 242, "tensorboard": 242, "subdirectori": 242, "logdir": 242, "startup": 242, "tree": [242, 266, 267, 269], "tfevent": 242, "encount": 242, "frontend": 242, "organ": [242, 264], "accordingli": [242, 272], "my_log_dir": 242, "view": 242, "entiti": 243, "bias": [243, 270, 273], "sent": 243, "usernam": 243, "my_ent": 243, "my_group": 243, "account": [243, 270, 273], "link": [243, 267, 269], "capecap": 243, "6053ofw0": 243, "torchtune_config_j67sb73v": 243, "soon": [244, 271], "readi": [244, 257, 265, 272], "grad": 244, "acwrappolicytyp": 245, "author": [245, 259, 268, 271, 273], "fsdp_adavnced_tutori": 245, "insid": 246, "contextmanag": 246, "debug_mod": 247, "pseudo": 247, "commonli": [247, 270, 271, 273], "numpi": 247, "determinist": 247, "global": [247, 266, 271], "warn": 247, "nondeterminist": 247, "cudnn": 247, "set_deterministic_debug_mod": 247, "profile_memori": 248, "with_stack": 248, "record_shap": 248, "with_flop": 248, "wait_step": 248, "warmup_step": 248, "active_step": 248, "profil": 248, "layout": 248, "trace": 248, "profileract": 248, "gradient_accumul": 248, "sensibl": 248, "default_schedul": 248, "reduct": [248, 261, 270], "iter": [248, 249, 250, 273], "scope": 248, "flop": 248, "wait": 248, "cycl": 248, "repeat": [248, 271], "model_named_paramet": 249, "force_overrid": 249, "behaviour": 249, "concret": [249, 271], "vocab_dim": 249, "named_param": 250, "inplac": [251, 270], "too": [251, 261, 269], "handler": 253, "_log": 253, "__version__": 254, "generated_examples_python": 255, "galleri": [255, 263], "sphinx": 255, "000": [256, 263, 269], "execut": [256, 263], "generated_exampl": 256, "mem": [256, 263], "mb": [256, 263], "gentl": 257, "introduct": 257, "first_finetune_tutori": 257, "torchvis": 258, "torchao": [258, 261, 267, 269, 272, 273], "latest": [258, 261, 268, 271, 273], "whl": 258, "cu121": 258, "cu118": 258, "cu124": 258, "And": [258, 267], "welcom": [258, 264], "greatest": [258, 268], "contributor": 258, "dev": 258, "commit": 258, "branch": 258, "therebi": [258, 271, 272, 273], "forc": 258, "reinstal": 258, "opt": [258, 268], "suffix": 258, "On": [259, 270], "pointer": 259, "emphas": 259, "simplic": 259, "component": 259, "prove": 259, "democrat": 259, "zoo": 259, "varieti": [259, 270], "integr": [259, 267, 268, 269, 270, 272, 273], "fsdp2": 259, "excit": 259, "checkout": 259, "quickstart": 259, "attain": 259, "embodi": 259, "philosophi": 259, "usabl": 259, "composit": 259, "hard": [259, 266], "outlin": 259, "unecessari": 259, "never": 259, "thoroughli": 259, "competit": 260, "grant": [260, 261, 268], "interest": [260, 261, 267], "8b_lora_single_devic": [260, 264, 265, 269, 271], "adjust": [260, 261, 266, 271, 272], "lever": [260, 261], "action": [260, 261], "degrad": [261, 271, 272, 273], "simul": [261, 271, 272], "compromis": 261, "blogpost": [261, 271], "qat_distribut": [261, 272], "8b_qat_ful": [261, 272], "least": [261, 269, 270, 272], "vram": [261, 269, 270, 271, 272], "80gb": [261, 272], "a100": 261, "h100": 261, "delai": 261, "fake": [261, 272], "empir": [261, 272], "potenti": [261, 270, 271], "fake_quant_after_n_step": [261, 272], "idea": [261, 273], "roughli": 261, "total_step": 261, "plan": [261, 267], "un": 261, "groupsiz": [261, 272], "256": [261, 269, 272], "hackabl": [262, 268], "singularli": [262, 268], "technic": [262, 268], "awar": [262, 271, 272], "tracker": 262, "short": 264, "subcommand": 264, "anytim": 264, "symlink": 264, "wrote": 264, "readm": [264, 267, 269], "md": 264, "lot": [264, 267, 271], "recent": 264, "agre": 264, "term": [264, 271], "perman": 264, "eat": 264, "bandwith": 264, "storag": [264, 273], "00030": 264, "ootb": 264, "full_finetune_single_devic": [264, 266, 267, 268], "7b_full_low_memori": [264, 267, 268], "8b_full_single_devic": [264, 266], "mini_full_low_memori": 264, "7b_full": [264, 267, 268], "13b_full": [264, 267, 268], "70b_full": 264, "edit": 264, "clobber": 264, "destin": 264, "lora_finetune_distribut": [264, 269, 270], "torchrun": 264, "launch": [264, 265, 268], "nproc": 264, "node": 264, "worker": 264, "nnode": [264, 270, 272], "minimum_nod": 264, "maximum_nod": 264, "fail": 264, "rdzv": 264, "rendezv": 264, "endpoint": 264, "8b_lora": [264, 269], "bypass": 264, "fancy_lora": 264, "8b_fancy_lora": 264, "nice": 265, "meet": 265, "overhaul": 265, "untrain": 265, "accompani": 265, "who": 265, "influenti": 265, "hip": 265, "hop": 265, "artist": 265, "2pac": 265, "rakim": 265, "flavor": [265, 266], "formatted_messag": [265, 266], "nyou": [265, 266], "nwho": 265, "why": [265, 268, 270], "518": 265, "25580": 265, "29962": 265, "3532": 265, "14816": 265, "29903": 265, "6778": 265, "_spm_model": 265, "piece_to_id": 265, "manual": [265, 273], "529": 265, "29879": 265, "29958": 265, "nhere": 265, "128000": [265, 272], "pure": 265, "mess": 265, "prime": 265, "strictli": 265, "ask": [265, 271], "untouch": 265, "though": 265, "robust": 265, "pretend": 265, "zuckerberg": 265, "seem": [265, 267], "good": [265, 270, 271], "altogeth": 265, "honor": 265, "copi": [265, 267, 268, 269, 272, 273], "custom_8b_lora_single_devic": 265, "steer": 266, "wheel": 266, "publicli": 266, "great": [266, 267, 271], "hood": [266, 273], "text_completion_dataset": [266, 272], "upper": 266, "constraint": [266, 270], "slow": [266, 271, 273], "signific": [266, 271, 272], "speedup": [266, 267, 269], "goal": [266, 272], "plant": 266, "miner": 266, "oak": 266, "copper": 266, "ore": 266, "eleph": 266, "customtempl": 266, "importlib": 266, "import_modul": 266, "mechan": 266, "search": 266, "often": [266, 270, 271], "runtim": [266, 271], "pythonpath": 266, "llama2chatformat": 266, "customchatformat": 266, "concatdataset": 266, "drive": 266, "rajpurkar": 266, "io": 266, "squad": 266, "explor": 266, "chatdataset": 266, "key_chosen": 266, "key_reject": 266, "c_mask": 266, "np": 266, "cross_entropy_ignore_idx": 266, "r_mask": 266, "stack_exchanged_paired_dataset": 266, "had": 266, "1024": [266, 272], "stackexchangedpairedtempl": 266, "response_j": 266, "response_k": 266, "rl": 266, "favorit": [267, 270], "seemlessli": 267, "connect": [267, 272], "amount": 267, "natur": 267, "export": 267, "leverag": [267, 269, 273], "percentag": 267, "16gb": [267, 270], "rtx": 267, "3090": 267, "4090": 267, "hour": 267, "7b_qlora_single_devic": [267, 268, 273], "473": 267, "98": [267, 273], "gb": [267, 269, 270, 272, 273], "484": 267, "01": [267, 268], "fact": [267, 269, 270, 271], "third": 267, "But": [267, 270], "realli": 267, "eleuther_ev": [267, 269, 272], "eleuther_evalu": [267, 269, 272], "lm_eval": [267, 269], "custom_eval_config": [267, 269], "truthfulqa_mc2": [267, 269, 270], "measur": [267, 269], "propens": [267, 269], "324": 267, "loglikelihood": 267, "195": 267, "121": 267, "197": 267, "acc": [267, 272], "388": 267, "489": 267, "custom_generation_config": [267, 269], "kick": 267, "300": 267, "bai": 267, "area": 267, "92": 267, "exploratorium": 267, "san": 267, "francisco": 267, "magazin": 267, "awesom": 267, "bridg": 267, "cool": 267, "96": [267, 273], "sec": [267, 269], "83": 267, "99": [267, 270], "72": 267, "littl": 267, "int8_weight_onli": [267, 269], "int8_dynamic_activation_int8_weight": [267, 269], "ao": [267, 269], "quant_api": [267, 269], "quantize_": [267, 269], "int4_weight_onli": [267, 269], "previous": [267, 269, 270], "benefit": 267, "clone": [267, 270, 272, 273], "assumpt": 267, "new_dir": 267, "output_dict": 267, "sd_1": 267, "sd_2": 267, "dump": 267, "convert_hf_checkpoint": 267, "checkpoint_path": 267, "justin": 267, "school": 267, "math": 267, "teacher": 267, "ws": 267, "94": [267, 269], "bandwidth": [267, 269], "1391": 267, "84": 267, "thats": 267, "seamlessli": 267, "authent": [267, 268], "hopefulli": 267, "gave": 267, "minut": 268, "agreement": 268, "depth": 268, "principl": 268, "boilerpl": 268, "substanti": [268, 270], "custom_config": 268, "replic": 268, "lorafinetunerecipesingledevic": 268, "lora_finetune_output": 268, "log_1713194212": 268, "3697006702423096": 268, "25880": [268, 273], "83it": 268, "monitor": 268, "tqdm": 268, "e2": 268, "focu": 269, "theta": 269, "observ": [269, 272], "consum": [269, 273], "overal": 269, "8b_qlora_single_devic": [269, 271], "coupl": [269, 270, 273], "meta_model_0": [269, 272], "122": 269, "sarah": 269, "busi": 269, "mum": 269, "young": 269, "children": 269, "live": 269, "north": 269, "east": 269, "england": 269, "135": 269, "88": 269, "138": 269, "346": 269, "09": 269, "139": 269, "broader": 269, "teach": 270, "straight": 270, "jump": 270, "unfamiliar": 270, "oppos": [270, 273], "momentum": [270, 271], "aghajanyan": 270, "et": 270, "al": 270, "hypothes": 270, "intrins": 270, "eight": 270, "practic": 270, "blue": 270, "although": [270, 272], "rememb": 270, "approx": 270, "15m": 270, "65k": 270, "requires_grad": [270, 273], "frozen_out": [270, 273], "lora_out": [270, 273], "omit": [270, 271], "base_model": 270, "lora_model": 270, "lora_llama_2_7b": [270, 273], "alon": 270, "bit": [270, 271, 272, 273], "in_featur": [270, 272], "out_featur": [270, 272], "validate_missing_and_unexpected_for_lora": 270, "peft_util": 270, "set_trainable_param": 270, "fetch": 270, "lora_param": 270, "total_param": 270, "trainable_param": 270, "2f": 270, "6742609920": 270, "4194304": 270, "7b_lora": 270, "my_model_checkpoint_path": [270, 272, 273], "tokenizer_checkpoint": [270, 272, 273], "my_tokenizer_checkpoint_path": [270, 272, 273], "factori": 270, "benefici": 270, "impact": [270, 271], "minor": 270, "lora_experiment_1": 270, "smooth": [270, 273], "curv": [270, 273], "500": 270, "ran": 270, "footprint": [270, 272], "commod": 270, "cogniz": 270, "ax": 270, "parallel": 270, "truthfulqa": 270, "475": 270, "87": 270, "508": 270, "86": 270, "504": 270, "04": 270, "514": 270, "lowest": 270, "absolut": 270, "4gb": 270, "tradeoff": 270, "salman": 271, "mohammadi": 271, "brief": 271, "glossari": 271, "struggl": 271, "constrain": [271, 272], "particularli": 271, "gradient_accumulation_step": 271, "throughput": 271, "cost": 271, "sebastian": 271, "raschka": 271, "fp16": 271, "sound": 271, "quot": 271, "aliv": 271, "region": 271, "enable_activation_checkpoint": 271, "bring": 271, "autograd": [271, 273], "saved_tensors_hook": 271, "offload_with_stream": 271, "hide": 271, "later": 271, "brought": 271, "enable_activation_offload": 271, "dev20240907": 271, "total_batch_s": 271, "count": 271, "suppos": 271, "log_every_n_step": 271, "translat": 271, "appear": 271, "frequent": 271, "slowli": 271, "num_devic": 271, "adamw8bit": 271, "pagedadamw": 271, "modern": 271, "converg": 271, "stateless": 271, "stochast": 271, "descent": 271, "sacrif": 271, "optimizer_in_bwd": 271, "greatli": 271, "lora_": 271, "lora_llama3": 271, "aim": 271, "_lora": 271, "firstli": 271, "secondli": 271, "affect": 271, "fashion": 271, "slower": [271, 273], "jointli": 271, "sens": 271, "novel": 271, "normalfloat": [271, 273], "8x": [271, 273], "worth": 271, "cast": [271, 272], "incur": [271, 272, 273], "penalti": 271, "qlora_": 271, "qlora_llama3_8b": 271, "_qlora": 271, "perplex": 272, "ptq": 272, "kept": 272, "nois": 272, "henc": 272, "x_q": 272, "int8": 272, "zp": 272, "x_float": 272, "qmin": 272, "qmax": 272, "clamp": 272, "x_fq": 272, "dequant": 272, "proce": 272, "prepared_model": 272, "swap": 272, "int8dynactint4weightqatlinear": 272, "int8dynactint4weightlinear": 272, "train_loop": 272, "converted_model": 272, "qat_distributed_recipe_label": 272, "recov": 272, "modif": 272, "custom_8b_qat_ful": 272, "2000": 272, "led": 272, "presum": 272, "mutat": 272, "5gb": 272, "custom_quant": 272, "poorli": 272, "custom_eleuther_evalu": 272, "fullmodeltorchtunecheckpoint": 272, "hellaswag": 272, "max_seq_length": 272, "my_eleuther_evalu": 272, "stderr": 272, "word_perplex": 272, "9148": 272, "byte_perplex": 272, "5357": 272, "bits_per_byt": 272, "6189": 272, "5687": 272, "0049": 272, "acc_norm": 272, "7536": 272, "0043": 272, "portion": [272, 273], "74": 272, "048": 272, "190": 272, "7735": 272, "5598": 272, "6413": 272, "5481": 272, "0050": 272, "7390": 272, "0044": 272, "7251": 272, "4994": 272, "5844": 272, "5740": 272, "7610": 272, "outperform": 272, "importantli": 272, "characterist": 272, "187": 272, "958": 272, "halv": 272, "int4weightonlyquant": 272, "motiv": 272, "edg": 272, "smartphon": 272, "executorch": 272, "xnnpack": 272, "export_llama": 272, "use_sdpa_with_kv_cach": 272, "qmode": 272, "group_siz": 272, "get_bos_id": 272, "get_eos_id": 272, "128001": 272, "output_nam": 272, "llama3_8da4w": 272, "pte": 272, "881": 272, "oneplu": 272, "709": 272, "tok": 272, "815": 272, "316": 272, "364": 272, "highli": 273, "vanilla": 273, "held": 273, "bespok": 273, "vast": 273, "major": 273, "normatfloat": 273, "deepdiv": 273, "distinct": 273, "de": 273, "counterpart": 273, "set_default_devic": 273, "qlora_linear": 273, "memory_alloc": 273, "177": 273, "152": 273, "del": 273, "empty_cach": 273, "lora_linear": 273, "081": 273, "344": 273, "qlora_llama2_7b": 273, "qlora_model": 273, "essenti": 273, "reparametrize_as_dtype_state_dict_post_hook": 273, "149": 273, "9157477021217346": 273, "02": 273, "08": 273, "15it": 273, "nightli": 273, "200": 273, "hundr": 273, "228": 273, "8158286809921265": 273, "95it": 273, "exercis": 273, "linear_nf4": 273, "to_nf4": 273, "linear_weight": 273, "incom": 273}, "objects": {"torchtune.config": [[24, 0, 1, "", "instantiate"], [25, 0, 1, "", "log_config"], [26, 0, 1, "", "parse"], [27, 0, 1, "", "validate"]], "torchtune.data": [[28, 1, 1, "", "ChatFormat"], [29, 1, 1, "", "ChatMLTemplate"], [30, 1, 1, "", "ChosenRejectedToMessages"], [31, 3, 1, "", "GrammarErrorCorrectionTemplate"], [32, 1, 1, "", "InputOutputToMessages"], [33, 1, 1, "", "InstructTemplate"], [34, 1, 1, "", "Message"], [35, 1, 1, "", "OpenAIToMessages"], [36, 1, 1, "", "PromptTemplate"], [37, 1, 1, "", "PromptTemplateInterface"], [38, 3, 1, "", "QuestionAnswerTemplate"], [39, 3, 1, "", "Role"], [40, 1, 1, "", "ShareGPTToMessages"], [41, 3, 1, "", "SummarizeTemplate"], [42, 0, 1, "", "format_content_with_images"], [43, 0, 1, "", "get_openai_messages"], [44, 0, 1, "", "get_sharegpt_messages"], [45, 0, 1, "", "left_pad_sequence"], [46, 0, 1, "", "load_image"], [47, 0, 1, "", "padded_collate"], [48, 0, 1, "", "padded_collate_dpo"], [49, 0, 1, "", "padded_collate_sft"], [50, 0, 1, "", "padded_collate_tiled_images_and_mask"], [51, 0, 1, "", "truncate"], [52, 0, 1, "", "validate_messages"]], "torchtune.data.ChatFormat": [[28, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[33, 2, 1, "", "format"]], "torchtune.data.Message": [[34, 4, 1, "", "contains_media"], [34, 2, 1, "", "from_dict"], [34, 2, 1, "", "get_media"], [34, 4, 1, "", "text_content"]], "torchtune.datasets": [[53, 3, 1, "", "ChatDataset"], [54, 1, 1, "", "ConcatDataset"], [55, 3, 1, "", "InstructDataset"], [56, 1, 1, "", "PackedDataset"], [57, 1, 1, "", "PreferenceDataset"], [58, 1, 1, "", "SFTDataset"], [59, 1, 1, "", "TextCompletionDataset"], [60, 0, 1, "", "alpaca_cleaned_dataset"], [61, 0, 1, "", "alpaca_dataset"], [62, 0, 1, "", "chat_dataset"], [63, 0, 1, "", "cnn_dailymail_articles_dataset"], [64, 0, 1, "", "grammar_dataset"], [65, 0, 1, "", "hh_rlhf_helpful_dataset"], [66, 0, 1, "", "instruct_dataset"], [69, 0, 1, "", "preference_dataset"], [70, 0, 1, "", "samsum_dataset"], [71, 0, 1, "", "slimorca_dataset"], [72, 0, 1, "", "stack_exchange_paired_dataset"], [73, 0, 1, "", "text_completion_dataset"], [74, 0, 1, "", "wikitext_dataset"]], "torchtune.datasets.multimodal": [[67, 0, 1, "", "llava_instruct_dataset"], [68, 0, 1, "", "the_cauldron_dataset"]], "torchtune.generation": [[75, 0, 1, "", "generate"], [76, 0, 1, "", "generate_next_token"], [77, 0, 1, "", "get_causal_mask_from_padding_mask"], [78, 0, 1, "", "get_position_ids_from_padding_mask"], [79, 0, 1, "", "sample"]], "torchtune.models.code_llama2": [[80, 0, 1, "", "code_llama2_13b"], [81, 0, 1, "", "code_llama2_70b"], [82, 0, 1, "", "code_llama2_7b"], [83, 0, 1, "", "lora_code_llama2_13b"], [84, 0, 1, "", "lora_code_llama2_70b"], [85, 0, 1, "", "lora_code_llama2_7b"], [86, 0, 1, "", "qlora_code_llama2_13b"], [87, 0, 1, "", "qlora_code_llama2_70b"], [88, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[89, 0, 1, "", "gemma"], [90, 0, 1, "", "gemma_2b"], [91, 0, 1, "", "gemma_7b"], [92, 0, 1, "", "gemma_tokenizer"], [93, 0, 1, "", "lora_gemma"], [94, 0, 1, "", "lora_gemma_2b"], [95, 0, 1, "", "lora_gemma_7b"], [96, 0, 1, "", "qlora_gemma_2b"], [97, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[98, 1, 1, "", "Llama2ChatTemplate"], [99, 0, 1, "", "llama2"], [100, 0, 1, "", "llama2_13b"], [101, 0, 1, "", "llama2_70b"], [102, 0, 1, "", "llama2_7b"], [103, 0, 1, "", "llama2_reward_7b"], [104, 0, 1, "", "llama2_tokenizer"], [105, 0, 1, "", "lora_llama2"], [106, 0, 1, "", "lora_llama2_13b"], [107, 0, 1, "", "lora_llama2_70b"], [108, 0, 1, "", "lora_llama2_7b"], [109, 0, 1, "", "lora_llama2_reward_7b"], [110, 0, 1, "", "qlora_llama2_13b"], [111, 0, 1, "", "qlora_llama2_70b"], [112, 0, 1, "", "qlora_llama2_7b"], [113, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama3": [[114, 0, 1, "", "llama3"], [115, 0, 1, "", "llama3_70b"], [116, 0, 1, "", "llama3_8b"], [117, 0, 1, "", "llama3_tokenizer"], [118, 0, 1, "", "lora_llama3"], [119, 0, 1, "", "lora_llama3_70b"], [120, 0, 1, "", "lora_llama3_8b"], [121, 0, 1, "", "qlora_llama3_70b"], [122, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3_1": [[123, 0, 1, "", "llama3_1"], [124, 0, 1, "", "llama3_1_405b"], [125, 0, 1, "", "llama3_1_70b"], [126, 0, 1, "", "llama3_1_8b"], [127, 0, 1, "", "lora_llama3_1"], [128, 0, 1, "", "lora_llama3_1_405b"], [129, 0, 1, "", "lora_llama3_1_70b"], [130, 0, 1, "", "lora_llama3_1_8b"], [131, 0, 1, "", "qlora_llama3_1_405b"], [132, 0, 1, "", "qlora_llama3_1_70b"], [133, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.llama3_2": [[134, 0, 1, "", "llama3_2_1b"], [135, 0, 1, "", "llama3_2_3b"], [136, 0, 1, "", "lora_llama3_2_1b"], [137, 0, 1, "", "lora_llama3_2_3b"], [138, 0, 1, "", "qlora_llama3_2_1b"], [139, 0, 1, "", "qlora_llama3_2_3b"]], "torchtune.models.llama3_2_vision": [[140, 1, 1, "", "Llama3VisionEncoder"], [141, 1, 1, "", "Llama3VisionProjectionHead"], [142, 1, 1, "", "Llama3VisionTransform"], [143, 0, 1, "", "llama3_2_vision_11b"], [144, 0, 1, "", "llama3_2_vision_decoder"], [145, 0, 1, "", "llama3_2_vision_encoder"], [146, 0, 1, "", "llama3_2_vision_transform"], [147, 0, 1, "", "lora_llama3_2_vision_11b"], [148, 0, 1, "", "lora_llama3_2_vision_decoder"], [149, 0, 1, "", "lora_llama3_2_vision_encoder"], [150, 0, 1, "", "qlora_llama3_2_vision_11b"]], "torchtune.models.llama3_2_vision.Llama3VisionEncoder": [[140, 2, 1, "", "forward"]], "torchtune.models.llama3_2_vision.Llama3VisionProjectionHead": [[141, 2, 1, "", "forward"]], "torchtune.models.llama3_2_vision.Llama3VisionTransform": [[142, 2, 1, "", "decode"], [142, 2, 1, "", "tokenize_message"], [142, 2, 1, "", "tokenize_messages"]], "torchtune.models.mistral": [[151, 1, 1, "", "MistralChatTemplate"], [152, 0, 1, "", "lora_mistral"], [153, 0, 1, "", "lora_mistral_7b"], [154, 0, 1, "", "lora_mistral_classifier"], [155, 0, 1, "", "lora_mistral_reward_7b"], [156, 0, 1, "", "mistral"], [157, 0, 1, "", "mistral_7b"], [158, 0, 1, "", "mistral_classifier"], [159, 0, 1, "", "mistral_reward_7b"], [160, 0, 1, "", "mistral_tokenizer"], [161, 0, 1, "", "qlora_mistral_7b"], [162, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.phi3": [[163, 0, 1, "", "lora_phi3"], [164, 0, 1, "", "lora_phi3_mini"], [165, 0, 1, "", "phi3"], [166, 0, 1, "", "phi3_mini"], [167, 0, 1, "", "phi3_mini_tokenizer"], [168, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.qwen2": [[169, 0, 1, "", "lora_qwen2"], [170, 0, 1, "", "lora_qwen2_0_5b"], [171, 0, 1, "", "lora_qwen2_1_5b"], [172, 0, 1, "", "lora_qwen2_7b"], [173, 0, 1, "", "qwen2"], [174, 0, 1, "", "qwen2_0_5b"], [175, 0, 1, "", "qwen2_1_5b"], [176, 0, 1, "", "qwen2_7b"], [177, 0, 1, "", "qwen2_tokenizer"]], "torchtune.modules": [[178, 1, 1, "", "FeedForward"], [179, 1, 1, "", "Fp32LayerNorm"], [180, 1, 1, "", "KVCache"], [181, 1, 1, "", "MultiHeadAttention"], [182, 1, 1, "", "RMSNorm"], [183, 1, 1, "", "RotaryPositionalEmbeddings"], [184, 1, 1, "", "TanhGate"], [185, 1, 1, "", "TiedLinear"], [186, 1, 1, "", "TransformerCrossAttentionLayer"], [187, 1, 1, "", "TransformerDecoder"], [188, 1, 1, "", "TransformerSelfAttentionLayer"], [189, 1, 1, "", "VisionTransformer"], [191, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.FeedForward": [[178, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[179, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[180, 2, 1, "", "reset"], [180, 2, 1, "", "update"]], "torchtune.modules.MultiHeadAttention": [[181, 2, 1, "", "forward"], [181, 2, 1, "", "reset_cache"], [181, 2, 1, "", "setup_cache"]], "torchtune.modules.RMSNorm": [[182, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[183, 2, 1, "", "forward"]], "torchtune.modules.TanhGate": [[184, 2, 1, "", "forward"]], "torchtune.modules.TransformerCrossAttentionLayer": [[186, 4, 1, "", "cache_enabled"], [186, 2, 1, "", "forward"], [186, 2, 1, "", "reset_cache"], [186, 2, 1, "", "setup_cache"]], "torchtune.modules.TransformerDecoder": [[187, 2, 1, "", "caches_are_enabled"], [187, 2, 1, "", "chunked_output"], [187, 2, 1, "", "forward"], [187, 2, 1, "", "reset_caches"], [187, 2, 1, "", "set_num_output_chunks"], [187, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerSelfAttentionLayer": [[188, 4, 1, "", "cache_enabled"], [188, 2, 1, "", "forward"], [188, 2, 1, "", "reset_cache"], [188, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[189, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[190, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[192, 1, 1, "", "CEWithChunkedOutputLoss"]], "torchtune.modules.loss.CEWithChunkedOutputLoss": [[192, 2, 1, "", "compute_cross_entropy"], [192, 2, 1, "", "forward"]], "torchtune.modules.model_fusion": [[193, 1, 1, "", "DeepFusionModel"], [194, 1, 1, "", "FusionEmbedding"], [195, 1, 1, "", "FusionLayer"], [196, 0, 1, "", "get_fusion_params"], [197, 0, 1, "", "register_fusion_module"]], "torchtune.modules.model_fusion.DeepFusionModel": [[193, 2, 1, "", "caches_are_enabled"], [193, 2, 1, "", "forward"], [193, 2, 1, "", "reset_caches"], [193, 2, 1, "", "set_num_output_chunks"], [193, 2, 1, "", "setup_caches"]], "torchtune.modules.model_fusion.FusionEmbedding": [[194, 2, 1, "", "forward"], [194, 2, 1, "", "fusion_params"]], "torchtune.modules.model_fusion.FusionLayer": [[195, 4, 1, "", "cache_enabled"], [195, 2, 1, "", "forward"], [195, 2, 1, "", "fusion_params"], [195, 2, 1, "", "reset_cache"], [195, 2, 1, "", "setup_cache"]], "torchtune.modules.peft": [[198, 1, 1, "", "AdapterModule"], [199, 1, 1, "", "LoRALinear"], [200, 0, 1, "", "disable_adapter"], [201, 0, 1, "", "get_adapter_params"], [202, 0, 1, "", "set_trainable_params"], [203, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [204, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[198, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[199, 2, 1, "", "adapter_params"], [199, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[205, 1, 1, "", "BaseTokenizer"], [206, 1, 1, "", "ModelTokenizer"], [207, 1, 1, "", "SentencePieceBaseTokenizer"], [208, 1, 1, "", "TikTokenBaseTokenizer"], [209, 0, 1, "", "parse_hf_tokenizer_json"], [210, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[205, 2, 1, "", "decode"], [205, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[206, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[207, 2, 1, "", "decode"], [207, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[208, 2, 1, "", "decode"], [208, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[211, 1, 1, "", "Transform"], [212, 1, 1, "", "VisionCrossAttentionMask"]], "torchtune.rlhf": [[213, 0, 1, "", "estimate_advantages"], [214, 0, 1, "", "get_rewards_ppo"], [219, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.rlhf.loss": [[215, 1, 1, "", "DPOLoss"], [216, 1, 1, "", "PPOLoss"], [217, 1, 1, "", "RSOLoss"], [218, 1, 1, "", "SimPOLoss"]], "torchtune.rlhf.loss.DPOLoss": [[215, 2, 1, "", "forward"]], "torchtune.rlhf.loss.PPOLoss": [[216, 2, 1, "", "forward"]], "torchtune.rlhf.loss.RSOLoss": [[217, 2, 1, "", "forward"]], "torchtune.rlhf.loss.SimPOLoss": [[218, 2, 1, "", "forward"]], "torchtune.training": [[220, 3, 1, "", "FSDPPolicyType"], [221, 1, 1, "", "FormattedCheckpointFiles"], [222, 1, 1, "", "FullModelHFCheckpointer"], [223, 1, 1, "", "FullModelMetaCheckpointer"], [224, 1, 1, "", "FullModelTorchTuneCheckpointer"], [225, 1, 1, "", "ModelType"], [226, 1, 1, "", "OptimizerInBackwardWrapper"], [227, 0, 1, "", "apply_selective_activation_checkpointing"], [228, 0, 1, "", "create_optim_in_bwd_wrapper"], [229, 0, 1, "", "get_dtype"], [230, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [231, 0, 1, "", "get_memory_stats"], [232, 0, 1, "", "get_quantizer_mode"], [233, 0, 1, "", "get_unmasked_sequence_lengths"], [234, 0, 1, "", "get_world_size_and_rank"], [235, 0, 1, "", "init_distributed"], [236, 0, 1, "", "is_distributed"], [237, 0, 1, "", "log_memory_stats"], [238, 0, 1, "", "lora_fsdp_wrap_policy"], [244, 0, 1, "", "register_optim_in_bwd_hooks"], [245, 0, 1, "", "set_activation_checkpointing"], [246, 0, 1, "", "set_default_dtype"], [247, 0, 1, "", "set_seed"], [248, 0, 1, "", "setup_torch_profiler"], [249, 0, 1, "", "update_state_dict_for_classifier"], [250, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.training.FormattedCheckpointFiles": [[221, 2, 1, "", "build_checkpoint_filenames"]], "torchtune.training.FullModelHFCheckpointer": [[222, 2, 1, "", "load_checkpoint"], [222, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelMetaCheckpointer": [[223, 2, 1, "", "load_checkpoint"], [223, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelTorchTuneCheckpointer": [[224, 2, 1, "", "load_checkpoint"], [224, 2, 1, "", "save_checkpoint"]], "torchtune.training.OptimizerInBackwardWrapper": [[226, 2, 1, "", "get_optim_key"], [226, 2, 1, "", "load_state_dict"], [226, 2, 1, "", "state_dict"]], "torchtune.training.metric_logging": [[239, 1, 1, "", "CometLogger"], [240, 1, 1, "", "DiskLogger"], [241, 1, 1, "", "StdoutLogger"], [242, 1, 1, "", "TensorBoardLogger"], [243, 1, 1, "", "WandBLogger"]], "torchtune.training.metric_logging.CometLogger": [[239, 2, 1, "", "close"], [239, 2, 1, "", "log"], [239, 2, 1, "", "log_config"], [239, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.DiskLogger": [[240, 2, 1, "", "close"], [240, 2, 1, "", "log"], [240, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.StdoutLogger": [[241, 2, 1, "", "close"], [241, 2, 1, "", "log"], [241, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.TensorBoardLogger": [[242, 2, 1, "", "close"], [242, 2, 1, "", "log"], [242, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.WandBLogger": [[243, 2, 1, "", "close"], [243, 2, 1, "", "log"], [243, 2, 1, "", "log_config"], [243, 2, 1, "", "log_dict"]], "torchtune.utils": [[251, 0, 1, "", "batch_to_device"], [252, 0, 1, "", "get_device"], [253, 0, 1, "", "get_logger"], [254, 0, 1, "", "torch_version_ge"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 7, 8, 19, 31, 38, 39, 41, 53, 55, 220, 257, 259, 264, 267, 269, 270, 272, 273], "config": [0, 21, 22, 264, 268], "data": [1, 10, 31, 38, 39, 41, 265], "text": [1, 2, 13, 15, 266, 269], "templat": [1, 9, 11, 13, 17, 18, 265, 266], "type": 1, "convert": 1, "messag": [1, 12, 13, 34], "transform": [1, 5, 12, 13, 14, 211], "collat": 1, "helper": 1, "function": 1, "dataset": [2, 9, 10, 11, 15, 16, 53, 55, 265, 266], "imag": [2, 13, 15], "gener": [2, 3, 75, 267, 269], "builder": 2, "class": [2, 17, 22], "model": [4, 5, 14, 18, 23, 264, 267, 268, 269, 270, 271, 272], "llama3": [4, 114, 265, 269, 272], "2": 4, "vision": [4, 5], "1": 4, "llama2": [4, 99, 265, 267, 270, 273], "code": 4, "llama": 4, "qwen": 4, "phi": 4, "3": 4, "mistral": [4, 156], "gemma": [4, 89], "modul": 5, "compon": [5, 21, 271], "build": [5, 258, 273], "block": 5, "loss": 5, "base": [5, 18], "token": [5, 13, 18, 265], "util": [5, 8], "peft": [5, 271], "fusion": 5, "rlhf": 6, "train": [7, 220, 261, 268], "checkpoint": [7, 19, 23, 267, 271], "reduc": 7, "precis": [7, 271], "distribut": [7, 261], "memori": [7, 266, 270, 271, 273], "manag": 7, "metric": [7, 20, 23], "log": [7, 20, 23], "perform": [7, 270], "profil": 7, "miscellan": [7, 8], "chat": [9, 265, 266], "exampl": [9, 11, 12, 14, 15, 16], "format": [9, 11, 13, 15, 16, 19, 266], "load": [9, 11, 15, 16, 18], "from": [9, 11, 15, 16, 18, 265, 273], "hug": [9, 11, 15, 16, 18, 266, 267], "face": [9, 11, 15, 16, 18, 266, 267], "local": [9, 11, 15, 16, 266], "remot": [9, 11, 15, 266], "specifi": 9, "convers": 9, "style": 9, "sharegpt": 9, "openai": 9, "renam": [9, 11], "column": [9, 11], "built": [9, 11, 15, 16, 264, 266], "overview": [10, 19, 259, 262, 267, 271], "pipelin": 10, "instruct": [11, 258, 266, 269], "configur": [12, 21, 266], "custom": [12, 17, 265, 266], "creat": [13, 14], "prompt": [13, 17, 18, 265], "access": [13, 269], "content": 13, "multimod": [14, 15], "us": [14, 17, 21, 22, 265, 267, 273], "interleav": 15, "prefer": 16, "defin": 17, "via": [17, 258, 269], "dotpath": 17, "string": 17, "dictionari": 17, "prompttempl": [17, 36], "download": [18, 264, 267, 268], "file": 18, "set": [18, 266], "max": [18, 266], "sequenc": [18, 266], "length": [18, 266], "special": [18, 265], "handl": 19, "differ": 19, "hfcheckpoint": 19, "metacheckpoint": 19, "torchtunecheckpoint": 19, "intermedi": 19, "vs": 19, "final": 19, "lora": [19, 260, 267, 270, 271, 273], "put": [19, 273], "thi": 19, "all": [19, 21, 273], "togeth": [19, 273], "comet": 20, "logger": [20, 23], "about": 21, "where": 21, "do": 21, "paramet": [21, 271], "live": 21, "write": 21, "instanti": [21, 24], "referenc": 21, "other": [21, 267], "field": 21, "interpol": 21, "valid": [21, 27, 264], "your": [21, 22, 267, 268], "best": 21, "practic": 21, "airtight": 21, "public": 21, "api": 21, "onli": 21, "command": 21, "line": 21, "overrid": 21, "remov": 21, "what": [22, 259, 270, 272, 273], "ar": 22, "recip": [22, 262, 264, 268, 270, 272], "script": 22, "run": [22, 264, 267], "cli": [22, 264], "pars": [22, 26], "weight": 23, "bias": 23, "w": 23, "b": 23, "log_config": 25, "chatformat": 28, "chatmltempl": 29, "chosenrejectedtomessag": 30, "grammarerrorcorrectiontempl": 31, "inputoutputtomessag": 32, "instructtempl": 33, "openaitomessag": 35, "prompttemplateinterfac": 37, "questionanswertempl": 38, "role": 39, "sharegpttomessag": 40, "summarizetempl": 41, "format_content_with_imag": 42, "get_openai_messag": 43, "get_sharegpt_messag": 44, "left_pad_sequ": 45, "load_imag": 46, "padded_col": 47, "padded_collate_dpo": 48, "padded_collate_sft": 49, "padded_collate_tiled_images_and_mask": 50, "truncat": 51, "validate_messag": 52, "chatdataset": 53, "concatdataset": 54, "instructdataset": 55, "packeddataset": 56, "preferencedataset": 57, "sftdataset": 58, "textcompletiondataset": 59, "alpaca_cleaned_dataset": 60, "alpaca_dataset": 61, "chat_dataset": 62, "cnn_dailymail_articles_dataset": 63, "grammar_dataset": 64, "hh_rlhf_helpful_dataset": 65, "instruct_dataset": 66, "llava_instruct_dataset": 67, "the_cauldron_dataset": 68, "preference_dataset": 69, "samsum_dataset": 70, "slimorca_dataset": 71, "stack_exchange_paired_dataset": 72, "text_completion_dataset": 73, "wikitext_dataset": 74, "generate_next_token": 76, "get_causal_mask_from_padding_mask": 77, "get_position_ids_from_padding_mask": 78, "sampl": [79, 266], "code_llama2_13b": 80, "code_llama2_70b": 81, "code_llama2_7b": 82, "lora_code_llama2_13b": 83, "lora_code_llama2_70b": 84, "lora_code_llama2_7b": 85, "qlora_code_llama2_13b": 86, "qlora_code_llama2_70b": 87, "qlora_code_llama2_7b": 88, "gemma_2b": 90, "gemma_7b": 91, "gemma_token": 92, "lora_gemma": 93, "lora_gemma_2b": 94, "lora_gemma_7b": 95, "qlora_gemma_2b": 96, "qlora_gemma_7b": 97, "llama2chattempl": 98, "llama2_13b": 100, "llama2_70b": 101, "llama2_7b": 102, "llama2_reward_7b": 103, "llama2_token": 104, "lora_llama2": 105, "lora_llama2_13b": 106, "lora_llama2_70b": 107, "lora_llama2_7b": 108, "lora_llama2_reward_7b": 109, "qlora_llama2_13b": 110, "qlora_llama2_70b": 111, "qlora_llama2_7b": 112, "qlora_llama2_reward_7b": 113, "llama3_70b": 115, "llama3_8b": 116, "llama3_token": 117, "lora_llama3": 118, "lora_llama3_70b": 119, "lora_llama3_8b": 120, "qlora_llama3_70b": 121, "qlora_llama3_8b": 122, "llama3_1": 123, "llama3_1_405b": 124, "llama3_1_70b": 125, "llama3_1_8b": 126, "lora_llama3_1": 127, "lora_llama3_1_405b": 128, "lora_llama3_1_70b": 129, "lora_llama3_1_8b": 130, "qlora_llama3_1_405b": 131, "qlora_llama3_1_70b": 132, "qlora_llama3_1_8b": 133, "llama3_2_1b": 134, "llama3_2_3b": 135, "lora_llama3_2_1b": 136, "lora_llama3_2_3b": 137, "qlora_llama3_2_1b": 138, "qlora_llama3_2_3b": 139, "llama3visionencod": 140, "llama3visionprojectionhead": 141, "llama3visiontransform": 142, "llama3_2_vision_11b": 143, "llama3_2_vision_decod": 144, "llama3_2_vision_encod": 145, "llama3_2_vision_transform": 146, "lora_llama3_2_vision_11b": 147, "lora_llama3_2_vision_decod": 148, "lora_llama3_2_vision_encod": 149, "qlora_llama3_2_vision_11b": 150, "mistralchattempl": 151, "lora_mistr": 152, "lora_mistral_7b": 153, "lora_mistral_classifi": 154, "lora_mistral_reward_7b": 155, "mistral_7b": 157, "mistral_classifi": 158, "mistral_reward_7b": 159, "mistral_token": 160, "qlora_mistral_7b": 161, "qlora_mistral_reward_7b": 162, "lora_phi3": 163, "lora_phi3_mini": 164, "phi3": 165, "phi3_mini": 166, "phi3_mini_token": 167, "qlora_phi3_mini": 168, "lora_qwen2": 169, "lora_qwen2_0_5b": 170, "lora_qwen2_1_5b": 171, "lora_qwen2_7b": 172, "qwen2": 173, "qwen2_0_5b": 174, "qwen2_1_5b": 175, "qwen2_7b": 176, "qwen2_token": 177, "feedforward": 178, "fp32layernorm": 179, "kvcach": 180, "multiheadattent": 181, "rmsnorm": 182, "rotarypositionalembed": 183, "tanhgat": 184, "tiedlinear": 185, "transformercrossattentionlay": 186, "transformerdecod": 187, "transformerselfattentionlay": 188, "visiontransform": 189, "reparametrize_as_dtype_state_dict_post_hook": 190, "get_cosine_schedule_with_warmup": 191, "cewithchunkedoutputloss": 192, "deepfusionmodel": 193, "fusionembed": 194, "fusionlay": 195, "get_fusion_param": 196, "register_fusion_modul": 197, "adaptermodul": 198, "loralinear": 199, "disable_adapt": 200, "get_adapter_param": 201, "set_trainable_param": 202, "validate_missing_and_unexpected_for_lora": 203, "validate_state_dict_for_lora": 204, "basetoken": 205, "modeltoken": 206, "sentencepiecebasetoken": 207, "tiktokenbasetoken": 208, "parse_hf_tokenizer_json": 209, "tokenize_messages_no_special_token": 210, "visioncrossattentionmask": 212, "estimate_advantag": 213, "get_rewards_ppo": 214, "dpoloss": 215, "ppoloss": 216, "rsoloss": 217, "simpoloss": 218, "truncate_sequence_at_first_stop_token": 219, "fsdppolicytyp": 220, "formattedcheckpointfil": 221, "fullmodelhfcheckpoint": 222, "fullmodelmetacheckpoint": 223, "fullmodeltorchtunecheckpoint": 224, "modeltyp": 225, "optimizerinbackwardwrapp": 226, "apply_selective_activation_checkpoint": 227, "create_optim_in_bwd_wrapp": 228, "get_dtyp": 229, "get_full_finetune_fsdp_wrap_polici": 230, "get_memory_stat": 231, "get_quantizer_mod": 232, "get_unmasked_sequence_length": 233, "get_world_size_and_rank": 234, "init_distribut": 235, "is_distribut": 236, "log_memory_stat": 237, "lora_fsdp_wrap_polici": 238, "cometlogg": 239, "disklogg": 240, "stdoutlogg": 241, "tensorboardlogg": 242, "wandblogg": 243, "register_optim_in_bwd_hook": 244, "set_activation_checkpoint": 245, "set_default_dtyp": 246, "set_se": 247, "setup_torch_profil": 248, "update_state_dict_for_classifi": 249, "validate_expected_param_dtyp": 250, "batch_to_devic": 251, "get_devic": 252, "get_logg": 253, "torch_version_g": 254, "comput": [256, 263], "time": [256, 263], "welcom": 257, "document": 257, "get": [257, 264, 269], "start": [257, 264], "tutori": 257, "instal": 258, "pre": 258, "requisit": 258, "pypi": 258, "git": 258, "clone": 258, "nightli": 258, "kei": 259, "concept": 259, "design": 259, "principl": 259, "singl": 260, "devic": [260, 272], "finetun": [260, 262, 267, 270, 272, 273], "quantiz": [261, 267, 269, 271, 272], "awar": 261, "qat": [261, 272], "list": 264, "copi": 264, "fine": [265, 266, 268, 269, 270, 271, 272, 273], "tune": [265, 266, 268, 269, 270, 271, 272, 273], "chang": 265, "when": 265, "should": 265, "i": 265, "pack": 266, "unstructur": 266, "corpu": 266, "multipl": 266, "fulli": 266, "end": 267, "workflow": 267, "7b": 267, "evalu": [267, 269, 272], "eleutherai": [267, 269], "s": [267, 269], "eval": [267, 269], "har": [267, 269], "speed": 267, "up": 267, "librari": 267, "upload": 267, "hub": 267, "first": 268, "llm": 268, "select": 268, "modifi": 268, "next": 268, "step": [268, 271], "meta": 269, "8b": 269, "our": 269, "faster": 269, "how": 270, "doe": 270, "work": 270, "appli": [270, 272], "trade": 270, "off": 270, "optim": 271, "activ": 271, "offload": 271, "gradient": 271, "accumul": 271, "lower": [271, 272], "fuse": 271, "backward": 271, "pass": 271, "effici": 271, "low": 271, "rank": 271, "adapt": 271, "qlora": [271, 273], "option": 272, "save": 273, "deep": 273, "dive": 273}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})