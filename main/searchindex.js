Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "lora_phi3_mini", "phi3_mini", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"support": [2, 6, 8, 9, 10, 20, 24, 26, 27, 28, 29, 30, 31, 32, 33, 52, 54, 64, 71, 74, 79, 92, 94, 95, 96, 97, 98, 99, 100], "sever": [2, 95], "wide": [2, 95], "us": [2, 4, 6, 9, 10, 11, 15, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 54, 55, 57, 58, 59, 60, 61, 67, 68, 69, 70, 71, 72, 73, 74, 75, 80, 81, 82, 83, 87, 90, 91, 92, 95, 97, 98, 99], "help": [2, 6, 18, 59, 70, 72, 90, 91, 92, 94, 95, 96, 97, 98, 100], "quickli": [2, 7, 94, 95], "bootstrap": [2, 95], "your": [2, 5, 9, 10, 24, 82, 83, 90, 91, 92, 94, 95, 98, 99, 100], "fine": [2, 6, 8, 9, 90, 92, 96, 99], "tune": [2, 3, 6, 7, 8, 9, 11, 90, 91, 92, 96, 99, 100], "also": [2, 6, 7, 8, 9, 10, 29, 54, 59, 64, 73, 75, 83, 91, 94, 95, 96, 97, 98, 99, 100], "common": [2, 4, 7, 94, 95, 98, 99], "format": [2, 5, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24, 26, 27, 28, 29, 31, 33, 70, 71, 94, 96, 97, 98, 99], "like": [2, 6, 7, 8, 9, 24, 91, 94, 95, 96, 97, 99], "chat": [2, 14, 15, 18, 19, 22, 24, 29, 33], "model": [2, 6, 7, 8, 10, 15, 20, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 70, 71, 75, 85, 86, 90, 92, 94, 95, 100], "instruct": [2, 3, 13, 15, 17, 19, 20, 26, 27, 28, 31, 52, 90, 94, 97, 99, 100], "These": [2, 4, 6, 7, 8, 10, 72, 94, 95, 96, 97, 98, 99, 100], "ar": [2, 4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29, 30, 31, 32, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 59, 64, 69, 70, 71, 74, 91, 92, 94, 95, 96, 97, 98, 99, 100], "especi": [2, 92, 96], "specifi": [2, 6, 7, 8, 10, 29, 54, 69, 75, 83, 86, 94, 95, 96, 97, 98, 100], "from": [2, 3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 49, 55, 59, 60, 62, 63, 65, 67, 70, 71, 72, 82, 83, 89, 91, 93, 95, 96, 97, 98, 99], "yaml": [2, 7, 8, 10, 11, 29, 31, 72, 83, 92, 94, 96, 97, 98, 99, 100], "config": [2, 6, 9, 10, 11, 12, 29, 31, 54, 70, 72, 83, 92, 94, 95, 96, 98, 99, 100], "represent": [2, 99, 100], "abov": [2, 6, 61, 91, 96, 98, 99, 100], "all": [3, 4, 8, 12, 24, 25, 29, 54, 55, 59, 61, 68, 70, 72, 88, 90, 92, 93, 94, 96, 97, 98, 99], "famili": [3, 8, 27, 28, 33, 92, 98], "download": [3, 6, 88, 91, 94, 98, 99, 100], "meta": [3, 6, 18, 70, 71, 94, 96, 97], "llama": [3, 6, 18, 24, 29, 57, 58, 70, 71, 94, 96, 97, 98, 99], "8b": [3, 44, 45, 46, 51, 94], "hf": [3, 6, 70, 94, 96, 97, 98], "token": [3, 6, 7, 8, 19, 24, 26, 27, 28, 29, 30, 31, 32, 33, 54, 58, 59, 60, 67, 68, 75, 95, 96, 97, 98, 99, 100], "access_token": 3, "pre": [3, 18, 91, 94], "train": [3, 5, 6, 8, 9, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 54, 61, 62, 70, 71, 74, 85, 90, 92, 94, 95, 96, 98, 99, 100], "can": [3, 4, 6, 7, 8, 9, 10, 12, 19, 24, 25, 26, 27, 28, 29, 31, 57, 58, 67, 69, 70, 72, 75, 82, 83, 86, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100], "hug": [3, 6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 62, 92, 95, 97, 98], "face": [3, 6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 62, 92, 95, 97, 98], "hub": [3, 6, 97], "follow": [3, 6, 8, 22, 24, 54, 62, 83, 90, 91, 95, 96, 97, 98, 99, 100], "command": [3, 8, 9, 72, 91, 94, 96, 97, 98, 99, 100], "2": [3, 6, 9, 23, 33, 54, 67, 70, 71, 84, 87, 94, 96, 97, 98, 99], "7b": [3, 6, 26, 27, 28, 31, 37, 40, 42, 48, 49, 70, 71, 94, 97, 98, 99, 100], "mini": [3, 51, 52, 53], "microsoft": [3, 52], "4k": [3, 52], "hf_token": 3, "ignor": [3, 6, 54, 55], "pattern": [3, 68], "ai": [3, 49, 54, 83, 94, 98], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 33, 54, 59, 62, 67, 68, 71, 82, 83, 84, 87, 94, 96, 97, 98, 99, 100], "googl": [3, 34], "2b": [3, 34], "offer": 5, "allow": [5, 25, 82, 100], "seamless": 5, "transit": 5, "between": [5, 6, 70, 96, 98, 99, 100], "interoper": [5, 6, 8, 92, 96, 100], "rest": [5, 94, 100], "ecosystem": [5, 6, 8, 92, 96, 98, 100], "For": [5, 6, 7, 8, 24, 25, 26, 27, 28, 29, 31, 54, 59, 72, 83, 86, 87, 91, 94, 95, 96, 97, 98, 99, 100], "comprehens": 5, "overview": [5, 7, 9, 97, 99, 100], "pleas": [5, 41, 42, 47, 50, 53, 69, 75, 86, 91, 100], "see": [5, 6, 9, 18, 20, 29, 33, 41, 42, 47, 50, 53, 56, 63, 69, 72, 75, 76, 83, 85, 86, 87, 91, 92, 94, 95, 96, 97, 98, 99, 100], "deep": [5, 6, 7, 8, 9, 92, 97, 98], "dive": [5, 6, 7, 8, 9, 92, 97, 98], "enabl": [5, 7, 8, 9, 25, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 64, 85, 87, 98, 99, 100], "work": [5, 6, 8, 72, 92, 96, 98, 100], "set": [5, 6, 7, 8, 9, 26, 27, 28, 30, 31, 32, 33, 58, 59, 66, 69, 73, 75, 86, 87, 92, 94, 95, 96, 97, 98, 99], "consumpt": [5, 25], "dure": [5, 6, 25, 26, 27, 28, 30, 32, 54, 56, 58, 59, 60, 61, 94, 96, 98, 99, 100], "provid": [5, 6, 7, 8, 10, 15, 20, 24, 25, 26, 33, 59, 72, 75, 83, 92, 94, 95, 96, 97, 98], "debug": [5, 6, 7, 8], "finetun": [5, 6, 7, 8, 38, 39, 40, 45, 46, 51, 79, 90, 92, 97, 98], "job": [5, 9, 87, 97], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 92, 97, 98], "walk": [6, 8, 82, 92, 94, 95, 96, 97, 100], "you": [6, 7, 8, 9, 10, 17, 18, 24, 26, 27, 28, 31, 72, 82, 83, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100], "through": [6, 7, 8, 9, 55, 92, 94, 95, 96, 97, 100], "design": [6, 8], "behavior": [6, 94, 95], "associ": [6, 7, 8, 96, 99], "util": [6, 7, 8, 9, 10, 25, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 92, 96, 97, 98, 100], "what": [6, 7, 9, 18, 20, 30, 32, 90, 94, 95, 96, 97, 98], "cover": [6, 7, 8, 9, 94, 96, 100], "how": [6, 7, 8, 9, 69, 86, 90, 94, 95, 96, 97, 98, 100], "we": [6, 7, 8, 9, 26, 27, 28, 31, 54, 56, 58, 59, 64, 67, 70, 71, 74, 92, 94, 95, 96, 97, 98, 99, 100], "them": [6, 7, 24, 25, 26, 31, 33, 55, 61, 67, 94, 95, 96, 99, 100], "scenario": [6, 25], "full": [6, 7, 8, 41, 42, 47, 50, 53, 67, 92, 98, 99], "compos": 6, "compon": [6, 8, 12, 85, 92, 95, 97, 99, 100], "which": [6, 8, 25, 26, 27, 28, 30, 32, 38, 39, 40, 45, 46, 48, 51, 54, 58, 59, 60, 62, 67, 70, 71, 74, 80, 83, 86, 92, 94, 95, 96, 97, 98, 99, 100], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 21, 22, 24, 26, 29, 31, 61, 65, 66, 67, 70, 71, 87, 94, 95, 96, 97, 98, 99], "recip": [6, 7, 9, 10, 11, 55, 70, 71, 92, 94, 95, 96, 98, 100], "evalu": [6, 8, 90, 92, 97, 99, 100], "gener": [6, 8, 13, 16, 21, 24, 26, 33, 67, 87, 88, 90, 94, 95, 99, 100], "each": [6, 8, 14, 17, 25, 38, 39, 40, 45, 46, 48, 51, 54, 58, 59, 67, 68, 87, 92, 95, 96, 97, 98, 99], "make": [6, 7, 8, 9, 54, 60, 92, 96, 97, 98, 99, 100], "easi": [6, 8, 92, 99], "understand": [6, 7, 8, 90, 92, 94, 95, 99, 100], "extend": [6, 8, 92], "befor": [6, 23, 26, 59, 60, 64, 70, 96], "let": [6, 7, 9, 94, 95, 96, 97, 98, 99, 100], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 20, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 45, 46, 48, 51, 54, 58, 59, 60, 61, 63, 65, 68, 69, 70, 71, 73, 75, 82, 85, 86, 92, 94, 95, 97, 99, 100], "defin": [6, 7, 8, 55, 63, 64, 65, 97, 99], "some": [6, 7, 15, 65, 66, 90, 92, 94, 96, 97, 99, 100], "concept": [6, 96, 97], "In": [6, 7, 8, 24, 58, 64, 69, 82, 83, 94, 96, 98, 99, 100], "ll": [6, 7, 8, 68, 92, 94, 95, 96, 97, 98, 100], "talk": 6, "about": [6, 8, 70, 83, 92, 94, 96, 97, 98, 99, 100], "take": [6, 7, 8, 10, 55, 56, 61, 70, 72, 73, 94, 95, 96, 97, 98, 99, 100], "close": [6, 8, 80, 81, 82, 83, 99], "look": [6, 7, 8, 82, 91, 94, 95, 96, 97, 98, 99], "veri": [6, 25, 59, 96], "simpli": [6, 7, 94, 95, 96, 98, 100], "dictat": 6, "state_dict": [6, 61, 70, 71, 99, 100], "store": [6, 25, 80, 83, 99, 100], "file": [6, 7, 8, 9, 10, 11, 67, 68, 70, 71, 72, 80, 83, 85, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100], "disk": [6, 80], "weight": [6, 8, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 61, 63, 64, 70, 71, 83, 90, 94, 96, 97, 98, 99, 100], "string": [6, 24, 26, 27, 28, 29, 30, 31, 32, 33, 63, 67, 68, 73, 74], "kei": [6, 7, 9, 24, 26, 31, 33, 54, 56, 59, 66, 70, 96, 97, 99, 100], "identifi": 6, "state": [6, 8, 61, 65, 66, 70, 71, 96, 98, 99, 100], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 19, 21, 22, 24, 26, 29, 31, 61, 65, 66, 70, 71, 78], "If": [6, 7, 12, 13, 16, 17, 20, 21, 23, 24, 26, 27, 28, 30, 31, 32, 33, 54, 61, 64, 70, 71, 73, 74, 75, 78, 82, 83, 87, 91, 94, 95, 96, 97, 98, 99], "don": [6, 7, 8, 83, 87, 94, 96, 97, 98, 100], "t": [6, 7, 8, 33, 68, 74, 83, 87, 94, 96, 97, 98, 100], "match": [6, 24, 26, 31, 33, 91, 96, 98, 99], "up": [6, 8, 9, 26, 27, 28, 31, 94, 95, 97, 98, 99, 100], "exactli": 6, "those": [6, 99], "definit": [6, 99], "either": [6, 70, 86, 99, 100], "run": [6, 7, 9, 11, 55, 56, 59, 61, 70, 71, 82, 83, 91, 92, 94, 97, 98, 99, 100], "explicit": 6, "error": [6, 7, 23, 70, 87], "load": [6, 8, 24, 25, 26, 70, 71, 72, 82, 94, 96, 98, 99], "rais": [6, 10, 12, 20, 23, 29, 33, 54, 59, 70, 71, 74, 78, 83, 87], "an": [6, 7, 8, 9, 10, 13, 19, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 54, 59, 63, 65, 66, 69, 70, 71, 75, 83, 92, 94, 95, 96, 97, 98, 99, 100], "except": [6, 19, 20, 95], "wors": 6, "silent": [6, 55], "succe": 6, "infer": [6, 18, 24, 54, 56, 58, 59, 60, 90, 94, 96, 97, 98, 100], "expect": [6, 7, 10, 13, 16, 17, 21, 24, 26, 29, 31, 58, 83, 94, 95, 99], "addit": [6, 7, 8, 10, 24, 26, 29, 31, 69, 70, 71, 74, 75, 78, 80, 82, 83, 86, 92, 94, 97, 99], "line": [6, 8, 72, 97, 98], "need": [6, 7, 8, 9, 17, 24, 33, 54, 55, 59, 82, 83, 91, 94, 95, 96, 97, 98, 99, 100], "shape": [6, 54, 56, 58, 59, 60, 64], "valu": [6, 7, 22, 33, 34, 35, 36, 37, 43, 44, 49, 54, 56, 57, 59, 62, 70, 72, 80, 81, 82, 83, 87, 97, 98, 99], "two": [6, 7, 23, 92, 96, 97, 98, 99, 100], "popular": [6, 92, 95, 96], "llama2": [6, 7, 8, 10, 18, 22, 24, 26, 27, 28, 31, 33, 35, 36, 37, 38, 39, 40, 41, 42, 55, 59, 60, 67, 90, 92, 97, 98], "offici": [6, 18, 94, 97, 98], "implement": [6, 8, 24, 26, 27, 28, 29, 30, 31, 32, 33, 55, 57, 58, 62, 63, 64, 70, 82, 92, 99, 100], "when": [6, 7, 8, 11, 19, 25, 59, 61, 62, 75, 82, 96, 98, 99, 100], "websit": 6, "get": [6, 7, 8, 9, 24, 67, 74, 76, 77, 91, 92, 94, 95, 96, 97, 99], "access": [6, 7, 8, 25, 70, 96, 97], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 54, 70, 71, 94, 95, 96, 97, 98, 99, 100], "pth": [6, 96, 98], "inspect": [6, 96, 99, 100], "content": [6, 19, 22, 24, 67, 94, 95], "easili": [6, 7, 92, 95, 99, 100], "torch": [6, 25, 56, 59, 61, 62, 73, 74, 78, 85, 86, 87, 96, 97, 98, 99, 100], "import": [6, 7, 10, 29, 82, 83, 94, 95, 96, 97, 99, 100], "consolid": [6, 98], "00": [6, 89, 93, 97, 98], "mmap": [6, 96], "true": [6, 7, 19, 26, 27, 28, 29, 30, 32, 41, 42, 47, 50, 53, 61, 67, 68, 69, 70, 71, 75, 78, 82, 94, 95, 96, 98, 99, 100], "weights_onli": 6, "map_loc": [6, 96], "cpu": [6, 8, 61, 74, 91, 96, 100], "tensor": [6, 54, 55, 56, 57, 58, 59, 60, 61, 64, 70, 80, 81, 82, 83, 84, 99, 100], "item": 6, "print": [6, 9, 25, 27, 28, 30, 32, 33, 67, 94, 95, 97, 99, 100], "f": [6, 9, 27, 28, 30, 32, 94, 96, 99, 100], "tok_embed": [6, 59], "size": [6, 8, 10, 27, 28, 30, 32, 54, 56, 57, 58, 59, 60, 77, 92, 95, 96, 97, 98, 99], "32000": [6, 10, 99], "4096": [6, 10, 26, 27, 28, 31, 54, 58, 99], "len": [6, 25, 27, 28, 30, 32, 59], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 23, 24, 25, 26, 30, 32, 33, 38, 39, 40, 45, 46, 51, 57, 58, 61, 62, 67, 68, 69, 70, 72, 73, 74, 76, 83, 85, 87, 91, 92, 94, 95, 96, 97, 98, 99, 100], "contain": [6, 19, 25, 54, 56, 58, 59, 60, 63, 65, 66, 67, 68, 70, 71, 72, 82, 84, 94, 96, 98, 99], "includ": [6, 7, 8, 14, 17, 64, 70, 71, 72, 92, 94, 96, 97, 98, 99, 100], "input": [6, 13, 14, 17, 24, 26, 27, 28, 29, 30, 31, 33, 54, 55, 57, 58, 59, 60, 64, 67, 70, 84, 87, 94, 95, 99, 100], "embed": [6, 54, 56, 57, 58, 59, 75, 94, 98], "tabl": [6, 94, 100], "call": [6, 10, 19, 55, 61, 72, 80, 81, 82, 83, 94, 99, 100], "layer": [6, 8, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 59, 60, 64, 69, 75, 92, 98, 99, 100], "have": [6, 7, 10, 54, 56, 63, 72, 75, 82, 85, 91, 94, 95, 96, 97, 98, 99, 100], "dim": [6, 54, 55, 57, 58, 59, 60], "most": [6, 7, 68, 94, 97, 99, 100], "within": [6, 7, 10, 24, 33, 55, 82, 87, 96, 98, 99, 100], "default": [6, 7, 15, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 54, 55, 57, 58, 59, 60, 61, 62, 64, 67, 68, 70, 71, 72, 74, 80, 83, 84, 85, 87, 91, 96, 98, 99, 100], "everi": [6, 8, 55, 82, 91, 100], "repo": [6, 70, 71, 96], "first": [6, 7, 10, 23, 59, 68, 70, 72, 90, 92, 94, 96, 98, 99, 100], "big": [6, 96], "split": [6, 94, 95, 96], "across": [6, 8, 25, 70, 82, 87, 96, 98], "bin": [6, 96], "To": [6, 7, 8, 9, 70, 91, 92, 94, 95, 96, 97, 98, 99, 100], "correctli": [6, 8, 12, 70, 91, 94, 97, 100], "piec": 6, "one": [6, 8, 23, 55, 67, 94, 95, 96, 97, 98, 100], "pytorch_model": [6, 96], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 19, 54, 58, 59, 60, 64, 65, 67, 71, 72, 74, 75, 96, 97, 98, 99, 100], "doe": [6, 20, 24, 52, 63, 70, 72, 94, 96], "fewer": [6, 54], "sinc": [6, 7, 10, 55, 70, 94, 96, 98], "instead": [6, 8, 29, 31, 55, 56, 64, 96, 98, 99], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 21, 24, 26, 29, 31, 33, 63, 66, 68, 70, 71, 72, 73, 80, 81, 82, 83, 94, 96, 98], "caus": [6, 67], "try": [6, 7, 94, 96, 97, 98, 100], "same": [6, 7, 38, 39, 40, 45, 46, 51, 54, 56, 60, 67, 72, 75, 83, 94, 96, 98, 99, 100], "As": [6, 7, 8, 9, 64, 92, 96, 98, 100], "re": [6, 7, 68, 92, 94, 96, 97, 98, 99], "care": [6, 55, 70, 96, 98, 99], "end": [6, 8, 19, 25, 68, 90, 92, 94, 98, 99], "number": [6, 8, 24, 26, 27, 28, 29, 31, 33, 54, 56, 59, 62, 70, 71, 77, 87, 97, 99], "just": [6, 13, 92, 94, 95, 97, 98, 99], "save": [6, 8, 9, 61, 70, 71, 75, 83, 90, 94, 96, 98, 99], "less": [6, 33, 96, 97, 98, 100], "prone": 6, "manag": [6, 25, 85, 94], "invari": 6, "accept": [6, 7, 33, 67, 69, 97, 100], "multipl": [6, 7, 8, 19, 24, 25, 64, 80, 81, 82, 83, 95, 97, 98], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 94, 95, 96], "worri": [6, 94, 97], "explicitli": [6, 63, 92, 99], "convert": [6, 22, 24, 70, 84, 94, 96, 100], "time": [6, 67, 80, 82, 94, 96, 98, 100], "produc": [6, 100], "back": [6, 23, 70, 99, 100], "origin": [6, 27, 28, 61, 64, 94, 95, 96, 98, 99, 100], "form": [6, 7, 8, 23], "One": [6, 96], "advantag": [6, 99], "being": [6, 70, 71, 73, 100], "should": [6, 7, 8, 14, 17, 18, 19, 20, 22, 29, 31, 38, 39, 40, 45, 46, 48, 51, 54, 55, 63, 69, 72, 80, 81, 82, 83, 91, 92, 95, 96, 97, 98, 99, 100], "abl": [6, 8, 95, 96, 97, 98], "post": [6, 100], "tool": [6, 96, 97], "quantiz": [6, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 64, 90, 97, 100], "eval": [6, 90, 92], "without": [6, 7, 9, 91, 92, 94, 95, 96, 99], "code": [6, 8, 59, 88, 92, 95, 97], "chang": [6, 7, 9, 13, 91, 95, 96, 97, 98, 99, 100], "OR": 6, "convers": [6, 14, 15, 18, 20, 22, 23, 24, 29, 33, 70, 92, 94, 95, 96, 98, 99, 100], "script": [6, 9, 96, 97, 98], "wai": [6, 7, 24, 94, 95, 96, 97, 98], "surround": [6, 8, 92], "load_checkpoint": [6, 8, 70, 71], "save_checkpoint": [6, 8, 9, 70, 71], "method": [6, 7, 8, 9, 11, 24, 26, 27, 28, 29, 30, 31, 32, 33, 61, 63, 65, 72, 91, 92, 96, 98, 99, 100], "convertor": 6, "avail": [6, 8, 72, 73, 74, 92, 96, 98, 99], "here": [6, 7, 9, 15, 30, 57, 58, 94, 95, 96, 97, 98, 99, 100], "three": [6, 8, 97], "hfcheckpoint": 6, "read": [6, 70, 71, 92], "write": [6, 8, 70, 71, 80, 94, 95, 97], "compat": [6, 70], "transform": [6, 8, 24, 26, 38, 39, 40, 45, 46, 48, 51, 59, 60, 62, 86, 99], "framework": [6, 8, 92], "mention": [6, 96, 100], "assum": [6, 13, 16, 17, 21, 26, 31, 62, 65, 68, 74, 96, 99], "checkpoint_dir": [6, 7, 70, 71, 96, 98], "necessari": [6, 33, 80, 81, 82, 83, 94, 99], "json": [6, 70, 85, 96], "easiest": [6, 96, 97], "sure": [6, 7, 96, 97, 98, 99, 100], "everyth": [6, 8, 72, 92, 97], "flow": [6, 24, 26, 100], "By": [6, 98, 99, 100], "safetensor": 6, "output": [6, 17, 27, 28, 30, 33, 38, 39, 40, 45, 46, 48, 51, 54, 55, 57, 58, 59, 60, 64, 66, 75, 81, 85, 91, 94, 95, 96, 97, 98, 99, 100], "dir": [6, 83, 91, 96, 97, 98], "output_dir": [6, 7, 70, 71, 85, 96, 98, 99, 100], "argument": [6, 7, 10, 17, 24, 26, 29, 31, 33, 41, 42, 47, 50, 53, 54, 69, 72, 75, 78, 80, 82, 83, 86, 94, 98, 99], "snippet": [6, 95], "explain": 6, "setup": [6, 7, 8, 59, 86, 96, 99, 100], "_component_": [6, 7, 9, 10, 29, 94, 95, 96, 98, 99], "fullmodelhfcheckpoint": [6, 96], "directori": [6, 7, 70, 71, 80, 82, 83, 96, 97, 98], "sort": [6, 70], "id": [6, 24, 26, 27, 28, 29, 31, 33, 67, 68, 70, 84, 94, 96], "so": [6, 7, 70, 72, 91, 92, 94, 96, 97, 98, 99, 100], "order": [6, 8, 70, 82, 83, 97], "matter": [6, 70, 99], "checkpoint_fil": [6, 7, 9, 70, 71, 96, 98, 99, 100], "restart": 6, "previou": [6, 70, 71], "more": [6, 7, 8, 29, 33, 56, 58, 69, 72, 83, 85, 86, 87, 92, 95, 96, 97, 98, 99, 100], "next": [6, 98, 100], "section": [6, 8, 90, 96, 98, 100], "recipe_checkpoint": [6, 70, 71], "null": [6, 7], "usual": [6, 58, 70, 83, 96, 99], "model_typ": [6, 70, 71, 96, 98], "resume_from_checkpoint": [6, 70, 71], "fals": [6, 7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 64, 67, 68, 70, 71, 85, 94, 95, 96, 98, 99, 100], "requir": [6, 7, 25, 33, 70, 82, 83, 87, 91, 94, 95, 97, 100], "param": [6, 8, 38, 39, 40, 45, 46, 51, 64, 65, 66, 70, 99, 100], "directli": [6, 7, 8, 10, 29, 31, 69, 70, 95, 96, 97, 98, 99, 100], "ensur": [6, 7, 12, 23, 33, 54, 70, 74, 92, 97], "out": [6, 7, 8, 24, 26, 27, 28, 29, 30, 32, 70, 71, 90, 92, 94, 96, 97, 98, 99, 100], "case": [6, 8, 9, 19, 54, 70, 74, 80, 86, 92, 94, 95, 96, 98, 99, 100], "discrep": [6, 70], "along": [6, 98, 99], "detail": [6, 29, 33, 56, 69, 75, 85, 87, 96, 97, 98, 99, 100], "found": [6, 7, 9, 57, 58, 99, 100], "metacheckpoint": 6, "github": [6, 10, 38, 39, 40, 45, 46, 51, 54, 57, 58, 62, 91, 97], "repositori": [6, 18, 96, 97], "fullmodelmetacheckpoint": [6, 98], "torchtunecheckpoint": 6, "perform": [6, 55, 92, 94, 96, 98, 100], "current": [6, 52, 54, 58, 59, 60, 71, 75, 77, 80, 82, 87, 96, 97, 98], "test": [6, 7, 8, 92, 94], "complet": [6, 8, 94, 95, 96, 97, 98], "written": [6, 7, 8, 70, 71, 80, 81, 82, 83, 92], "begin": [6, 67, 68, 94, 98, 100], "partit": [6, 100], "ha": [6, 63, 65, 67, 95, 96, 97, 98, 99, 100], "standard": [6, 81, 92, 94, 96, 98], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 96], "inform": [6, 19, 83, 86, 92, 96, 97, 98], "subsequ": [6, 8], "recipe_st": [6, 70, 71], "pt": [6, 9, 70, 71, 96, 98], "epoch": [6, 8, 9, 62, 70, 71, 94, 96, 97, 98], "optim": [6, 7, 8, 25, 52, 62, 94, 96, 97, 98, 99, 100], "etc": [6, 8, 70, 97], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 19, 59, 63, 67, 70, 85, 87, 94, 95, 96, 99, 100], "updat": [6, 7, 8, 91, 94, 96, 97, 98, 99, 100], "hf_model_0001_0": [6, 96], "hf_model_0002_0": [6, 96], "both": [6, 25, 96, 99, 100], "adapt": [6, 63, 64, 65, 66, 70, 71, 94, 96, 99, 100], "merg": [6, 10, 70, 96, 98, 100], "would": [6, 7, 9, 59, 91, 94, 95, 96, 99, 100], "our": [6, 8, 92, 94, 95, 96, 97, 99, 100], "tutori": [6, 86, 92, 94, 95, 96, 97, 98, 99, 100], "primari": [6, 7, 8, 97], "want": [6, 7, 8, 9, 10, 24, 91, 94, 96, 97, 98, 99], "resum": [6, 8, 62, 70, 71, 100], "initi": [6, 8, 11, 25, 34, 35, 36, 37, 43, 44, 49, 78, 97, 99, 100], "frozen": [6, 99, 100], "base": [6, 10, 26, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 58, 62, 64, 66, 70, 72, 75, 80, 90, 94, 96, 97, 98, 99, 100], "well": [6, 7, 8, 92, 95, 96, 98, 100], "learnt": [6, 94, 96], "someth": [6, 8, 9, 94, 96], "NOT": 6, "refer": [6, 7, 8, 57, 58, 92, 99], "adapter_checkpoint": [6, 70, 71], "adapter_0": [6, 96], "now": [6, 67, 94, 95, 96, 97, 98, 99, 100], "knowledg": 6, "creat": [6, 7, 10, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 56, 62, 69, 70, 71, 80, 82, 94, 95, 96, 98, 100], "simpl": [6, 8, 90, 97, 99, 100], "forward": [6, 8, 54, 55, 57, 58, 59, 60, 64, 98, 99, 100], "13b": [6, 35, 38, 41], "modeltyp": [6, 70, 71], "llama2_13b": [6, 38], "right": [6, 70, 96, 98, 99], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 99], "successfulli": [6, 97], "vocab": [6, 10, 59, 98], "70": [6, 43], "x": [6, 54, 55, 57, 58, 59, 60, 64, 99, 100], "randint": 6, "0": [6, 8, 38, 39, 40, 41, 42, 54, 59, 62, 64, 67, 82, 83, 84, 87, 89, 93, 94, 96, 97, 98, 99, 100], "no_grad": 6, "6": [6, 57, 84, 96, 100], "3989": 6, "9": [6, 96, 100], "0531": 6, "3": [6, 51, 52, 68, 72, 76, 84, 94, 96, 97, 98, 100], "2375": 6, "5": [6, 62, 84, 85, 96, 97, 98], "2822": 6, "4": [6, 33, 54, 84, 92, 96, 98, 99, 100], "4872": 6, "7469": 6, "8": [6, 27, 28, 30, 32, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 96, 99, 100], "6737": 6, "11": [6, 96, 98, 100], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 84], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 24, 26, 31, 33, 83, 94, 95, 96, 97, 98, 99], "find": [6, 8, 9, 96, 97, 99], "list": [6, 7, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 63, 64, 67, 68, 70, 71, 72, 76, 79, 84, 94, 95, 97, 98], "builder": [6, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 94, 95, 100], "hope": 6, "deeper": [6, 97], "insight": [6, 96], "happi": [6, 96], "thi": [7, 8, 9, 10, 19, 24, 25, 26, 27, 28, 29, 31, 33, 52, 54, 55, 58, 59, 60, 61, 62, 63, 67, 69, 70, 71, 72, 73, 74, 80, 82, 83, 85, 86, 87, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100], "guid": [7, 9, 92, 94, 95, 97, 99], "pars": [7, 10, 68, 72, 94, 97], "effect": 7, "cli": [7, 9, 11, 91, 96, 97], "prerequisit": [7, 94, 95, 96, 97, 98, 99, 100], "Be": [7, 94, 96, 97, 98, 99, 100], "familiar": [7, 94, 96, 97, 98, 99, 100], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 91, 94, 95, 97], "instal": [7, 9, 82, 83, 90, 96, 97, 98, 99, 100], "fundament": 7, "There": [7, 14, 23, 94, 96, 97, 98, 99], "entri": [7, 8, 97], "point": [7, 8, 22, 95, 96, 97, 98, 99, 100], "locat": [7, 98, 99, 100], "thei": [7, 8, 19, 25, 59, 72, 94, 95, 99], "truth": [7, 96, 98], "reproduc": 7, "overridden": [7, 55, 72], "quick": [7, 25], "experiment": 7, "modifi": [7, 8, 9, 61, 92, 96, 98, 99, 100], "serv": [7, 69, 95, 99], "particular": [7, 24, 25, 33, 69, 95, 99, 100], "seed": [7, 8, 9, 87, 97], "shuffl": 7, "devic": [7, 8, 73, 74, 94, 96, 97, 98, 99], "cuda": [7, 73, 74, 91, 96, 100], "dtype": [7, 8, 56, 61, 74, 79, 96, 100], "fp32": [7, 100], "enable_fsdp": 7, "mani": [7, 95, 96], "object": [7, 10, 14, 15, 18, 20, 54, 69, 94, 95], "keyword": [7, 10, 24, 26, 29, 31, 33, 61, 94], "loss": [7, 8, 26, 27, 28, 30, 32, 97, 99, 100], "function": [7, 8, 10, 11, 24, 54, 55, 61, 69, 73, 77, 87, 92, 94, 95, 100], "exampl": [7, 8, 9, 10, 11, 15, 18, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 54, 63, 67, 69, 70, 71, 82, 83, 84, 88, 89, 91, 93, 94, 95, 96, 98, 99, 100], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 96], "path": [7, 8, 9, 10, 24, 26, 27, 28, 29, 30, 31, 32, 33, 67, 68, 70, 71, 72, 85, 94, 96, 98, 99], "normal": [7, 24, 57, 59, 60, 67, 94, 95, 99, 100], "python": [7, 68, 72, 76, 83, 87, 88, 96], "alpaca_dataset": [7, 27, 95], "custom": [7, 8, 24, 26, 29, 31, 86, 92, 96, 97, 98, 99], "train_on_input": [7, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 95], "onc": [7, 96, 97, 98, 99, 100], "ve": [7, 56, 68, 94, 95, 96, 98, 99], "instanc": [7, 10, 25, 55, 61, 65, 66, 99], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 29, 96, 100], "under": [7, 96, 98, 100], "preced": [7, 10, 95, 98, 99], "actual": [7, 9, 24, 94], "throw": 7, "notic": [7, 94, 95, 99], "miss": [7, 99], "posit": [7, 10, 54, 58, 59, 60, 98], "anoth": [7, 96], "handl": [7, 11, 19, 25, 67, 94, 96, 99, 100], "def": [7, 8, 9, 11, 69, 94, 95, 99, 100], "dictconfig": [7, 8, 10, 11, 12, 83], "arg": [7, 10, 59, 61, 63, 72, 81], "tupl": [7, 10, 25, 33, 61, 67, 68, 69, 72, 77, 84], "kwarg": [7, 10, 61, 63, 72, 78, 80, 81, 82, 83, 86], "str": [7, 10, 13, 16, 17, 19, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 61, 63, 64, 65, 66, 67, 68, 70, 71, 72, 73, 74, 76, 79, 80, 81, 82, 83, 85, 87, 94], "mean": [7, 57, 94, 97, 99], "pass": [7, 10, 24, 25, 26, 29, 31, 54, 55, 61, 69, 74, 75, 78, 82, 83, 86, 94, 99, 100], "add": [7, 9, 24, 68, 72, 95, 96, 98, 99, 100], "d": [7, 19, 54, 59, 60, 68, 94, 95, 99], "llama2_token": [7, 96], "tmp": [7, 94, 97, 98], "option": [7, 8, 13, 16, 17, 21, 24, 26, 29, 31, 33, 38, 39, 40, 45, 46, 48, 51, 54, 58, 59, 60, 61, 67, 68, 70, 71, 73, 74, 76, 80, 83, 85, 87, 91, 92, 96], "bool": [7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 61, 64, 67, 68, 69, 70, 71, 75, 78, 82, 85, 86, 100], "max_seq_len": [7, 10, 24, 26, 27, 28, 29, 31, 33, 54, 56, 58, 59, 67, 68, 94, 95], "int": [7, 9, 24, 25, 26, 27, 28, 29, 31, 33, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 56, 57, 58, 59, 62, 64, 67, 68, 69, 70, 71, 75, 77, 80, 81, 82, 83, 84, 86, 87, 94, 95, 99, 100], "512": [7, 27, 28, 95, 100], "instructdataset": [7, 27, 28, 30, 31, 32, 95], "alreadi": [7, 78, 91, 96, 99], "overwrit": [7, 91], "duplic": [7, 8, 92], "sometim": 7, "than": [7, 23, 33, 54, 69, 94, 96, 97, 98, 99, 100], "resolv": [7, 97], "alpaca": [7, 13, 27, 28, 38, 39, 40, 45, 46, 51, 95], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 80, 81, 82, 83], "disklogg": 7, "log_dir": [7, 80, 82, 83], "conveni": [7, 8], "verifi": [7, 73, 74, 75, 94, 97, 99], "properli": 7, "experi": [7, 83, 90, 92, 94, 98, 99], "wa": [7, 94, 96, 98, 99, 100], "cp": [7, 91, 94, 96, 97, 98], "7b_lora_single_devic": [7, 96, 97, 99, 100], "my_config": 7, "discuss": [7, 97, 99], "guidelin": 7, "while": [7, 8, 38, 39, 40, 45, 46, 51, 55, 92, 96, 100], "mai": [7, 9, 75, 85, 94, 95, 97, 99], "tempt": 7, "put": [7, 8, 97, 99], "much": [7, 96, 98, 99, 100], "give": [7, 99], "maximum": [7, 24, 26, 27, 28, 29, 31, 33, 54, 56, 58, 59, 68], "flexibl": [7, 25, 95], "switch": 7, "encourag": [7, 99], "clariti": 7, "significantli": 7, "easier": [7, 96, 97], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 100], "expos": [7, 8, 94, 95, 97], "parent": 7, "modul": [7, 10, 33, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 75, 86, 87, 94, 97, 99, 100], "__init__": [7, 8, 99, 100], "py": [7, 10, 38, 39, 40, 45, 46, 51, 54, 56, 57, 58, 62, 96, 98], "guarante": 7, "stabil": [7, 92, 100], "underscor": 7, "_alpaca": 7, "collect": [7, 97], "differ": [7, 9, 24, 25, 26, 67, 92, 94, 96, 98, 99, 100], "itself": 7, "via": [7, 9, 29, 64, 99, 100], "pair": [7, 84, 95], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 94, 96, 97, 98, 99, 100], "checkpoint": [7, 8, 61, 68, 70, 71, 83, 86, 92, 98, 99, 100], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 29, 31, 33, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65, 66, 67, 68, 70, 71, 72, 80, 81, 82, 83, 94, 95, 97, 99, 100], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 54, 58, 59, 60], "core": [8, 92, 95, 97, 100], "i": [8, 18, 20, 61, 66, 68, 95, 96, 98, 100], "structur": [8, 14, 15, 18, 20, 24, 94, 95, 96], "new": [8, 49, 80, 82, 94, 96, 97, 98, 99, 100], "user": [8, 14, 15, 18, 19, 20, 22, 23, 24, 54, 67, 94, 95, 97], "thought": [8, 92, 97, 100], "target": [8, 92], "pipelin": [8, 92], "llm": [8, 90, 92, 95, 96, 99], "eg": [8, 59, 70, 92], "meaning": [8, 92, 96], "featur": [8, 9, 91, 92, 96, 97], "fsdp": [8, 69, 75, 92, 97, 98], "activ": [8, 55, 86, 92, 100], "gradient": [8, 92, 96, 98, 99, 100], "accumul": [8, 92], "mix": [8, 96], "precis": [8, 61, 74, 92, 97, 100], "appli": [8, 24, 26, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 57, 58, 59, 60, 86, 92, 100], "given": [8, 10, 17, 23, 64, 73, 74, 92, 99], "complex": 8, "becom": [8, 91, 95], "harder": 8, "anticip": 8, "architectur": [8, 18, 20, 59, 95], "methodolog": 8, "reason": [8, 96], "possibl": [8, 24, 29, 95], "trade": 8, "off": [8, 67, 96], "memori": [8, 25, 26, 27, 28, 31, 61, 75, 90, 92, 95, 96, 97, 98], "vs": [8, 97], "qualiti": [8, 96, 99], "believ": 8, "best": [8, 94], "suit": [8, 97], "specif": [8, 10, 75, 94, 95, 96, 100], "b": [8, 54, 58, 59, 60, 64, 83, 99, 100], "fit": [8, 24, 26, 27, 28, 31], "solut": 8, "result": [8, 67, 96, 98, 99, 100], "meant": [8, 61], "depend": [8, 9, 13, 96, 99, 100], "level": [8, 76, 92, 100], "expertis": 8, "routin": 8, "yourself": [8, 98, 99], "exist": [8, 91, 95, 96, 97, 98, 100], "ad": [8, 67, 94, 99, 100], "ones": 8, "modular": [8, 92], "build": [8, 29, 31, 92, 98, 99], "block": [8, 38, 39, 40, 45, 46, 48, 51, 92], "wandb": [8, 9, 83, 97], "log": [8, 76, 80, 81, 82, 83, 96, 97, 98, 100], "fulli": [8, 25], "nativ": [8, 90, 92, 99, 100], "pytorch": [8, 59, 61, 69, 82, 85, 86, 87, 90, 91, 92, 98, 99, 100], "correct": [8, 16, 30, 57, 58, 59, 73, 92, 94, 95], "numer": [8, 92], "pariti": [8, 92], "verif": 8, "extens": [8, 92], "comparison": [8, 99, 100], "benchmark": [8, 87, 92, 96, 98, 99], "limit": 8, "hidden": [8, 55], "behind": 8, "100": [8, 26, 27, 28, 30, 32, 33, 84, 85, 99, 100], "flag": [8, 26, 27, 28, 30, 32, 69, 75, 100], "prefer": [8, 92, 95], "over": [8, 62, 72, 92, 95, 96, 98, 99, 100], "unnecessari": 8, "abstract": [8, 14, 17, 92, 97, 100], "No": [8, 92], "inherit": [8, 72, 92], "go": [8, 18, 20, 67, 92, 95, 96, 97, 100], "upon": [8, 25, 98], "figur": [8, 99, 100], "spectrum": 8, "decid": 8, "interact": [8, 90, 97], "start": [8, 9, 25, 68, 91, 92, 94, 95, 96, 97], "paradigm": 8, "consist": [8, 97], "configur": [8, 26, 27, 28, 29, 30, 31, 32, 33, 60, 92, 94, 97, 98, 99, 100], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 80, 81, 82, 83, 84, 85, 86, 87, 90, 92, 94, 95, 96, 97, 98, 99, 100], "overrid": [8, 11, 96, 97, 98, 100], "togeth": [8, 83, 97, 99], "valid": [8, 23, 91, 96, 97], "environ": [8, 91, 96, 97], "logic": [8, 70, 92, 97, 99], "api": [8, 9, 41, 42, 47, 50, 53, 94, 96, 97, 98, 100], "closer": [8, 99], "monolith": [8, 92], "trainer": [8, 77], "A": [8, 9, 22, 25, 61, 64, 67, 68, 69, 70, 72, 84, 89, 90, 93, 94, 96, 99, 100], "wrapper": [8, 67, 68, 99], "around": [8, 24, 67, 68, 85, 94, 96, 99, 100], "extern": 8, "primarili": [8, 25, 99], "eleutherai": [8, 92, 99], "har": [8, 92, 99], "control": [8, 26, 27, 28, 30, 32, 87, 96], "multi": [8, 24, 54, 98], "stage": 8, "distil": 8, "oper": [8, 25, 85, 87], "turn": [8, 19, 23, 24, 68, 94], "dataload": [8, 27, 28, 30, 32], "applic": [8, 54, 70, 71, 83], "clean": [8, 9, 27], "after": [8, 56, 57, 80, 81, 82, 83, 94, 100], "process": [8, 9, 61, 87, 97, 100], "group": [8, 54, 80, 81, 82, 83, 98], "init_process_group": [8, 78], "backend": 8, "gloo": 8, "els": [8, 72, 83, 92, 100], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 25, 72, 75, 95, 97, 98, 99], "stuff": 8, "carri": 8, "relev": [8, 19, 96, 99], "interfac": [8, 14, 17, 25, 63], "metric": [8, 97], "logger": [8, 76, 80, 81, 82, 83, 97], "self": [8, 9, 38, 39, 40, 45, 46, 48, 51, 54, 59, 60, 63, 95, 99, 100], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 69, 75, 85, 86, 94], "_model": 8, "_setup_model": 8, "_token": [8, 95], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 100], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 87, 98], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 27, 28, 30, 32, 54, 56, 58, 59, 60, 67, 84, 92, 95, 97, 98, 99], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 24, 26, 27, 28, 29, 31, 33, 84], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 80, 81, 82, 83], "step": [8, 59, 62, 68, 80, 81, 82, 83, 85, 90, 96, 99, 100], "learn": [8, 25, 62, 92, 94, 95, 97, 98, 99, 100], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 20, 21, 23, 24, 26, 29, 31, 54, 58, 59, 60, 66, 67, 68, 70, 71, 73, 74, 76, 80, 81, 82, 83, 86, 87, 94, 96], "fullfinetunerecip": 8, "direct": [8, 91], "wandblogg": [9, 99, 100], "workspac": 9, "seen": [9, 99, 100], "screenshot": 9, "below": [9, 58, 69, 95, 98, 99, 100], "packag": [9, 82, 83, 91], "pip": [9, 82, 83, 91, 96, 98], "Then": [9, 97], "login": [9, 83, 96], "built": [9, 91, 94, 95, 97, 100], "project": [9, 38, 39, 40, 45, 46, 48, 51, 54, 55, 75, 83, 90, 99, 100], "grab": [9, 98], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 94], "exit": [9, 91], "resourc": [9, 80, 81, 82, 83], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 31, 33, 94, 95, 96], "desir": [9, 24, 94], "suggest": 9, "approach": [9, 25, 95], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 96], "_output_dir": [9, 70, 71], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 19, 22, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 54, 56, 57, 58, 59, 60, 61, 64, 65, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 85, 86, 96, 99, 100], "descript": [9, 29, 33], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 19, 22, 24, 27, 28, 30, 32], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 24, 26, 29, 31, 34, 35, 36, 37, 38, 39, 40, 41, 42, 45, 46, 47, 49, 50, 51, 52, 53, 54, 57, 58, 62, 69, 70, 71, 72, 76, 82, 83, 85, 86, 87, 91, 96], "com": [10, 38, 39, 40, 45, 46, 51, 54, 57, 58, 62, 91], "facebookresearch": [10, 57, 58], "blob": [10, 38, 39, 40, 45, 46, 51, 54, 57, 58, 62], "main": [10, 11, 54, 57, 58, 91, 96, 98], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 59], "32": [10, 98, 99, 100], "num_head": [10, 54, 56, 58, 59], "num_kv_head": [10, 54, 56], "vocab_s": 10, "must": [10, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 63, 68, 72, 100], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52, 54, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 75, 76, 77, 78, 79, 84, 85, 87, 94, 95, 99, 100], "nn": [10, 54, 55, 56, 59, 60, 61, 63, 65, 66, 69, 86, 99, 100], "parsed_yaml": 10, "embed_dim": [10, 54, 58, 60, 99], "valueerror": [10, 20, 23, 29, 33, 54, 59, 70, 71, 74, 87], "callabl": [11, 24, 26, 59, 69, 75, 86], "With": [11, 96, 99, 100], "my_recip": 11, "foo": 11, "bar": [11, 92, 97], "instanti": [12, 34, 35, 36, 37, 38, 39, 40, 43, 44, 45, 46, 48, 49, 51, 52], "configerror": 12, "cannot": [12, 98], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 80, 81, 82, 83, 95, 96, 100], "prompt": [13, 14, 16, 17, 18, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 59, 67, 95, 96, 98], "templat": [13, 14, 16, 17, 21, 24, 26, 27, 28, 30, 31, 32, 33], "style": [13, 27, 28, 29, 33, 100], "slightli": 13, "classmethod": [13, 14, 15, 16, 17, 18, 19, 20, 21], "map": [13, 16, 17, 21, 22, 24, 25, 26, 31, 33, 66, 70, 80, 81, 82, 83, 94, 96, 99], "column_map": [13, 16, 17, 21, 24, 26, 31, 33, 95], "placehold": [13, 14, 16, 17, 21, 24, 26, 31, 33], "column": [13, 16, 17, 21, 24, 26, 31, 33, 94], "ident": [13, 16, 17, 20, 21, 26, 31, 96], "role": [14, 19, 22, 24, 67, 94, 95], "system": [14, 15, 18, 19, 20, 22, 23, 24, 67, 94, 95], "assist": [14, 15, 18, 19, 22, 23, 24, 67, 94, 95], "messag": [14, 15, 18, 20, 22, 23, 24, 29, 67, 68, 91, 94, 95], "accord": [14, 20, 94], "openai": 15, "markup": 15, "languag": [15, 64, 99], "It": [15, 20, 94, 95, 100], "huggingfac": [15, 24, 26, 29, 31, 52, 62, 70, 71, 95, 96], "im_start": 15, "context": [15, 52, 85, 95], "im_end": 15, "goe": 15, "respons": [15, 67, 95, 96, 97, 98], "appropri": [15, 18, 20, 25, 62, 95, 100], "tag": [15, 18, 20, 24, 68, 80, 81, 82, 83, 94], "grammar": [16, 30, 95], "sentenc": 16, "alwai": [17, 72], "human": [18, 22, 94], "taken": [18, 99, 100], "inst": [18, 20, 24, 94, 95], "sy": [18, 94, 95], "respect": [18, 25, 66, 94, 95], "honest": [18, 94, 95], "am": [18, 20, 94, 95, 96, 98], "pari": [18, 20, 95], "capit": [18, 20, 95], "franc": [18, 20, 95], "known": [18, 20, 67, 95], "its": [18, 20, 87, 95, 96, 98, 99], "stun": [18, 20, 95], "liter": [19, 38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53], "mask": [19, 26, 27, 28, 30, 32, 54, 60, 67, 68, 94, 95], "ipython": 19, "eot": 19, "dataclass": [19, 94], "repres": [19, 94], "individu": [19, 83, 86, 94, 95], "tiktoken": [19, 68, 98], "special": [19, 24, 68], "variabl": [19, 24, 25, 26, 31, 33, 100], "writer": 19, "whether": [19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 38, 39, 40, 45, 46, 48, 51, 61, 64, 67, 68, 69, 74, 94], "correspond": [19, 63, 65, 74, 97, 98], "consecut": [19, 23], "from_dict": [19, 94], "construct": [19, 99], "dictionari": [19, 80, 81, 82, 83, 96], "mistral": [20, 24, 48, 49, 50, 94, 96, 97], "llama2chatformat": [20, 33, 94, 95], "summar": [21, 32, 94, 95], "task": [21, 25, 94, 95, 96, 98, 99, 100], "dialogu": [21, 32, 94], "dialog": 21, "adher": [22, 33], "sharegpt": [22, 29, 95], "gpt": [22, 54, 96], "remain": [22, 62, 99], "unmask": 22, "forth": 23, "come": [23, 63, 99], "empti": 23, "shorter": 23, "length": [23, 25, 26, 27, 28, 31, 33, 52, 54, 56, 58, 59, 60, 67, 68, 71, 84], "min": [23, 99], "invalid": 23, "convert_to_messag": [24, 94], "chat_format": [24, 29, 33, 94, 95], "chatformat": [24, 29, 33, 95], "load_dataset_kwarg": [24, 26, 29, 31], "multiturn": [24, 94], "foreach": 24, "prepar": [24, 94], "truncat": [24, 26, 31, 33, 67, 68], "encod": [24, 26, 27, 28, 29, 30, 31, 32, 33, 67, 68, 94], "decod": [24, 26, 27, 28, 29, 30, 31, 32, 33, 59, 67, 68, 94], "anyth": [24, 26, 27, 28, 29, 30, 31, 32, 33], "load_dataset": [24, 26, 27, 28, 29, 30, 31, 32, 33, 94], "co": [24, 26, 29, 31, 52, 70, 71, 96], "doc": [24, 26, 29, 31, 69, 72, 76, 82, 83, 85, 87, 96], "en": [24, 26, 29, 31], "package_refer": [24, 26, 29, 31], "loading_method": [24, 26, 29, 31], "text": [24, 67, 68, 94, 95, 96], "extra": [24, 91, 99, 100], "still": [24, 72, 99, 100], "llama3": [24, 43, 44, 45, 46, 47, 75, 90], "where": [24, 25, 27, 28, 30, 32, 54, 59, 64, 67, 95], "unless": 24, "check": [24, 29, 59, 74, 90, 94, 96, 97, 99], "concaten": [25, 67, 95], "sub": [25, 82], "unifi": 25, "were": [25, 94, 97], "simplifi": [25, 99], "simultan": 25, "intern": [25, 72], "aggreg": 25, "transpar": 25, "index": [25, 62, 84, 91, 94, 96], "howev": [25, 91], "constitu": 25, "might": [25, 96], "larg": [25, 64, 100], "comput": [25, 54, 55, 58, 59, 87, 96, 100], "cumul": 25, "maintain": [25, 100], "indic": [25, 69, 94, 95], "deleg": 25, "retriev": [25, 75], "lead": [25, 67], "high": [25, 92, 99], "scale": [25, 38, 39, 40, 45, 46, 48, 51, 64, 99, 100], "consid": 25, "strategi": 25, "stream": [25, 76], "demand": 25, "deriv": [25, 55, 59, 60], "_dataset": 25, "_len": 25, "total": [25, 62, 77, 89, 93, 96, 98, 99], "combin": [25, 95], "_index": 25, "lookup": 25, "dataset1": 25, "mycustomdataset": 25, "params1": 25, "dataset2": 25, "params2": 25, "concat_dataset": 25, "data_point": 25, "1500": 25, "element": [25, 68, 96], "focus": [25, 97], "enhanc": [25, 100], "divers": 25, "machin": [25, 73, 96], "instructtempl": [26, 95], "contribut": [26, 27, 28, 30, 32], "replac": [26, 27, 28, 30, 32, 61, 99], "disabl": [26, 31, 87], "recommend": [26, 27, 28, 31, 82, 94, 96, 100], "highest": [26, 27, 28, 31], "sequenc": [26, 27, 28, 31, 33, 54, 56, 58, 59, 60, 67, 68, 84, 94], "yahma": 27, "codebas": [27, 28, 30, 32, 96], "alpaca_d": [27, 28], "batch_siz": [27, 28, 30, 32, 54, 60, 96], "tatsu": [28, 95], "lab": [28, 95], "conversation_styl": [29, 95], "chatdataset": [29, 33, 94], "made": [29, 31, 58, 96], "friendli": [29, 31, 94], "huggingfaceh4": 29, "no_robot": 29, "chatmlformat": 29, "2096": 29, "accomplish": 29, "liweili": 30, "c4_200m": 30, "variant": [30, 32], "mirror": [30, 32], "llama_recip": [30, 32], "grammar_d": 30, "samsum": [32, 95], "summari": [32, 95], "samsum_d": 32, "_util": 33, "open": [33, 34, 95, 96], "orca": [33, 95], "slimorca": [33, 95], "dedup": [33, 95], "_chat_format": 33, "1024": [33, 95], "doesn": [33, 96], "prescrib": 33, "least": [33, 98, 99], "though": [33, 94], "max": [33, 59, 62, 67, 99], "ds": 33, "10": [33, 84, 96, 98, 100], "351": 33, "82": [33, 96], "391": 33, "221": 33, "220": 33, "193": 33, "12": [33, 91], "471": 33, "gemma": 34, "gemmatransformerdecod": 34, "w": [34, 35, 36, 37, 43, 44, 49, 82, 83, 94, 96, 99, 100], "blog": 34, "technolog": 34, "develop": [34, 100], "transformerdecod": [35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 99], "arxiv": [35, 36, 37, 41, 42, 47, 50, 53, 54, 57, 58], "org": [35, 36, 37, 41, 42, 47, 50, 53, 54, 57, 58, 69, 72, 76, 82, 85, 86, 87, 91], "ab": [35, 36, 37, 41, 42, 47, 50, 53, 58], "2307": [35, 36, 37], "09288": [35, 36, 37], "70b": [36, 39, 43, 45, 98], "lora_attn_modul": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99, 100], "q_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 99, 100], "k_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 99, 100], "v_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 99, 100], "output_proj": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 99, 100], "apply_lora_to_mlp": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99], "apply_lora_to_output": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99], "lora_rank": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99], "lora_alpha": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99], "float": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 54, 57, 62, 64, 80, 81, 82, 83, 99, 100], "16": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 99, 100], "lora_dropout": [38, 39, 40, 41, 42], "05": [38, 39, 40, 41, 42], "quantize_bas": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 64, 100], "lora": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 64, 90, 92, 94, 97, 98], "tloen": [38, 39, 40, 45, 46, 51], "8bb8579e403dc78e37fe81ffbb253c413007323f": [38, 39, 40, 45, 46, 51], "l41": [38, 39, 40, 45, 46, 51], "l43": [38, 39, 40, 45, 46, 51], "linear": [38, 39, 40, 41, 42, 45, 46, 47, 48, 50, 51, 53, 59, 63, 64, 99, 100], "attent": [38, 39, 40, 45, 46, 48, 51, 52, 54, 56, 58, 59, 60, 98, 99, 100], "mlp": [38, 39, 40, 45, 46, 48, 51, 59, 60, 98, 99], "final": [38, 39, 40, 45, 46, 48, 51, 55, 59, 68, 96, 98, 99, 100], "rank": [38, 39, 40, 45, 46, 48, 51, 64, 77, 87, 97, 99, 100], "low": [38, 39, 40, 45, 46, 48, 51, 64, 96, 99, 100], "approxim": [38, 39, 40, 45, 46, 48, 51, 64, 99], "factor": [38, 39, 40, 45, 46, 48, 51, 64, 96], "llama2_70b": 39, "llama2_7b": [40, 99], "qlora": [41, 42, 47, 50, 53, 61, 90, 92, 98, 99], "per": [41, 42, 47, 50, 53, 56, 61, 98, 100], "paper": [41, 42, 47, 50, 53, 99, 100], "2305": [41, 42, 47, 50, 53, 54], "14314": [41, 42, 47, 50, 53], "lora_llama2_13b": 41, "lora_llama2_7b": [42, 99], "llama3_70b": 45, "llama3_8b": [46, 98], "lora_llama3_8b": 47, "announc": 49, "lora_mistral_7b": 50, "phi3": [51, 52, 53], "phi3_mini": 51, "ref": [52, 83], "phi": 52, "128k": 52, "nor": 52, "slide": 52, "window": 52, "lora_phi3_mini": 53, "head_dim": [54, 56, 58, 59], "pos_embed": [54, 99], "kv_cach": 54, "kvcach": [54, 59], "attn_dropout": [54, 59], "head": [54, 56, 58, 59, 98], "queri": [54, 56, 59, 98], "gqa": 54, "introduc": [54, 57, 64, 94, 99, 100], "pdf": [54, 57], "13245v1": 54, "version": [54, 91, 98, 100], "multihead": 54, "mha": [54, 59], "n": [54, 67, 68, 89, 93, 94, 95], "extrem": 54, "share": [54, 95, 96], "mqa": 54, "credit": 54, "document": [54, 69, 75], "lightn": 54, "lit": 54, "lit_gpt": 54, "v": [54, 59, 99], "k": [54, 99], "q": [54, 99], "n_kv_head": 54, "dimens": [54, 56, 58, 59, 64, 98, 99, 100], "calcul": [54, 98], "e": [54, 61, 63, 66, 91, 96, 98, 99, 100], "g": [54, 63, 98, 99, 100], "rotarypositionalembed": [54, 99], "cach": [54, 56, 58, 91], "rope": [54, 58], "dropout": [54, 64, 99, 100], "onto": 54, "scaled_dot_product_attent": 54, "input_po": [54, 58, 59, 60], "seq_length": [54, 60], "seq_len": [54, 58], "bigger": 54, "n_h": [54, 58], "num": [54, 58], "n_kv": 54, "kv": [54, 56, 59], "emb": [54, 59, 60], "h_d": [54, 58], "gate_proj": 55, "down_proj": 55, "up_proj": 55, "silu": 55, "feed": [55, 60], "network": [55, 99, 100], "fed": [55, 94], "multipli": 55, "subclass": [55, 72], "although": [55, 99], "afterward": 55, "former": 55, "regist": [55, 61, 100], "hook": [55, 61, 100], "latter": 55, "max_batch_s": 56, "standalon": 56, "past": 56, "becaus": [56, 59, 94, 96, 98], "expand": 56, "dpython": [56, 61], "ep": 57, "1e": 57, "06": [57, 99], "root": [57, 82, 83], "squar": 57, "1910": 57, "07467": 57, "verfic": [57, 58], "small": [57, 96], "avoid": [57, 61, 87, 100], "divis": 57, "zero": [57, 96, 98], "10000": 58, "rotari": [58, 98], "propos": 58, "2104": 58, "09864": 58, "l450": 58, "upto": 58, "init": [58, 83, 100], "exceed": 58, "freq": 58, "recomput": 58, "geometr": 58, "progress": [58, 97], "rotat": 58, "angl": 58, "bsz": 58, "todo": 58, "effici": [58, 75, 90, 92, 96, 97, 99], "transformerdecoderlay": 59, "norm": [59, 60], "move": 59, "space": 59, "belong": 59, "reduc": [59, 92, 95, 99, 100], "statement": 59, "improv": [59, 75, 96, 98, 99], "readabl": [59, 96], "At": 59, "arang": 59, "prompt_length": 59, "causal_mask": 59, "m_": 59, "seq": 59, "attn": [60, 99, 100], "causalselfattent": [60, 99], "sa_norm": 60, "mlp_norm": 60, "ff": 60, "common_util": 61, "bfloat16": [61, 96, 97, 98, 99], "offload_to_cpu": 61, "nf4": [61, 100], "restor": 61, "higher": [61, 98, 100], "offload": [61, 100], "increas": [61, 62, 98, 99], "peak": [61, 96, 98, 99, 100], "gpu": [61, 96, 97, 98, 99, 100], "usag": [61, 91, 96, 97, 98, 100], "_register_state_dict_hook": 61, "m": [61, 68, 94], "mymodul": 61, "_after_": 61, "nf4tensor": [61, 100], "unquant": [61, 96, 100], "unus": 61, "num_warmup_step": 62, "num_training_step": 62, "num_cycl": 62, "last_epoch": 62, "lambdalr": 62, "rate": [62, 92, 97], "schedul": [62, 85, 97], "linearli": 62, "lr": 62, "decreas": [62, 99, 100], "cosin": 62, "v4": 62, "23": [62, 98], "src": 62, "l104": 62, "warmup": [62, 85], "phase": 62, "wave": 62, "half": 62, "last": 62, "lr_schedul": 62, "peft": [63, 64, 65, 66, 99, 100], "protocol": 63, "adapter_param": [63, 64, 65, 66], "proj": 63, "in_dim": [63, 64, 99, 100], "out_dim": [63, 64, 99, 100], "bia": [63, 64, 99, 100], "loralinear": [63, 99, 100], "alpha": [64, 99, 100], "use_bia": 64, "perturb": 64, "decomposit": [64, 99], "matric": [64, 99, 100], "trainabl": [64, 66, 99, 100], "mapsto": 64, "w_0x": 64, "r": [64, 68, 99], "bax": 64, "probabl": [64, 96], "lora_a": [64, 99, 100], "lora_b": [64, 99, 100], "subset": 65, "get_adapter_param": [66, 99], "sentencepieceprocessor": 67, "pretrain": [67, 68, 94, 97, 99, 100], "non": 67, "spm_model": [67, 94], "tokenized_text": 67, "hello": [67, 94, 96, 98], "world": [67, 77, 96], "add_bo": [67, 68, 94], "add_eo": [67, 68, 94], "31587": 67, "29644": 67, "102": 67, "trim_leading_whitespac": 67, "prefix": 67, "unbatch": 67, "prepend": [67, 68], "bo": [67, 68, 94], "append": [67, 91], "eo": [67, 68, 94], "trim": 67, "whitespac": 67, "underli": [67, 100], "sentencepiec": [67, 98], "s1": 67, "s2": 67, "due": [67, 99, 100], "tokenize_messag": [67, 68, 94, 95], "problem": 67, "slice": 67, "tokenizer_path": 67, "separ": [67, 70, 94, 97, 98, 99, 100], "concat": 67, "1788": 67, "2643": 67, "13": [67, 96, 98, 100], "1792": 67, "9508": 67, "465": 67, "22137": 67, "2933": 67, "join": 67, "attribut": 67, "llama3_tiktoken": 68, "p": [68, 69, 99, 100], "l": 68, "all_special_token": 68, "bos_token": 68, "begin_of_text": [68, 94], "eos_token": 68, "end_of_text": 68, "start_header_id": [68, 94], "end_header_id": [68, 94], "step_id": 68, "eom_id": 68, "eot_id": [68, 94], "python_tag": 68, "identif": 68, "regex": 68, "second": [68, 96, 98, 99, 100], "uniqu": 68, "256": [68, 95, 96, 98], "header": [68, 94], "token_id": 68, "truncate_at_eo": 68, "tokenize_head": 68, "datatyp": [69, 100], "polici": [69, 75, 86], "denot": 69, "boolean": 69, "integ": [69, 84, 87], "auto_wrap_polici": [69, 75, 86], "submodul": 69, "obei": 69, "contract": 69, "get_fsdp_polici": 69, "modules_to_wrap": [69, 75], "min_num_param": 69, "my_fsdp_polici": 69, "recurs": [69, 82], "isinst": 69, "sum": [69, 99], "numel": [69, 99], "1000": 69, "functool": 69, "partial": 69, "stabl": [69, 82, 85, 87, 91], "html": [69, 72, 76, 82, 85, 86, 87], "alia": 69, "few": [70, 95, 98, 99, 100], "0001_of_0003": 70, "0002_of_0003": 70, "preserv": [70, 100], "weight_map": [70, 96], "intermediate_checkpoint": [70, 71], "parit": 70, "_weight_map": 70, "shard": [71, 98], "wip": 71, "argpars": 72, "argumentpars": 72, "builtin": 72, "said": 72, "noth": 72, "treat": [72, 94], "consult": 72, "info": [72, 97], "librari": [72, 76, 87, 90, 92, 100], "parse_known_arg": 72, "namespac": 72, "act": 72, "precid": 72, "parse_arg": 72, "properti": [72, 99], "too": [72, 98], "availab": 73, "distribut": [73, 78, 86, 87, 92, 97, 98], "bf16": [74, 100], "request": [74, 95, 96], "inde": [74, 96], "kernel": 74, "runtimeerror": [74, 78], "float32": 74, "done": [74, 99, 100], "isn": 74, "hardwar": [74, 92, 96, 99], "memory_efficient_fsdp_wrap": 75, "maxim": [75, 90, 92], "been": [75, 98], "workload": 75, "15": [75, 94, 96, 99, 100], "alongsid": 75, "ac": 75, "fullyshardeddataparallel": 75, "const": 75, "fsdppolicytyp": 75, "handler": 76, "aka": 77, "filenam": 80, "log_": 80, "unixtimestamp": 80, "txt": [80, 97], "thread": 80, "safe": 80, "flush": [80, 81, 82, 83], "union": [80, 81, 82, 83, 86, 87], "ndarrai": [80, 81, 82, 83], "scalar": [80, 81, 82, 83], "record": [80, 81, 82, 83], "payload": [80, 81, 82, 83], "organize_log": 82, "tensorboard": 82, "subdirectori": 82, "compar": [82, 96, 99, 100], "logdir": 82, "startup": 82, "tree": [82, 95, 96], "tfevent": 82, "encount": 82, "frontend": 82, "organ": 82, "accordingli": 82, "my_log_dir": 82, "view": [82, 96, 97], "my_metr": [82, 83], "termin": [82, 83], "entiti": 83, "bias": 83, "sent": 83, "usernam": 83, "my_project": 83, "my_ent": 83, "my_group": 83, "importerror": 83, "account": [83, 99, 100], "log_config": 83, "local": [83, 87, 91, 94, 96, 97], "link": [83, 96], "capecap": 83, "6053ofw0": 83, "torchtune_config_j67sb73v": 83, "padding_idx": 84, "ignore_idx": 84, "pad": 84, "longest": 84, "tokenpair": 84, "collat": 84, "token_pair": 84, "torchtune_perf_trac": 85, "contextmanag": 85, "wait": 85, "trace": 85, "speed": [85, 98, 100], "reduct": [85, 99], "acwrappolicytyp": 86, "describ": [86, 95], "author": [86, 92, 97, 100], "intermedi": [86, 98, 100], "fsdp_adavnced_tutori": 86, "debug_mod": 87, "pseudo": 87, "random": [87, 97], "commonli": [87, 96, 99, 100], "numpi": 87, "own": [87, 94, 95, 96, 99], "determinist": 87, "global": 87, "warn": 87, "nondeterminist": 87, "addition": [87, 99], "cudnn": 87, "set_deterministic_debug_mod": 87, "algorithm": 87, "outsid": [87, 96, 98, 99], "generated_examples_python": 88, "zip": 88, "galleri": [88, 93], "sphinx": 88, "000": [89, 93, 98], "execut": [89, 93], "generated_exampl": 89, "mem": [89, 93], "mb": [89, 93], "topic": 90, "gentl": 90, "introduct": 90, "readi": [90, 94], "workflow": [90, 95, 97, 99], "requisit": 91, "proper": [91, 97], "host": [91, 97], "page": [91, 92, 97, 98], "latest": [91, 97, 100], "confirm": 91, "And": [91, 96, 98], "h": 91, "ls": [91, 96, 97, 98], "welcom": 91, "show": [91, 94, 99], "greatest": [91, 97], "contributor": 91, "cd": [91, 96], "even": [91, 94, 98, 99, 100], "commit": 91, "branch": 91, "url": 91, "whl": 91, "therebi": [91, 100], "forc": 91, "reinstal": 91, "opt": [91, 97], "suffix": 91, "cu121": 91, "On": [92, 99], "pointer": 92, "emphas": 92, "aspect": 92, "simplic": 92, "component": 92, "reus": 92, "prove": 92, "democrat": 92, "box": [92, 100], "zoo": 92, "varieti": [92, 99], "techniqu": [92, 96, 97, 99], "integr": [92, 96, 97, 98, 99, 100], "excit": 92, "checkout": 92, "quickstart": 92, "attain": 92, "better": [92, 94, 95, 96], "chekckpoint": 92, "hyperparamet": [92, 97, 99, 100], "embodi": 92, "philosophi": 92, "usabl": 92, "composit": 92, "hard": 92, "outlin": 92, "unecessari": 92, "never": 92, "thoroughli": 92, "unit": 92, "know": [94, 95, 96, 98, 99], "align": 94, "intend": 94, "hi": 94, "nice": 94, "meet": 94, "overhaul": 94, "entir": [94, 100], "sai": [94, 95, 97], "accompani": 94, "who": 94, "influenti": 94, "hip": 94, "hop": 94, "artist": [94, 98], "2pac": 94, "rakim": 94, "c": 94, "na": 94, "flavor": [94, 95], "certain": 94, "msg": 94, "formatted_messag": [94, 95], "nyou": [94, 95], "nwho": 94, "sentencepiecetoken": 94, "why": [94, 97, 99], "user_messag": 94, "518": 94, "25580": 94, "29962": 94, "3532": 94, "14816": 94, "29903": 94, "6778": 94, "piece_to_id": 94, "reserv": [94, 100], "vector": 94, "place": 94, "manual": [94, 100], "529": 94, "29879": 94, "29958": 94, "tiktokentoken": 94, "nhere": 94, "_encode_special_token": 94, "128000": 94, "128009": 94, "part": [94, 100], "pure": 94, "That": 94, "won": [94, 96, 98], "mess": 94, "govern": 94, "prime": 94, "strictli": 94, "summarizetempl": [94, 95], "lightweight": 94, "ask": 94, "untouch": 94, "nsummari": 94, "long": [94, 99], "robust": 94, "enough": 94, "csv": 94, "question": [94, 95, 96, 98], "answer": [94, 96, 98], "onlin": 94, "forum": 94, "panda": 94, "pd": 94, "df": 94, "read_csv": 94, "your_fil": 94, "nrow": 94, "tolist": 94, "row": 94, "iloc": 94, "gp": 94, "receiv": 94, "commun": [94, 96], "satellit": 94, "thing": [94, 100], "message_convert": 94, "input_msg": 94, "output_msg": 94, "assistant_messag": 94, "But": [94, 96, 98, 99], "mistralchatformat": 94, "custom_dataset": 94, "2048": 94, "data_fil": 94, "honor": 94, "copi": [94, 96, 97, 98, 100], "8b_lora_single_devic": [94, 98], "my": [94, 96, 98], "launch": [94, 97], "custom_8b_lora_single_devic": 94, "steer": 95, "wheel": 95, "publicli": 95, "great": [95, 96], "iter": [95, 100], "knob": 95, "tweak": 95, "footprint": [95, 99], "could": [95, 99], "achiev": [95, 96, 98, 99, 100], "concatdataset": 95, "instruct_dataset": 95, "vicgal": 95, "gpt4": 95, "alpacainstructtempl": 95, "demonstr": 95, "fix": 95, "goal": 95, "agnost": 95, "condit": 95, "respond": 95, "further": [95, 99, 100], "classifi": 95, "anim": 95, "plant": 95, "miner": 95, "oak": 95, "copper": 95, "ore": 95, "eleph": 95, "mydataset": 95, "onthehub": 95, "customtempl": 95, "similar": [95, 96, 98, 99, 100], "quit": [95, 100], "similarli": 95, "chat_dataset": 95, "incorpor": 95, "advanc": 95, "preferencedataset": 95, "rlhf": 95, "adjust": 95, "chosen": 95, "reject": 95, "chosen_messag": 95, "transformed_sampl": 95, "key_chosen": 95, "rejected_messag": 95, "key_reject": 95, "chosen_input_id": 95, "c_mask": 95, "chosen_label": 95, "np": 95, "cross_entropy_ignore_idx": 95, "rejected_input_id": 95, "r_mask": 95, "rejected_label": 95, "purpos": [95, 97, 98], "stack_exchanged_paired_dataset": 95, "had": 95, "lvwerra": 95, "stack": 95, "exchang": 95, "stackexchangedpairedtempl": 95, "response_j": 95, "response_k": 95, "data_dir": 95, "rl": 95, "favorit": [96, 98, 99], "seemlessli": 96, "beyond": [96, 100], "connect": 96, "larger": [96, 98], "amount": 96, "natur": 96, "export": 96, "mobil": 96, "phone": 96, "leverag": [96, 98, 100], "mode": 96, "lot": 96, "plai": 96, "freez": [96, 99], "percentag": 96, "learnabl": 96, "keep": [96, 99], "16gb": [96, 99], "rtx": 96, "3090": 96, "4090": 96, "hour": 96, "full_finetune_single_devic": [96, 97], "7b_full_low_memori": [96, 97], "full_finetune_distribut": [96, 97], "7b_full": [96, 97], "13b_full": [96, 97], "7b_qlora_single_devic": [96, 97, 100], "473": 96, "98": [96, 100], "gb": [96, 98, 99, 100], "50": 96, "484": 96, "01": [96, 97], "fact": [96, 98, 99], "third": 96, "smaller": [96, 98, 99, 100], "realli": 96, "eleuther_ev": [96, 98], "eleuther_evalu": [96, 98], "lm_eval": [96, 98], "plan": 96, "custom_eval_config": [96, 98], "truthfulqa_mc2": [96, 98, 99], "measur": [96, 98], "propens": [96, 98], "shot": [96, 98], "accuraci": [96, 98, 99, 100], "baselin": [96, 99], "324": 96, "loglikelihood": 96, "195": 96, "121": 96, "27": 96, "197": 96, "acc": 96, "388": 96, "38": 96, "shown": 96, "489": 96, "48": [96, 100], "seem": 96, "custom_generation_config": [96, 98], "kick": 96, "top_k": 96, "300": 96, "temperatur": 96, "interest": 96, "site": 96, "visit": 96, "bai": 96, "area": 96, "92": [96, 98], "exploratorium": 96, "san": 96, "francisco": 96, "magazin": 96, "awesom": 96, "bridg": 96, "pretti": 96, "cool": 96, "96": [96, 100], "61": 96, "sec": [96, 98], "25": 96, "83": 96, "99": [96, 99], "72": 96, "littl": 96, "saw": 96, "took": [96, 98], "torchao": [96, 98, 100], "bit": [96, 98, 99, 100], "custom_quantization_config": [96, 98], "68": 96, "19": [96, 98, 100], "76": 96, "69": 96, "95": [96, 98], "67": 96, "4w": [96, 98], "unlik": [96, 98], "engin": [96, 98], "fullmodeltorchtunecheckpoint": [96, 98], "int4weightonlyquant": [96, 98], "groupsiz": [96, 98], "did": [96, 98, 100], "park": 96, "sit": 96, "top": [96, 100], "hill": 96, "beauti": 96, "62": [96, 98], "17": [96, 99], "85": 96, "compil": [96, 98, 100], "hood": [96, 100], "sped": 96, "almost": [96, 98, 99], "3x": [96, 98], "benefit": 96, "yet": 96, "fast": 96, "clone": [96, 99, 100], "assumpt": 96, "satisfi": 96, "new_dir": 96, "output_dict": 96, "sd_1": 96, "sd_2": 96, "dump": 96, "convert_hf_checkpoint": 96, "checkpoint_path": 96, "justin": 96, "school": 96, "math": 96, "teacher": 96, "ws": 96, "94": [96, 98], "103": 96, "28": 96, "bandwidth": [96, 98], "1391": 96, "84": 96, "thats": 96, "seamlessli": 96, "authent": [96, 97], "hopefulli": 96, "gave": 96, "gate": 97, "grant": 97, "minut": 97, "agreement": 97, "altern": 97, "hackabl": 97, "singularli": 97, "technic": 97, "depth": 97, "principl": 97, "minim": [97, 99, 100], "boilerpl": 97, "hold": 97, "substanti": [97, 99], "custom_config": 97, "replic": 97, "lorafinetunerecipesingledevic": 97, "lora_finetune_output": 97, "log_1713194212": 97, "sampler": 97, "52": 97, "3697006702423096": 97, "25880": [97, 100], "24": [97, 98], "55": 97, "83it": 97, "monitor": 97, "tqdm": 97, "interv": 97, "e2": 97, "releas": 98, "focu": 98, "128": [98, 99], "theta": 98, "gain": 98, "illustr": 98, "basic": 98, "observ": 98, "18": 98, "consum": [98, 100], "vram": [98, 99], "overal": 98, "nproc_per_nod": [98, 99], "lora_finetune_distribut": [98, 99], "8b_lora": 98, "8b_qlora_single_devic": 98, "alloc": [98, 100], "coupl": [98, 99, 100], "llama3_token": 98, "122": 98, "sarah": 98, "busi": 98, "mum": 98, "young": 98, "children": 98, "live": 98, "north": 98, "east": 98, "england": 98, "135": 98, "88": 98, "138": 98, "346": 98, "09": 98, "139": 98, "31": 98, "far": 98, "drill": 98, "90": 98, "93": 98, "91": 98, "104": 98, "four": [98, 99], "again": 98, "jake": 98, "disciplin": 98, "passion": 98, "draw": 98, "paint": 98, "57": [98, 99, 100], "speedup": 98, "broader": 98, "teach": 99, "straight": 99, "jump": 99, "neural": [99, 100], "unfamiliar": 99, "oppos": [99, 100], "momentum": 99, "adamw": 99, "arbitrari": 99, "relat": 99, "aghajanyan": 99, "et": 99, "al": 99, "hypothes": 99, "intrins": 99, "lower": 99, "down": [99, 100], "often": 99, "eight": 99, "practic": 99, "imag": 99, "left": 99, "blue": 99, "rememb": 99, "approx": 99, "15m": 99, "8192": 99, "65k": 99, "requires_grad": [99, 100], "frozen_out": [99, 100], "lora_out": [99, 100], "omit": 99, "base_model": 99, "choos": 99, "lora_model": 99, "lora_llama_2_7b": [99, 100], "alon": 99, "in_featur": 99, "out_featur": 99, "inplac": 99, "feel": 99, "free": 99, "strict": 99, "whenev": 99, "validate_state_dict_for_lora": 99, "peft_util": 99, "set_trainable_param": 99, "fetch": 99, "lora_param": 99, "total_param": 99, "trainable_param": 99, "2f": 99, "6742609920": 99, "4194304": 99, "nnode": 99, "7b_lora": 99, "my_model_checkpoint_path": [99, 100], "tokenizer_checkpoint": [99, 100], "my_tokenizer_checkpoint_path": [99, 100], "constraint": 99, "factori": 99, "benefici": 99, "impact": 99, "rel": 99, "minor": 99, "good": 99, "64": 99, "lora_experiment_1": 99, "smooth": [99, 100], "curv": [99, 100], "500": 99, "ran": 99, "commod": 99, "cogniz": 99, "ax": 99, "parallel": 99, "truthfulqa": 99, "previous": 99, "475": 99, "87": 99, "508": 99, "86": 99, "504": 99, "04": 99, "514": 99, "lowest": 99, "absolut": 99, "4gb": 99, "tradeoff": 99, "potenti": 99, "highli": 100, "vanilla": 100, "held": 100, "therefor": 100, "bespok": 100, "normalfloat": 100, "8x": 100, "retain": 100, "vast": 100, "major": 100, "highlight": 100, "degrad": 100, "normatfloat": 100, "doubl": 100, "themselv": 100, "prune": 100, "deepdiv": 100, "idea": 100, "distinct": 100, "storag": 100, "de": 100, "incur": 100, "counterpart": 100, "set_default_devic": 100, "qlora_linear": 100, "memory_alloc": 100, "177": 100, "152": 100, "byte": 100, "del": 100, "empty_cach": 100, "lora_linear": 100, "081": 100, "344": 100, "qlora_llama2_7b": 100, "qlora_model": 100, "essenti": 100, "reparametrize_as_dtype_state_dict_post_hook": 100, "stat": 100, "against": 100, "35": 100, "40": 100, "29": 100, "slow": 100, "slower": 100, "149": 100, "9157477021217346": 100, "02": 100, "08": 100, "14": 100, "15it": 100, "nightli": 100, "200": 100, "hundr": 100, "228": 100, "8158286809921265": 100, "59": 100, "95it": 100, "exercis": 100, "portion": 100, "augment": 100, "linear_nf4": 100, "to_nf4": 100, "linear_weight": 100, "autograd": 100, "regular": 100, "incom": 100}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "Message"], [20, 1, 1, "", "MistralChatFormat"], [21, 1, 1, "", "SummarizeTemplate"], [22, 0, 1, "", "sharegpt_to_llama2_messages"], [23, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.Message": [[19, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[20, 2, 1, "", "format"], [20, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[21, 2, 1, "", "format"]], "torchtune.datasets": [[24, 1, 1, "", "ChatDataset"], [25, 1, 1, "", "ConcatDataset"], [26, 1, 1, "", "InstructDataset"], [27, 0, 1, "", "alpaca_cleaned_dataset"], [28, 0, 1, "", "alpaca_dataset"], [29, 0, 1, "", "chat_dataset"], [30, 0, 1, "", "grammar_dataset"], [31, 0, 1, "", "instruct_dataset"], [32, 0, 1, "", "samsum_dataset"], [33, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[34, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[35, 0, 1, "", "llama2_13b"], [36, 0, 1, "", "llama2_70b"], [37, 0, 1, "", "llama2_7b"], [38, 0, 1, "", "lora_llama2_13b"], [39, 0, 1, "", "lora_llama2_70b"], [40, 0, 1, "", "lora_llama2_7b"], [41, 0, 1, "", "qlora_llama2_13b"], [42, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[43, 0, 1, "", "llama3_70b"], [44, 0, 1, "", "llama3_8b"], [45, 0, 1, "", "lora_llama3_70b"], [46, 0, 1, "", "lora_llama3_8b"], [47, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[48, 0, 1, "", "lora_mistral_7b"], [49, 0, 1, "", "mistral_7b"], [50, 0, 1, "", "qlora_mistral_7b"]], "torchtune.models.phi3": [[51, 0, 1, "", "lora_phi3_mini"], [52, 0, 1, "", "phi3_mini"], [53, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[54, 1, 1, "", "CausalSelfAttention"], [55, 1, 1, "", "FeedForward"], [56, 1, 1, "", "KVCache"], [57, 1, 1, "", "RMSNorm"], [58, 1, 1, "", "RotaryPositionalEmbeddings"], [59, 1, 1, "", "TransformerDecoder"], [60, 1, 1, "", "TransformerDecoderLayer"], [62, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[54, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[55, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[57, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[58, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[59, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[60, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[61, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[63, 1, 1, "", "AdapterModule"], [64, 1, 1, "", "LoRALinear"], [65, 0, 1, "", "get_adapter_params"], [66, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[63, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[64, 2, 1, "", "adapter_params"], [64, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[67, 1, 1, "", "SentencePieceTokenizer"], [68, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[67, 2, 1, "", "decode"], [67, 2, 1, "", "encode"], [67, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[68, 2, 1, "", "decode"], [68, 2, 1, "", "encode"], [68, 2, 1, "", "tokenize_message"], [68, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[69, 4, 1, "", "FSDPPolicyType"], [70, 1, 1, "", "FullModelHFCheckpointer"], [71, 1, 1, "", "FullModelMetaCheckpointer"], [72, 1, 1, "", "TuneRecipeArgumentParser"], [73, 0, 1, "", "get_device"], [74, 0, 1, "", "get_dtype"], [75, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [76, 0, 1, "", "get_logger"], [77, 0, 1, "", "get_world_size_and_rank"], [78, 0, 1, "", "init_distributed"], [79, 0, 1, "", "list_dtypes"], [84, 0, 1, "", "padded_collate"], [85, 0, 1, "", "profiler"], [86, 0, 1, "", "set_activation_checkpointing"], [87, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[70, 2, 1, "", "load_checkpoint"], [70, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[71, 2, 1, "", "load_checkpoint"], [71, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[72, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[80, 1, 1, "", "DiskLogger"], [81, 1, 1, "", "StdoutLogger"], [82, 1, 1, "", "TensorBoardLogger"], [83, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[80, 2, 1, "", "close"], [80, 2, 1, "", "log"], [80, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[81, 2, 1, "", "close"], [81, 2, 1, "", "log"], [81, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[82, 2, 1, "", "close"], [82, 2, 1, "", "log"], [82, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[83, 2, 1, "", "close"], [83, 2, 1, "", "log"], [83, 2, 1, "", "log_config"], [83, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 69, 90, 92, 96, 98, 99, 100], "config": [0, 7, 8, 97], "data": [1, 5, 94], "instruct": [1, 91, 95, 98], "templat": [1, 94, 95], "chat": [1, 94, 95], "format": [1, 6, 95], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 94, 95], "exampl": 2, "gener": [2, 96, 98], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 96, 97, 98, 99], "llama3": [3, 94, 98], "llama2": [3, 94, 96, 99, 100], "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 91, 100], "block": 4, "token": [4, 94], "peft": 4, "util": [4, 5, 69], "checkpoint": [5, 6, 9, 96], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 99, 100], "manag": 5, "perform": [5, 99], "profil": [5, 85], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 92, 96], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 96, 99, 100], "put": [6, 100], "thi": 6, "all": [6, 7, 100], "togeth": [6, 100], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 95], "us": [7, 8, 94, 96, 100], "instanti": [7, 10], "referenc": 7, "other": [7, 96], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 96, 97], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 92, 99, 100], "ar": 8, "recip": [8, 97, 99], "script": 8, "run": [8, 96], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "messag": 19, "mistralchatformat": 20, "summarizetempl": 21, "sharegpt_to_llama2_messag": 22, "validate_messag": 23, "chatdataset": 24, "concatdataset": 25, "instructdataset": 26, "alpaca_cleaned_dataset": 27, "alpaca_dataset": 28, "chat_dataset": 29, "grammar_dataset": 30, "instruct_dataset": 31, "samsum_dataset": 32, "slimorca_dataset": 33, "gemma_2b": 34, "llama2_13b": 35, "llama2_70b": 36, "llama2_7b": 37, "lora_llama2_13b": 38, "lora_llama2_70b": 39, "lora_llama2_7b": 40, "qlora_llama2_13b": 41, "qlora_llama2_7b": 42, "llama3_70b": 43, "llama3_8b": 44, "lora_llama3_70b": 45, "lora_llama3_8b": 46, "qlora_llama3_8b": 47, "lora_mistral_7b": 48, "mistral_7b": 49, "qlora_mistral_7b": 50, "lora_phi3_mini": 51, "phi3_mini": 52, "qlora_phi3_mini": 53, "causalselfattent": 54, "todo": [54, 60], "feedforward": 55, "kvcach": 56, "rmsnorm": 57, "rotarypositionalembed": 58, "transformerdecod": 59, "transformerdecoderlay": 60, "reparametrize_as_dtype_state_dict_post_hook": 61, "get_cosine_schedule_with_warmup": 62, "adaptermodul": 63, "loralinear": 64, "get_adapter_param": 65, "set_trainable_param": 66, "sentencepiecetoken": 67, "tiktokentoken": 68, "fsdppolicytyp": 69, "fullmodelhfcheckpoint": 70, "fullmodelmetacheckpoint": 71, "tunerecipeargumentpars": 72, "get_devic": 73, "get_dtyp": 74, "get_full_finetune_fsdp_wrap_polici": 75, "get_logg": 76, "get_world_size_and_rank": 77, "init_distribut": 78, "list_dtyp": 79, "disklogg": 80, "stdoutlogg": 81, "tensorboardlogg": 82, "wandblogg": 83, "padded_col": 84, "set_activation_checkpoint": 86, "set_se": 87, "comput": [89, 93], "time": [89, 93], "welcom": 90, "document": 90, "get": [90, 98], "start": 90, "tutori": 90, "instal": 91, "via": [91, 98], "pypi": 91, "git": 91, "clone": 91, "nightli": 91, "kei": 92, "concept": 92, "design": 92, "principl": 92, "fine": [94, 95, 97, 98], "tune": [94, 95, 97, 98], "chang": 94, "from": [94, 100], "prompt": 94, "special": 94, "when": 94, "should": 94, "i": 94, "custom": [94, 95], "fulli": 95, "end": 96, "workflow": 96, "download": [96, 97], "7b": 96, "finetun": [96, 99, 100], "evalu": [96, 98], "eleutherai": [96, 98], "s": [96, 98], "eval": [96, 98], "har": [96, 98], "speed": 96, "up": 96, "quantiz": [96, 98], "librari": 96, "upload": 96, "hug": 96, "face": 96, "hub": 96, "first": 97, "llm": 97, "select": 97, "modifi": 97, "train": 97, "next": 97, "step": 97, "meta": 98, "8b": 98, "access": 98, "text": 98, "our": 98, "faster": 98, "how": 99, "doe": 99, "work": 99, "appli": 99, "trade": 99, "off": 99, "qlora": 100, "save": 100, "deep": 100, "dive": 100}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})