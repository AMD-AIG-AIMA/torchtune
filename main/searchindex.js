Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.DPOLoss", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.modules.tokenizers.Tokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.padded_collate_dpo", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_classifier_7b.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.DPOLoss.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.modules.tokenizers.Tokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.padded_collate_dpo.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "StackExchangedPairedTemplate", "SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_tokenizer", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "lora_mistral_7b", "lora_mistral_classifier_7b", "mistral_7b", "mistral_classifier_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_classifier_7b", "lora_phi3_mini", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "DPOLoss", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "SentencePieceTokenizer", "TikTokenTokenizer", "Tokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "padded_collate_dpo", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 14, 16, 18, 20, 21, 22, 30, 31, 32, 34, 35, 39, 43, 81, 86, 87, 144, 148, 149, 152, 154, 155], "prompt": [1, 14, 15, 17, 18, 19, 21, 22, 23, 24, 25, 28, 30, 32, 34, 35, 36, 38, 39, 40, 41, 43, 94, 106, 117, 150, 151, 153], "chat": [1, 2, 15, 16, 19, 20, 24, 25, 28, 36, 41, 87], "includ": [1, 6, 7, 8, 15, 18, 87, 100, 110, 111, 115, 146, 148, 149, 150, 151, 152, 153, 154, 155], "some": [1, 6, 7, 16, 102, 103, 144, 146, 148, 149, 150, 151, 152, 154, 155], "specif": [1, 7, 8, 10, 120, 149, 150, 151, 155], "format": [1, 2, 5, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 28, 30, 32, 34, 35, 36, 39, 41, 108, 110, 111, 112, 113, 148, 149, 151, 152, 153, 154], "differ": [1, 7, 9, 28, 29, 30, 32, 106, 113, 134, 141, 146, 148, 149, 151, 153, 154, 155], "dataset": [1, 5, 7, 14, 17, 18, 20, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 146, 152, 153], "model": [1, 2, 6, 7, 8, 10, 16, 21, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 98, 99, 100, 101, 102, 103, 104, 105, 110, 111, 112, 113, 116, 117, 120, 122, 128, 135, 136, 144, 146, 149, 150, 155], "from": [1, 2, 3, 6, 7, 8, 9, 10, 14, 17, 18, 19, 20, 22, 23, 25, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 54, 55, 61, 62, 63, 80, 81, 90, 94, 95, 97, 99, 102, 105, 106, 110, 111, 112, 114, 115, 116, 117, 131, 132, 135, 143, 145, 147, 148, 150, 151, 152, 153, 154], "common": [1, 2, 4, 7, 148, 149, 150, 153, 154], "json": [1, 6, 24, 25, 87, 110, 148, 150, 151], "messag": [1, 15, 16, 19, 21, 24, 25, 27, 28, 36, 87, 106, 107, 108, 145, 148, 149, 150], "miscellan": 1, "function": [1, 7, 8, 10, 12, 28, 89, 90, 96, 98, 101, 104, 105, 109, 110, 117, 118, 124, 128, 134, 138, 146, 149, 150, 155], "us": [1, 2, 4, 6, 9, 10, 12, 16, 19, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 89, 90, 92, 93, 94, 95, 96, 98, 101, 104, 106, 107, 109, 110, 111, 113, 114, 115, 117, 118, 119, 120, 122, 128, 129, 130, 131, 132, 138, 144, 145, 146, 148, 150, 152, 153, 154], "modifi": [1, 7, 8, 9, 96, 146, 151, 153, 154, 155], "For": [2, 5, 6, 7, 8, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 43, 44, 89, 94, 110, 115, 116, 123, 132, 136, 138, 145, 148, 149, 150, 151, 152, 153, 154, 155], "detail": [2, 6, 36, 41, 87, 91, 109, 120, 128, 138, 148, 151, 152, 153, 154, 155], "usag": [2, 96, 113, 114, 139, 145, 148, 150, 151, 152, 153, 155], "guid": [2, 7, 9, 146, 149, 150, 152, 154], "pleas": [2, 5, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 109, 120, 128, 136, 145, 155], "see": [2, 5, 6, 9, 19, 21, 36, 41, 44, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 87, 88, 91, 99, 109, 113, 115, 120, 121, 128, 132, 136, 138, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "our": [2, 6, 8, 146, 149, 150, 151, 152, 154, 155], "tutori": [2, 6, 136, 146, 149, 150, 151, 152, 153, 154, 155], "support": [2, 6, 8, 9, 10, 21, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 86, 87, 89, 100, 111, 112, 114, 119, 122, 146, 148, 149, 150, 151, 152, 153, 154, 155], "sever": 2, "wide": 2, "help": [2, 6, 19, 94, 110, 115, 144, 145, 146, 148, 149, 150, 151, 152, 153, 155], "quickli": [2, 7, 33, 149, 150], "bootstrap": 2, "your": [2, 5, 9, 10, 14, 17, 22, 23, 28, 33, 131, 132, 144, 145, 146, 148, 149, 150, 153, 154, 155], "fine": [2, 6, 8, 9, 31, 144, 146, 151, 154], "tune": [2, 3, 6, 7, 8, 9, 12, 31, 144, 145, 146, 148, 151, 154, 155], "also": [2, 6, 7, 8, 9, 10, 36, 39, 43, 87, 89, 94, 100, 118, 120, 122, 128, 132, 145, 148, 149, 150, 151, 152, 153, 154, 155], "like": [2, 6, 7, 8, 9, 28, 87, 112, 145, 148, 149, 150, 151, 152, 154], "These": [2, 4, 6, 7, 8, 10, 31, 115, 149, 150, 151, 152, 153, 154, 155], "ar": [2, 4, 6, 7, 9, 10, 14, 17, 18, 19, 20, 21, 22, 23, 27, 30, 31, 32, 34, 35, 36, 38, 39, 40, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 94, 100, 101, 104, 105, 109, 110, 111, 113, 114, 116, 117, 119, 122, 126, 128, 134, 139, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "especi": [2, 146, 148, 151], "specifi": [2, 6, 7, 8, 10, 36, 89, 94, 95, 98, 109, 117, 120, 123, 128, 132, 136, 139, 148, 149, 150, 151, 152, 153, 155], "yaml": [2, 7, 8, 10, 11, 12, 36, 39, 43, 115, 132, 146, 148, 149, 150, 151, 152, 153, 154, 155], "config": [2, 6, 9, 10, 11, 12, 13, 36, 39, 43, 89, 104, 110, 114, 115, 132, 139, 146, 149, 150, 151, 153, 154, 155], "represent": [2, 154, 155], "abov": [2, 6, 96, 126, 145, 151, 153, 154, 155], "all": [3, 4, 8, 13, 28, 29, 31, 36, 89, 90, 94, 96, 101, 107, 110, 114, 115, 116, 126, 135, 141, 142, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154], "famili": [3, 8, 34, 35, 37, 41, 42, 44, 113, 146, 148, 153], "download": [3, 6, 142, 145, 149, 150, 153, 154, 155], "meta": [3, 6, 19, 110, 111, 148, 149, 151, 152], "8b": [3, 72, 75, 77, 85, 148, 149], "hf": [3, 6, 98, 110, 148, 149, 151, 152, 153], "token": [3, 6, 7, 8, 20, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 56, 64, 73, 82, 87, 89, 93, 94, 95, 106, 107, 117, 120, 133, 148, 150, 151, 152, 153, 154, 155], "access_token": 3, "pre": [3, 19, 31, 145, 149, 150], "train": [3, 5, 6, 8, 9, 19, 28, 29, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 89, 93, 94, 95, 96, 97, 110, 111, 112, 119, 122, 128, 139, 144, 146, 148, 149, 150, 151, 153, 154, 155], "can": [3, 4, 6, 7, 8, 9, 10, 13, 20, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 43, 44, 92, 93, 101, 106, 109, 110, 113, 115, 120, 128, 131, 132, 136, 139, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "hug": [3, 6, 16, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 97, 146, 148, 152, 153], "face": [3, 6, 16, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 97, 146, 148, 152, 153], "hub": [3, 6, 148, 150, 152], "follow": [3, 6, 8, 24, 25, 28, 31, 89, 97, 112, 113, 114, 126, 132, 139, 144, 145, 148, 150, 151, 152, 153, 154, 155], "command": [3, 8, 9, 115, 145, 148, 149, 150, 151, 152, 153, 154, 155], "2": [3, 6, 9, 27, 31, 41, 89, 106, 110, 111, 133, 134, 137, 138, 139, 140, 149, 151, 152, 153, 154], "7b": [3, 6, 30, 32, 33, 34, 35, 37, 39, 43, 44, 47, 50, 53, 55, 58, 63, 67, 70, 78, 79, 80, 81, 110, 111, 149, 152, 153, 154, 155], "codellama": 3, "mini": [3, 85, 86, 87, 88], "microsoft": [3, 86, 87], "4k": [3, 86, 87], "hf_token": 3, "ignor": [3, 6, 87, 89, 90, 148], "pattern": [3, 107, 148], "ai": [3, 80, 89, 132, 149, 153], "mistralai": [3, 148], "v0": 3, "1": [3, 6, 8, 31, 41, 89, 94, 97, 98, 106, 107, 111, 113, 117, 126, 131, 132, 133, 134, 137, 138, 148, 149, 151, 152, 153, 154, 155], "size": [3, 6, 8, 10, 34, 35, 38, 40, 89, 91, 92, 93, 94, 124, 126, 146, 148, 150, 151, 152, 153, 154], "2b": [3, 54, 57], "googl": [3, 54, 55], "offer": 5, "allow": [5, 29, 104, 131, 148, 155], "seamless": 5, "transit": 5, "between": [5, 6, 110, 113, 150, 151, 153, 154, 155], "interoper": [5, 6, 8, 146, 151, 155], "rest": [5, 149, 155], "ecosystem": [5, 6, 8, 146, 151, 153, 155], "comprehens": 5, "overview": [5, 7, 9, 144, 152, 154, 155], "deep": [5, 6, 7, 8, 9, 146, 152, 153], "dive": [5, 6, 7, 8, 9, 146, 152, 153], "enabl": [5, 7, 8, 9, 29, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 100, 138, 139, 153, 154, 155], "work": [5, 6, 8, 115, 146, 148, 151, 153, 155], "set": [5, 6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 43, 44, 93, 94, 101, 103, 109, 118, 120, 126, 128, 136, 137, 138, 139, 146, 148, 149, 151, 152, 153, 154], "consumpt": [5, 29], "dure": [5, 6, 29, 30, 31, 34, 35, 38, 40, 89, 91, 93, 94, 95, 96, 122, 149, 151, 153, 154, 155], "provid": [5, 6, 7, 8, 10, 14, 16, 21, 26, 28, 29, 30, 31, 32, 41, 94, 101, 112, 115, 118, 120, 132, 139, 146, 148, 149, 150, 151, 152, 153], "debug": [5, 6, 7, 8, 148], "finetun": [5, 6, 7, 8, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 144, 146, 152, 153], "job": [5, 9, 138, 152], "variou": [5, 18], "walk": [6, 8, 131, 146, 149, 150, 151, 152, 155], "you": [6, 7, 8, 9, 10, 18, 19, 23, 28, 30, 32, 33, 34, 35, 37, 39, 43, 44, 113, 115, 117, 131, 132, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "through": [6, 7, 8, 9, 90, 101, 146, 148, 149, 150, 151, 152, 155], "design": [6, 8], "behavior": [6, 128, 149, 150], "associ": [6, 7, 8, 117, 151, 154], "util": [6, 7, 8, 9, 10, 29, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 146, 151, 152, 153, 155], "what": [6, 7, 9, 19, 21, 22, 33, 38, 40, 144, 149, 150, 151, 152, 153], "cover": [6, 7, 8, 9, 149, 151, 155], "how": [6, 7, 8, 9, 23, 109, 136, 144, 148, 149, 150, 151, 152, 153, 155], "we": [6, 7, 8, 9, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 89, 91, 93, 94, 100, 106, 110, 111, 112, 117, 119, 123, 128, 135, 146, 148, 149, 150, 151, 152, 153, 154, 155], "them": [6, 7, 28, 29, 30, 32, 39, 90, 96, 106, 148, 149, 150, 151, 154, 155], "scenario": [6, 29], "full": [6, 7, 8, 36, 39, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 104, 105, 106, 146, 148, 150, 153, 154], "compos": 6, "compon": [6, 8, 13, 134, 146, 150, 152, 154, 155], "which": [6, 7, 8, 29, 30, 31, 33, 34, 35, 38, 40, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 89, 93, 94, 95, 97, 104, 105, 106, 110, 111, 112, 114, 119, 129, 132, 136, 146, 148, 149, 150, 151, 152, 153, 154, 155], "plug": 6, "ani": [6, 7, 8, 10, 12, 13, 14, 17, 18, 22, 23, 24, 25, 26, 28, 30, 32, 33, 36, 37, 39, 43, 44, 96, 102, 103, 104, 105, 106, 110, 111, 112, 114, 117, 125, 128, 138, 141, 148, 149, 150, 151, 152, 153, 154], "recip": [6, 7, 9, 10, 11, 12, 90, 104, 110, 111, 112, 146, 149, 150, 151, 153, 155], "evalu": [6, 8, 144, 146, 152, 154, 155], "gener": [6, 8, 14, 17, 22, 23, 28, 30, 31, 32, 37, 41, 101, 106, 137, 138, 139, 142, 144, 149, 150, 154, 155], "each": [6, 8, 15, 18, 29, 31, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 89, 93, 94, 95, 98, 104, 105, 106, 107, 134, 138, 139, 146, 148, 150, 151, 152, 153, 154], "make": [6, 7, 8, 9, 89, 95, 146, 148, 151, 152, 153, 154, 155], "easi": [6, 8, 146, 150, 154], "understand": [6, 7, 8, 144, 146, 149, 150, 154, 155], "extend": [6, 8, 146], "befor": [6, 27, 30, 31, 32, 89, 94, 95, 100, 110, 148, 151], "let": [6, 7, 9, 148, 149, 150, 151, 152, 153, 154, 155], "s": [6, 7, 8, 9, 10, 12, 14, 15, 16, 19, 21, 24, 25, 27, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 48, 49, 50, 65, 66, 67, 74, 75, 78, 79, 85, 87, 89, 91, 93, 94, 95, 96, 98, 99, 102, 104, 105, 107, 109, 110, 111, 114, 118, 120, 122, 128, 131, 136, 137, 146, 148, 149, 150, 152, 154, 155], "defin": [6, 7, 8, 90, 99, 100, 102, 150, 152, 154], "concept": [6, 151, 152], "In": [6, 7, 8, 28, 93, 100, 109, 128, 131, 132, 149, 151, 153, 154, 155], "ll": [6, 7, 8, 107, 117, 123, 146, 149, 150, 151, 152, 153, 155], "talk": 6, "about": [6, 8, 98, 132, 146, 148, 149, 151, 152, 153, 154, 155], "take": [6, 7, 8, 10, 90, 91, 96, 110, 112, 115, 118, 134, 149, 150, 151, 152, 153, 154, 155], "close": [6, 8, 129, 130, 131, 132, 154], "look": [6, 7, 8, 116, 131, 145, 149, 150, 151, 152, 153, 154], "veri": [6, 29, 94, 148, 151], "simpli": [6, 7, 31, 148, 149, 150, 151, 153, 155], "dictat": 6, "state_dict": [6, 96, 104, 110, 111, 112, 113, 114, 154, 155], "store": [6, 29, 129, 132, 154, 155], "file": [6, 7, 8, 9, 10, 11, 12, 106, 107, 110, 111, 112, 115, 129, 132, 139, 143, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155], "disk": [6, 33, 129], "weight": [6, 8, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 96, 99, 100, 104, 110, 111, 112, 113, 123, 128, 132, 144, 148, 149, 151, 152, 153, 154, 155], "string": [6, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 99, 106, 107, 108, 118, 119, 123, 148, 150], "kei": [6, 7, 9, 14, 17, 22, 23, 24, 28, 30, 32, 39, 89, 91, 94, 95, 103, 104, 105, 110, 112, 114, 134, 139, 148, 151, 152, 154, 155], "identifi": 6, "state": [6, 8, 96, 102, 103, 104, 105, 110, 111, 112, 114, 116, 151, 153, 154, 155], "dict": [6, 7, 8, 9, 10, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 36, 37, 39, 43, 44, 96, 102, 103, 104, 105, 110, 111, 112, 114, 116, 122, 125, 127, 133, 134, 135, 150], "If": [6, 7, 13, 14, 17, 18, 21, 22, 23, 24, 26, 27, 28, 30, 32, 34, 35, 38, 39, 40, 41, 89, 93, 94, 95, 96, 98, 100, 105, 110, 111, 112, 113, 114, 117, 118, 119, 120, 122, 123, 125, 131, 132, 138, 141, 145, 148, 149, 150, 151, 152, 153, 154], "don": [6, 7, 8, 132, 138, 148, 149, 150, 151, 152, 153, 155], "t": [6, 7, 8, 107, 119, 132, 138, 148, 149, 150, 151, 152, 153, 155], "match": [6, 28, 30, 32, 39, 105, 145, 148, 150, 151, 153, 154], "up": [6, 8, 9, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 116, 139, 148, 149, 150, 152, 153, 154, 155], "exactli": [6, 105], "those": [6, 113, 154], "definit": [6, 154], "either": [6, 105, 110, 117, 136, 148, 154, 155], "run": [6, 7, 9, 12, 90, 91, 94, 96, 110, 111, 112, 114, 116, 126, 131, 132, 135, 145, 146, 149, 150, 152, 153, 154, 155], "explicit": 6, "error": [6, 7, 27, 110, 138, 148], "load": [6, 8, 28, 29, 30, 31, 32, 33, 104, 110, 111, 112, 114, 115, 131, 149, 150, 151, 153, 154], "rais": [6, 10, 13, 21, 24, 27, 36, 41, 89, 91, 94, 98, 104, 105, 110, 111, 112, 114, 119, 122, 125, 132, 134, 138, 141], "an": [6, 7, 8, 9, 10, 14, 20, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 89, 94, 98, 99, 101, 102, 103, 109, 110, 111, 112, 114, 118, 120, 132, 139, 146, 148, 149, 150, 151, 152, 153, 154, 155], "except": [6, 20, 21, 150], "wors": 6, "silent": [6, 90], "succe": 6, "infer": [6, 19, 28, 89, 91, 93, 94, 95, 118, 144, 149, 151, 152, 153, 155], "expect": [6, 7, 10, 14, 17, 18, 22, 23, 28, 30, 32, 36, 39, 93, 105, 114, 132, 141, 149, 150, 154], "addit": [6, 7, 8, 10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 104, 109, 110, 111, 112, 119, 120, 125, 128, 129, 131, 132, 136, 139, 146, 149, 152, 154], "line": [6, 8, 14, 115, 148, 150, 152, 153], "need": [6, 7, 8, 9, 18, 28, 31, 41, 89, 90, 94, 128, 131, 132, 135, 145, 148, 149, 150, 151, 152, 153, 154, 155], "shape": [6, 89, 91, 93, 94, 95, 98, 100, 117, 139], "valu": [6, 7, 25, 41, 45, 46, 47, 54, 55, 61, 62, 63, 71, 72, 80, 81, 89, 91, 92, 94, 95, 97, 104, 110, 113, 114, 115, 117, 129, 130, 131, 132, 134, 138, 148, 150, 152, 153, 154], "two": [6, 7, 27, 146, 151, 152, 153, 154, 155], "popular": [6, 146, 150, 151], "llama2": [6, 7, 8, 10, 19, 28, 30, 32, 33, 34, 35, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 90, 94, 95, 106, 113, 144, 146, 148, 152, 153], "offici": [6, 19, 149, 152, 153], "implement": [6, 8, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 90, 92, 93, 97, 98, 99, 100, 110, 123, 131, 146, 150, 154, 155], "when": [6, 7, 8, 12, 20, 29, 31, 33, 89, 93, 94, 95, 96, 97, 104, 117, 120, 131, 135, 148, 151, 153, 154, 155], "llama": [6, 19, 28, 92, 93, 110, 111, 148, 149, 151, 152, 153, 154], "websit": 6, "get": [6, 7, 8, 9, 28, 106, 119, 121, 122, 124, 145, 146, 149, 150, 151, 152, 154], "access": [6, 7, 8, 29, 110, 116, 148, 151, 152], "singl": [6, 7, 10, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 33, 89, 104, 110, 111, 112, 114, 116, 148, 149, 150, 151, 152, 153, 154, 155], "pth": [6, 151], "inspect": [6, 151, 154, 155], "content": [6, 20, 24, 25, 28, 106, 149, 150], "easili": [6, 7, 146, 150, 154, 155], "torch": [6, 7, 29, 91, 94, 96, 97, 98, 112, 114, 116, 117, 118, 119, 122, 125, 126, 134, 135, 136, 137, 138, 139, 140, 141, 151, 152, 153, 154, 155], "import": [6, 7, 10, 36, 39, 43, 131, 132, 149, 150, 151, 152, 154, 155], "consolid": [6, 153], "00": [6, 143, 147, 152], "mmap": [6, 151], "true": [6, 7, 20, 30, 31, 34, 35, 36, 38, 39, 40, 43, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 89, 94, 95, 96, 101, 106, 107, 109, 110, 111, 112, 120, 122, 125, 126, 128, 131, 139, 140, 148, 149, 150, 151, 153, 154, 155], "weights_onli": [6, 112], "map_loc": [6, 151], "cpu": [6, 8, 96, 119, 139, 145, 148, 151, 155], "tensor": [6, 89, 90, 91, 92, 93, 94, 95, 96, 98, 100, 110, 117, 129, 130, 131, 132, 133, 134, 137, 154, 155], "item": 6, "print": [6, 9, 29, 34, 35, 38, 40, 41, 106, 117, 140, 149, 150, 152, 154, 155], "f": [6, 9, 34, 35, 38, 40, 149, 151, 154, 155], "tok_embed": [6, 94], "32000": [6, 10, 154], "4096": [6, 10, 30, 32, 33, 34, 35, 37, 39, 43, 44, 89, 93, 150, 154], "len": [6, 29, 34, 35, 38, 40, 94], "292": 6, "The": [6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 27, 28, 29, 30, 31, 32, 38, 40, 41, 42, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 92, 93, 96, 97, 98, 101, 106, 107, 109, 110, 112, 115, 118, 119, 121, 123, 132, 137, 139, 140, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "contain": [6, 20, 24, 29, 31, 33, 43, 89, 91, 93, 94, 95, 99, 102, 103, 104, 106, 107, 110, 111, 112, 114, 115, 116, 122, 127, 131, 133, 134, 139, 149, 151, 153, 154], "input": [6, 14, 15, 17, 18, 22, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 89, 90, 92, 93, 94, 95, 100, 106, 110, 112, 133, 134, 138, 141, 149, 150, 154, 155], "embed": [6, 89, 91, 92, 93, 94, 120, 149, 153], "tabl": [6, 149, 155], "call": [6, 10, 20, 90, 96, 104, 115, 129, 130, 131, 132, 135, 139, 149, 150, 154, 155], "layer": [6, 8, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 94, 95, 100, 104, 105, 109, 120, 146, 153, 154, 155], "have": [6, 7, 10, 89, 91, 99, 105, 112, 114, 115, 120, 128, 131, 141, 145, 149, 150, 151, 152, 153, 154, 155], "dim": [6, 89, 90, 92, 93, 94], "most": [6, 7, 107, 149, 152, 154, 155], "within": [6, 7, 10, 28, 31, 41, 90, 117, 131, 138, 139, 148, 150, 151, 153, 154, 155], "default": [6, 7, 16, 20, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 57, 58, 61, 62, 63, 65, 66, 67, 71, 72, 74, 75, 78, 79, 80, 81, 85, 89, 90, 92, 93, 94, 95, 96, 97, 98, 100, 104, 106, 107, 110, 111, 112, 115, 117, 119, 124, 128, 129, 132, 133, 134, 137, 138, 139, 145, 148, 149, 150, 151, 153, 154, 155], "everi": [6, 8, 90, 131, 139, 145, 148, 155], "repo": [6, 110, 111, 113, 148, 151], "first": [6, 7, 10, 27, 31, 91, 94, 107, 110, 115, 144, 146, 149, 150, 151, 153, 154, 155], "big": [6, 151], "split": [6, 31, 149, 150, 151], "across": [6, 8, 29, 110, 131, 138, 151, 153], "bin": [6, 148, 151], "To": [6, 7, 8, 9, 31, 110, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "correctli": [6, 8, 13, 104, 110, 145, 149, 152, 155], "piec": 6, "one": [6, 8, 27, 90, 98, 106, 112, 149, 150, 151, 152, 153, 155], "pytorch_model": [6, 151], "00001": [6, 148], "00002": [6, 148], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 20, 31, 37, 100, 102, 104, 106, 111, 112, 114, 115, 117, 119, 120, 122, 123, 128, 148, 150, 151, 152, 153, 154, 155], "doe": [6, 21, 24, 28, 31, 86, 89, 94, 95, 99, 110, 112, 114, 115, 148, 149, 151], "fewer": [6, 89], "sinc": [6, 7, 10, 90, 110, 112, 149, 151, 153], "instead": [6, 8, 31, 36, 39, 43, 90, 91, 100, 148, 151, 153, 154], "mismatch": 6, "name": [6, 7, 9, 11, 14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 41, 43, 44, 99, 103, 105, 107, 110, 111, 112, 113, 114, 115, 116, 117, 118, 129, 130, 131, 132, 141, 148, 149, 151, 153], "caus": [6, 106], "try": [6, 7, 149, 151, 152, 153, 155], "same": [6, 7, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 89, 91, 95, 106, 114, 115, 120, 132, 148, 149, 151, 153, 154, 155], "As": [6, 7, 8, 9, 100, 146, 151, 153, 155], "re": [6, 7, 107, 112, 146, 149, 151, 152, 153, 154], "care": [6, 90, 110, 112, 151, 153, 154], "end": [6, 8, 20, 29, 106, 107, 144, 146, 149, 153, 154], "number": [6, 8, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 89, 91, 94, 97, 110, 111, 112, 117, 124, 138, 139, 148, 152, 154], "just": [6, 14, 146, 148, 149, 150, 152, 153, 154], "save": [6, 8, 9, 96, 110, 111, 112, 114, 120, 128, 132, 144, 148, 149, 150, 151, 153, 154], "less": [6, 41, 151, 152, 153, 155], "prone": 6, "manag": [6, 29, 101, 137, 149], "invari": 6, "accept": [6, 7, 41, 106, 109, 150, 152, 155], "multipl": [6, 7, 8, 20, 28, 29, 89, 94, 95, 100, 129, 130, 131, 132, 134, 139, 152, 153], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 149, 150, 151], "worri": [6, 149, 152], "explicitli": [6, 99, 146, 154], "convert": [6, 24, 25, 28, 110, 133, 149, 151, 155], "time": [6, 106, 129, 131, 139, 148, 149, 150, 151, 153, 155], "produc": [6, 114, 155], "back": [6, 27, 101, 110, 150, 154, 155], "origin": [6, 34, 35, 96, 100, 149, 151, 153, 154, 155], "form": [6, 7, 8, 27, 148], "One": [6, 151], "advantag": [6, 154], "being": [6, 110, 111, 112, 116, 118, 155], "should": [6, 7, 8, 14, 15, 18, 19, 20, 21, 24, 25, 31, 36, 39, 43, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 89, 90, 98, 99, 104, 105, 109, 115, 127, 129, 130, 131, 132, 145, 146, 150, 151, 152, 153, 154, 155], "abl": [6, 8, 151, 152, 153], "post": [6, 135, 139, 155], "tool": [6, 150, 151, 152], "quantiz": [6, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 100, 112, 123, 144, 152, 155], "eval": [6, 144, 146], "without": [6, 7, 9, 14, 104, 145, 146, 149, 151, 154], "code": [6, 8, 45, 46, 47, 48, 49, 50, 51, 52, 53, 94, 142, 146, 150, 152], "chang": [6, 7, 9, 14, 112, 145, 151, 152, 153, 154, 155], "OR": [6, 24], "convers": [6, 15, 16, 19, 21, 24, 25, 27, 28, 36, 41, 110, 112, 113, 146, 149, 150, 151, 153, 154, 155], "script": [6, 9, 148, 151, 152, 153], "wai": [6, 7, 28, 104, 148, 149, 150, 151, 152, 153], "surround": [6, 8, 146], "load_checkpoint": [6, 8, 110, 111, 112, 113], "save_checkpoint": [6, 8, 9, 110, 111, 112], "method": [6, 7, 8, 9, 12, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 96, 99, 102, 104, 114, 115, 123, 145, 146, 150, 151, 153, 154, 155], "convertor": 6, "avail": [6, 8, 44, 115, 118, 119, 126, 146, 148, 151, 153, 154], "here": [6, 7, 9, 14, 16, 17, 22, 23, 38, 92, 93, 148, 149, 150, 151, 152, 153, 154, 155], "three": [6, 8, 98, 152], "hfcheckpoint": 6, "read": [6, 110, 111, 112, 146], "write": [6, 8, 14, 110, 111, 112, 129, 149, 150, 152], "compat": [6, 110, 112], "transform": [6, 8, 28, 30, 32, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 94, 95, 97, 136, 154], "framework": [6, 8, 146], "mention": [6, 151, 155], "assum": [6, 14, 17, 18, 22, 23, 30, 32, 39, 89, 93, 94, 95, 97, 102, 107, 114, 116, 119, 128, 151, 154], "checkpoint_dir": [6, 7, 110, 111, 112, 151, 153], "necessari": [6, 41, 129, 130, 131, 132, 149, 154], "easiest": [6, 151, 152], "sure": [6, 7, 151, 152, 153, 154, 155], "everyth": [6, 8, 115, 146, 152], "flow": [6, 28, 30, 31, 32, 155], "By": [6, 148, 153, 154, 155], "safetensor": [6, 110, 148], "output": [6, 18, 34, 35, 38, 41, 48, 49, 50, 65, 66, 67, 74, 75, 78, 79, 85, 89, 90, 92, 93, 94, 95, 100, 103, 104, 105, 112, 117, 120, 130, 139, 145, 148, 149, 150, 151, 152, 153, 154, 155], "dir": [6, 132, 145, 148, 151, 152, 153], "output_dir": [6, 7, 110, 111, 112, 139, 151, 153, 154, 155], "argument": [6, 7, 10, 18, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 89, 109, 115, 120, 125, 129, 131, 132, 136, 148, 149, 150, 153, 154], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 94, 139, 148, 150, 151, 154, 155], "_component_": [6, 7, 9, 10, 36, 39, 43, 149, 150, 151, 153, 154], "fullmodelhfcheckpoint": [6, 151], "directori": [6, 7, 110, 111, 112, 129, 131, 132, 139, 148, 151, 152, 153], "sort": [6, 110, 112], "id": [6, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 89, 93, 94, 95, 106, 107, 108, 110, 112, 117, 133, 134, 149, 150, 151], "so": [6, 7, 31, 110, 115, 145, 146, 149, 151, 152, 153, 154, 155], "order": [6, 8, 110, 112, 131, 132, 152], "matter": [6, 110, 112, 148, 154], "checkpoint_fil": [6, 7, 9, 110, 111, 112, 151, 153, 154, 155], "restart": [6, 148], "previou": [6, 31, 110, 111, 112], "more": [6, 7, 8, 36, 41, 87, 91, 93, 104, 109, 112, 115, 132, 136, 138, 146, 148, 150, 151, 152, 153, 154, 155], "next": [6, 31, 117, 153, 155], "section": [6, 8, 122, 144, 151, 153, 155], "recipe_checkpoint": [6, 110, 111, 112], "null": [6, 7], "usual": [6, 93, 110, 132, 148, 151, 154], "model_typ": [6, 110, 111, 112, 151, 153], "resume_from_checkpoint": [6, 110, 111, 112], "fals": [6, 7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 94, 95, 100, 101, 104, 106, 107, 110, 111, 112, 126, 139, 148, 149, 150, 151, 153, 154, 155], "requir": [6, 7, 29, 33, 41, 43, 110, 112, 114, 125, 126, 128, 131, 132, 134, 138, 139, 145, 148, 149, 150, 152, 155], "param": [6, 8, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 100, 102, 103, 105, 110, 128, 154, 155], "directli": [6, 7, 8, 10, 36, 39, 43, 109, 110, 148, 151, 152, 153, 154, 155], "ensur": [6, 7, 13, 27, 41, 89, 110, 112, 119, 146, 150, 152], "out": [6, 7, 8, 28, 30, 34, 35, 36, 38, 40, 110, 111, 144, 146, 148, 149, 151, 152, 153, 154, 155], "case": [6, 8, 9, 20, 89, 110, 114, 119, 123, 128, 129, 136, 146, 148, 149, 150, 151, 153, 154, 155], "discrep": [6, 110], "along": [6, 153, 154], "found": [6, 7, 9, 92, 93, 148, 154, 155], "metacheckpoint": 6, "github": [6, 10, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 89, 92, 93, 97, 98, 104, 145, 150, 152], "repositori": [6, 19, 151, 152], "fullmodelmetacheckpoint": [6, 153], "torchtunecheckpoint": 6, "perform": [6, 31, 87, 90, 101, 117, 146, 149, 151, 153, 155], "current": [6, 31, 86, 87, 89, 91, 93, 94, 95, 111, 112, 120, 124, 129, 131, 135, 138, 151, 152, 153], "test": [6, 7, 8, 146, 149], "complet": [6, 8, 14, 31, 37, 87, 149, 150, 151, 152, 153], "written": [6, 7, 8, 110, 111, 129, 130, 131, 132, 146], "begin": [6, 31, 106, 107, 149, 153, 155], "partit": [6, 110, 155], "ha": [6, 99, 101, 102, 105, 106, 112, 114, 141, 150, 151, 152, 153, 154, 155], "standard": [6, 17, 24, 130, 146, 149, 151, 153], "key_1": [6, 112], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 151], "inform": [6, 20, 132, 136, 146, 148, 151, 152, 153], "subsequ": [6, 8], "recipe_st": [6, 110, 111, 112], "pt": [6, 9, 110, 111, 112, 151, 153], "epoch": [6, 8, 9, 97, 110, 111, 112, 148, 149, 151, 152, 153], "optim": [6, 7, 8, 29, 86, 97, 98, 112, 114, 116, 122, 134, 135, 139, 149, 151, 152, 153, 154, 155], "etc": [6, 8, 110, 122, 152], "prevent": [6, 31, 148], "flood": 6, "overwritten": 6, "note": [6, 7, 18, 20, 94, 99, 106, 114, 135, 138, 139, 149, 150, 151, 154, 155], "updat": [6, 7, 8, 91, 114, 139, 145, 149, 151, 152, 153, 154, 155], "hf_model_0001_0": [6, 151], "hf_model_0002_0": [6, 151], "both": [6, 29, 105, 148, 151, 154, 155], "adapt": [6, 99, 100, 101, 102, 103, 110, 111, 112, 149, 151, 154, 155], "merg": [6, 10, 11, 110, 151, 153, 155], "would": [6, 7, 9, 31, 94, 145, 149, 150, 151, 154, 155], "primari": [6, 7, 8, 152], "want": [6, 7, 8, 9, 10, 28, 117, 145, 148, 149, 150, 151, 152, 153, 154], "resum": [6, 8, 97, 110, 111, 112, 155], "initi": [6, 8, 12, 29, 31, 45, 46, 47, 54, 55, 61, 62, 63, 71, 72, 80, 81, 114, 125, 126, 152, 154, 155], "frozen": [6, 154, 155], "base": [6, 10, 30, 32, 41, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 93, 97, 98, 100, 101, 103, 104, 105, 110, 115, 118, 120, 128, 129, 144, 149, 151, 152, 153, 154, 155], "well": [6, 7, 8, 146, 148, 150, 151, 153, 155], "learnt": [6, 149, 151], "someth": [6, 8, 9, 149, 151], "NOT": 6, "refer": [6, 7, 8, 92, 93, 98, 101, 146, 154], "adapter_checkpoint": [6, 110, 111, 112], "adapter_0": [6, 151], "now": [6, 106, 114, 116, 149, 150, 151, 152, 153, 154, 155], "knowledg": 6, "creat": [6, 7, 10, 31, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 88, 91, 97, 109, 110, 111, 112, 116, 129, 131, 148, 149, 150, 151, 153, 155], "simpl": [6, 8, 14, 17, 22, 23, 144, 150, 152, 154, 155], "forward": [6, 8, 89, 90, 92, 93, 94, 95, 98, 100, 122, 139, 153, 154, 155], "13b": [6, 45, 48, 51, 61, 65, 68], "modeltyp": [6, 110, 111, 112], "llama2_13b": [6, 65], "right": [6, 110, 151, 153, 154], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 104, 114, 154], "successfulli": [6, 148, 152], "vocab": [6, 10, 94, 153], "70": [6, 71], "x": [6, 89, 90, 92, 93, 94, 95, 100, 117, 137, 154, 155], "randint": 6, "0": [6, 8, 31, 48, 49, 50, 51, 52, 53, 65, 66, 67, 68, 69, 70, 89, 94, 97, 98, 100, 106, 117, 131, 132, 133, 134, 138, 140, 143, 147, 149, 150, 151, 152, 153, 154, 155], "no_grad": 6, "6": [6, 31, 92, 133, 134, 151, 155], "3989": 6, "9": [6, 134, 151, 155], "0531": 6, "3": [6, 31, 85, 86, 87, 107, 113, 115, 121, 133, 134, 137, 148, 149, 151, 152, 153, 155], "2375": 6, "5": [6, 7, 14, 97, 98, 133, 134, 151, 152, 153], "2822": 6, "4": [6, 7, 41, 89, 133, 134, 140, 146, 148, 150, 151, 153, 154, 155], "4872": 6, "7469": 6, "8": [6, 34, 35, 38, 40, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 134, 151, 154, 155], "6737": 6, "11": [6, 134, 151, 153, 155], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 133, 134], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 28, 30, 32, 39, 41, 104, 132, 148, 149, 150, 151, 152, 153, 154], "find": [6, 8, 9, 148, 151, 152, 154], "list": [6, 7, 15, 16, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 115, 117, 121, 133, 134, 149, 150, 152, 153], "builder": [6, 37, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 88, 149, 150, 155], "hope": 6, "deeper": [6, 152], "insight": [6, 151], "happi": [6, 151], "thi": [7, 8, 9, 10, 17, 20, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 43, 44, 86, 87, 89, 90, 93, 94, 95, 96, 97, 99, 101, 104, 105, 106, 109, 110, 111, 112, 114, 115, 117, 118, 119, 122, 126, 128, 129, 131, 132, 134, 135, 136, 138, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155], "pars": [7, 10, 11, 107, 115, 149, 152], "effect": 7, "cli": [7, 9, 11, 12, 145, 151, 152], "prerequisit": [7, 149, 150, 151, 152, 153, 154, 155], "Be": [7, 149, 151, 152, 153, 154, 155], "familiar": [7, 149, 151, 152, 153, 154, 155], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 145, 149, 150, 152], "instal": [7, 9, 126, 131, 132, 144, 148, 151, 152, 153, 154, 155], "fundament": 7, "There": [7, 15, 27, 149, 151, 152, 153, 154], "entri": [7, 8, 152], "point": [7, 8, 24, 25, 150, 151, 152, 153, 154, 155], "locat": [7, 148, 153, 154, 155], "thei": [7, 8, 20, 29, 94, 105, 115, 120, 148, 149, 150, 154], "truth": [7, 151, 153], "reproduc": 7, "overridden": [7, 90, 115, 139], "quick": [7, 29], "experiment": 7, "serv": [7, 109, 150, 154], "particular": [7, 28, 29, 41, 109, 150, 154, 155], "seed": [7, 8, 9, 138, 152], "shuffl": [7, 31], "devic": [7, 8, 104, 114, 118, 119, 122, 148, 149, 151, 152, 153, 154], "cuda": [7, 118, 119, 122, 139, 145, 151, 155], "dtype": [7, 8, 91, 94, 96, 119, 137, 141, 151, 155], "fp32": [7, 155], "enable_fsdp": 7, "mani": [7, 31, 150, 151], "object": [7, 10, 11, 15, 16, 19, 21, 89, 109, 123, 149], "keyword": [7, 10, 28, 30, 32, 33, 36, 37, 39, 41, 43, 44, 96, 149, 150], "loss": [7, 8, 30, 34, 35, 38, 40, 98, 152, 154, 155], "exampl": [7, 8, 9, 10, 12, 14, 17, 22, 23, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 89, 98, 99, 101, 106, 109, 110, 111, 113, 114, 117, 123, 131, 132, 133, 134, 137, 140, 142, 143, 145, 147, 148, 149, 150, 151, 153, 154, 155], "subfield": 7, "dotpath": 7, "wish": [7, 150], "exact": [7, 10, 151], "path": [7, 8, 9, 10, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 56, 64, 73, 82, 87, 106, 107, 110, 111, 112, 115, 139, 148, 149, 150, 151, 153, 154], "normal": [7, 28, 31, 92, 94, 95, 106, 149, 150, 154, 155], "python": [7, 107, 115, 121, 132, 138, 142, 148, 151], "alpaca_dataset": [7, 34, 150], "custom": [7, 8, 28, 30, 32, 36, 39, 43, 136, 146, 148, 151, 152, 153, 154], "train_on_input": [7, 24, 25, 28, 30, 34, 35, 36, 38, 39, 40, 41, 149, 150], "onc": [7, 101, 151, 152, 153, 154, 155], "ve": [7, 91, 107, 149, 150, 151, 153, 154], "instanc": [7, 10, 29, 90, 96, 102, 103, 154], "cfg": [7, 8, 11, 12, 13], "automat": [7, 9, 10, 36, 148, 151, 155], "under": [7, 139, 150, 151, 153, 155], "preced": [7, 10, 148, 153, 154], "actual": [7, 9, 14, 17, 22, 23, 28, 149], "throw": 7, "notic": [7, 149, 150, 154], "miss": [7, 104, 105, 139, 154], "posit": [7, 10, 31, 89, 91, 93, 94, 95, 153], "anoth": [7, 151], "handl": [7, 12, 20, 29, 106, 149, 151, 154, 155], "def": [7, 8, 9, 12, 109, 113, 149, 150, 154, 155], "dictconfig": [7, 8, 10, 11, 12, 13, 132, 139], "arg": [7, 10, 94, 96, 99, 108, 115, 130, 139], "tupl": [7, 10, 29, 41, 91, 96, 98, 106, 107, 109, 115, 124, 133, 134, 139, 141], "kwarg": [7, 10, 96, 99, 108, 115, 125, 129, 130, 131, 132, 136, 139, 150], "str": [7, 10, 11, 14, 17, 18, 20, 22, 23, 24, 25, 28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 56, 64, 73, 82, 87, 96, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 110, 111, 112, 114, 115, 118, 119, 121, 122, 123, 125, 127, 129, 130, 131, 132, 133, 134, 138, 139, 140, 141, 149, 150], "mean": [7, 89, 92, 94, 95, 128, 148, 149, 150, 152, 154], "pass": [7, 10, 28, 29, 30, 32, 33, 36, 37, 39, 43, 44, 89, 90, 96, 101, 105, 109, 112, 119, 120, 122, 125, 128, 131, 132, 136, 139, 148, 149, 150, 154, 155], "add": [7, 9, 28, 31, 107, 112, 113, 115, 150, 151, 153, 154, 155], "d": [7, 20, 89, 91, 94, 107, 148, 149, 154], "llama2_token": [7, 151], "tmp": [7, 114, 149, 152, 153], "option": [7, 8, 14, 17, 18, 22, 23, 26, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 89, 93, 94, 95, 96, 104, 105, 106, 107, 110, 111, 112, 117, 118, 119, 121, 123, 129, 132, 138, 139, 145, 146, 148, 150, 151], "bool": [7, 20, 24, 25, 28, 30, 31, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 96, 100, 104, 105, 106, 107, 108, 109, 110, 111, 112, 120, 122, 125, 126, 128, 131, 136, 139, 140, 149, 155], "max_seq_len": [7, 10, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 89, 91, 93, 94, 106, 107, 149, 150], "int": [7, 9, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 91, 92, 93, 94, 97, 100, 106, 107, 108, 109, 110, 111, 112, 117, 120, 124, 128, 129, 130, 131, 132, 133, 134, 136, 138, 139, 148, 149, 150, 154, 155], "512": [7, 34, 35, 150, 155], "instructdataset": [7, 34, 35, 38, 39, 40, 150], "alreadi": [7, 113, 125, 128, 145, 148, 150, 151, 154], "overwrit": [7, 112, 145], "duplic": [7, 8, 146, 148], "sometim": 7, "than": [7, 27, 41, 89, 91, 109, 112, 113, 140, 141, 149, 150, 151, 152, 153, 154, 155], "resolv": [7, 11, 152], "alpaca": [7, 14, 34, 35, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 150], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 129, 130, 131, 132], "disklogg": 7, "log_dir": [7, 129, 131, 132], "conveni": [7, 8, 148], "verifi": [7, 118, 119, 120, 149, 152, 154], "properli": [7, 104, 126, 148], "experi": [7, 132, 144, 146, 149, 153, 154], "wa": [7, 104, 149, 151, 153, 154, 155], "cp": [7, 145, 148, 149, 151, 152, 153], "7b_lora_single_devic": [7, 151, 152, 154, 155], "my_config": 7, "discuss": [7, 87, 152, 154], "guidelin": 7, "while": [7, 8, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 90, 146, 151, 155], "mai": [7, 9, 120, 149, 150, 152, 154], "tempt": 7, "put": [7, 8, 152, 154], "much": [7, 151, 153, 154, 155], "give": [7, 150, 154], "maximum": [7, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 89, 91, 93, 94, 107, 148], "flexibl": [7, 29, 150], "switch": 7, "encourag": [7, 154], "clariti": 7, "significantli": 7, "easier": [7, 151, 152], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 31, 33, 43, 87, 98, 150, 155], "expos": [7, 8, 112, 149, 152], "parent": [7, 148], "modul": [7, 10, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 113, 116, 120, 128, 135, 136, 138, 149, 152, 154, 155], "__init__": [7, 8, 154, 155], "py": [7, 10, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 89, 91, 92, 93, 97, 98, 148, 151, 153], "guarante": 7, "stabil": [7, 146, 155], "underscor": 7, "_alpaca": 7, "collect": [7, 117, 152], "itself": 7, "via": [7, 9, 36, 39, 43, 100, 110, 154, 155], "pair": [7, 14, 42, 133, 134, 150], "k1": [7, 8], "v1": [7, 8, 44], "k2": [7, 8], "v2": [7, 8, 150], "lora_finetune_single_devic": [7, 148, 149, 151, 152, 153, 154, 155], "checkpoint": [7, 8, 96, 107, 110, 111, 112, 113, 114, 132, 136, 146, 148, 153, 154, 155], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 36, 39, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 102, 103, 106, 107, 108, 110, 111, 112, 113, 114, 115, 129, 130, 131, 132, 149, 150, 152, 154, 155], "assign": [7, 33], "nest": 7, "dot": 7, "notat": [7, 89, 93, 94], "certain": [7, 139, 149], "flag": [7, 8, 30, 34, 35, 38, 40, 109, 112, 120, 148, 155], "built": [7, 9, 42, 145, 149, 152, 155], "bitsandbyt": 7, "pagedadamw8bit": 7, "delet": 7, "foreach": [7, 28], "pytorch": [7, 8, 94, 96, 104, 109, 126, 131, 136, 138, 139, 144, 145, 146, 153, 154, 155], "llama3": [7, 28, 41, 71, 72, 73, 74, 75, 76, 77, 113, 117, 120, 144, 148, 150], "8b_full": [7, 148, 150], "adamw": [7, 154], "lr": [7, 97], "2e": 7, "fuse": [7, 135], "nproc_per_nod": [7, 150, 153, 154], "full_finetune_distribut": [7, 148, 150, 151, 152], "core": [8, 146, 150, 152, 155], "i": [8, 19, 21, 89, 94, 95, 96, 103, 107, 114, 117, 150, 151, 153, 155], "structur": [8, 15, 16, 19, 21, 24, 25, 28, 36, 149, 150, 151], "new": [8, 37, 80, 91, 113, 129, 131, 149, 151, 152, 153, 154, 155], "user": [8, 15, 16, 19, 20, 21, 24, 25, 27, 28, 89, 106, 149, 150, 152], "thought": [8, 146, 152, 155], "target": [8, 146], "pipelin": [8, 146], "llm": [8, 144, 146, 150, 151, 154], "eg": [8, 94, 110, 146], "meaning": [8, 146, 151], "featur": [8, 9, 145, 146, 151, 152], "fsdp": [8, 109, 114, 120, 128, 146, 152, 153], "activ": [8, 90, 122, 127, 136, 139, 146, 155], "gradient": [8, 128, 135, 139, 146, 151, 153, 154, 155], "accumul": [8, 135, 139, 146], "mix": [8, 148, 150, 151], "precis": [8, 96, 119, 146, 152, 155], "appli": [8, 28, 30, 32, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 92, 93, 94, 95, 104, 105, 136, 146, 155], "given": [8, 10, 14, 17, 18, 22, 23, 27, 100, 101, 108, 117, 118, 119, 123, 128, 135, 140, 146, 154], "complex": 8, "becom": [8, 145, 150], "harder": 8, "anticip": 8, "architectur": [8, 19, 21, 94, 113, 148, 150], "methodolog": 8, "reason": [8, 117, 151], "possibl": [8, 28, 31, 36, 148, 150], "trade": 8, "off": [8, 106, 151], "memori": [8, 29, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 96, 104, 120, 122, 127, 128, 139, 144, 146, 151, 152, 153], "vs": [8, 152], "qualiti": [8, 151, 154], "believ": 8, "best": [8, 149], "suit": [8, 152], "b": [8, 89, 91, 93, 94, 95, 100, 128, 132, 154, 155], "fit": [8, 28, 30, 31, 32, 33, 34, 35, 37, 39, 43, 44, 150], "solut": 8, "result": [8, 106, 139, 151, 153, 154, 155], "meant": [8, 96, 114], "depend": [8, 9, 14, 110, 139, 148, 150, 151, 154, 155], "level": [8, 116, 121, 128, 146, 155], "expertis": 8, "routin": 8, "yourself": [8, 148, 153, 154], "exist": [8, 145, 148, 151, 152, 153, 155], "ad": [8, 106, 112, 113, 149, 154, 155], "ones": 8, "modular": [8, 146], "build": [8, 36, 39, 43, 146, 153, 154], "block": [8, 31, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 104, 105, 146], "wandb": [8, 9, 132, 152], "log": [8, 11, 98, 121, 122, 127, 129, 130, 131, 132, 151, 152, 153, 155], "fulli": [8, 29], "nativ": [8, 144, 146, 154, 155], "correct": [8, 17, 38, 92, 93, 94, 118, 146, 149, 150], "numer": [8, 146], "pariti": [8, 146], "verif": 8, "extens": [8, 112, 146], "comparison": [8, 154, 155], "benchmark": [8, 138, 146, 151, 153, 154], "limit": [8, 114, 150], "hidden": [8, 90], "behind": 8, "100": [8, 30, 34, 35, 38, 40, 41, 117, 133, 134, 154, 155], "prefer": [8, 22, 42, 98, 134, 146, 148, 150], "over": [8, 97, 115, 146, 151, 153, 154, 155], "unnecessari": 8, "abstract": [8, 15, 18, 108, 146, 152, 155], "No": [8, 112, 146], "inherit": [8, 115, 146, 150], "go": [8, 19, 21, 106, 146, 150, 151, 152, 155], "upon": [8, 29, 153], "figur": [8, 154, 155], "spectrum": 8, "decid": 8, "interact": [8, 144, 152], "start": [8, 9, 29, 107, 113, 145, 146, 149, 150, 151, 152], "paradigm": 8, "consist": [8, 44, 152], "configur": [8, 30, 32, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 95, 146, 149, 152, 153, 154, 155], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 85, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 109, 110, 111, 112, 114, 116, 117, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 144, 146, 148, 149, 150, 151, 152, 153, 154, 155], "overrid": [8, 11, 12, 148, 151, 152, 153, 155], "togeth": [8, 31, 132, 152, 154], "valid": [8, 27, 104, 105, 141, 145, 151, 152], "environ": [8, 118, 126, 145, 148, 151, 152], "logic": [8, 113, 146, 152, 154], "api": [8, 9, 24, 51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 104, 148, 149, 151, 152, 153, 155], "closer": [8, 154], "monolith": [8, 146], "trainer": [8, 98], "A": [8, 9, 24, 25, 29, 31, 89, 94, 95, 96, 98, 100, 104, 106, 107, 109, 114, 115, 122, 123, 127, 128, 133, 134, 143, 144, 147, 148, 149, 151, 154, 155], "wrapper": [8, 106, 107, 114, 116, 148, 154], "around": [8, 28, 106, 107, 122, 148, 149, 151, 154, 155], "extern": [8, 150], "primarili": [8, 29, 154], "eleutherai": [8, 146, 154], "har": [8, 146, 154], "control": [8, 30, 34, 35, 38, 40, 101, 138, 151], "multi": [8, 28, 89, 104, 153], "stage": 8, "distil": 8, "oper": [8, 29, 101, 138], "turn": [8, 20, 27, 28, 107, 149], "dataload": [8, 31, 34, 35, 38, 40], "applic": [8, 89, 110, 111, 132], "clean": [8, 9, 34], "after": [8, 89, 91, 92, 94, 95, 104, 128, 129, 130, 131, 132, 149, 155], "process": [8, 9, 96, 124, 125, 138, 150, 152, 155], "group": [8, 89, 124, 125, 129, 130, 131, 132, 148, 153], "init_process_group": [8, 125], "backend": [8, 148], "gloo": 8, "els": [8, 115, 132, 146, 155], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 29, 112, 115, 120, 139, 150, 152, 153, 154], "stuff": 8, "carri": 8, "relev": [8, 20, 148, 151, 154], "interfac": [8, 15, 18, 29, 99], "metric": [8, 152], "logger": [8, 121, 127, 129, 130, 131, 132, 152], "self": [8, 9, 31, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 89, 94, 95, 99, 104, 105, 110, 113, 114, 150, 154, 155], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 109, 120, 128, 136, 149], "_model": [8, 114], "_setup_model": 8, "_token": [8, 150], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 114, 116, 135, 139, 155], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 98, 138, 148, 153], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 31], "batch": [8, 31, 34, 35, 38, 40, 89, 91, 93, 94, 98, 106, 133, 134, 139, 146, 150, 152, 153, 154], "enumer": 8, "_autocast": 8, "logit": [8, 117], "label": [8, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 98, 133, 134], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 129, 130, 131, 132], "step": [8, 31, 94, 97, 107, 116, 129, 130, 131, 132, 135, 139, 144, 151, 154, 155], "learn": [8, 29, 97, 146, 149, 150, 152, 153, 154, 155], "decor": [8, 12], "recipe_main": [8, 12], "none": [8, 9, 11, 13, 14, 17, 18, 21, 22, 23, 26, 27, 28, 30, 31, 32, 33, 36, 37, 39, 41, 43, 44, 89, 91, 93, 94, 95, 101, 103, 104, 105, 106, 107, 110, 111, 112, 113, 117, 118, 119, 121, 123, 127, 129, 130, 131, 132, 135, 136, 137, 138, 139, 141, 149, 150, 151], "fullfinetunerecip": 8, "direct": [8, 98, 134, 145], "wandblogg": [9, 154, 155], "workspac": 9, "seen": [9, 154, 155], "screenshot": 9, "below": [9, 14, 93, 109, 150, 153, 154, 155], "packag": [9, 131, 132, 145], "pip": [9, 131, 132, 145, 151, 153], "Then": [9, 101, 152], "login": [9, 132, 148, 151], "project": [9, 48, 49, 50, 65, 66, 67, 74, 75, 78, 79, 85, 89, 90, 104, 105, 120, 132, 144, 154, 155], "grab": [9, 153], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 126, 149], "exit": [9, 145, 148], "resourc": [9, 129, 130, 131, 132], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 39, 41, 43, 89, 93, 94, 95, 117, 149, 151], "desir": [9, 28, 137, 149], "suggest": 9, "approach": [9, 29, 150], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 151], "_output_dir": [9, 110, 111, 112], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": [9, 139], "type": [9, 10, 12, 20, 24, 25, 26, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 85, 86, 87, 89, 91, 92, 93, 94, 95, 96, 98, 100, 102, 106, 107, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 134, 136, 137, 138, 139, 140, 150, 151, 154, 155], "descript": [9, 36, 41, 148], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 18, 20, 24, 25, 28, 31, 34, 35, 38, 40, 127, 150], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 28, 30, 32, 33, 36, 37, 39, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 80, 81, 83, 84, 85, 86, 87, 88, 89, 92, 93, 97, 98, 104, 109, 110, 111, 115, 121, 126, 131, 132, 136, 138, 145, 150, 151], "com": [10, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 89, 92, 93, 97, 98, 104, 145], "facebookresearch": [10, 92, 93], "blob": [10, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85, 87, 89, 92, 93, 97, 98], "main": [10, 12, 87, 89, 92, 93, 145, 151, 153], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 94], "32": [10, 153, 154, 155], "num_head": [10, 89, 91, 93, 94], "num_kv_head": [10, 89, 91], "vocab_s": 10, "must": [10, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 99, 107, 115, 155], "return": [10, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 85, 86, 87, 89, 91, 92, 93, 94, 95, 97, 98, 99, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 112, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 137, 138, 139, 140, 149, 150, 154, 155], "nn": [10, 89, 90, 91, 94, 95, 96, 99, 101, 102, 103, 109, 116, 128, 135, 136, 141, 154, 155], "parsed_yaml": 10, "embed_dim": [10, 89, 93, 95, 154], "valueerror": [10, 21, 24, 27, 36, 41, 89, 91, 94, 98, 110, 111, 112, 119, 122, 138, 141], "recipe_nam": 11, "rank": [11, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 100, 124, 126, 138, 152, 154, 155], "zero": [11, 91, 92, 151, 153], "displai": 11, "callabl": [12, 28, 30, 32, 94, 101, 109, 117, 120, 123, 128, 136], "With": [12, 151, 154, 155], "my_recip": 12, "foo": 12, "bar": [12, 146, 152], "instanti": [13, 45, 46, 47, 48, 49, 50, 54, 55, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 78, 79, 80, 81, 82, 85, 86, 87, 114], "configerror": 13, "cannot": [13, 112, 153], "data": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 122, 129, 130, 131, 132, 150, 151, 155], "templat": [14, 15, 17, 18, 22, 23, 28, 30, 32, 34, 35, 36, 38, 39, 40, 41], "style": [14, 31, 34, 35, 36, 41, 155], "slightli": 14, "describ": [14, 136, 150], "task": [14, 23, 29, 37, 149, 150, 151, 153, 154, 155], "further": [14, 148, 150, 154, 155], "context": [14, 16, 86, 101, 137, 139, 150], "respons": [14, 16, 98, 106, 150, 151, 152, 153], "appropri": [14, 16, 19, 21, 29, 97, 110, 150, 155], "request": [14, 119, 150, 151], "Or": 14, "instruciton": 14, "classmethod": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 150], "map": [14, 17, 18, 22, 23, 24, 25, 28, 29, 30, 31, 32, 39, 103, 110, 114, 116, 129, 130, 131, 132, 135, 139, 149, 150, 151, 154], "column_map": [14, 17, 18, 22, 23, 28, 30, 32, 39, 150], "placehold": [14, 15, 17, 18, 22, 23, 28, 30, 32, 39, 150], "column": [14, 17, 18, 22, 23, 28, 30, 32, 33, 39, 43, 89, 94, 95, 149, 150], "ident": [14, 17, 18, 21, 22, 23, 30, 31, 32, 39, 151], "poem": 14, "n": [14, 22, 89, 106, 107, 143, 147, 149, 150], "nwrite": 14, "long": [14, 31, 149, 154], "where": [14, 17, 22, 23, 28, 29, 34, 35, 38, 40, 89, 94, 100, 106, 120, 128, 134, 150], "me": 14, "role": [15, 20, 24, 25, 28, 106, 149, 150], "system": [15, 16, 19, 20, 21, 24, 25, 27, 28, 87, 106, 149, 150], "assist": [15, 16, 19, 20, 24, 25, 27, 28, 87, 106, 117, 149, 150], "accord": [15, 21, 149], "openai": [16, 24, 36, 150], "markup": 16, "languag": [16, 100, 117, 154], "It": [16, 21, 148, 149, 150, 155], "im_start": 16, "im_end": 16, "goe": [16, 101], "tag": [16, 19, 21, 28, 107, 129, 130, 131, 132, 149], "grammar": [17, 38, 150], "english": 17, "sentenc": [17, 31, 106], "quik": 17, "brown": 17, "fox": 17, "jump": [17, 154], "lazi": 17, "dog": 17, "alwai": [18, 115], "human": [19, 25, 149], "taken": [19, 154, 155], "inst": [19, 21, 28, 149, 150], "sy": [19, 149, 150], "respect": [19, 29, 103, 139, 149, 150], "honest": [19, 149, 150], "am": [19, 21, 149, 150, 151, 153], "pari": [19, 21, 23, 150], "capit": [19, 21, 22, 23, 150], "franc": [19, 21, 22, 23, 150], "known": [19, 21, 106, 123, 150], "its": [19, 21, 31, 89, 93, 94, 95, 135, 138, 148, 150, 151, 153, 154], "stun": [19, 21, 150], "liter": [20, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 104, 105], "mask": [20, 30, 31, 34, 35, 38, 40, 89, 94, 95, 106, 107, 149, 150], "ipython": 20, "eot": 20, "dataclass": [20, 149], "repres": [20, 134, 149], "individu": [20, 31, 122, 132, 136, 149, 150], "tiktoken": [20, 107, 153], "special": [20, 28, 87, 106, 107, 114, 150], "variabl": [20, 28, 29, 30, 32, 39, 126, 155], "writer": 20, "whether": [20, 24, 25, 28, 30, 34, 35, 36, 38, 39, 40, 41, 43, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 96, 100, 104, 105, 106, 107, 109, 119, 122, 149, 150], "correspond": [20, 99, 102, 119, 134, 152, 153], "consecut": [20, 27], "from_dict": [20, 149], "construct": [20, 154], "dictionari": [20, 31, 122, 127, 129, 130, 131, 132, 134, 151], "mistral": [21, 28, 41, 78, 79, 80, 81, 82, 83, 84, 113, 148, 149, 151, 152], "llama2chatformat": [21, 149, 150], "similar": [22, 37, 42, 44, 104, 150, 151, 153, 154, 155], "stackexchangedpair": 22, "question": [22, 149, 150, 151, 153], "answer": [22, 149, 151, 153], "nanswer": 22, "summar": [23, 40, 149, 150], "dialogu": [23, 40, 149], "summari": [23, 40, 122, 150], "dialog": 23, "hello": [23, 106, 149, 151, 153], "did": [23, 151, 153, 155], "know": [23, 149, 150, 151, 153, 154], "adher": [24, 25], "could": [24, 154], "remain": [24, 25, 97, 154], "unmask": [24, 25], "sharegpt": [25, 36], "gpt": [25, 89, 151], "eos_id": 26, "length": [26, 27, 29, 30, 31, 32, 33, 34, 35, 37, 39, 41, 43, 44, 86, 89, 91, 93, 94, 106, 107, 111, 133, 134], "last": [26, 31, 97, 150], "replac": [26, 30, 34, 35, 38, 40, 96, 154], "forth": [27, 150], "come": [27, 99, 154], "empti": [27, 148], "shorter": 27, "min": [27, 154], "invalid": 27, "convert_to_messag": [28, 149], "chat_format": [28, 36, 41, 149, 150], "chatformat": [28, 36, 150], "load_dataset_kwarg": [28, 30, 32, 33, 36, 37, 39, 43, 44], "multiturn": [28, 149], "prepar": [28, 149], "truncat": [28, 30, 31, 32, 33, 37, 39, 41, 43, 44, 106, 107, 150], "encod": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 98, 106, 107, 108, 149], "decod": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 94, 106, 107, 108, 117, 149], "anyth": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "load_dataset": [28, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 149, 150], "huggingfac": [28, 30, 32, 33, 36, 37, 39, 43, 44, 81, 86, 87, 97, 98, 110, 111, 148, 151], "co": [28, 30, 32, 33, 36, 37, 39, 43, 44, 81, 86, 87, 110, 111, 151], "doc": [28, 30, 32, 33, 36, 37, 39, 43, 44, 109, 115, 121, 126, 131, 132, 138, 148, 151], "en": [28, 30, 32, 33, 36, 37, 39, 43, 44], "package_refer": [28, 30, 32, 33, 36, 37, 39, 43, 44], "loading_method": [28, 30, 32, 33, 36, 37, 39, 43, 44], "text": [28, 31, 33, 36, 37, 43, 44, 106, 107, 108, 149, 151], "extra": [28, 145, 154, 155], "still": [28, 115, 154, 155], "unless": 28, "check": [28, 36, 94, 104, 119, 126, 140, 144, 149, 151, 152, 154], "concaten": [29, 106, 108, 134], "sub": [29, 131], "unifi": [29, 81], "were": [29, 101, 149, 152], "simplifi": [29, 148, 154], "simultan": 29, "intern": [29, 115], "aggreg": 29, "transpar": 29, "index": [29, 89, 93, 94, 95, 97, 133, 134, 145, 149, 151], "howev": [29, 87, 145], "constitu": 29, "might": [29, 148, 151], "larg": [29, 100, 139, 148, 155], "comput": [29, 89, 90, 93, 94, 98, 122, 138, 151, 155], "cumul": 29, "maintain": [29, 155], "indic": [29, 31, 89, 93, 94, 95, 109, 126, 149], "deleg": 29, "retriev": [29, 120], "lead": [29, 106], "high": [29, 146, 154], "scale": [29, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 100, 117, 154, 155], "consid": 29, "strategi": 29, "stream": [29, 121], "demand": 29, "deriv": [29, 90, 94, 95], "_dataset": 29, "_len": 29, "total": [29, 97, 124, 143, 147, 151, 153, 154], "combin": 29, "_index": 29, "lookup": 29, "dataset1": 29, "mycustomdataset": 29, "params1": 29, "dataset2": 29, "params2": 29, "concat_dataset": 29, "data_point": 29, "1500": 29, "element": [29, 107, 151], "focus": [29, 152], "enhanc": [29, 155], "divers": 29, "machin": [29, 118, 148, 151], "instructtempl": [30, 32, 150], "contribut": [30, 34, 35, 38, 40], "disabl": [30, 32, 33, 37, 39, 43, 44, 101, 138], "recommend": [30, 32, 33, 34, 35, 37, 39, 43, 44, 131, 149, 151, 155], "highest": [30, 32, 33, 34, 35, 37, 39, 43, 44], "sequenc": [30, 31, 32, 33, 34, 35, 37, 39, 41, 43, 44, 89, 91, 93, 94, 106, 107, 133, 134, 149], "ds": [31, 41], "max_pack": 31, "split_across_pack": 31, "greedi": 31, "pack": [31, 34, 35, 36, 38, 39, 40, 41, 43, 89, 93, 94, 95], "done": [31, 104, 119, 128, 154, 155], "preprocess": 31, "outsid": [31, 138, 139, 151, 153, 154], "sampler": [31, 152], "part": [31, 149, 155], "buffer": 31, "enough": [31, 149], "attent": [31, 48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 86, 89, 91, 93, 94, 95, 104, 105, 153, 154, 155], "lower": [31, 154], "triangular": 31, "cross": 31, "attend": [31, 89, 94, 95], "rel": [31, 89, 93, 94, 95, 122, 154], "pad": [31, 117, 133, 134, 150], "max": [31, 41, 94, 97, 106, 148, 154], "wise": 31, "collat": [31, 133, 150], "made": [31, 36, 39, 43, 93, 151], "smaller": [31, 151, 153, 154, 155], "jam": 31, "vari": 31, "s1": [31, 106], "s2": [31, 106], "s3": 31, "s4": 31, "contamin": 31, "input_po": [31, 89, 91, 93, 94, 95], "matrix": 31, "causal": [31, 89, 94, 95], "continu": [31, 150], "increment": 31, "move": [31, 94], "entir": [31, 128, 149, 155], "avoid": [31, 92, 96, 138, 148, 155], "freeform": [33, 43], "unstructur": [33, 44], "corpu": [33, 37, 44], "local": [33, 43, 132, 138, 145, 148, 149, 151, 152], "tabular": [33, 43], "yahma": [34, 39], "codebas": [34, 35, 38, 40, 151], "prior": [34, 35, 36, 38, 39, 40, 41, 43], "alpaca_d": [34, 35], "batch_siz": [34, 35, 38, 40, 89, 91, 94, 95, 98, 151], "tatsu": 35, "lab": 35, "conversation_styl": [36, 150], "chatdataset": [36, 41, 149, 150], "friendli": [36, 39, 43, 117, 149], "huggingfaceh4": 36, "no_robot": 36, "chatmlformat": 36, "2096": [36, 39, 43], "accomplish": [36, 39, 43], "packeddataset": [36, 39, 43, 150], "ccdv": 37, "cnn_dailymail": 37, "textcompletiondataset": [37, 43, 44, 150], "cnn": 37, "dailymail": 37, "articl": [37, 44], "extract": 37, "highlight": [37, 155], "liweili": 38, "c4_200m": 38, "variant": [38, 40], "mirror": [38, 40], "llama_recip": [38, 40], "grammar_d": 38, "alpaca_clean": 39, "alpacainstructtempl": [39, 150], "samsum": [40, 150], "samsum_d": 40, "open": [41, 54, 55, 150, 151], "orca": 41, "slimorca": 41, "dedup": 41, "1024": [41, 42, 150], "prescrib": 41, "least": [41, 153, 154], "though": [41, 149], "10": [41, 133, 134, 151, 153, 155], "351": 41, "82": [41, 151], "391": 41, "221": 41, "220": 41, "193": 41, "12": [41, 134, 145], "471": 41, "lvwerra": [42, 150], "stack": [42, 139, 150], "exchang": [42, 150], "preferencedataset": [42, 150], "stackexchangepair": 42, "textdataset": 43, "omit": [43, 154], "allenai": [43, 150], "c4": [43, 150], "data_dir": [43, 150], "realnewslik": [43, 150], "wikitext": 44, "subset": [44, 102], "103": [44, 151], "raw": 44, "wikipedia": 44, "page": [44, 145, 146, 148, 152, 153], "code_llama2": [45, 46, 47, 48, 49, 50, 51, 52, 53, 148], "transformerdecod": [45, 46, 47, 48, 49, 50, 51, 52, 53, 61, 62, 63, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 88, 117, 154], "w": [45, 46, 47, 54, 55, 61, 62, 63, 71, 72, 80, 81, 131, 132, 149, 151, 154, 155], "arxiv": [45, 46, 47, 51, 52, 53, 59, 60, 61, 62, 63, 68, 69, 70, 76, 77, 83, 84, 88, 89, 92, 93, 98], "org": [45, 46, 47, 51, 52, 53, 59, 60, 61, 62, 63, 68, 69, 70, 76, 77, 83, 84, 88, 89, 92, 93, 98, 109, 115, 121, 126, 131, 136, 138, 145], "pdf": [45, 46, 47, 89, 92], "2308": [45, 46, 47], "12950": [45, 46, 47], "70b": [46, 49, 52, 62, 66, 69, 71, 74, 76, 153], "lora_attn_modul": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 104, 105, 154, 155], "q_proj": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 104, 105, 154, 155], "k_proj": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 104, 105, 154, 155], "v_proj": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 104, 105, 154, 155], "output_proj": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 104, 105, 154, 155], "apply_lora_to_mlp": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 104, 105, 154], "apply_lora_to_output": [48, 49, 50, 51, 52, 53, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 104, 105, 154], "lora_rank": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 154], "lora_alpha": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 154], "float": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 89, 92, 97, 98, 100, 117, 122, 127, 129, 130, 131, 132, 154, 155], "16": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 134, 154, 155], "lora_dropout": [48, 49, 50, 51, 52, 53, 65, 66, 67, 68, 69, 70], "05": [48, 49, 50, 51, 52, 53, 65, 66, 67, 68, 69, 70], "quantize_bas": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 100, 155], "lora": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 100, 101, 104, 105, 110, 128, 144, 146, 149, 152, 153], "code_llama2_13b": 48, "tloen": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85], "8bb8579e403dc78e37fe81ffbb253c413007323f": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85], "l41": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85], "l43": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 85], "linear": [48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 65, 66, 67, 68, 69, 70, 74, 75, 76, 77, 78, 79, 83, 84, 85, 88, 94, 99, 100, 104, 105, 154, 155], "mlp": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 94, 95, 104, 105, 153, 154], "final": [48, 49, 50, 65, 66, 67, 74, 75, 78, 79, 85, 90, 94, 101, 104, 105, 107, 151, 153, 154, 155], "low": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 100, 151, 154, 155], "approxim": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 100, 154], "factor": [48, 49, 50, 57, 58, 65, 66, 67, 74, 75, 78, 79, 85, 100, 151], "code_llama2_70b": 49, "code_llama2_7b": 50, "qlora": [51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 96, 144, 146, 153, 154], "per": [51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 91, 96, 148, 153, 155], "paper": [51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 154, 155], "ab": [51, 52, 53, 59, 60, 61, 62, 63, 68, 69, 70, 76, 77, 83, 84, 88, 93, 98], "2305": [51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88, 89, 98], "14314": [51, 52, 53, 59, 60, 68, 69, 70, 76, 77, 83, 84, 88], "lora_code_llama2_13b": 51, "lora_code_llama2_70b": 52, "lora_code_llama2_7b": 53, "gemma": [54, 55, 56, 57, 58, 59, 60, 113], "gemmatransformerdecod": [54, 55, 57, 58, 59, 60], "blog": [54, 55], "technolog": [54, 55], "develop": [54, 55, 155], "sentencepiecetoken": [56, 64, 82, 149], "gemma_2b": 57, "gemma_7b": 58, "lora_gemma_2b": 59, "lora_gemma_7b": 60, "2307": [61, 62, 63], "09288": [61, 62, 63], "llama2_70b": 66, "llama2_7b": [67, 154], "lora_llama2_13b": 68, "lora_llama2_70b": 69, "lora_llama2_7b": [70, 154], "tiktokentoken": [73, 149], "llama3_70b": 74, "llama3_8b": [75, 117, 153], "lora_llama3_70b": 76, "lora_llama3_8b": 77, "classifi": [79, 81, 84, 150], "announc": 80, "ray2333": 81, "reward": [81, 98], "feedback": 81, "transformerclassifi": 81, "lora_mistral_7b": 83, "lora_mistral_classifier_7b": 84, "phi3": [85, 86, 87, 88, 113, 148], "phi3_mini": [85, 113], "ref": [86, 87, 132], "phi": [86, 87, 113], "128k": 86, "nor": 86, "slide": 86, "window": [86, 150], "phi3minisentencepiecetoken": 87, "tokenizer_config": 87, "spm": 87, "lm": 87, "eo": [87, 106, 107, 149, 150], "bo": [87, 106, 107, 149, 150], "unk": 87, "augment": [87, 155], "endoftext": 87, "opt": [87, 145, 152], "cite": 87, "better": [87, 146, 149, 150, 151], "51": 87, "lora_phi3_mini": 88, "head_dim": [89, 91, 94], "pos_embed": [89, 154], "kv_cach": 89, "kvcach": [89, 94], "attn_dropout": [89, 94], "head": [89, 91, 93, 94, 113, 153], "queri": [89, 91, 94, 95, 153], "gqa": 89, "introduc": [89, 92, 100, 149, 150, 154, 155], "13245v1": 89, "version": [89, 117, 140, 145, 149, 153, 155], "multihead": 89, "mha": [89, 94], "extrem": 89, "share": [89, 150, 151], "mqa": 89, "credit": 89, "document": [89, 109, 120, 128, 148, 150], "lightn": 89, "lit": 89, "lit_gpt": 89, "v": [89, 94, 154], "k": [89, 154], "q": [89, 154], "n_kv_head": 89, "dimens": [89, 91, 93, 94, 100, 153, 154, 155], "calcul": [89, 94, 153], "e": [89, 96, 99, 103, 110, 114, 122, 139, 145, 151, 153, 154, 155], "g": [89, 99, 110, 122, 139, 153, 154, 155], "rotarypositionalembed": [89, 154], "cach": [89, 91, 93, 94, 145, 148], "rope": [89, 93], "dropout": [89, 100, 154, 155], "onto": 89, "scaled_dot_product_attent": 89, "seq_length": [89, 95, 117], "boolean": [89, 94, 95, 109], "softmax": [89, 94, 95], "row": [89, 94, 95, 149], "j": [89, 94, 95], "seq_len": 89, "bigger": 89, "n_h": [89, 93], "num": [89, 93], "n_kv": 89, "kv": [89, 91, 94], "emb": [89, 94], "h_d": [89, 93], "gate_proj": 90, "down_proj": 90, "up_proj": 90, "silu": 90, "feed": [90, 95], "network": [90, 101, 154, 155], "fed": [90, 149], "multipli": 90, "subclass": [90, 115], "although": [90, 154], "afterward": 90, "former": 90, "regist": [90, 96, 135, 155], "hook": [90, 96, 135, 155], "latter": 90, "standalon": 91, "past": 91, "becaus": [91, 94, 112, 148, 149, 151, 153], "expand": 91, "dpython": [91, 94, 96], "reset": [91, 94, 122], "k_val": 91, "v_val": 91, "h": [91, 145, 148], "longer": [91, 150], "ep": 92, "1e": 92, "06": [92, 154], "root": [92, 131, 132], "squar": 92, "1910": 92, "07467": 92, "verfic": [92, 93], "small": [92, 151], "divis": 92, "10000": 93, "rotari": [93, 153], "propos": 93, "2104": 93, "09864": 93, "l450": 93, "upto": 93, "init": [93, 122, 132, 155], "exceed": 93, "freq": 93, "recomput": 93, "geometr": 93, "progress": [93, 152], "rotat": 93, "angl": 93, "todo": 93, "effici": [93, 104, 120, 144, 146, 151, 152, 154], "transformerdecoderlay": 94, "norm": [94, 95], "space": 94, "belong": [94, 116], "reduc": [94, 146, 150, 154, 155], "statement": 94, "improv": [94, 120, 151, 153, 154], "readabl": [94, 151], "At": 94, "arang": 94, "prompt_length": 94, "causal_mask": 94, "m_": 94, "seq": 94, "reset_cach": 94, "setup_cach": 94, "attn": [95, 154, 155], "causalselfattent": [95, 154], "sa_norm": 95, "mlp_norm": 95, "ff": 95, "common_util": 96, "bfloat16": [96, 137, 151, 152, 153, 154], "offload_to_cpu": 96, "nf4": [96, 155], "restor": 96, "higher": [96, 153, 155], "offload": [96, 155], "increas": [96, 97, 153, 154], "peak": [96, 122, 127, 151, 153, 154, 155], "gpu": [96, 148, 151, 152, 153, 154, 155], "_register_state_dict_hook": 96, "m": [96, 107, 117, 149], "mymodul": 96, "_after_": 96, "nf4tensor": [96, 155], "unquant": [96, 151, 155], "unus": 96, "num_warmup_step": 97, "num_training_step": 97, "num_cycl": [97, 139], "last_epoch": 97, "lambdalr": 97, "rate": [97, 146, 152], "schedul": [97, 139, 152], "linearli": 97, "decreas": [97, 150, 154, 155], "cosin": 97, "v4": 97, "23": [97, 153], "src": 97, "l104": 97, "warmup": [97, 139], "phase": 97, "wave": 97, "half": 97, "lr_schedul": 97, "beta": 98, "label_smooth": 98, "loss_typ": 98, "sigmoid": 98, "dpo": [98, 101, 134], "18290": 98, "trl": 98, "librari": [98, 115, 119, 121, 138, 144, 146, 148, 150, 155], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 98, "dpo_train": 98, "l844": 98, "temperatur": [98, 117, 151], "uncertainti": 98, "hing": 98, "ipo": 98, "kto_pair": 98, "policy_chosen_logp": 98, "policy_rejected_logp": 98, "reference_chosen_logp": 98, "reference_rejected_logp": 98, "polici": [98, 101, 109, 120, 128, 136], "probabl": [98, 100, 117, 151], "chosen": [98, 139, 150], "reject": [98, 150], "chosen_reward": 98, "rejected_reward": 98, "unknown": 98, "peft": [99, 100, 101, 102, 103, 104, 105, 110, 154, 155], "protocol": 99, "adapter_param": [99, 100, 101, 102, 103], "proj": 99, "in_dim": [99, 100, 154, 155], "out_dim": [99, 100, 154, 155], "bia": [99, 100, 154, 155], "loralinear": [99, 154, 155], "alpha": [100, 154, 155], "use_bia": 100, "perturb": 100, "decomposit": [100, 154], "matric": [100, 128, 154, 155], "trainabl": [100, 103, 128, 154, 155], "mapsto": 100, "w_0x": 100, "r": [100, 107, 154], "bax": 100, "lora_a": [100, 154, 155], "lora_b": [100, 154, 155], "temporarili": 101, "neural": [101, 154, 155], "treat": [101, 115, 149], "attribut": [101, 106, 116], "caller": 101, "whose": [101, 135], "yield": 101, "get_adapter_param": [103, 154], "base_miss": 104, "base_unexpect": 104, "lora_miss": 104, "lora_unexpect": 104, "validate_state_dict_for_lora": [104, 154], "unlik": [104, 151, 153], "reli": 104, "unexpect": 104, "strict": [104, 154], "pull": [104, 148], "120600": 104, "assertionerror": [104, 105, 134], "nonempti": 104, "full_model_state_dict_kei": 105, "lora_state_dict_kei": 105, "base_model_state_dict_kei": 105, "confirm": [105, 145], "determin": 105, "lora_modul": 105, "complement": 105, "disjoint": 105, "union": [105, 129, 130, 131, 132, 136, 138], "non": [105, 106], "overlap": 105, "sentencepieceprocessor": 106, "pretrain": [106, 107, 148, 149, 152, 154, 155], "spm_model": [106, 149], "tokenized_text": 106, "world": [106, 124, 126, 151], "add_bo": [106, 107, 108, 149], "add_eo": [106, 107, 108, 149], "31587": 106, "29644": 106, "102": 106, "trim_leading_whitespac": 106, "prefix": 106, "unbatch": 106, "prepend": [106, 107], "append": [106, 145], "trim": 106, "whitespac": 106, "underli": [106, 155], "sentencepiec": [106, 153], "due": [106, 154, 155], "tokenize_messag": [106, 107, 108, 149, 150], "problem": 106, "slice": 106, "tokenizer_path": 106, "separ": [106, 110, 149, 152, 153, 154, 155], "concat": 106, "1788": 106, "2643": 106, "13": [106, 134, 151, 153, 155], "1792": 106, "9508": 106, "465": 106, "22137": 106, "2933": 106, "join": 106, "llama3_tiktoken": 107, "p": [107, 109, 114, 154, 155], "l": 107, "all_special_token": 107, "bos_token": 107, "begin_of_text": [107, 149], "eos_token": 107, "end_of_text": 107, "start_header_id": [107, 149], "end_header_id": [107, 149], "step_id": 107, "eom_id": 107, "eot_id": [107, 149], "python_tag": 107, "identif": 107, "regex": 107, "second": [107, 151, 153, 154, 155], "uniqu": [107, 113], "256": [107, 151, 153], "header": [107, 149], "token_id": [107, 108], "truncate_at_eo": 107, "tokenize_head": 107, "datatyp": [109, 155], "denot": 109, "integ": [109, 133, 138], "auto_wrap_polici": [109, 120, 136], "submodul": [109, 128], "obei": 109, "contract": 109, "get_fsdp_polici": 109, "modules_to_wrap": [109, 120, 128], "min_num_param": 109, "my_fsdp_polici": 109, "recurs": [109, 128, 131], "isinst": [109, 150], "sum": [109, 154], "numel": [109, 154], "1000": 109, "functool": 109, "partial": 109, "stabl": [109, 126, 131, 138, 145], "html": [109, 115, 121, 126, 131, 136, 138, 144], "alia": 109, "safe_seri": 110, "from_pretrain": 110, "0001_of_0003": 110, "0002_of_0003": 110, "preserv": [110, 155], "weight_map": [110, 151], "convert_weight": 110, "_model_typ": [110, 113], "intermediate_checkpoint": [110, 111, 112], "_weight_map": 110, "shard": [111, 153], "wip": 111, "larger": [112, 151, 153], "present": 112, "down": [112, 150, 154, 155], "intermedi": [112, 136, 153, 155], "qualnam": 113, "boundari": 113, "distinguish": 113, "gate": [113, 148, 152], "my_new_model": 113, "my_custom_state_dict_map": 113, "mistral_reward": 113, "classif": 113, "mistral_classifi": 113, "optim_map": 114, "bare": 114, "bone": 114, "distribut": [114, 118, 125, 126, 136, 138, 146, 148, 152, 153], "optim_dict": [114, 116, 135], "cfg_optim": 114, "ckpt": 114, "optim_ckpt": 114, "placeholder_optim_dict": 114, "optiminbackwardwrapp": 114, "get_optim_kei": 114, "arbitrari": [114, 154], "hyperparamet": [114, 146, 152, 154, 155], "optim_ckpt_map": 114, "runtimeerror": [114, 119, 125], "loadabl": 114, "argpars": 115, "argumentpars": 115, "builtin": 115, "said": 115, "noth": 115, "consult": 115, "info": [115, 152], "parse_known_arg": 115, "namespac": 115, "act": 115, "precid": 115, "parse_arg": 115, "properti": [115, 154], "too": [115, 153], "optimizerinbackwardwrapp": 116, "top": [116, 151, 155], "named_paramet": 116, "max_generated_token": 117, "pad_id": 117, "top_k": [117, 151], "stop_token": 117, "custom_generate_next_token": 117, "condit": [117, 126, 148, 150], "bsz": 117, "predict": 117, "prune": [117, 155], "stop": 117, "compil": [117, 151, 153, 155], "generate_next_token": 117, "llama3_token": [117, 153], "hi": [117, 149], "my": [117, 148, 149, 150, 151, 153], "jeremi": 117, "float32": 119, "bf16": [119, 155], "inde": [119, 151], "kernel": 119, "isn": [119, 148], "hardwar": [119, 146, 150, 151, 154], "memory_efficient_fsdp_wrap": 120, "maxim": [120, 128, 144, 146], "been": [120, 153], "workload": 120, "15": [120, 134, 149, 151, 154, 155], "alongsid": 120, "ac": 120, "fullyshardeddataparallel": [120, 128], "fsdppolicytyp": [120, 128], "handler": 121, "reset_stat": 122, "track": 122, "alloc": [122, 127, 128, 153, 155], "reserv": [122, 127, 149, 155], "stat": [122, 127, 155], "int4": 123, "4w": [123, 151, 153], "recogn": 123, "mode": [123, 151], "aka": 124, "master": 126, "port": [126, 148], "address": 126, "hold": [126, 152], "peak_memory_act": 127, "peak_memory_alloc": 127, "peak_memory_reserv": 127, "get_memory_stat": 127, "own": [128, 138, 148, 149, 150, 151, 154], "unit": [128, 146], "hierarch": 128, "requires_grad": [128, 154, 155], "filenam": 129, "log_": 129, "unixtimestamp": 129, "txt": [129, 150, 152], "thread": 129, "safe": 129, "flush": [129, 130, 131, 132], "ndarrai": [129, 130, 131, 132], "scalar": [129, 130, 131, 132], "record": [129, 130, 131, 132, 139], "payload": [129, 130, 131, 132], "organize_log": 131, "tensorboard": 131, "subdirectori": 131, "compar": [131, 140, 151, 154, 155], "logdir": 131, "startup": 131, "tree": [131, 150, 151], "tfevent": 131, "encount": 131, "frontend": 131, "organ": [131, 148], "accordingli": 131, "my_log_dir": 131, "view": [131, 151, 152], "my_metr": [131, 132], "termin": [131, 132], "entiti": 132, "bias": 132, "sent": 132, "usernam": 132, "my_project": 132, "my_ent": 132, "my_group": 132, "importerror": 132, "account": [132, 154, 155], "log_config": 132, "link": [132, 151], "capecap": 132, "6053ofw0": 132, "torchtune_config_j67sb73v": 132, "padding_idx": [133, 134], "ignore_idx": [133, 134], "longest": 133, "token_pair": 133, "input_id": 134, "chosen_input_id": [134, 150], "chosen_label": [134, 150], "rejected_input_id": [134, 150], "rejected_label": [134, 150], "14": [134, 155], "17": [134, 151, 154], "18": [134, 153], "19": [134, 151, 153, 155], "20": 134, "soon": 135, "readi": [135, 144, 149], "grad": 135, "achiev": [135, 151, 153, 154, 155], "acwrappolicytyp": 136, "author": [136, 146, 152, 155], "fsdp_adavnced_tutori": 136, "insid": 137, "contextmanag": 137, "debug_mod": 138, "pseudo": 138, "random": [138, 152], "commonli": [138, 151, 154, 155], "numpi": 138, "determinist": 138, "global": [138, 150], "warn": 138, "nondeterminist": 138, "addition": [138, 150, 154], "cudnn": 138, "set_deterministic_debug_mod": 138, "algorithm": 138, "profile_memori": 139, "with_stack": 139, "record_shap": 139, "with_flop": 139, "wait_step": 139, "warmup_step": 139, "active_step": 139, "profil": 139, "layout": 139, "trace": 139, "profileract": 139, "flop": 139, "wait": 139, "cycl": 139, "repeat": 139, "speed": [139, 153, 155], "reduct": [139, 154], "iter": [139, 141, 155], "scope": 139, "gradient_accumul": 139, "sensibl": 139, "similarli": [139, 150], "default_schedul": 139, "greater": 140, "equal": 140, "against": [140, 155], "__version__": 140, "named_param": 141, "generated_examples_python": 142, "zip": 142, "galleri": [142, 147], "sphinx": 142, "000": [143, 147, 153], "execut": [143, 147], "generated_exampl": 143, "mem": [143, 147], "mb": [143, 147], "topic": 144, "gentl": 144, "introduct": 144, "first_finetune_tutori": 144, "workflow": [144, 150, 152, 154], "requisit": 145, "proper": [145, 152], "host": [145, 148, 152], "latest": [145, 152, 155], "And": [145, 151, 153], "ls": [145, 148, 151, 152, 153], "welcom": [145, 148], "show": [145, 148, 149, 154], "greatest": [145, 152], "contributor": 145, "cd": [145, 151], "even": [145, 148, 149, 150, 153, 154, 155], "commit": 145, "branch": 145, "url": 145, "whl": 145, "therebi": [145, 155], "forc": 145, "reinstal": 145, "suffix": 145, "cu121": 145, "On": [146, 154], "pointer": 146, "emphas": 146, "aspect": 146, "simplic": 146, "component": 146, "reus": 146, "prove": 146, "democrat": 146, "box": [146, 155], "zoo": 146, "varieti": [146, 154], "techniqu": [146, 151, 152, 154], "integr": [146, 151, 152, 153, 154, 155], "excit": 146, "checkout": 146, "quickstart": 146, "attain": 146, "chekckpoint": 146, "embodi": 146, "philosophi": 146, "usabl": 146, "composit": 146, "hard": [146, 150], "outlin": 146, "unecessari": 146, "never": 146, "thoroughli": 146, "short": 148, "subcommand": 148, "anytim": 148, "symlink": 148, "auto": 148, "wrote": 148, "readm": 148, "md": 148, "lot": [148, 151], "recent": 148, "releas": [148, 153], "agre": 148, "term": 148, "perman": 148, "eat": 148, "bandwith": 148, "storag": [148, 155], "00030": 148, "ootb": 148, "full_finetune_single_devic": [148, 150, 151, 152], "7b_full_low_memori": [148, 151, 152], "8b_full_single_devic": [148, 150], "mini_full_low_memori": 148, "7b_full": [148, 151, 152], "13b_full": [148, 151, 152], "70b_full": 148, "edit": 148, "destin": 148, "lora_finetune_distribut": [148, 153, 154], "torchrun": 148, "8b_lora_single_devic": [148, 149, 153], "launch": [148, 149, 152], "nproc": 148, "node": 148, "worker": 148, "nnode": [148, 154], "minimum_nod": 148, "maximum_nod": 148, "fail": 148, "rdzv": 148, "rendezv": 148, "endpoint": 148, "8b_lora": [148, 153], "bypass": 148, "vice": 148, "versa": 148, "fancy_lora": 148, "8b_fancy_lora": 148, "sai": [148, 149, 152], "align": 149, "intend": 149, "nice": 149, "meet": 149, "overhaul": 149, "accompani": 149, "who": 149, "influenti": 149, "hip": 149, "hop": 149, "artist": [149, 153], "2pac": 149, "rakim": 149, "c": 149, "na": 149, "flavor": [149, 150], "msg": 149, "formatted_messag": [149, 150], "nyou": [149, 150], "nwho": 149, "why": [149, 152, 154], "user_messag": 149, "518": 149, "25580": 149, "29962": 149, "3532": 149, "14816": 149, "29903": 149, "6778": 149, "piece_to_id": 149, "vector": 149, "place": 149, "manual": [149, 155], "529": 149, "29879": 149, "29958": 149, "nhere": 149, "_encode_special_token": 149, "128000": 149, "128009": 149, "pure": 149, "That": 149, "won": [149, 151, 153], "mess": 149, "govern": 149, "prime": 149, "strictli": 149, "summarizetempl": [149, 150], "lightweight": 149, "ask": 149, "untouch": 149, "nsummari": 149, "robust": 149, "csv": [149, 150], "onlin": 149, "forum": 149, "panda": 149, "pd": 149, "df": 149, "read_csv": 149, "your_fil": 149, "nrow": 149, "tolist": 149, "iloc": 149, "gp": 149, "receiv": 149, "commun": [149, 150, 151], "satellit": 149, "thing": [149, 155], "message_convert": 149, "input_msg": 149, "output_msg": 149, "assistant_messag": 149, "But": [149, 151, 153, 154], "mistralchatformat": 149, "custom_dataset": 149, "2048": 149, "data_fil": [149, 150], "honor": 149, "copi": [149, 151, 152, 153, 155], "custom_8b_lora_single_devic": 149, "steer": 150, "wheel": 150, "publicli": 150, "great": [150, 151], "hood": [150, 151, 155], "text_completion_dataset": 150, "padded_col": 150, "upper": 150, "constraint": [150, 154], "slow": [150, 155], "signific": 150, "speedup": [150, 153], "minim": [150, 152, 154, 155], "my_data": 150, "instruct_dataset": 150, "fix": 150, "goal": 150, "agnost": 150, "respond": 150, "anim": 150, "plant": 150, "miner": 150, "oak": 150, "copper": 150, "ore": 150, "eleph": 150, "customtempl": 150, "cl": 150, "chat_dataset": 150, "quit": [150, 155], "incorpor": 150, "advanc": 150, "customchatformat": 150, "vicgal": 150, "gpt4": 150, "drive": 150, "rajpurkar": 150, "io": 150, "squad": 150, "explor": 150, "rlhf": 150, "few": [150, 153, 154, 155], "adjust": 150, "chosen_messag": 150, "transformed_sampl": 150, "key_chosen": 150, "rejected_messag": 150, "key_reject": 150, "c_mask": 150, "np": 150, "cross_entropy_ignore_idx": 150, "r_mask": 150, "stack_exchanged_paired_dataset": 150, "had": 150, "stackexchangedpairedtempl": 150, "response_j": 150, "response_k": 150, "rl": 150, "favorit": [151, 153, 154], "seemlessli": 151, "beyond": [151, 155], "connect": 151, "amount": 151, "natur": 151, "export": 151, "mobil": 151, "phone": 151, "leverag": [151, 153, 155], "plai": 151, "freez": [151, 154], "percentag": 151, "learnabl": 151, "keep": [151, 154], "16gb": [151, 154], "rtx": 151, "3090": 151, "4090": 151, "hour": 151, "7b_qlora_single_devic": [151, 152, 155], "473": 151, "98": [151, 155], "gb": [151, 153, 154, 155], "50": 151, "484": 151, "01": [151, 152], "fact": [151, 153, 154], "third": 151, "realli": 151, "eleuther_ev": [151, 153], "eleuther_evalu": [151, 153], "lm_eval": [151, 153], "plan": 151, "custom_eval_config": [151, 153], "truthfulqa_mc2": [151, 153, 154], "measur": [151, 153], "propens": [151, 153], "shot": [151, 153], "accuraci": [151, 153, 154, 155], "baselin": [151, 154], "324": 151, "loglikelihood": 151, "195": 151, "121": 151, "27": 151, "197": 151, "acc": 151, "388": 151, "38": 151, "shown": 151, "489": 151, "48": [151, 155], "seem": 151, "custom_generation_config": [151, 153], "kick": 151, "300": 151, "interest": 151, "site": 151, "visit": 151, "bai": 151, "area": 151, "92": [151, 153], "exploratorium": 151, "san": 151, "francisco": 151, "magazin": 151, "awesom": 151, "bridg": 151, "pretti": 151, "cool": 151, "96": [151, 155], "61": 151, "sec": [151, 153], "25": 151, "83": 151, "99": [151, 154], "72": 151, "littl": 151, "saw": 151, "took": [151, 153], "torchao": [151, 153, 155], "bit": [151, 153, 154, 155], "custom_quantization_config": [151, 153], "68": 151, "76": 151, "69": 151, "95": [151, 153], "67": 151, "engin": [151, 153], "fullmodeltorchtunecheckpoint": [151, 153], "int4weightonlyquant": [151, 153], "groupsiz": [151, 153], "park": 151, "sit": 151, "hill": 151, "beauti": 151, "62": [151, 153], "85": 151, "sped": 151, "almost": [151, 153, 154], "3x": [151, 153], "benefit": 151, "doesn": 151, "yet": 151, "fast": 151, "clone": [151, 154, 155], "assumpt": 151, "satisfi": 151, "new_dir": 151, "output_dict": 151, "sd_1": 151, "sd_2": 151, "dump": 151, "convert_hf_checkpoint": 151, "checkpoint_path": 151, "justin": 151, "school": 151, "math": 151, "teacher": 151, "ws": 151, "94": [151, 153], "28": 151, "bandwidth": [151, 153], "1391": 151, "84": 151, "thats": 151, "seamlessli": 151, "authent": [151, 152], "hopefulli": 151, "gave": 151, "grant": 152, "minut": 152, "agreement": 152, "altern": 152, "hackabl": 152, "singularli": 152, "technic": 152, "purpos": [152, 153], "depth": 152, "principl": 152, "boilerpl": 152, "substanti": [152, 154], "custom_config": 152, "replic": 152, "lorafinetunerecipesingledevic": 152, "lora_finetune_output": 152, "log_1713194212": 152, "52": 152, "3697006702423096": 152, "25880": [152, 155], "24": [152, 153], "55": 152, "83it": 152, "monitor": 152, "tqdm": 152, "interv": 152, "e2": 152, "focu": 153, "128": [153, 154], "theta": 153, "gain": 153, "illustr": 153, "basic": 153, "observ": 153, "consum": [153, 155], "vram": [153, 154], "overal": 153, "8b_qlora_single_devic": 153, "coupl": [153, 154, 155], "meta_model_0": 153, "122": 153, "sarah": 153, "busi": 153, "mum": 153, "young": 153, "children": 153, "live": 153, "north": 153, "east": 153, "england": 153, "135": 153, "88": 153, "138": 153, "346": 153, "09": 153, "139": 153, "31": 153, "far": 153, "drill": 153, "90": 153, "93": 153, "91": 153, "104": 153, "four": [153, 154], "again": 153, "jake": 153, "disciplin": 153, "passion": 153, "draw": 153, "paint": 153, "57": [153, 154, 155], "broader": 153, "teach": 154, "straight": 154, "unfamiliar": 154, "oppos": [154, 155], "momentum": 154, "relat": 154, "aghajanyan": 154, "et": 154, "al": 154, "hypothes": 154, "intrins": 154, "often": 154, "eight": 154, "practic": 154, "imag": 154, "left": 154, "blue": 154, "rememb": 154, "approx": 154, "15m": 154, "8192": 154, "65k": 154, "frozen_out": [154, 155], "lora_out": [154, 155], "base_model": 154, "choos": 154, "lora_model": 154, "lora_llama_2_7b": [154, 155], "alon": 154, "in_featur": 154, "out_featur": 154, "inplac": 154, "feel": 154, "free": 154, "whenev": 154, "peft_util": 154, "set_trainable_param": 154, "fetch": 154, "lora_param": 154, "total_param": 154, "trainable_param": 154, "2f": 154, "6742609920": 154, "4194304": 154, "7b_lora": 154, "my_model_checkpoint_path": [154, 155], "tokenizer_checkpoint": [154, 155], "my_tokenizer_checkpoint_path": [154, 155], "factori": 154, "benefici": 154, "impact": 154, "minor": 154, "good": 154, "64": 154, "lora_experiment_1": 154, "smooth": [154, 155], "curv": [154, 155], "500": 154, "ran": 154, "footprint": 154, "commod": 154, "cogniz": 154, "ax": 154, "parallel": 154, "truthfulqa": 154, "previous": 154, "475": 154, "87": 154, "508": 154, "86": 154, "504": 154, "04": 154, "514": 154, "lowest": 154, "absolut": 154, "4gb": 154, "tradeoff": 154, "potenti": 154, "highli": 155, "vanilla": 155, "held": 155, "therefor": 155, "bespok": 155, "normalfloat": 155, "8x": 155, "retain": 155, "vast": 155, "major": 155, "degrad": 155, "normatfloat": 155, "doubl": 155, "themselv": 155, "deepdiv": 155, "idea": 155, "distinct": 155, "de": 155, "incur": 155, "counterpart": 155, "set_default_devic": 155, "qlora_linear": 155, "memory_alloc": 155, "177": 155, "152": 155, "byte": 155, "del": 155, "empty_cach": 155, "lora_linear": 155, "081": 155, "344": 155, "qlora_llama2_7b": 155, "qlora_model": 155, "essenti": 155, "reparametrize_as_dtype_state_dict_post_hook": 155, "35": 155, "40": 155, "29": 155, "slower": 155, "149": 155, "9157477021217346": 155, "02": 155, "08": 155, "15it": 155, "nightli": 155, "200": 155, "hundr": 155, "228": 155, "8158286809921265": 155, "59": 155, "95it": 155, "exercis": 155, "portion": 155, "linear_nf4": 155, "to_nf4": 155, "linear_weight": 155, "autograd": 155, "regular": 155, "incom": 155}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "log_config"], [12, 0, 1, "", "parse"], [13, 0, 1, "", "validate"]], "torchtune.data": [[14, 1, 1, "", "AlpacaInstructTemplate"], [15, 1, 1, "", "ChatFormat"], [16, 1, 1, "", "ChatMLFormat"], [17, 1, 1, "", "GrammarErrorCorrectionTemplate"], [18, 1, 1, "", "InstructTemplate"], [19, 1, 1, "", "Llama2ChatFormat"], [20, 1, 1, "", "Message"], [21, 1, 1, "", "MistralChatFormat"], [22, 1, 1, "", "StackExchangedPairedTemplate"], [23, 1, 1, "", "SummarizeTemplate"], [24, 0, 1, "", "get_openai_messages"], [25, 0, 1, "", "get_sharegpt_messages"], [26, 0, 1, "", "truncate"], [27, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[14, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[15, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[16, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[18, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[19, 2, 1, "", "format"]], "torchtune.data.Message": [[20, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[21, 2, 1, "", "format"], [21, 3, 1, "", "system"]], "torchtune.data.StackExchangedPairedTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.SummarizeTemplate": [[23, 2, 1, "", "format"]], "torchtune.datasets": [[28, 1, 1, "", "ChatDataset"], [29, 1, 1, "", "ConcatDataset"], [30, 1, 1, "", "InstructDataset"], [31, 1, 1, "", "PackedDataset"], [32, 1, 1, "", "PreferenceDataset"], [33, 1, 1, "", "TextCompletionDataset"], [34, 0, 1, "", "alpaca_cleaned_dataset"], [35, 0, 1, "", "alpaca_dataset"], [36, 0, 1, "", "chat_dataset"], [37, 0, 1, "", "cnn_dailymail_articles_dataset"], [38, 0, 1, "", "grammar_dataset"], [39, 0, 1, "", "instruct_dataset"], [40, 0, 1, "", "samsum_dataset"], [41, 0, 1, "", "slimorca_dataset"], [42, 0, 1, "", "stack_exchanged_paired_dataset"], [43, 0, 1, "", "text_completion_dataset"], [44, 0, 1, "", "wikitext_dataset"]], "torchtune.models.code_llama2": [[45, 0, 1, "", "code_llama2_13b"], [46, 0, 1, "", "code_llama2_70b"], [47, 0, 1, "", "code_llama2_7b"], [48, 0, 1, "", "lora_code_llama2_13b"], [49, 0, 1, "", "lora_code_llama2_70b"], [50, 0, 1, "", "lora_code_llama2_7b"], [51, 0, 1, "", "qlora_code_llama2_13b"], [52, 0, 1, "", "qlora_code_llama2_70b"], [53, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[54, 0, 1, "", "gemma_2b"], [55, 0, 1, "", "gemma_7b"], [56, 0, 1, "", "gemma_tokenizer"], [57, 0, 1, "", "lora_gemma_2b"], [58, 0, 1, "", "lora_gemma_7b"], [59, 0, 1, "", "qlora_gemma_2b"], [60, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.llama2": [[61, 0, 1, "", "llama2_13b"], [62, 0, 1, "", "llama2_70b"], [63, 0, 1, "", "llama2_7b"], [64, 0, 1, "", "llama2_tokenizer"], [65, 0, 1, "", "lora_llama2_13b"], [66, 0, 1, "", "lora_llama2_70b"], [67, 0, 1, "", "lora_llama2_7b"], [68, 0, 1, "", "qlora_llama2_13b"], [69, 0, 1, "", "qlora_llama2_70b"], [70, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[71, 0, 1, "", "llama3_70b"], [72, 0, 1, "", "llama3_8b"], [73, 0, 1, "", "llama3_tokenizer"], [74, 0, 1, "", "lora_llama3_70b"], [75, 0, 1, "", "lora_llama3_8b"], [76, 0, 1, "", "qlora_llama3_70b"], [77, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[78, 0, 1, "", "lora_mistral_7b"], [79, 0, 1, "", "lora_mistral_classifier_7b"], [80, 0, 1, "", "mistral_7b"], [81, 0, 1, "", "mistral_classifier_7b"], [82, 0, 1, "", "mistral_tokenizer"], [83, 0, 1, "", "qlora_mistral_7b"], [84, 0, 1, "", "qlora_mistral_classifier_7b"]], "torchtune.models.phi3": [[85, 0, 1, "", "lora_phi3_mini"], [86, 0, 1, "", "phi3_mini"], [87, 0, 1, "", "phi3_mini_tokenizer"], [88, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[89, 1, 1, "", "CausalSelfAttention"], [90, 1, 1, "", "FeedForward"], [91, 1, 1, "", "KVCache"], [92, 1, 1, "", "RMSNorm"], [93, 1, 1, "", "RotaryPositionalEmbeddings"], [94, 1, 1, "", "TransformerDecoder"], [95, 1, 1, "", "TransformerDecoderLayer"], [97, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[89, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[90, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[91, 2, 1, "", "reset"], [91, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[92, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[93, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[94, 2, 1, "", "forward"], [94, 2, 1, "", "reset_caches"], [94, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[95, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[96, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[98, 1, 1, "", "DPOLoss"]], "torchtune.modules.loss.DPOLoss": [[98, 2, 1, "", "forward"]], "torchtune.modules.peft": [[99, 1, 1, "", "AdapterModule"], [100, 1, 1, "", "LoRALinear"], [101, 0, 1, "", "disable_adapter"], [102, 0, 1, "", "get_adapter_params"], [103, 0, 1, "", "set_trainable_params"], [104, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [105, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[99, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[100, 2, 1, "", "adapter_params"], [100, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[106, 1, 1, "", "SentencePieceTokenizer"], [107, 1, 1, "", "TikTokenTokenizer"], [108, 1, 1, "", "Tokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[106, 2, 1, "", "decode"], [106, 2, 1, "", "encode"], [106, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[107, 2, 1, "", "decode"], [107, 2, 1, "", "encode"], [107, 2, 1, "", "tokenize_message"], [107, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.Tokenizer": [[108, 2, 1, "", "decode"], [108, 2, 1, "", "encode"], [108, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[109, 4, 1, "", "FSDPPolicyType"], [110, 1, 1, "", "FullModelHFCheckpointer"], [111, 1, 1, "", "FullModelMetaCheckpointer"], [112, 1, 1, "", "FullModelTorchTuneCheckpointer"], [113, 1, 1, "", "ModelType"], [114, 1, 1, "", "OptimizerInBackwardWrapper"], [115, 1, 1, "", "TuneRecipeArgumentParser"], [116, 0, 1, "", "create_optim_in_bwd_wrapper"], [117, 0, 1, "", "generate"], [118, 0, 1, "", "get_device"], [119, 0, 1, "", "get_dtype"], [120, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [121, 0, 1, "", "get_logger"], [122, 0, 1, "", "get_memory_stats"], [123, 0, 1, "", "get_quantizer_mode"], [124, 0, 1, "", "get_world_size_and_rank"], [125, 0, 1, "", "init_distributed"], [126, 0, 1, "", "is_distributed"], [127, 0, 1, "", "log_memory_stats"], [128, 0, 1, "", "lora_fsdp_wrap_policy"], [133, 0, 1, "", "padded_collate"], [134, 0, 1, "", "padded_collate_dpo"], [135, 0, 1, "", "register_optim_in_bwd_hooks"], [136, 0, 1, "", "set_activation_checkpointing"], [137, 0, 1, "", "set_default_dtype"], [138, 0, 1, "", "set_seed"], [139, 0, 1, "", "setup_torch_profiler"], [140, 0, 1, "", "torch_version_ge"], [141, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[110, 2, 1, "", "load_checkpoint"], [110, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[111, 2, 1, "", "load_checkpoint"], [111, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[112, 2, 1, "", "load_checkpoint"], [112, 2, 1, "", "save_checkpoint"]], "torchtune.utils.ModelType": [[113, 3, 1, "", "GEMMA"], [113, 3, 1, "", "LLAMA2"], [113, 3, 1, "", "LLAMA3"], [113, 3, 1, "", "MISTRAL"], [113, 3, 1, "", "MISTRAL_REWARD"], [113, 3, 1, "", "PHI3_MINI"]], "torchtune.utils.OptimizerInBackwardWrapper": [[114, 2, 1, "", "get_optim_key"], [114, 2, 1, "", "load_state_dict"], [114, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[115, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[129, 1, 1, "", "DiskLogger"], [130, 1, 1, "", "StdoutLogger"], [131, 1, 1, "", "TensorBoardLogger"], [132, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[129, 2, 1, "", "close"], [129, 2, 1, "", "log"], [129, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[130, 2, 1, "", "close"], [130, 2, 1, "", "log"], [130, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[131, 2, 1, "", "close"], [131, 2, 1, "", "log"], [131, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[132, 2, 1, "", "close"], [132, 2, 1, "", "log"], [132, 2, 1, "", "log_config"], [132, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 109, 144, 146, 148, 151, 153, 154, 155], "config": [0, 7, 8, 148, 152], "data": [1, 5, 149], "text": [1, 150, 153], "templat": [1, 149, 150], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 149, 150], "exampl": 2, "gener": [2, 117, 151, 153], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 148, 151, 152, 153, 154], "llama3": [3, 149, 153], "llama2": [3, 149, 151, 154, 155], "code": 3, "llama": 3, "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 145, 155], "block": 4, "token": [4, 108, 149], "peft": 4, "util": [4, 5, 109], "loss": 4, "checkpoint": [5, 6, 9, 151], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 150, 154, 155], "manag": 5, "perform": [5, 154], "profil": 5, "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 146, 151], "format": [6, 150], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 151, 154, 155], "put": [6, 155], "thi": 6, "all": [6, 7, 155], "togeth": [6, 155], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 150], "us": [7, 8, 149, 151, 155], "instanti": [7, 10], "referenc": 7, "other": [7, 151], "field": 7, "interpol": 7, "valid": [7, 13, 148], "your": [7, 8, 151, 152], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "remov": 7, "what": [8, 146, 154, 155], "ar": 8, "recip": [8, 148, 152, 154], "script": 8, "run": [8, 148, 151], "cli": [8, 148], "pars": [8, 12], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "log_config": 11, "alpacainstructtempl": 14, "chatformat": 15, "chatmlformat": 16, "grammarerrorcorrectiontempl": 17, "instructtempl": 18, "llama2chatformat": 19, "messag": 20, "mistralchatformat": 21, "stackexchangedpairedtempl": 22, "summarizetempl": 23, "get_openai_messag": 24, "get_sharegpt_messag": 25, "truncat": 26, "validate_messag": 27, "chatdataset": 28, "concatdataset": 29, "instructdataset": 30, "packeddataset": 31, "preferencedataset": 32, "textcompletiondataset": 33, "alpaca_cleaned_dataset": 34, "alpaca_dataset": 35, "chat_dataset": 36, "cnn_dailymail_articles_dataset": 37, "grammar_dataset": 38, "instruct_dataset": 39, "samsum_dataset": 40, "slimorca_dataset": 41, "stack_exchanged_paired_dataset": 42, "text_completion_dataset": 43, "wikitext_dataset": 44, "code_llama2_13b": 45, "code_llama2_70b": 46, "code_llama2_7b": 47, "lora_code_llama2_13b": 48, "lora_code_llama2_70b": 49, "lora_code_llama2_7b": 50, "qlora_code_llama2_13b": 51, "qlora_code_llama2_70b": 52, "qlora_code_llama2_7b": 53, "gemma_2b": 54, "gemma_7b": 55, "gemma_token": 56, "lora_gemma_2b": 57, "lora_gemma_7b": 58, "qlora_gemma_2b": 59, "qlora_gemma_7b": 60, "llama2_13b": 61, "llama2_70b": 62, "llama2_7b": 63, "llama2_token": 64, "lora_llama2_13b": 65, "lora_llama2_70b": 66, "lora_llama2_7b": 67, "qlora_llama2_13b": 68, "qlora_llama2_70b": 69, "qlora_llama2_7b": 70, "llama3_70b": 71, "llama3_8b": 72, "llama3_token": 73, "lora_llama3_70b": 74, "lora_llama3_8b": 75, "qlora_llama3_70b": 76, "qlora_llama3_8b": 77, "lora_mistral_7b": 78, "lora_mistral_classifier_7b": 79, "mistral_7b": 80, "mistral_classifier_7b": 81, "mistral_token": 82, "qlora_mistral_7b": 83, "qlora_mistral_classifier_7b": 84, "lora_phi3_mini": 85, "phi3_mini": 86, "phi3_mini_token": 87, "qlora_phi3_mini": 88, "causalselfattent": 89, "todo": [89, 95], "feedforward": 90, "kvcach": 91, "rmsnorm": 92, "rotarypositionalembed": 93, "transformerdecod": 94, "transformerdecoderlay": 95, "reparametrize_as_dtype_state_dict_post_hook": 96, "get_cosine_schedule_with_warmup": 97, "dpoloss": 98, "adaptermodul": 99, "loralinear": 100, "disable_adapt": 101, "get_adapter_param": 102, "set_trainable_param": 103, "validate_missing_and_unexpected_for_lora": 104, "validate_state_dict_for_lora": 105, "sentencepiecetoken": 106, "tiktokentoken": 107, "fsdppolicytyp": 109, "fullmodelhfcheckpoint": 110, "fullmodelmetacheckpoint": 111, "fullmodeltorchtunecheckpoint": 112, "modeltyp": 113, "optimizerinbackwardwrapp": 114, "tunerecipeargumentpars": 115, "create_optim_in_bwd_wrapp": 116, "get_devic": 118, "get_dtyp": 119, "get_full_finetune_fsdp_wrap_polici": 120, "get_logg": 121, "get_memory_stat": 122, "get_quantizer_mod": 123, "get_world_size_and_rank": 124, "init_distribut": 125, "is_distribut": 126, "log_memory_stat": 127, "lora_fsdp_wrap_polici": 128, "disklogg": 129, "stdoutlogg": 130, "tensorboardlogg": 131, "wandblogg": 132, "padded_col": 133, "padded_collate_dpo": 134, "register_optim_in_bwd_hook": 135, "set_activation_checkpoint": 136, "set_default_dtyp": 137, "set_se": 138, "setup_torch_profil": 139, "torch_version_g": 140, "validate_expected_param_dtyp": 141, "comput": [143, 147], "time": [143, 147], "welcom": 144, "document": 144, "get": [144, 148, 153], "start": [144, 148], "tutori": 144, "instal": 145, "instruct": [145, 150, 153], "via": [145, 153], "pypi": 145, "git": 145, "clone": 145, "nightli": 145, "kei": 146, "concept": 146, "design": 146, "principl": 146, "download": [148, 151, 152], "list": 148, "built": [148, 150], "copi": 148, "fine": [149, 150, 152, 153], "tune": [149, 150, 152, 153], "chat": [149, 150], "chang": 149, "from": [149, 155], "prompt": 149, "special": 149, "when": 149, "should": 149, "i": 149, "custom": [149, 150], "hug": [150, 151], "face": [150, 151], "set": 150, "max": 150, "sequenc": 150, "length": 150, "sampl": 150, "pack": 150, "unstructur": 150, "corpu": 150, "multipl": 150, "local": 150, "remot": 150, "fulli": 150, "end": 151, "workflow": 151, "7b": 151, "finetun": [151, 154, 155], "evalu": [151, 153], "eleutherai": [151, 153], "s": [151, 153], "eval": [151, 153], "har": [151, 153], "speed": 151, "up": 151, "quantiz": [151, 153], "librari": 151, "upload": 151, "hub": 151, "first": 152, "llm": 152, "select": 152, "modifi": 152, "train": 152, "next": 152, "step": 152, "meta": 153, "8b": 153, "access": 153, "our": 153, "faster": 153, "how": 154, "doe": 154, "work": 154, "appli": 154, "trade": 154, "off": 154, "qlora": 155, "save": 155, "deep": 155, "dive": 155}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})