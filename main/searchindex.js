Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.StackExchangedPairedTemplate", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchanged_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.Qwen2Tokenizer", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.left_padded_collate", "generated/torchtune.modules.rlhf.loss.DPOLoss", "generated/torchtune.modules.rlhf.loss.IPOLoss", "generated/torchtune.modules.rlhf.loss.PPOLoss", "generated/torchtune.modules.rlhf.loss.RSOLoss", "generated/torchtune.modules.rlhf.loss.SimPOLoss", "generated/torchtune.modules.rlhf.padded_collate_dpo", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.modules.transforms.find_supported_resolutions", "generated/torchtune.modules.transforms.get_canvas_best_fit", "generated/torchtune.modules.transforms.get_inscribed_size", "generated/torchtune.modules.transforms.resize_with_pad", "generated/torchtune.modules.transforms.tile_crop", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.FullModelTorchTuneCheckpointer", "generated/torchtune.utils.ModelType", "generated/torchtune.utils.OptimizerInBackwardWrapper", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.create_optim_in_bwd_wrapper", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_memory_stats", "generated/torchtune.utils.get_quantizer_mode", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.is_distributed", "generated/torchtune.utils.log_memory_stats", "generated/torchtune.utils.lora_fsdp_wrap_policy", "generated/torchtune.utils.metric_logging.CometLogger", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.register_optim_in_bwd_hooks", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_default_dtype", "generated/torchtune.utils.set_seed", "generated/torchtune.utils.setup_torch_profiler", "generated/torchtune.utils.torch_version_ge", "generated/torchtune.utils.validate_expected_param_dtype", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.StackExchangedPairedTemplate.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchanged_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.Qwen2Tokenizer.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.left_padded_collate.rst", "generated/torchtune.modules.rlhf.loss.DPOLoss.rst", "generated/torchtune.modules.rlhf.loss.IPOLoss.rst", "generated/torchtune.modules.rlhf.loss.PPOLoss.rst", "generated/torchtune.modules.rlhf.loss.RSOLoss.rst", "generated/torchtune.modules.rlhf.loss.SimPOLoss.rst", "generated/torchtune.modules.rlhf.padded_collate_dpo.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.modules.transforms.find_supported_resolutions.rst", "generated/torchtune.modules.transforms.get_canvas_best_fit.rst", "generated/torchtune.modules.transforms.get_inscribed_size.rst", "generated/torchtune.modules.transforms.resize_with_pad.rst", "generated/torchtune.modules.transforms.tile_crop.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.utils.ModelType.rst", "generated/torchtune.utils.OptimizerInBackwardWrapper.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.create_optim_in_bwd_wrapper.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_memory_stats.rst", "generated/torchtune.utils.get_quantizer_mode.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.is_distributed.rst", "generated/torchtune.utils.log_memory_stats.rst", "generated/torchtune.utils.lora_fsdp_wrap_policy.rst", "generated/torchtune.utils.metric_logging.CometLogger.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.register_optim_in_bwd_hooks.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_default_dtype.rst", "generated/torchtune.utils.set_seed.rst", "generated/torchtune.utils.setup_torch_profiler.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated/torchtune.utils.validate_expected_param_dtype.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "ChatMLTemplate", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Llama2ChatFormat", "Message", "MistralChatFormat", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.Role", "ShareGPTToMessages", "StackExchangedPairedTemplate", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "truncate", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchanged_paired_dataset", "text_completion_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralChatTemplate", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "Qwen2Tokenizer", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "CausalSelfAttention", "FeedForward", "Fp32LayerNorm", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "left_padded_collate", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "padded_collate_dpo", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "find_supported_resolutions", "get_canvas_best_fit", "get_inscribed_size", "resize_with_pad", "tile_crop", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "TuneRecipeArgumentParser", "create_optim_in_bwd_wrapper", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_memory_stats", "get_quantizer_mode", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "torch_version_ge", "validate_expected_param_dtype", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 15, 17, 18, 21, 25, 30, 38, 39, 40, 41, 44, 48, 52, 94, 113, 122, 129, 130, 131, 139, 140, 141, 219, 223, 224, 227, 229, 230, 231], "prompt": [1, 15, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 36, 38, 40, 41, 44, 45, 47, 48, 49, 50, 67, 77, 78, 94, 113, 114, 126, 133, 149, 176, 192, 225, 226, 228], "chat": [1, 2, 16, 17, 18, 22, 23, 29, 32, 33, 36, 41, 45, 77, 78, 131, 133], "includ": [1, 6, 8, 9, 16, 21, 26, 27, 41, 57, 68, 78, 79, 95, 104, 119, 131, 138, 155, 171, 180, 185, 186, 190, 221, 223, 224, 225, 226, 227, 228, 229, 231], "some": [1, 6, 8, 17, 18, 117, 157, 158, 219, 221, 223, 224, 225, 226, 227, 229, 230, 231], "specif": [1, 4, 8, 9, 11, 41, 47, 49, 50, 172, 195, 224, 225, 226, 230, 231], "format": [1, 2, 5, 15, 16, 17, 21, 23, 24, 25, 30, 32, 36, 38, 40, 41, 44, 45, 47, 48, 49, 50, 77, 78, 94, 113, 133, 172, 181, 182, 185, 186, 187, 188, 223, 224, 226, 227, 228, 229], "differ": [1, 8, 10, 37, 38, 40, 54, 55, 56, 114, 151, 164, 169, 173, 188, 216, 221, 223, 224, 226, 228, 229, 230, 231], "dataset": [1, 5, 8, 15, 20, 21, 22, 24, 29, 30, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 164, 165, 221, 227, 228, 230], "model": [1, 2, 6, 7, 8, 9, 11, 17, 18, 20, 24, 25, 36, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 146, 147, 148, 149, 150, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 171, 172, 175, 176, 177, 185, 186, 187, 188, 191, 192, 195, 197, 203, 204, 210, 211, 219, 221, 224, 225, 231], "from": [1, 2, 3, 6, 7, 8, 9, 10, 11, 15, 21, 22, 23, 24, 29, 30, 33, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 69, 70, 77, 80, 81, 82, 83, 98, 114, 120, 122, 131, 133, 139, 140, 141, 142, 144, 149, 150, 151, 153, 154, 157, 160, 164, 165, 167, 168, 173, 175, 178, 180, 181, 182, 185, 186, 187, 189, 190, 191, 192, 204, 207, 208, 210, 218, 220, 222, 223, 225, 226, 227, 228, 229, 230], "common": [1, 2, 4, 8, 176, 223, 224, 225, 228, 229, 230], "json": [1, 6, 22, 29, 32, 33, 36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 98, 131, 133, 142, 175, 185, 223, 225, 226, 230], "schema": 1, "convers": [1, 6, 16, 17, 23, 25, 29, 32, 33, 35, 36, 41, 45, 50, 185, 187, 188, 221, 224, 225, 226, 229, 231], "list": [1, 6, 8, 16, 17, 23, 24, 25, 26, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 57, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 78, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 98, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 123, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136, 137, 151, 154, 155, 159, 160, 163, 169, 171, 172, 173, 174, 176, 178, 179, 180, 185, 186, 187, 190, 192, 196, 204, 209, 224, 225, 226, 227, 228, 230], "miscellan": 1, "us": [1, 2, 3, 4, 6, 7, 10, 11, 13, 17, 18, 20, 23, 24, 26, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 77, 78, 79, 85, 95, 98, 99, 104, 108, 127, 131, 133, 134, 138, 142, 143, 144, 147, 148, 149, 150, 151, 152, 156, 159, 161, 162, 164, 166, 168, 173, 174, 178, 179, 182, 184, 185, 186, 188, 189, 190, 192, 193, 194, 195, 197, 203, 204, 205, 206, 207, 208, 213, 219, 220, 221, 223, 225, 227, 228, 229, 230], "modifi": [1, 8, 9, 10, 152, 221, 226, 228, 229, 230, 231], "For": [2, 5, 6, 8, 9, 22, 26, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 68, 72, 78, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 149, 151, 179, 180, 185, 190, 191, 198, 204, 208, 211, 213, 220, 223, 224, 225, 226, 227, 228, 229, 230, 231], "detail": [2, 6, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 78, 121, 146, 151, 166, 184, 195, 203, 213, 223, 225, 226, 227, 228, 229, 230, 231], "usag": [2, 152, 188, 189, 214, 220, 223, 225, 226, 227, 228, 230, 231], "guid": [2, 7, 8, 10, 168, 204, 221, 224, 225, 227, 229], "pleas": [2, 5, 19, 31, 54, 55, 56, 57, 64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 151, 184, 195, 203, 211, 220, 226, 228, 231], "see": [2, 5, 6, 7, 10, 19, 23, 25, 31, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 50, 52, 53, 64, 65, 66, 75, 76, 77, 78, 90, 91, 92, 93, 102, 103, 111, 112, 113, 121, 124, 125, 132, 133, 146, 151, 154, 184, 188, 190, 195, 196, 203, 204, 208, 211, 213, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "our": [2, 6, 9, 221, 224, 225, 226, 227, 229, 230, 231], "tutori": [2, 6, 78, 211, 221, 224, 225, 226, 227, 228, 229, 230, 231], "support": [2, 3, 6, 7, 9, 10, 11, 24, 25, 36, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 72, 85, 99, 108, 113, 115, 117, 127, 130, 131, 134, 143, 145, 151, 155, 167, 179, 182, 186, 187, 189, 194, 197, 198, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "sever": 2, "wide": 2, "help": [2, 6, 23, 77, 149, 151, 185, 190, 204, 219, 220, 221, 223, 224, 225, 226, 227, 230, 231], "quickli": [2, 8, 26, 42, 224, 225], "bootstrap": 2, "your": [2, 5, 7, 10, 11, 15, 26, 30, 36, 42, 55, 56, 57, 78, 151, 204, 207, 208, 219, 220, 221, 223, 224, 225, 228, 229, 230, 231], "fine": [2, 6, 7, 9, 10, 24, 39, 41, 52, 78, 219, 221, 226, 229, 230], "tune": [2, 3, 6, 7, 8, 9, 10, 13, 24, 39, 41, 52, 78, 219, 220, 221, 223, 226, 229, 230, 231], "also": [2, 6, 7, 8, 9, 10, 11, 37, 45, 48, 52, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 131, 134, 138, 143, 149, 155, 168, 193, 195, 197, 203, 204, 208, 220, 223, 224, 225, 226, 227, 228, 229, 230, 231], "like": [2, 4, 6, 7, 8, 9, 10, 36, 131, 151, 187, 220, 223, 224, 225, 226, 227, 229, 230], "These": [2, 4, 6, 8, 9, 11, 39, 151, 178, 190, 224, 225, 226, 227, 228, 229, 230, 231], "ar": [2, 4, 6, 7, 8, 10, 11, 15, 16, 21, 22, 23, 25, 26, 27, 29, 30, 35, 38, 39, 40, 41, 44, 45, 47, 48, 49, 50, 55, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 77, 78, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 149, 150, 151, 155, 156, 159, 160, 162, 169, 178, 180, 184, 185, 186, 188, 189, 191, 192, 194, 197, 201, 203, 214, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "especi": [2, 221, 223, 226], "specifi": [2, 6, 8, 9, 11, 20, 41, 45, 79, 85, 95, 99, 104, 108, 134, 138, 143, 149, 150, 184, 192, 195, 198, 203, 208, 211, 214, 223, 224, 225, 226, 227, 228, 230, 231], "yaml": [2, 8, 9, 11, 12, 13, 37, 45, 48, 52, 190, 208, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "config": [2, 6, 7, 10, 11, 12, 13, 14, 37, 45, 48, 52, 143, 159, 185, 189, 190, 204, 208, 214, 221, 224, 225, 226, 228, 229, 230, 231], "represent": [2, 229, 230, 231], "abov": [2, 3, 6, 152, 180, 201, 220, 226, 228, 229, 230, 231], "all": [3, 4, 9, 14, 26, 37, 39, 41, 45, 57, 98, 131, 133, 142, 143, 144, 149, 151, 152, 156, 177, 179, 180, 185, 189, 190, 191, 201, 210, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230], "famili": [3, 6, 9, 44, 46, 50, 51, 53, 188, 221, 223, 228], "request": [3, 15, 194, 225, 226], "access": [3, 6, 8, 9, 37, 185, 191, 223, 225, 226, 227], "hug": [3, 6, 17, 18, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 98, 131, 142, 153, 175, 221, 223, 227, 228], "face": [3, 6, 17, 18, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 98, 131, 142, 153, 175, 221, 223, 227, 228], "To": [3, 6, 8, 9, 10, 39, 151, 185, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "download": [3, 6, 217, 220, 224, 225, 228, 229, 230, 231], "8b": [3, 97, 101, 103, 107, 110, 112, 128, 223, 224, 230], "meta": [3, 6, 23, 77, 78, 94, 148, 185, 186, 223, 224, 226, 227], "hf": [3, 6, 126, 164, 165, 167, 185, 223, 224, 226, 227, 228], "token": [3, 6, 8, 9, 24, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 67, 68, 71, 72, 78, 79, 84, 85, 94, 95, 98, 99, 104, 108, 114, 115, 117, 119, 121, 123, 126, 127, 129, 131, 133, 134, 138, 142, 143, 148, 149, 150, 151, 162, 163, 166, 170, 171, 172, 173, 174, 175, 176, 178, 192, 195, 209, 223, 225, 226, 227, 228, 229, 230, 231], "hf_token": 3, "70b": [3, 59, 62, 65, 81, 87, 91, 96, 100, 102, 106, 109, 111, 228], "ignor": [3, 6, 52, 126, 143, 144, 223], "pattern": [3, 174, 223], "origin": [3, 6, 43, 44, 152, 155, 224, 226, 228, 229, 230, 231], "consolid": [3, 6], "weight": [3, 6, 9, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 143, 152, 154, 155, 159, 164, 173, 185, 186, 187, 188, 198, 203, 208, 219, 223, 224, 226, 227, 228, 229, 230, 231], "you": [3, 6, 7, 8, 9, 10, 11, 21, 23, 24, 26, 36, 38, 40, 41, 42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 77, 151, 180, 188, 190, 192, 204, 207, 208, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "can": [3, 4, 6, 7, 8, 9, 10, 11, 14, 24, 26, 27, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 78, 114, 147, 148, 151, 156, 173, 174, 179, 181, 182, 184, 185, 188, 190, 195, 203, 204, 207, 208, 211, 214, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "instead": [3, 6, 9, 39, 45, 48, 52, 57, 133, 144, 146, 151, 155, 168, 223, 228, 229, 230], "405b": [3, 105], "The": [3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 17, 18, 21, 23, 24, 25, 30, 35, 36, 37, 38, 39, 40, 41, 47, 49, 51, 54, 55, 56, 57, 61, 62, 63, 67, 72, 73, 74, 78, 85, 86, 87, 88, 89, 94, 99, 100, 101, 108, 109, 110, 114, 115, 117, 126, 127, 128, 133, 134, 135, 136, 137, 145, 147, 148, 151, 152, 153, 156, 161, 163, 164, 165, 166, 167, 168, 171, 172, 173, 174, 175, 176, 178, 180, 181, 182, 183, 184, 185, 187, 190, 193, 194, 196, 198, 204, 208, 212, 214, 215, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "reus": [3, 221], "llama3_token": [3, 192, 224, 228], "builder": [3, 6, 43, 46, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 73, 74, 75, 76, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 100, 101, 102, 103, 105, 106, 107, 109, 110, 111, 112, 116, 118, 120, 122, 124, 125, 128, 130, 132, 135, 136, 137, 139, 140, 141, 224, 225, 231], "class": [3, 8, 10, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 36, 37, 38, 39, 40, 41, 42, 45, 48, 54, 55, 56, 67, 77, 78, 83, 94, 113, 114, 117, 121, 122, 126, 133, 143, 144, 145, 146, 147, 148, 149, 150, 151, 154, 155, 157, 158, 164, 165, 166, 167, 168, 171, 172, 173, 174, 177, 178, 185, 186, 187, 188, 189, 190, 204, 205, 206, 207, 208, 224, 225, 227, 229, 231], "7b": [3, 6, 38, 40, 42, 44, 46, 48, 52, 53, 60, 63, 66, 70, 74, 82, 83, 88, 89, 92, 93, 116, 118, 120, 122, 125, 137, 141, 185, 186, 224, 227, 228, 229, 231], "13b": [3, 6, 58, 61, 64, 80, 86, 90], "codellama": 3, "size": [3, 6, 9, 11, 44, 47, 49, 55, 56, 57, 133, 143, 146, 147, 148, 149, 150, 151, 161, 162, 178, 179, 181, 182, 183, 199, 201, 221, 223, 225, 226, 227, 228, 229, 230], "0": [3, 6, 9, 39, 57, 61, 62, 63, 64, 65, 66, 67, 68, 72, 78, 79, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 99, 104, 108, 114, 115, 117, 119, 121, 126, 127, 129, 133, 134, 135, 136, 137, 138, 139, 140, 143, 149, 151, 153, 155, 163, 164, 165, 166, 167, 168, 169, 170, 176, 180, 192, 198, 204, 207, 208, 209, 213, 215, 218, 222, 224, 225, 226, 227, 228, 229, 230, 231], "5b": [3, 135, 136, 139, 140], "qwen2": [3, 133, 134, 135, 136, 137, 139, 140, 141, 142, 188], "exampl": [3, 6, 7, 8, 9, 10, 11, 13, 15, 22, 26, 30, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 57, 67, 78, 94, 114, 126, 133, 143, 151, 154, 156, 163, 164, 165, 167, 168, 169, 170, 173, 174, 176, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 192, 198, 204, 207, 208, 209, 212, 215, 217, 218, 220, 222, 223, 224, 225, 226, 228, 229, 230, 231], "output": [3, 6, 20, 21, 37, 38, 41, 44, 47, 49, 50, 57, 61, 62, 63, 68, 72, 79, 83, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 122, 127, 128, 134, 137, 138, 143, 144, 145, 147, 148, 149, 150, 151, 155, 158, 159, 160, 178, 181, 182, 187, 192, 195, 206, 214, 220, 223, 224, 225, 226, 227, 228, 229, 231], "dir": [3, 6, 208, 220, 223, 226, 227, 228, 230], "tmp": [3, 8, 189, 224, 227], "none": [3, 9, 10, 12, 14, 15, 20, 21, 22, 29, 30, 34, 35, 36, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 57, 67, 71, 78, 79, 84, 85, 94, 95, 98, 99, 104, 108, 114, 123, 126, 131, 133, 142, 143, 146, 148, 149, 150, 151, 156, 158, 159, 160, 161, 162, 166, 173, 176, 181, 182, 185, 186, 187, 188, 192, 193, 194, 196, 198, 202, 204, 205, 206, 207, 208, 210, 211, 212, 213, 214, 216, 223, 224, 225, 226, 230], "mini": [3, 126, 128, 129, 130, 131, 132], "4k": [3, 129, 130, 131], "microsoft": [3, 130, 131], "ai": [3, 41, 120, 143, 208, 224, 228], "v0": [3, 113], "mistralai": [3, 223], "2b": [3, 69, 73], "googl": [3, 69, 70], "gguf": 3, "vision": [3, 41, 57], "compon": [3, 6, 9, 14, 41, 169, 221, 225, 227, 229, 231], "multimod": [3, 24, 41], "encod": [3, 4, 41, 57, 67, 78, 94, 114, 126, 133, 164, 168, 171, 173, 174, 176, 178, 224], "perform": [4, 6, 39, 78, 144, 151, 156, 168, 177, 192, 221, 224, 226, 228, 230, 231], "direct": [4, 9, 164, 169, 220], "text": [4, 24, 26, 27, 36, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 78, 94, 114, 126, 133, 171, 173, 174, 176, 178, 224, 226, 230], "id": [4, 6, 36, 38, 39, 40, 42, 44, 45, 46, 48, 51, 52, 53, 78, 94, 114, 126, 133, 143, 148, 149, 150, 163, 169, 171, 172, 173, 174, 175, 176, 178, 185, 187, 192, 204, 209, 224, 225, 226], "decod": [4, 68, 72, 79, 85, 94, 95, 99, 104, 108, 114, 115, 117, 119, 121, 126, 127, 129, 133, 134, 138, 149, 171, 173, 174, 192, 224], "typic": [4, 8, 39, 41, 42, 52, 131, 164, 168, 225, 230, 231], "byte": [4, 133, 174, 231], "pair": [4, 8, 15, 51, 165, 169, 174, 209, 225], "underli": [4, 114, 173, 231], "helper": 4, "method": [4, 6, 8, 9, 10, 13, 36, 38, 40, 42, 44, 45, 46, 48, 51, 52, 53, 133, 152, 154, 157, 159, 171, 172, 182, 189, 190, 198, 220, 221, 225, 229, 231], "ani": [4, 6, 8, 9, 11, 13, 14, 15, 21, 26, 30, 32, 33, 34, 36, 38, 40, 41, 42, 45, 46, 48, 52, 53, 56, 78, 114, 145, 152, 157, 158, 159, 160, 171, 172, 173, 176, 185, 186, 187, 189, 192, 200, 203, 204, 213, 216, 223, 224, 225, 227, 229, 230], "function": [4, 6, 8, 9, 11, 13, 36, 55, 56, 57, 143, 144, 151, 152, 156, 159, 160, 164, 166, 169, 184, 185, 192, 193, 199, 203, 213, 221, 224, 225, 231], "preprocess": [4, 39, 151], "imag": [4, 24, 41, 54, 55, 56, 57, 151, 178, 179, 180, 181, 182, 183, 229], "algorithm": [4, 161, 168, 213], "ppo": [4, 161, 162, 164, 166], "offer": 5, "allow": [5, 37, 159, 180, 207, 223, 230, 231], "seamless": 5, "transit": 5, "between": [5, 6, 133, 162, 165, 166, 168, 185, 188, 204, 225, 226, 228, 229, 230, 231], "train": [5, 6, 7, 9, 10, 20, 23, 36, 37, 38, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 77, 78, 143, 145, 148, 149, 150, 152, 153, 164, 168, 185, 186, 187, 194, 197, 203, 204, 214, 219, 221, 223, 224, 225, 226, 228, 229, 230, 231], "interoper": [5, 6, 9, 221, 226, 231], "rest": [5, 224, 231], "ecosystem": [5, 6, 9, 221, 226, 228, 231], "comprehens": 5, "overview": [5, 8, 10, 219, 227, 229, 231], "deep": [5, 6, 7, 8, 9, 10, 221, 227, 228], "dive": [5, 6, 7, 8, 9, 10, 221, 227, 228], "enabl": [5, 7, 8, 9, 10, 37, 61, 62, 63, 64, 65, 66, 73, 74, 75, 76, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 109, 110, 111, 112, 116, 118, 124, 125, 128, 132, 135, 136, 137, 139, 140, 155, 213, 214, 228, 229, 231], "work": [5, 6, 9, 190, 221, 223, 226, 228, 231], "set": [5, 6, 7, 8, 9, 10, 24, 38, 39, 40, 42, 44, 46, 47, 48, 49, 50, 52, 53, 79, 85, 94, 95, 99, 104, 108, 115, 117, 119, 121, 126, 127, 129, 133, 134, 138, 143, 148, 149, 156, 158, 181, 182, 184, 193, 195, 201, 203, 204, 211, 212, 213, 214, 221, 223, 224, 226, 227, 228, 229, 230], "consumpt": [5, 37], "dure": [5, 6, 38, 39, 44, 47, 49, 50, 143, 146, 148, 149, 150, 151, 152, 168, 197, 224, 226, 228, 229, 230, 231], "provid": [5, 6, 8, 9, 11, 15, 17, 18, 22, 25, 34, 37, 38, 39, 40, 57, 149, 151, 156, 164, 187, 190, 193, 195, 204, 208, 214, 221, 223, 224, 225, 226, 227, 228], "debug": [5, 6, 8, 9, 204, 223], "finetun": [5, 6, 8, 9, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137, 219, 221, 227, 228], "job": [5, 10, 213, 227], "variou": [5, 21], "walk": [6, 9, 207, 221, 224, 225, 226, 227, 230, 231], "through": [6, 7, 8, 9, 10, 57, 144, 151, 156, 221, 223, 224, 225, 226, 227, 230, 231], "design": [6, 9, 168], "behavior": [6, 203, 224, 225], "associ": [6, 8, 9, 57, 68, 79, 95, 104, 119, 138, 192, 204, 226, 229], "util": [6, 7, 8, 9, 10, 11, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 221, 226, 227, 228, 230, 231], "what": [6, 7, 8, 10, 23, 25, 30, 41, 47, 49, 77, 113, 151, 219, 224, 225, 226, 227, 228], "cover": [6, 7, 8, 9, 10, 224, 226, 231], "how": [6, 7, 8, 9, 10, 151, 184, 204, 211, 219, 223, 224, 225, 226, 227, 228, 230, 231], "we": [6, 7, 8, 9, 10, 38, 39, 40, 41, 42, 44, 46, 48, 52, 53, 78, 114, 133, 143, 146, 148, 149, 151, 155, 164, 168, 179, 180, 185, 186, 187, 192, 194, 198, 203, 210, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "them": [6, 8, 37, 38, 40, 48, 67, 78, 114, 126, 144, 151, 152, 176, 223, 224, 225, 226, 229, 230, 231], "scenario": [6, 37, 41], "full": [6, 8, 9, 19, 31, 45, 48, 53, 64, 65, 66, 67, 75, 76, 78, 90, 91, 92, 93, 102, 103, 111, 112, 114, 124, 125, 126, 132, 159, 160, 176, 221, 223, 225, 226, 228, 229, 230], "compos": [6, 151], "which": [6, 8, 9, 37, 38, 39, 42, 44, 47, 49, 50, 52, 61, 62, 63, 71, 72, 73, 74, 84, 85, 86, 87, 88, 89, 94, 98, 99, 100, 101, 108, 109, 110, 113, 114, 115, 116, 117, 118, 123, 127, 128, 131, 134, 135, 136, 137, 143, 148, 149, 150, 151, 153, 159, 160, 173, 180, 185, 186, 187, 189, 194, 205, 208, 211, 221, 223, 224, 225, 226, 227, 229, 230, 231], "plug": 6, "recip": [6, 7, 8, 10, 11, 12, 13, 144, 159, 185, 186, 187, 221, 224, 225, 226, 228, 231], "evalu": [6, 9, 219, 221, 227, 229, 231], "gener": [6, 9, 15, 30, 36, 38, 39, 40, 46, 52, 78, 114, 133, 156, 161, 204, 212, 213, 214, 217, 219, 224, 225, 229, 230, 231], "each": [6, 9, 16, 21, 26, 27, 37, 39, 41, 54, 55, 56, 57, 61, 62, 63, 67, 72, 73, 74, 78, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 114, 115, 116, 117, 118, 126, 127, 128, 134, 135, 136, 137, 143, 148, 149, 150, 151, 159, 160, 161, 162, 164, 165, 167, 168, 169, 176, 178, 180, 183, 213, 214, 221, 223, 225, 226, 227, 229, 230], "make": [6, 7, 8, 9, 10, 143, 150, 151, 221, 223, 224, 226, 227, 228, 229, 230, 231], "easi": [6, 9, 221, 225, 229], "understand": [6, 8, 9, 219, 221, 224, 225, 229, 231], "extend": [6, 9, 221], "befor": [6, 26, 35, 38, 39, 40, 54, 55, 57, 68, 72, 143, 149, 150, 151, 155, 174, 185, 204, 223, 226, 230], "let": [6, 8, 10, 223, 224, 225, 226, 227, 228, 229, 231], "s": [6, 8, 9, 10, 11, 13, 15, 16, 17, 18, 22, 23, 25, 29, 32, 33, 35, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 61, 62, 63, 67, 77, 78, 85, 86, 87, 88, 89, 94, 99, 100, 101, 108, 109, 110, 113, 114, 115, 116, 117, 118, 126, 127, 128, 131, 134, 137, 138, 143, 146, 148, 149, 150, 151, 152, 154, 157, 159, 160, 164, 165, 167, 168, 170, 174, 180, 184, 185, 186, 189, 193, 195, 197, 203, 204, 207, 211, 212, 221, 223, 224, 225, 227, 229, 230, 231], "defin": [6, 8, 9, 26, 36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 144, 154, 155, 157, 162, 225, 227, 229], "concept": [6, 226, 227], "In": [6, 8, 9, 36, 55, 56, 57, 148, 151, 155, 165, 180, 184, 203, 207, 208, 224, 226, 228, 229, 230, 231], "ll": [6, 8, 9, 192, 198, 221, 224, 225, 226, 227, 228, 230, 231], "talk": 6, "about": [6, 9, 151, 164, 168, 204, 208, 221, 223, 224, 226, 227, 228, 229, 230, 231], "take": [6, 8, 9, 11, 41, 144, 146, 151, 152, 169, 185, 187, 190, 193, 224, 225, 226, 227, 228, 229, 231], "close": [6, 9, 204, 205, 206, 207, 208, 229], "look": [6, 8, 9, 191, 207, 220, 224, 225, 226, 227, 228, 229, 230], "veri": [6, 37, 149, 223, 226], "simpli": [6, 8, 22, 39, 41, 164, 165, 223, 224, 225, 226, 228, 231], "dictat": 6, "state_dict": [6, 152, 159, 185, 186, 187, 188, 189, 229, 231], "store": [6, 41, 204, 205, 208, 229, 231], "file": [6, 7, 8, 9, 10, 11, 12, 13, 36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 67, 78, 94, 98, 114, 126, 131, 133, 142, 173, 174, 175, 185, 186, 187, 190, 205, 208, 214, 218, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231], "disk": [6, 42, 205], "string": [6, 24, 26, 44, 45, 46, 51, 67, 78, 94, 114, 126, 133, 154, 171, 173, 174, 176, 193, 194, 198, 204, 223, 225], "kei": [6, 8, 10, 15, 30, 32, 36, 38, 40, 41, 48, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 149, 150, 158, 159, 160, 168, 169, 185, 187, 189, 204, 214, 223, 226, 227, 229, 231], "identifi": [6, 204], "state": [6, 9, 151, 152, 157, 158, 159, 160, 161, 164, 185, 186, 187, 189, 191, 226, 228, 229, 231], "dict": [6, 8, 9, 10, 11, 15, 20, 21, 22, 24, 26, 29, 30, 32, 33, 36, 38, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 94, 126, 133, 152, 157, 158, 159, 160, 163, 169, 171, 172, 174, 175, 177, 185, 186, 187, 189, 191, 197, 200, 202, 204, 209, 210, 225], "If": [6, 8, 14, 15, 21, 22, 24, 25, 29, 30, 32, 34, 35, 36, 38, 40, 41, 44, 47, 48, 49, 50, 57, 79, 85, 94, 95, 99, 104, 108, 126, 134, 138, 143, 148, 149, 150, 151, 152, 155, 160, 180, 181, 182, 185, 186, 187, 188, 189, 192, 193, 194, 195, 197, 198, 200, 204, 207, 208, 213, 214, 216, 220, 223, 224, 225, 226, 227, 228, 229, 230], "don": [6, 8, 9, 208, 213, 223, 224, 225, 226, 227, 231], "t": [6, 8, 9, 165, 194, 208, 213, 223, 224, 225, 226, 227, 231], "match": [6, 38, 40, 48, 126, 160, 180, 204, 220, 223, 225, 226, 228, 229], "up": [6, 7, 9, 10, 38, 39, 40, 42, 44, 46, 48, 52, 53, 133, 174, 178, 179, 182, 191, 204, 214, 223, 224, 225, 227, 228, 229, 231], "exactli": [6, 160, 230], "those": [6, 188, 226, 228, 229], "definit": [6, 229], "either": [6, 41, 160, 185, 192, 204, 211, 223, 229, 230, 231], "run": [6, 7, 8, 10, 13, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 133, 134, 138, 144, 146, 149, 152, 185, 186, 187, 189, 191, 201, 204, 207, 208, 210, 220, 221, 224, 225, 227, 228, 229, 230, 231], "explicit": 6, "error": [6, 8, 19, 35, 133, 146, 185, 213, 223], "load": [6, 9, 36, 37, 38, 39, 40, 41, 42, 44, 46, 47, 49, 50, 51, 52, 53, 159, 185, 186, 187, 189, 190, 207, 224, 225, 226, 228, 229], "rais": [6, 11, 14, 25, 32, 35, 36, 38, 45, 126, 134, 143, 146, 149, 151, 159, 160, 169, 176, 185, 186, 187, 189, 194, 197, 200, 204, 208, 213, 216], "an": [6, 7, 8, 9, 10, 11, 15, 35, 36, 37, 38, 42, 47, 49, 52, 53, 54, 55, 56, 85, 99, 108, 115, 117, 121, 127, 133, 134, 135, 136, 139, 140, 143, 146, 149, 151, 154, 156, 157, 158, 164, 178, 179, 180, 181, 182, 184, 185, 186, 187, 189, 193, 195, 204, 208, 214, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "except": [6, 24, 25, 113, 176, 225], "wors": 6, "silent": [6, 144], "succe": 6, "infer": [6, 23, 36, 41, 68, 77, 78, 119, 143, 146, 148, 149, 150, 193, 219, 224, 226, 227, 228, 230, 231], "expect": [6, 8, 11, 15, 20, 21, 22, 29, 30, 36, 38, 40, 41, 45, 47, 48, 49, 50, 148, 160, 189, 204, 208, 216, 224, 225, 229, 230], "addit": [6, 8, 9, 11, 36, 38, 40, 41, 42, 45, 46, 48, 52, 53, 78, 113, 159, 164, 184, 185, 186, 187, 194, 195, 200, 203, 204, 205, 207, 208, 211, 221, 224, 227, 229], "line": [6, 7, 9, 15, 190, 223, 225, 227, 228], "need": [6, 7, 8, 9, 10, 21, 26, 36, 39, 41, 143, 144, 149, 151, 168, 203, 204, 207, 208, 210, 220, 223, 224, 225, 226, 227, 228, 229, 231], "shape": [6, 54, 55, 56, 57, 143, 146, 148, 149, 150, 151, 155, 161, 162, 163, 164, 165, 166, 167, 168, 170, 178, 180, 183, 192, 214], "valu": [6, 8, 29, 33, 58, 59, 60, 68, 69, 70, 72, 79, 80, 81, 82, 83, 85, 95, 96, 97, 99, 104, 105, 106, 107, 108, 115, 117, 119, 120, 121, 122, 127, 129, 134, 138, 139, 140, 141, 143, 146, 147, 149, 150, 153, 159, 161, 162, 166, 169, 170, 185, 188, 189, 190, 192, 204, 205, 206, 207, 208, 213, 223, 225, 227, 228, 229, 230], "two": [6, 8, 20, 35, 55, 151, 170, 178, 180, 221, 226, 227, 228, 229, 230, 231], "popular": [6, 221, 225, 226], "llama2": [6, 8, 9, 11, 23, 36, 38, 40, 41, 42, 44, 46, 48, 52, 53, 58, 59, 60, 61, 62, 63, 64, 65, 66, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 144, 149, 150, 188, 219, 221, 223, 227, 228, 230], "offici": [6, 23, 77, 133, 224, 227, 228], "implement": [6, 9, 36, 38, 40, 42, 44, 45, 46, 48, 51, 52, 53, 67, 78, 114, 126, 144, 147, 148, 151, 153, 154, 155, 164, 165, 166, 167, 168, 171, 172, 185, 198, 207, 221, 225, 229, 230, 231], "when": [6, 8, 9, 13, 37, 39, 41, 42, 52, 78, 133, 143, 148, 149, 150, 151, 152, 153, 159, 162, 179, 181, 182, 192, 195, 207, 210, 223, 226, 228, 229, 230, 231], "llama": [6, 23, 36, 77, 78, 94, 147, 148, 185, 186, 223, 224, 226, 227, 228, 229], "websit": 6, "get": [6, 7, 8, 9, 10, 36, 41, 78, 114, 133, 194, 196, 197, 199, 204, 220, 221, 224, 225, 226, 227, 229, 230], "singl": [6, 8, 11, 15, 16, 17, 21, 23, 25, 30, 32, 33, 37, 39, 41, 42, 52, 55, 56, 57, 71, 83, 84, 94, 98, 122, 123, 131, 133, 143, 151, 159, 185, 186, 187, 188, 189, 191, 223, 224, 225, 226, 227, 228, 229, 231], "pth": [6, 226], "inspect": [6, 226, 229, 231], "content": [6, 16, 22, 24, 26, 27, 29, 32, 33, 36, 41, 67, 78, 114, 126, 176, 224, 225], "easili": [6, 8, 221, 225, 229, 230, 231], "torch": [6, 8, 54, 55, 56, 143, 145, 146, 149, 150, 151, 152, 153, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 180, 182, 183, 187, 189, 191, 192, 193, 194, 197, 198, 200, 201, 209, 210, 211, 212, 213, 214, 215, 216, 226, 227, 228, 229, 231], "import": [6, 8, 11, 45, 48, 52, 151, 164, 204, 207, 208, 224, 225, 226, 227, 228, 229, 230, 231], "00": [6, 218, 222, 227], "mmap": [6, 226], "true": [6, 8, 24, 37, 38, 39, 42, 43, 44, 45, 47, 48, 49, 50, 52, 53, 57, 64, 65, 66, 67, 68, 72, 75, 76, 78, 90, 91, 92, 93, 94, 102, 103, 111, 112, 114, 124, 125, 126, 132, 133, 143, 149, 150, 152, 156, 161, 166, 170, 173, 174, 176, 178, 180, 184, 185, 186, 187, 195, 197, 200, 201, 203, 204, 207, 214, 215, 223, 224, 225, 226, 228, 229, 230, 231], "weights_onli": [6, 187], "map_loc": [6, 226], "cpu": [6, 9, 152, 194, 214, 220, 223, 226, 231], "tensor": [6, 54, 55, 56, 57, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 155, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 180, 182, 183, 185, 192, 204, 205, 206, 207, 208, 209, 212, 229, 231], "item": 6, "print": [6, 10, 37, 44, 47, 49, 50, 67, 78, 94, 114, 126, 133, 151, 173, 174, 176, 192, 215, 224, 225, 227, 229, 230, 231], "f": [6, 10, 44, 47, 49, 224, 226, 229, 231], "tok_embed": [6, 149], "32000": [6, 11, 229], "4096": [6, 11, 38, 40, 42, 44, 46, 48, 52, 53, 143, 148, 225, 229, 230], "len": [6, 37, 44, 47, 49, 149, 151], "292": 6, "contain": [6, 20, 24, 32, 39, 41, 42, 52, 67, 78, 94, 98, 114, 126, 131, 133, 142, 143, 146, 148, 149, 150, 154, 157, 158, 159, 161, 163, 169, 170, 174, 176, 179, 185, 186, 187, 189, 190, 191, 197, 202, 207, 209, 214, 224, 226, 228, 229], "input": [6, 15, 20, 21, 30, 36, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 71, 84, 94, 98, 114, 123, 126, 131, 134, 138, 143, 144, 145, 147, 148, 149, 150, 151, 155, 163, 169, 173, 174, 178, 182, 183, 185, 187, 209, 213, 216, 224, 225, 229, 231], "embed": [6, 54, 55, 56, 57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 147, 148, 149, 151, 195, 224, 228, 230], "tabl": [6, 224, 226, 228, 231], "call": [6, 11, 24, 26, 41, 113, 143, 144, 151, 152, 159, 190, 204, 205, 206, 207, 208, 210, 214, 224, 225, 229, 231], "layer": [6, 9, 57, 61, 62, 63, 64, 65, 66, 68, 72, 73, 74, 75, 76, 79, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 121, 122, 124, 125, 127, 128, 129, 132, 134, 135, 136, 137, 138, 143, 149, 150, 151, 155, 159, 160, 184, 195, 221, 228, 229, 230, 231], "have": [6, 8, 11, 20, 55, 56, 57, 143, 146, 151, 154, 160, 168, 178, 180, 187, 189, 190, 195, 203, 207, 216, 220, 224, 225, 226, 227, 228, 229, 230, 231], "dim": [6, 143, 144, 147, 148, 149], "most": [6, 8, 26, 179, 224, 227, 229, 231], "within": [6, 8, 11, 36, 39, 54, 72, 85, 99, 108, 115, 117, 127, 134, 144, 151, 181, 192, 207, 213, 214, 223, 225, 229, 231], "hub": [6, 41, 223, 225, 227], "default": [6, 8, 17, 18, 20, 22, 24, 29, 32, 33, 34, 36, 38, 39, 40, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 147, 148, 149, 150, 152, 153, 155, 159, 161, 162, 163, 164, 168, 169, 173, 174, 176, 185, 186, 187, 190, 192, 194, 199, 203, 204, 205, 208, 209, 212, 213, 214, 220, 223, 224, 225, 226, 228, 229, 230, 231], "everi": [6, 9, 54, 55, 56, 144, 151, 207, 214, 220, 223, 231], "2": [6, 10, 35, 39, 50, 54, 55, 67, 78, 94, 113, 114, 126, 133, 143, 151, 163, 165, 166, 168, 169, 170, 173, 174, 176, 179, 180, 181, 182, 185, 186, 198, 209, 212, 213, 214, 215, 224, 226, 227, 228, 229, 230], "repo": [6, 185, 186, 188, 223, 226], "first": [6, 8, 11, 35, 39, 57, 146, 149, 151, 170, 185, 190, 219, 221, 224, 225, 226, 228, 229, 230, 231], "big": 6, "split": [6, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 133, 174, 224, 225, 226, 230], "across": [6, 9, 37, 185, 207, 213, 226, 228, 230], "bin": [6, 223, 226], "correctli": [6, 9, 14, 159, 185, 220, 224, 227, 231], "piec": 6, "one": [6, 9, 20, 35, 67, 78, 114, 126, 133, 144, 151, 176, 180, 181, 187, 204, 224, 225, 226, 227, 228, 231], "pytorch_model": [6, 226], "00001": [6, 223], "00002": [6, 223], "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 7, 10, 24, 39, 41, 46, 57, 72, 85, 99, 108, 113, 115, 117, 127, 134, 151, 155, 157, 159, 173, 185, 186, 187, 189, 190, 192, 194, 195, 197, 198, 203, 223, 225, 226, 227, 229, 230, 231], "doe": [6, 25, 32, 36, 39, 52, 68, 78, 113, 119, 130, 143, 149, 150, 154, 176, 185, 187, 189, 190, 223, 224, 226, 230], "fewer": [6, 143], "sinc": [6, 8, 11, 41, 144, 180, 181, 182, 185, 187, 224, 226, 228, 230], "mismatch": 6, "name": [6, 7, 8, 10, 12, 15, 20, 21, 22, 29, 30, 38, 40, 42, 47, 48, 49, 50, 52, 53, 154, 158, 160, 174, 185, 186, 187, 188, 189, 190, 191, 192, 193, 204, 205, 206, 207, 208, 216, 223, 224, 226, 228, 230], "caus": [6, 114, 173, 182], "try": [6, 8, 224, 226, 227, 228, 231], "same": [6, 8, 26, 54, 55, 61, 62, 63, 67, 73, 74, 78, 86, 87, 88, 89, 100, 101, 109, 110, 114, 126, 128, 135, 136, 137, 146, 150, 151, 166, 168, 170, 176, 189, 190, 195, 208, 223, 224, 226, 228, 229, 230, 231], "As": [6, 8, 9, 10, 155, 221, 226, 231], "re": [6, 8, 168, 187, 221, 224, 226, 227, 229], "care": [6, 144, 185, 187, 226, 228, 229], "end": [6, 9, 24, 42, 52, 94, 114, 133, 174, 176, 219, 221, 224, 228, 229, 230], "number": [6, 9, 36, 38, 39, 40, 42, 44, 45, 46, 48, 51, 52, 53, 54, 55, 57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 149, 151, 153, 178, 179, 185, 186, 187, 192, 199, 213, 214, 223, 227, 229], "just": [6, 15, 221, 223, 224, 225, 227, 228, 229, 230], "save": [6, 9, 10, 152, 185, 186, 187, 189, 195, 203, 208, 219, 223, 224, 225, 226, 228, 229, 230], "less": [6, 181, 226, 227, 228, 231], "prone": 6, "manag": [6, 37, 156, 204, 212, 224], "invari": 6, "accept": [6, 8, 184, 225, 227, 231], "multipl": [6, 8, 9, 24, 36, 37, 41, 143, 149, 150, 151, 155, 169, 179, 180, 204, 205, 206, 207, 208, 214, 227, 228], "sourc": [6, 8, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 224, 225, 226, 230], "worri": [6, 224, 227], "explicitli": [6, 154, 221, 229], "convert": [6, 20, 22, 29, 32, 33, 36, 41, 47, 49, 50, 133, 185, 209, 224, 226, 230, 231], "time": [6, 67, 68, 78, 114, 119, 126, 161, 176, 205, 207, 214, 223, 224, 225, 226, 228, 231], "produc": [6, 189, 230, 231], "back": [6, 35, 156, 185, 225, 229, 231], "form": [6, 8, 9, 35, 41, 181, 223], "One": [6, 230], "advantag": [6, 161, 166, 229], "being": [6, 41, 185, 186, 187, 191, 193, 230, 231], "should": [6, 8, 9, 15, 16, 21, 22, 23, 24, 25, 26, 29, 32, 33, 39, 45, 48, 52, 61, 62, 63, 72, 73, 74, 77, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 113, 115, 116, 117, 118, 119, 121, 127, 128, 129, 133, 134, 135, 136, 137, 138, 143, 144, 151, 154, 159, 160, 161, 166, 183, 184, 190, 202, 204, 205, 206, 207, 208, 220, 221, 225, 226, 227, 228, 229, 230, 231], "abl": [6, 9, 226, 227, 230], "post": [6, 151, 210, 214, 226, 228, 230, 231], "tool": [6, 24, 26, 41, 113, 204, 225, 226, 227], "quantiz": [6, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 155, 187, 198, 219, 227, 231], "eval": [6, 219, 221, 230], "without": [6, 8, 10, 15, 159, 180, 182, 220, 221, 224, 226, 229, 230], "code": [6, 9, 58, 59, 60, 61, 62, 63, 64, 65, 66, 149, 204, 217, 221, 225, 227], "chang": [6, 7, 8, 10, 15, 20, 187, 220, 223, 226, 227, 228, 229, 230, 231], "OR": [6, 32], "script": [6, 10, 223, 225, 226, 227, 228], "wai": [6, 8, 36, 41, 159, 223, 224, 225, 226, 227, 228], "surround": [6, 9, 221], "load_checkpoint": [6, 9, 185, 186, 187, 188], "save_checkpoint": [6, 9, 10, 185, 186, 187], "map": [6, 15, 20, 21, 22, 26, 29, 30, 32, 33, 36, 37, 38, 39, 40, 47, 48, 49, 50, 94, 126, 158, 174, 175, 185, 189, 191, 204, 205, 206, 207, 208, 210, 214, 224, 225, 226, 229], "appli": [6, 9, 36, 38, 40, 41, 61, 62, 63, 64, 65, 66, 68, 72, 73, 74, 75, 76, 78, 79, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 124, 125, 127, 128, 132, 133, 134, 135, 136, 137, 138, 143, 147, 148, 149, 150, 159, 160, 211, 221, 231], "permut": 6, "certain": [6, 8, 214, 224], "ensur": [6, 8, 14, 35, 41, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 185, 187, 194, 221, 225, 227], "behav": 6, "further": [6, 15, 151, 168, 223, 225, 229, 230, 231], "illustr": [6, 228], "whilst": 6, "other": [6, 9, 11, 20, 26, 37, 165, 187, 190, 195, 214, 225, 227, 228, 229, 230], "phi3": [6, 126, 127, 128, 130, 131, 132, 188, 223], "own": [6, 26, 203, 213, 223, 224, 225, 226, 228, 229], "found": [6, 7, 8, 10, 147, 148, 185, 186, 187, 223, 229, 231], "folder": [6, 224], "three": [6, 9, 41, 164, 165, 167, 168, 227], "read": [6, 185, 186, 187, 221], "write": [6, 9, 15, 185, 186, 187, 205, 224, 225, 227], "compat": [6, 185, 187, 230], "transform": [6, 9, 20, 22, 36, 38, 40, 41, 47, 49, 50, 57, 61, 62, 63, 68, 72, 73, 74, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 121, 127, 128, 129, 133, 134, 135, 136, 137, 138, 149, 150, 151, 153, 178, 179, 180, 181, 182, 183, 211, 229, 230], "framework": [6, 9, 221], "mention": [6, 226, 231], "assum": [6, 15, 21, 22, 29, 30, 38, 40, 47, 48, 49, 50, 143, 148, 149, 150, 153, 157, 174, 189, 191, 194, 203, 224, 226, 229], "checkpoint_dir": [6, 8, 185, 186, 187, 226, 228, 230], "necessari": [6, 41, 204, 205, 206, 207, 208, 224, 229], "easiest": [6, 226, 227], "sure": [6, 8, 224, 226, 227, 228, 229, 230, 231], "everyth": [6, 9, 190, 221, 227], "follow": [6, 9, 22, 24, 26, 29, 32, 33, 36, 39, 41, 133, 143, 153, 166, 178, 179, 187, 188, 189, 201, 208, 214, 219, 220, 223, 225, 226, 227, 228, 229, 230, 231], "flow": [6, 36, 38, 39, 40, 230, 231], "By": [6, 133, 223, 229, 230, 231], "safetensor": [6, 185, 223], "output_dir": [6, 8, 185, 186, 187, 214, 226, 228, 229, 230, 231], "here": [6, 7, 8, 10, 15, 17, 18, 30, 47, 147, 148, 223, 224, 225, 226, 227, 228, 229, 230, 231], "argument": [6, 8, 11, 19, 21, 31, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 143, 184, 190, 195, 200, 204, 205, 207, 208, 211, 223, 224, 225, 229, 230], "snippet": 6, "explain": 6, "setup": [6, 8, 9, 143, 149, 150, 214, 223, 225, 226, 229, 231], "_component_": [6, 7, 8, 10, 11, 37, 45, 48, 52, 214, 224, 225, 226, 228, 229, 230], "fullmodelhfcheckpoint": [6, 226], "directori": [6, 8, 185, 186, 187, 205, 207, 208, 214, 223, 224, 225, 226, 227, 228], "sort": [6, 185, 187], "so": [6, 8, 39, 151, 180, 185, 190, 220, 221, 224, 226, 227, 228, 229, 230, 231], "order": [6, 7, 9, 185, 187, 207, 208, 227], "matter": [6, 185, 187, 223, 229], "checkpoint_fil": [6, 8, 10, 185, 186, 187, 226, 228, 229, 230, 231], "restart": [6, 223], "previou": [6, 39, 185, 186, 187], "more": [6, 8, 9, 26, 36, 38, 40, 41, 42, 43, 45, 47, 48, 49, 50, 52, 53, 78, 133, 146, 148, 151, 159, 184, 187, 190, 204, 208, 211, 213, 221, 223, 225, 226, 227, 228, 229, 230, 231], "next": [6, 39, 52, 57, 151, 178, 192, 228, 231], "section": [6, 9, 197, 219, 226, 228, 231], "recipe_checkpoint": [6, 185, 186, 187, 230], "null": [6, 8, 230], "usual": [6, 148, 170, 185, 208, 223, 226, 229], "model_typ": [6, 185, 186, 187, 226, 228, 230], "resume_from_checkpoint": [6, 185, 186, 187], "fals": [6, 8, 20, 22, 24, 29, 32, 33, 36, 37, 38, 39, 43, 44, 45, 47, 48, 49, 50, 52, 53, 57, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 124, 125, 126, 127, 128, 132, 133, 134, 135, 136, 137, 138, 143, 149, 150, 155, 156, 159, 170, 173, 180, 185, 186, 187, 201, 214, 223, 224, 225, 226, 228, 229, 230, 231], "requir": [6, 8, 37, 41, 42, 52, 78, 133, 169, 185, 187, 189, 198, 200, 201, 203, 204, 207, 208, 213, 214, 220, 223, 224, 225, 227, 230, 231], "param": [6, 9, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137, 155, 157, 158, 160, 185, 203, 229, 230, 231], "directli": [6, 8, 9, 11, 41, 45, 48, 52, 164, 184, 185, 223, 226, 227, 228, 229, 230, 231], "out": [6, 8, 9, 38, 44, 45, 47, 49, 50, 178, 185, 186, 219, 221, 223, 224, 226, 227, 228, 229, 231], "case": [6, 9, 10, 24, 26, 55, 56, 57, 151, 185, 189, 194, 198, 203, 205, 211, 221, 223, 224, 225, 226, 228, 229, 231], "discrep": [6, 185], "along": [6, 181, 229], "github": [6, 11, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 133, 135, 136, 137, 143, 147, 148, 153, 159, 164, 165, 166, 167, 168, 220, 225, 226, 227, 228], "repositori": [6, 23, 36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 77, 226, 227], "fullmodelmetacheckpoint": [6, 228, 230], "current": [6, 39, 68, 72, 85, 99, 108, 115, 117, 119, 127, 130, 134, 143, 146, 148, 149, 150, 166, 186, 187, 195, 198, 199, 205, 207, 210, 213, 225, 227, 228, 230], "test": [6, 8, 9, 221, 224], "complet": [6, 9, 15, 39, 46, 52, 131, 165, 224, 225, 226, 227, 228], "written": [6, 8, 9, 185, 186, 204, 205, 206, 207, 208, 221], "begin": [6, 39, 52, 78, 114, 133, 151, 174, 224, 228, 231], "partit": [6, 185, 231], "ha": [6, 78, 114, 151, 154, 156, 157, 160, 170, 187, 189, 216, 224, 225, 226, 227, 228, 229, 231], "standard": [6, 19, 32, 41, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 206, 221, 224, 226, 228], "key_1": [6, 187], "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 226], "inform": [6, 133, 204, 208, 211, 221, 223, 226, 227], "subsequ": [6, 9, 151, 178], "recipe_st": [6, 185, 186, 187], "pt": [6, 10, 185, 186, 187, 226, 228, 230], "epoch": [6, 9, 10, 153, 185, 186, 187, 223, 224, 226, 227, 228, 230], "optim": [6, 8, 9, 37, 68, 78, 119, 130, 153, 164, 166, 167, 168, 169, 187, 189, 191, 197, 210, 214, 224, 226, 227, 228, 229, 231], "etc": [6, 9, 185, 197, 227], "prevent": [6, 39, 164, 223], "flood": 6, "overwritten": 6, "note": [6, 8, 21, 72, 133, 149, 154, 189, 210, 213, 224, 225, 226, 229, 230, 231], "updat": [6, 8, 9, 26, 146, 164, 166, 177, 189, 214, 220, 224, 226, 227, 228, 229, 230, 231], "hf_model_0001_0": [6, 226], "hf_model_0002_0": [6, 226], "both": [6, 37, 160, 223, 226, 229, 230, 231], "adapt": [6, 154, 155, 156, 157, 158, 185, 186, 187, 224, 226, 229, 231], "merg": [6, 11, 12, 133, 142, 185, 226, 228, 231], "would": [6, 8, 10, 26, 39, 149, 151, 165, 220, 224, 225, 226, 229, 231], "addition": [6, 168, 173, 174, 213, 225, 229], "option": [6, 8, 9, 15, 20, 21, 22, 29, 30, 34, 36, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 56, 57, 61, 62, 63, 67, 71, 72, 73, 74, 78, 79, 84, 85, 86, 87, 88, 89, 94, 95, 98, 99, 100, 101, 104, 108, 109, 110, 114, 115, 116, 117, 118, 123, 126, 127, 128, 131, 133, 134, 135, 136, 137, 138, 142, 143, 148, 149, 150, 151, 152, 159, 160, 161, 162, 166, 171, 173, 176, 180, 181, 182, 185, 186, 187, 192, 193, 194, 196, 198, 204, 205, 208, 213, 214, 220, 221, 223, 225, 226], "save_adapter_weights_onli": 6, "choos": [6, 229], "primari": [6, 8, 9, 41, 227], "want": [6, 8, 9, 10, 11, 36, 41, 179, 180, 192, 220, 223, 224, 225, 226, 227, 228, 229], "resum": [6, 9, 153, 185, 186, 187, 231], "initi": [6, 9, 13, 37, 39, 58, 59, 60, 69, 70, 80, 81, 82, 83, 96, 97, 105, 106, 107, 120, 122, 139, 140, 141, 164, 189, 200, 201, 227, 229, 231], "frozen": [6, 164, 229, 231], "base": [6, 11, 24, 26, 38, 40, 41, 61, 62, 63, 64, 65, 66, 68, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 121, 124, 125, 127, 128, 129, 132, 133, 134, 135, 136, 137, 138, 148, 153, 155, 156, 158, 159, 160, 162, 164, 165, 167, 168, 185, 190, 193, 195, 203, 205, 219, 224, 226, 227, 228, 229, 231], "well": [6, 8, 9, 221, 223, 225, 226, 228, 231], "learnt": [6, 224, 226], "someth": [6, 9, 10, 224, 226, 230], "NOT": [6, 68, 119], "refer": [6, 8, 9, 147, 148, 151, 156, 162, 164, 165, 166, 167, 168, 204, 221, 229, 230], "adapter_checkpoint": [6, 185, 186, 187], "adapter_0": [6, 226], "now": [6, 189, 191, 224, 225, 226, 227, 228, 229, 230, 231], "knowledg": 6, "creat": [6, 8, 11, 22, 26, 39, 41, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 73, 74, 75, 76, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 96, 97, 100, 101, 102, 103, 105, 106, 107, 109, 110, 111, 112, 116, 118, 120, 122, 124, 125, 128, 130, 132, 135, 136, 137, 139, 140, 141, 146, 151, 153, 184, 185, 186, 187, 191, 204, 205, 207, 223, 224, 225, 226, 231], "simpl": [6, 9, 15, 30, 151, 168, 219, 225, 227, 229, 230, 231], "forward": [6, 9, 54, 55, 56, 143, 144, 145, 147, 148, 149, 150, 151, 155, 164, 165, 166, 167, 168, 197, 214, 228, 229, 231], "modeltyp": [6, 185, 186, 187], "llama2_13b": [6, 86], "right": [6, 185, 226, 228, 229], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 159, 189, 229], "successfulli": [6, 223, 227], "vocab": [6, 11, 133, 142, 149, 228], "70": [6, 96], "x": [6, 54, 55, 56, 143, 144, 145, 147, 148, 149, 150, 151, 155, 192, 212, 229, 230, 231], "randint": 6, "1": [6, 9, 39, 50, 54, 55, 67, 78, 79, 85, 94, 95, 99, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 119, 121, 126, 127, 129, 133, 134, 135, 136, 138, 139, 140, 143, 149, 151, 153, 163, 164, 165, 166, 167, 168, 169, 173, 174, 176, 180, 181, 182, 186, 188, 192, 201, 204, 207, 208, 209, 212, 213, 223, 224, 226, 227, 229, 230, 231], "no_grad": 6, "6": [6, 39, 68, 72, 147, 151, 163, 169, 183, 209, 230, 231], "3989": 6, "9": [6, 151, 169, 226, 230, 231], "0531": 6, "3": [6, 39, 57, 94, 113, 128, 130, 131, 133, 151, 163, 169, 180, 181, 182, 183, 188, 190, 196, 198, 209, 212, 223, 224, 226, 227, 228, 230, 231], "2375": 6, "5": [6, 8, 15, 151, 153, 163, 164, 168, 169, 170, 180, 209, 226, 227, 228], "2822": 6, "4": [6, 8, 57, 143, 151, 163, 169, 179, 198, 209, 215, 221, 223, 225, 226, 228, 229, 230, 231], "4872": 6, "7469": 6, "8": [6, 44, 47, 49, 61, 62, 63, 64, 65, 66, 73, 74, 75, 76, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 109, 110, 111, 112, 116, 118, 124, 125, 128, 132, 133, 135, 136, 137, 151, 163, 169, 226, 229, 230, 231], "6737": 6, "11": [6, 151, 169, 226, 230, 231], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 151, 163, 166, 169, 178, 209], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": [6, 151], "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 7, 9, 24, 38, 40, 48, 133, 159, 176, 204, 208, 223, 224, 225, 226, 227, 228, 229, 230], "find": [6, 7, 9, 10, 164, 223, 226, 227, 229], "hope": 6, "deeper": [6, 227], "insight": [6, 226], "happi": [6, 226], "thi": [7, 8, 9, 10, 11, 19, 20, 22, 24, 31, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 68, 72, 78, 79, 85, 94, 95, 99, 104, 108, 113, 114, 115, 117, 119, 121, 126, 127, 129, 130, 131, 133, 134, 138, 143, 144, 148, 149, 150, 151, 152, 153, 154, 156, 159, 160, 162, 164, 165, 166, 168, 169, 173, 174, 176, 178, 184, 185, 186, 187, 189, 190, 192, 193, 194, 197, 201, 203, 204, 205, 207, 208, 210, 211, 213, 219, 220, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 220, 224, 225, 227], "start": [7, 9, 10, 176, 188, 204, 220, 221, 224, 225, 226, 227, 230], "cometlogg": 7, "checkpoint": [7, 8, 9, 152, 174, 185, 186, 187, 188, 189, 208, 211, 221, 223, 228, 229, 230, 231], "workspac": [7, 10, 204], "seen": [7, 10, 229, 231], "screenshot": [7, 10], "below": [7, 10, 15, 148, 184, 225, 228, 229, 231], "instal": [7, 8, 10, 201, 204, 207, 208, 219, 223, 225, 226, 227, 228, 229, 230, 231], "comet_ml": [7, 204], "packag": [7, 10, 204, 207, 208, 220, 225], "featur": [7, 9, 10, 220, 221, 226, 227], "via": [7, 8, 10, 45, 48, 52, 155, 185, 229, 231], "pip": [7, 10, 204, 207, 208, 220, 226, 228], "login": [7, 10, 204, 208, 223, 226], "data": [7, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 44, 45, 47, 48, 49, 50, 51, 52, 53, 77, 78, 151, 164, 167, 177, 197, 204, 205, 206, 207, 208, 225, 226, 230, 231], "command": [7, 9, 10, 190, 220, 223, 224, 225, 226, 227, 228, 229, 230, 231], "add": [7, 8, 10, 36, 39, 41, 42, 52, 78, 94, 113, 133, 151, 174, 176, 187, 188, 190, 225, 226, 228, 229, 231], "built": [7, 8, 10, 51, 220, 224, 227, 231], "metric_logg": [7, 8, 9, 10], "metric_log": [7, 8, 10, 204, 205, 206, 207, 208], "project": [7, 10, 57, 61, 62, 63, 68, 72, 79, 83, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 122, 127, 128, 134, 137, 138, 143, 144, 151, 159, 160, 188, 195, 204, 208, 219, 224, 229, 231], "experiment_nam": [7, 204], "my": [7, 192, 223, 224, 225, 226, 228], "experi": [7, 8, 204, 208, 219, 221, 224, 228, 229], "automat": [7, 8, 10, 11, 45, 223, 226, 231], "grab": [7, 10, 228], "hyperparamet": [7, 168, 189, 221, 227, 229, 231], "tab": [7, 10], "actual": [7, 8, 10, 15, 20, 30, 36, 41, 224, 230], "asset": 7, "artifact": [7, 10, 214], "click": [7, 10], "sampl": [7, 10, 15, 16, 17, 20, 21, 22, 23, 24, 25, 29, 30, 32, 33, 36, 38, 39, 40, 41, 42, 48, 50, 52, 143, 148, 149, 150, 151, 167, 177, 178, 192, 224, 226], "after": [7, 9, 26, 41, 71, 84, 94, 98, 123, 126, 131, 143, 146, 147, 149, 150, 159, 170, 203, 204, 205, 206, 207, 208, 224, 226, 228, 230, 231], "pars": [8, 11, 12, 175, 190, 224, 227], "effect": [8, 168, 230], "cli": [8, 10, 12, 13, 220, 226, 227], "prerequisit": [8, 224, 225, 226, 227, 228, 229, 230, 231], "Be": [8, 224, 226, 227, 228, 229, 230, 231], "familiar": [8, 224, 226, 227, 228, 229, 230, 231], "fundament": [8, 230], "There": [8, 35, 55, 180, 224, 227, 228, 229], "entri": [8, 9, 227], "point": [8, 9, 32, 33, 176, 225, 226, 227, 228, 229, 230, 231], "locat": [8, 223, 225, 228, 229, 230, 231], "thei": [8, 9, 37, 57, 149, 151, 160, 190, 195, 223, 224, 225, 229, 230], "truth": [8, 226, 228], "reproduc": [8, 204], "overridden": [8, 144, 190, 214], "quick": 8, "experiment": 8, "serv": [8, 176, 184, 225, 229], "particular": [8, 36, 37, 41, 184, 225, 229, 231], "seed": [8, 9, 10, 213, 227, 230], "shuffl": [8, 39, 230], "devic": [8, 9, 159, 189, 193, 194, 197, 223, 224, 226, 227, 228, 229], "cuda": [8, 193, 194, 197, 214, 220, 226, 231], "dtype": [8, 9, 143, 146, 149, 150, 152, 194, 212, 216, 226, 230, 231], "fp32": [8, 230, 231], "enable_fsdp": 8, "mani": [8, 39, 225, 226], "object": [8, 11, 12, 16, 17, 23, 25, 47, 49, 57, 133, 143, 164, 168, 184, 198, 224], "keyword": [8, 11, 36, 38, 40, 41, 42, 45, 46, 48, 52, 53, 152, 224, 225], "loss": [8, 9, 24, 26, 38, 41, 44, 47, 49, 50, 164, 165, 166, 167, 168, 227, 229, 231], "subfield": 8, "dotpath": [8, 225], "wish": [8, 225], "exact": [8, 11, 226], "path": [8, 9, 10, 11, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 67, 71, 78, 84, 94, 98, 114, 123, 126, 131, 133, 142, 173, 174, 175, 185, 186, 187, 190, 214, 223, 224, 225, 226, 228, 229], "normal": [8, 36, 39, 41, 78, 114, 145, 147, 149, 150, 173, 224, 225, 229, 230, 231], "python": [8, 133, 190, 196, 204, 208, 213, 217, 223, 225, 226, 230], "alpaca_dataset": [8, 43, 225], "custom": [8, 9, 26, 36, 38, 40, 41, 45, 48, 52, 211, 221, 223, 227, 228, 229], "train_on_input": [8, 20, 22, 29, 32, 33, 36, 37, 38, 43, 44, 45, 47, 48, 49, 50, 224, 225], "onc": [8, 26, 156, 226, 227, 228, 229, 231], "ve": [8, 146, 223, 224, 225, 226, 228, 229], "instanc": [8, 11, 36, 37, 38, 85, 99, 108, 115, 117, 127, 134, 135, 136, 139, 140, 144, 152, 157, 158, 229], "cfg": [8, 9, 12, 13, 14], "under": [8, 214, 225, 231], "preced": [8, 11, 223, 228, 229], "throw": 8, "notic": [8, 54, 55, 56, 151, 224, 225, 229], "miss": [8, 159, 160, 214, 229], "posit": [8, 11, 39, 54, 55, 56, 57, 68, 72, 115, 117, 119, 121, 127, 129, 143, 146, 148, 149, 150, 151, 228], "anoth": [8, 41, 204, 226], "handl": [8, 13, 37, 41, 78, 114, 173, 174, 224, 226, 229, 231], "def": [8, 9, 10, 13, 184, 188, 224, 225, 229, 231], "dictconfig": [8, 9, 11, 12, 13, 14, 204, 208, 214], "arg": [8, 11, 18, 27, 56, 77, 113, 145, 149, 152, 154, 171, 172, 177, 190, 206, 214, 230], "tupl": [8, 11, 26, 56, 67, 78, 94, 114, 126, 133, 146, 151, 152, 161, 162, 164, 165, 166, 167, 168, 169, 170, 172, 176, 179, 180, 181, 182, 184, 190, 199, 214, 216], "kwarg": [8, 11, 18, 27, 77, 113, 142, 145, 152, 154, 171, 172, 177, 190, 200, 204, 205, 206, 207, 208, 211, 214, 225], "str": [8, 11, 12, 15, 20, 21, 22, 24, 26, 29, 30, 32, 33, 36, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 67, 71, 78, 84, 94, 98, 114, 123, 126, 131, 133, 142, 152, 154, 155, 157, 158, 159, 160, 163, 169, 171, 172, 173, 174, 175, 185, 186, 187, 188, 189, 190, 193, 194, 196, 197, 198, 200, 202, 204, 205, 206, 207, 208, 209, 213, 214, 215, 216, 224, 225], "mean": [8, 143, 147, 149, 150, 161, 203, 223, 224, 225, 227, 229, 230], "pass": [8, 11, 24, 26, 36, 37, 38, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 144, 152, 156, 160, 166, 174, 184, 187, 194, 195, 197, 200, 203, 204, 207, 208, 211, 214, 223, 224, 225, 229, 230, 231], "d": [8, 24, 143, 146, 149, 223, 224, 229, 230], "llama2_token": [8, 224, 226], "llama2token": [8, 84], "modeltoken": [8, 24, 36, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 176, 224, 225], "bool": [8, 20, 22, 24, 29, 32, 33, 36, 38, 39, 42, 43, 44, 45, 47, 48, 49, 50, 52, 53, 57, 61, 62, 63, 64, 65, 66, 67, 68, 72, 73, 74, 75, 76, 78, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 124, 125, 126, 127, 128, 132, 133, 134, 135, 136, 137, 138, 149, 150, 152, 155, 159, 160, 161, 170, 172, 173, 174, 176, 180, 184, 185, 186, 187, 195, 197, 200, 201, 203, 204, 207, 211, 214, 215, 224, 231], "max_seq_len": [8, 11, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 67, 68, 71, 72, 78, 79, 84, 85, 94, 95, 98, 99, 104, 108, 114, 115, 117, 119, 121, 123, 126, 127, 129, 131, 133, 134, 138, 142, 143, 146, 148, 149, 163, 176, 224, 225, 230], "int": [8, 10, 34, 36, 38, 39, 40, 42, 43, 44, 45, 46, 48, 51, 52, 53, 54, 55, 56, 57, 61, 62, 63, 64, 65, 66, 67, 68, 71, 72, 73, 74, 75, 76, 78, 79, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 121, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 142, 143, 146, 147, 148, 149, 150, 151, 153, 155, 163, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 192, 195, 199, 203, 204, 205, 206, 207, 208, 209, 211, 213, 214, 223, 224, 225, 229, 231], "512": [8, 43, 44, 57, 225, 231], "instructdataset": [8, 43, 44, 48, 225], "alreadi": [8, 143, 188, 200, 203, 220, 223, 225, 226, 229], "overwrit": [8, 187, 220, 223], "duplic": [8, 9, 221, 223], "sometim": 8, "than": [8, 35, 143, 146, 151, 164, 181, 184, 187, 188, 215, 216, 224, 225, 226, 227, 228, 229, 231], "resolv": [8, 12, 227], "alpaca": [8, 15, 37, 43, 44, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137, 225], "disklogg": 8, "log_dir": [8, 205, 207, 208], "conveni": [8, 9, 223], "verifi": [8, 193, 194, 195, 224, 227, 229], "properli": [8, 159, 201, 223], "wa": [8, 55, 56, 57, 151, 159, 181, 224, 229, 230, 231], "cp": [8, 220, 223, 224, 226, 227, 228, 230], "7b_lora_single_devic": [8, 226, 227, 229, 231], "my_config": [8, 223], "discuss": [8, 226, 227, 228, 229], "guidelin": 8, "while": [8, 9, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137, 144, 221, 226, 230, 231], "mai": [8, 10, 151, 195, 224, 225, 227, 229], "tempt": 8, "put": [8, 9, 227, 229, 230], "much": [8, 168, 226, 228, 229, 230, 231], "give": [8, 225, 229], "maximum": [8, 34, 36, 38, 39, 40, 42, 44, 45, 46, 48, 51, 52, 53, 54, 55, 57, 68, 71, 72, 79, 84, 85, 94, 95, 98, 99, 104, 108, 115, 117, 119, 121, 123, 127, 129, 131, 134, 138, 143, 146, 148, 149, 163, 179, 180, 181, 182, 223], "flexibl": [8, 37, 225], "switch": 8, "encourag": [8, 78, 168, 229], "clariti": 8, "significantli": [8, 164], "easier": [8, 226, 227], "dont": 8, "slimorca_dataset": 8, "privat": 8, "expos": [8, 9, 187, 224, 227], "parent": [8, 223], "modul": [8, 11, 47, 49, 54, 55, 56, 57, 117, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 188, 191, 195, 203, 210, 211, 213, 225, 227, 229, 231], "__init__": [8, 9, 229, 231], "py": [8, 11, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 133, 135, 136, 137, 143, 146, 147, 148, 153, 164, 165, 166, 167, 168, 223, 226, 228], "guarante": 8, "stabil": [8, 221, 230, 231], "underscor": 8, "_alpaca": 8, "collect": [8, 192, 227], "itself": 8, "k1": [8, 9], "v1": [8, 9, 53], "k2": [8, 9], "v2": [8, 9, 204, 225], "lora_finetune_single_devic": [8, 223, 224, 226, 227, 228, 229, 231], "home": 8, "my_model_checkpoint": 8, "file_1": 8, "file_2": 8, "my_tokenizer_path": 8, "assign": [8, 41], "nest": 8, "dot": 8, "notat": [8, 143, 148, 149, 161, 162], "flag": [8, 9, 24, 38, 44, 47, 49, 50, 184, 187, 195, 223, 231], "bitsandbyt": 8, "pagedadamw8bit": 8, "delet": 8, "foreach": [8, 36], "pytorch": [8, 9, 78, 149, 152, 159, 184, 201, 207, 211, 213, 214, 219, 220, 221, 226, 228, 229, 230, 231], "llama3": [8, 36, 94, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 127, 188, 192, 195, 219, 223, 225, 226], "8b_full": [8, 223, 225], "adamw": [8, 229], "lr": [8, 153], "2e": 8, "fuse": [8, 210, 230], "nproc_per_nod": [8, 225, 228, 229, 230], "full_finetune_distribut": [8, 223, 225, 226, 227], "core": [9, 41, 221, 225, 227, 231], "i": [9, 23, 24, 25, 77, 113, 143, 149, 150, 151, 152, 158, 189, 192, 225, 226, 228, 230, 231], "structur": [9, 16, 17, 22, 23, 25, 27, 29, 32, 33, 36, 41, 45, 98, 131, 142, 178, 224, 225, 226, 230], "new": [9, 22, 29, 46, 47, 49, 50, 120, 146, 188, 204, 205, 207, 224, 226, 227, 228, 229, 231], "user": [9, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 33, 35, 36, 41, 67, 78, 79, 85, 95, 99, 104, 108, 114, 115, 117, 119, 121, 126, 127, 129, 133, 134, 138, 143, 176, 181, 182, 224, 225, 227, 230], "thought": [9, 221, 227, 231], "target": [9, 168, 181, 221], "pipelin": [9, 221], "llm": [9, 219, 221, 225, 226, 228, 229], "eg": [9, 149, 185, 221], "meaning": [9, 221, 226], "fsdp": [9, 184, 189, 195, 203, 221, 227, 228], "activ": [9, 144, 197, 202, 211, 214, 221, 230, 231], "gradient": [9, 203, 210, 214, 221, 226, 228, 229, 231], "accumul": [9, 210, 214, 221], "mix": [9, 145, 223, 225, 226], "precis": [9, 145, 152, 194, 221, 227, 231], "given": [9, 11, 15, 21, 30, 35, 44, 46, 47, 49, 50, 51, 53, 133, 155, 156, 162, 165, 171, 172, 192, 193, 194, 198, 203, 210, 215, 221, 229], "complex": 9, "becom": [9, 151, 165, 220, 225], "harder": 9, "anticip": 9, "architectur": [9, 23, 25, 77, 113, 149, 151, 188, 223, 225], "methodolog": 9, "reason": [9, 192, 226, 230], "possibl": [9, 39, 45, 179, 180, 223, 225], "trade": 9, "off": [9, 26, 78, 114, 226, 230], "memori": [9, 37, 38, 39, 40, 42, 44, 46, 48, 52, 53, 133, 152, 159, 195, 197, 202, 203, 214, 219, 221, 226, 227, 228, 230], "vs": [9, 165, 227], "qualiti": [9, 226, 229, 230], "believ": 9, "best": [9, 180, 224], "suit": [9, 227], "b": [9, 143, 146, 148, 149, 150, 155, 161, 162, 168, 203, 208, 229, 231], "fit": [9, 36, 38, 39, 40, 42, 44, 46, 48, 52, 53, 151, 164, 165, 180, 181, 182, 225], "solut": [9, 165], "result": [9, 57, 67, 78, 114, 126, 151, 176, 178, 214, 226, 228, 229, 230, 231], "meant": [9, 152, 189], "depend": [9, 10, 15, 185, 214, 223, 225, 226, 229, 231], "level": [9, 41, 133, 177, 191, 196, 203, 221, 231], "expertis": 9, "routin": 9, "yourself": [9, 223, 228, 229], "exist": [9, 204, 220, 223, 226, 227, 228, 231], "ad": [9, 26, 54, 55, 56, 114, 121, 151, 173, 187, 188, 224, 225, 229, 230, 231], "ones": 9, "modular": [9, 221], "build": [9, 45, 48, 52, 57, 68, 79, 95, 104, 119, 121, 138, 221, 228, 229], "block": [9, 39, 61, 62, 63, 68, 72, 73, 74, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 127, 128, 134, 135, 136, 137, 138, 159, 160, 221], "wandb": [9, 10, 208, 227], "log": [9, 12, 164, 165, 166, 167, 168, 196, 197, 202, 204, 205, 206, 207, 208, 226, 227, 228, 229, 231], "fulli": [9, 37], "nativ": [9, 219, 221, 229, 230, 231], "correct": [9, 19, 47, 147, 148, 149, 193, 221, 224, 225], "numer": [9, 221, 230], "pariti": [9, 221], "verif": 9, "extens": [9, 187, 221], "comparison": [9, 229, 231], "benchmark": [9, 213, 221, 226, 228, 229, 230], "limit": [9, 180, 181, 182, 189, 225, 230], "hidden": [9, 57, 144, 151], "behind": 9, "100": [9, 38, 44, 47, 49, 50, 169, 192, 209, 229, 231], "prefer": [9, 30, 51, 164, 165, 166, 167, 168, 169, 221, 223, 225], "over": [9, 41, 153, 164, 165, 190, 221, 223, 226, 229, 231], "unnecessari": 9, "abstract": [9, 16, 21, 171, 172, 221, 227, 231], "No": [9, 187, 221], "inherit": [9, 190, 221, 225], "go": [9, 23, 25, 57, 67, 77, 78, 113, 114, 126, 151, 176, 221, 225, 226, 227, 231], "upon": [9, 37, 228], "figur": [9, 229, 231], "spectrum": 9, "decid": 9, "interact": [9, 219, 227], "avail": [9, 53, 190, 193, 194, 201, 221, 223, 226, 228, 229], "paradigm": [9, 133], "consist": [9, 53, 227], "configur": [9, 38, 40, 41, 44, 45, 46, 47, 48, 49, 50, 52, 53, 72, 85, 94, 99, 108, 115, 126, 127, 134, 150, 204, 221, 224, 227, 228, 229, 230, 231], "paramet": [9, 11, 12, 13, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 189, 191, 192, 193, 194, 195, 196, 197, 198, 200, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 219, 221, 223, 224, 225, 226, 227, 228, 229, 230, 231], "overrid": [9, 12, 13, 223, 226, 227, 228, 231], "togeth": [9, 39, 208, 227, 229, 230], "valid": [9, 35, 159, 160, 162, 216, 220, 226, 227], "environ": [9, 193, 201, 204, 220, 223, 225, 226, 227, 230], "logic": [9, 41, 172, 188, 221, 227, 229], "api": [9, 10, 19, 31, 32, 41, 64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 159, 204, 223, 224, 227, 228, 231], "closer": [9, 229], "monolith": [9, 221], "trainer": [9, 164, 165, 167, 168], "A": [9, 10, 19, 31, 32, 33, 37, 39, 57, 67, 78, 114, 126, 133, 142, 143, 149, 150, 151, 152, 155, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 173, 174, 176, 178, 180, 184, 188, 189, 190, 197, 198, 202, 203, 209, 218, 219, 222, 223, 224, 229, 230, 231], "wrapper": [9, 145, 173, 174, 189, 191, 223, 229], "around": [9, 36, 41, 78, 114, 145, 173, 174, 197, 223, 224, 226, 229, 230, 231], "extern": [9, 225], "primarili": [9, 37, 229], "eleutherai": [9, 53, 221, 229, 230], "har": [9, 221, 229, 230], "control": [9, 38, 44, 47, 49, 50, 156, 165, 204, 213, 226], "multi": [9, 36, 143, 159, 228], "stage": [9, 151], "distil": 9, "oper": [9, 133, 151, 156, 177, 213, 230], "turn": [9, 24, 35, 36, 224], "dataload": [9, 39, 44, 47, 49], "applic": [9, 143, 185, 186, 208], "clean": [9, 10, 43], "process": [9, 10, 41, 57, 133, 151, 152, 199, 200, 213, 225, 227, 230, 231], "group": [9, 143, 199, 200, 204, 205, 206, 207, 208, 223, 228, 230], "init_process_group": [9, 200], "backend": [9, 223, 230], "gloo": 9, "els": [9, 190, 208, 221, 231], "nccl": 9, "fullfinetunerecipedistribut": 9, "cleanup": 9, "stuff": 9, "carri": [9, 41], "relev": [9, 223, 226, 229], "interfac": [9, 16, 21, 26, 27, 37, 154, 177, 225], "metric": [9, 227, 230], "logger": [9, 196, 202, 204, 205, 206, 207, 208, 227], "self": [9, 10, 39, 61, 62, 63, 68, 72, 73, 74, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 121, 127, 128, 129, 134, 135, 136, 137, 138, 143, 149, 150, 154, 159, 160, 185, 188, 189, 225, 229, 231], "_devic": 9, "get_devic": 9, "_dtype": 9, "get_dtyp": 9, "ckpt_dict": 9, "wrap": [9, 184, 195, 203, 211, 224], "_model": [9, 189], "_setup_model": 9, "_token": [9, 225], "_setup_token": 9, "_optim": 9, "_setup_optim": 9, "_loss_fn": 9, "_setup_loss": 9, "_sampler": 9, "_dataload": 9, "_setup_data": 9, "backward": [9, 189, 191, 210, 214, 231], "zero_grad": 9, "curr_epoch": 9, "rang": [9, 164, 166, 168, 213, 223, 228, 230], "epochs_run": [9, 10], "total_epoch": [9, 10], "idx": [9, 39], "batch": [9, 39, 44, 47, 49, 55, 143, 146, 148, 149, 150, 151, 161, 162, 163, 164, 165, 167, 168, 169, 209, 214, 221, 225, 227, 228, 229], "enumer": 9, "_autocast": 9, "logit": [9, 192], "label": [9, 36, 38, 39, 40, 42, 44, 45, 46, 48, 50, 51, 52, 53, 164, 168, 169, 209], "global_step": 9, "_log_every_n_step": 9, "_metric_logg": 9, "log_dict": [9, 204, 205, 206, 207, 208], "step": [9, 39, 41, 149, 153, 161, 191, 204, 205, 206, 207, 208, 210, 214, 219, 226, 229, 230, 231], "learn": [9, 37, 153, 165, 221, 224, 225, 227, 228, 229, 230, 231], "decor": [9, 13], "recipe_main": [9, 13], "fullfinetunerecip": 9, "wandblogg": [10, 229, 231], "Then": [10, 156, 227], "tip": 10, "straggler": 10, "background": 10, "crash": 10, "otherwis": [10, 55, 56, 57, 151, 201, 204, 224, 230], "exit": [10, 220, 223], "resourc": [10, 204, 205, 206, 207, 208, 230], "kill": 10, "ps": 10, "aux": 10, "grep": 10, "awk": 10, "xarg": 10, "desir": [10, 36, 41, 181, 182, 212, 224], "suggest": 10, "approach": [10, 37, 225], "full_finetun": 10, "joinpath": 10, "_checkpoint": [10, 226], "_output_dir": [10, 185, 186, 187], "torchtune_model_": 10, "with_suffix": 10, "wandb_at": 10, "type": [10, 11, 13, 24, 32, 33, 34, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 104, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 155, 157, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 203, 209, 211, 212, 213, 214, 215, 216, 225, 226, 229, 230, 231], "descript": [10, 45, 223], "whatev": 10, "metadata": [10, 230], "seed_kei": 10, "epochs_kei": 10, "total_epochs_kei": 10, "max_steps_kei": 10, "max_steps_per_epoch": [10, 230], "add_fil": 10, "log_artifact": 10, "field": [11, 20, 21, 24, 32, 33, 36, 39, 41, 44, 47, 49, 202, 225], "hydra": 11, "facebook": 11, "research": 11, "http": [11, 36, 38, 40, 42, 45, 46, 48, 52, 53, 58, 59, 60, 61, 62, 63, 64, 65, 66, 69, 70, 73, 74, 75, 76, 78, 80, 81, 82, 83, 86, 87, 88, 89, 90, 91, 92, 93, 94, 100, 101, 102, 103, 109, 110, 111, 112, 120, 122, 124, 125, 128, 130, 131, 132, 133, 135, 136, 137, 139, 140, 141, 143, 147, 148, 151, 153, 159, 161, 164, 165, 166, 167, 168, 178, 184, 185, 186, 190, 196, 201, 204, 207, 208, 211, 213, 220, 225, 226, 228], "com": [11, 61, 62, 63, 73, 74, 78, 86, 87, 88, 89, 94, 100, 101, 109, 110, 128, 133, 135, 136, 137, 143, 147, 148, 153, 159, 164, 165, 166, 167, 168, 204, 220, 226, 228], "facebookresearch": [11, 147], "blob": [11, 61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 131, 133, 135, 136, 137, 143, 147, 148, 153, 164, 165, 166, 167, 168], "main": [11, 13, 78, 131, 143, 147, 148, 220, 226, 228], "_intern": 11, "_instantiate2": 11, "l148": 11, "omegaconf": 11, "num_lay": [11, 57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 149, 151], "32": [11, 151, 204, 228, 229, 230, 231], "num_head": [11, 57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 148, 149], "num_kv_head": [11, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146], "vocab_s": [11, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138], "must": [11, 26, 37, 154, 190, 204, 231], "return": [11, 13, 15, 16, 17, 21, 23, 24, 25, 26, 30, 32, 33, 34, 36, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 187, 189, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 209, 212, 213, 214, 215, 224, 225, 229, 231], "nn": [11, 143, 144, 146, 149, 150, 151, 152, 154, 156, 157, 158, 184, 191, 203, 210, 211, 216, 229, 231], "parsed_yaml": 11, "embed_dim": [11, 54, 55, 56, 57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 148, 150, 151, 229], "valueerror": [11, 25, 32, 35, 36, 38, 45, 126, 134, 143, 149, 151, 185, 186, 187, 194, 197, 213, 216], "recipe_nam": 12, "rank": [12, 61, 62, 63, 72, 73, 74, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 115, 116, 117, 118, 127, 128, 134, 135, 136, 137, 155, 199, 201, 213, 227, 229, 231], "zero": [12, 146, 147, 226, 228, 230], "displai": 12, "callabl": [13, 36, 38, 40, 41, 149, 156, 184, 192, 195, 198, 203, 211], "With": [13, 226, 229, 230, 231], "my_recip": 13, "foo": 13, "bar": [13, 221, 227], "instanti": [14, 26, 58, 59, 60, 61, 62, 63, 68, 69, 70, 71, 72, 73, 74, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 115, 116, 117, 118, 119, 120, 121, 122, 123, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 140, 141, 142, 189], "configerror": 14, "cannot": [14, 133, 187, 228], "templat": [15, 18, 19, 21, 26, 27, 30, 31, 36, 37, 38, 40, 41, 44, 45, 47, 48, 49, 50, 77, 78, 113, 133], "style": [15, 39, 43, 44, 45, 50, 231], "slightli": 15, "describ": [15, 78, 94, 211, 225], "task": [15, 19, 31, 37, 41, 46, 224, 225, 226, 228, 229, 230, 231], "context": [15, 17, 18, 130, 156, 212, 214, 225], "respons": [15, 17, 18, 20, 24, 41, 67, 78, 114, 126, 161, 162, 164, 165, 167, 168, 176, 225, 226, 227, 228], "appropri": [15, 17, 23, 24, 25, 37, 77, 153, 185, 225, 231], "Or": 15, "instruciton": 15, "classmethod": [15, 16, 17, 21, 23, 24, 25, 30, 225], "column_map": [15, 20, 21, 22, 29, 30, 37, 38, 40, 47, 48, 49, 50, 225], "placehold": [15, 21, 30, 38, 40, 48, 225], "column": [15, 20, 21, 22, 29, 30, 38, 40, 41, 42, 47, 48, 49, 50, 52, 143, 149, 150, 224, 225, 230], "ident": [15, 21, 22, 25, 29, 30, 38, 39, 40, 47, 48, 49, 50, 113, 165, 226, 230], "poem": 15, "n": [15, 19, 26, 30, 31, 67, 78, 114, 126, 143, 151, 176, 180, 218, 222, 223, 224, 225, 230], "nwrite": 15, "long": [15, 39, 133, 174, 224, 225, 229], "where": [15, 26, 30, 36, 44, 47, 49, 55, 78, 83, 114, 122, 143, 149, 151, 155, 161, 164, 165, 166, 169, 170, 173, 178, 180, 195, 203, 225], "me": 15, "tag": [16, 17, 23, 25, 26, 36, 41, 77, 113, 204, 205, 206, 207, 208, 224], "system": [16, 17, 18, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 35, 36, 41, 67, 77, 78, 113, 114, 126, 133, 176, 224, 225], "assist": [16, 17, 18, 20, 22, 23, 24, 26, 27, 28, 29, 32, 33, 35, 36, 41, 67, 77, 78, 114, 126, 131, 133, 176, 192, 224, 225], "role": [16, 22, 24, 26, 27, 29, 32, 33, 36, 41, 67, 78, 114, 126, 176, 224, 225], "prepend": [16, 26, 27, 78, 94, 114, 173], "append": [16, 26, 27, 94, 114, 126, 173, 204, 220, 225], "messag": [16, 17, 18, 20, 22, 23, 25, 26, 27, 29, 32, 33, 35, 36, 41, 45, 47, 49, 50, 67, 71, 78, 84, 94, 98, 114, 123, 126, 131, 133, 172, 176, 220, 223, 224, 225], "accord": [16, 25, 113, 224], "openai": [17, 18, 32, 45, 166, 225], "markup": [17, 18], "languag": [17, 18, 133, 155, 164, 192, 229], "It": [17, 18, 24, 25, 26, 41, 113, 151, 164, 168, 181, 204, 223, 224, 225, 231], "im_start": [17, 18], "im_end": [17, 18], "goe": [17, 18, 156], "functool": [19, 31, 184], "partial": [19, 31, 184], "_prompt_templ": [19, 31, 47, 49], "prompttempl": [19, 31, 41, 47, 49, 50], "english": 19, "ncorrect": 19, "grammar": [19, 47, 225], "user_messag": [19, 31, 224], "assistant_messag": [19, 31, 224], "equival": [20, 55, 165, 167, 168], "respect": [20, 23, 37, 77, 158, 180, 214, 224, 225], "whether": [20, 22, 24, 29, 32, 33, 36, 38, 42, 44, 45, 47, 48, 49, 50, 52, 53, 61, 62, 63, 68, 72, 73, 74, 85, 86, 87, 88, 89, 94, 99, 100, 101, 108, 109, 110, 114, 115, 116, 117, 118, 126, 127, 128, 133, 134, 135, 136, 137, 138, 152, 155, 159, 160, 173, 174, 184, 194, 197, 204, 224, 225], "keep": [20, 226, 229], "alwai": [21, 47, 49, 50, 165, 190, 204], "dataclass": [22, 224], "remain": [22, 29, 32, 33, 153, 229], "unmask": [22, 29, 32, 33], "human": [23, 24, 29, 33, 77, 164, 166, 167, 224], "pre": [23, 39, 41, 52, 77, 78, 151, 220, 224, 225], "taken": [23, 77, 229, 231], "inst": [23, 25, 36, 41, 77, 78, 113, 224, 225], "sy": [23, 77, 78, 224, 225], "honest": [23, 77, 224, 225], "am": [23, 25, 77, 113, 224, 225, 226, 228], "pari": [23, 25, 77, 113, 225], "capit": [23, 25, 30, 77, 113, 225], "franc": [23, 25, 30, 77, 113, 225], "known": [23, 25, 77, 78, 113, 114, 198, 225, 230], "its": [23, 25, 39, 77, 113, 117, 143, 148, 149, 150, 165, 210, 213, 223, 224, 225, 226, 228, 229], "stun": [23, 25, 77, 113, 225], "liter": [24, 26, 28, 61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 159, 160], "ipython": [24, 26, 28, 41], "union": [24, 47, 49, 50, 52, 53, 134, 138, 160, 204, 205, 206, 207, 208, 211, 213], "mask": [24, 26, 38, 39, 41, 44, 47, 49, 50, 67, 78, 94, 114, 126, 133, 143, 149, 150, 161, 166, 172, 176, 178, 224, 225], "eot": [24, 94], "repres": [24, 54, 55, 151, 169, 180, 224, 230], "individu": [24, 39, 197, 208, 211, 224, 225], "interleav": [24, 178], "tokenize_messag": [24, 36, 38, 40, 42, 44, 45, 46, 48, 51, 52, 53, 67, 78, 94, 114, 126, 133, 172, 176, 224, 225], "attach": 24, "special": [24, 36, 41, 78, 94, 98, 114, 126, 131, 133, 142, 151, 171, 172, 174, 175, 176, 178, 189, 225], "writer": 24, "dictionari": [24, 26, 39, 41, 163, 169, 197, 202, 204, 205, 206, 207, 208, 209, 226], "hello": [24, 67, 78, 94, 114, 126, 133, 173, 174, 224, 226, 228], "world": [24, 67, 78, 94, 114, 126, 133, 173, 174, 199, 201, 226], "calcul": [24, 26, 143, 149, 150, 151, 161, 162, 166, 180, 181, 228], "correspond": [24, 154, 157, 161, 166, 169, 194, 227, 228, 230], "consecut": [24, 35, 178], "e": [24, 36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 133, 143, 151, 152, 154, 158, 178, 180, 185, 189, 197, 214, 220, 226, 228, 229, 230, 231], "properti": [24, 150, 190, 229], "contains_media": 24, "non": [24, 160, 162], "from_dict": [24, 224], "construct": [24, 133, 178, 229], "text_cont": [24, 224], "mistral": [25, 36, 41, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 188, 223, 224, 226, 227], "llama2chatformat": [25, 78, 224, 225], "achiev": [26, 210, 226, 228, 229, 230, 231], "prepend_tag": 26, "append_tag": 26, "thu": [26, 41, 165, 230], "consid": [26, 37, 41, 55, 56, 57, 151], "come": [26, 35, 154, 229], "alia": [28, 184], "adher": [29, 32, 33], "sharegpt": [29, 33, 45], "gpt": [29, 33, 133, 143, 226], "similar": [30, 46, 51, 52, 53, 159, 164, 225, 226, 228, 229, 231], "stackexchangedpair": 30, "question": [30, 224, 225, 226, 228], "answer": [30, 224, 226, 228], "nanswer": 30, "summar": [31, 49, 224, 225], "dialogu": [31, 49, 224], "nsummari": [31, 224], "summari": [31, 37, 49, 151, 197, 225], "could": [32, 229], "eos_id": [34, 94, 174, 176], "length": [34, 35, 37, 38, 39, 40, 42, 44, 46, 48, 52, 53, 67, 68, 71, 72, 78, 79, 84, 85, 94, 95, 98, 99, 104, 108, 114, 115, 117, 119, 121, 123, 126, 127, 129, 130, 131, 133, 134, 138, 142, 143, 146, 148, 149, 161, 162, 163, 169, 174, 176, 178, 186, 204, 209], "last": [34, 39, 52, 153, 162, 225], "replac": [34, 38, 44, 47, 49, 50, 133, 152, 229], "forth": [35, 225], "empti": [35, 223], "shorter": 35, "min": [35, 180, 229], "invalid": 35, "convert_to_messag": [36, 224], "chat_format": [36, 45, 224, 225], "chatformat": [36, 45, 225], "load_dataset_kwarg": [36, 38, 40, 41, 42, 45, 46, 48, 52, 53], "multiturn": [36, 224], "prepar": [36, 224, 230], "truncat": [36, 38, 39, 40, 42, 46, 48, 52, 53, 67, 71, 78, 84, 94, 98, 114, 123, 126, 131, 133, 142, 170, 174, 176, 225], "local": [36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 98, 131, 142, 204, 208, 213, 220, 223, 224, 226, 227], "g": [36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 133, 143, 151, 154, 178, 180, 185, 197, 214, 228, 229, 230, 231], "csv": [36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 224, 225], "filepath": [36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53], "data_fil": [36, 38, 40, 41, 42, 45, 47, 48, 49, 50, 52, 53, 224, 225], "load_dataset": [36, 38, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 224, 225], "huggingfac": [36, 38, 40, 42, 45, 46, 48, 52, 53, 122, 130, 131, 133, 139, 140, 141, 153, 164, 165, 167, 168, 185, 186, 223, 226], "co": [36, 38, 40, 42, 45, 46, 48, 52, 53, 122, 130, 131, 139, 140, 141, 185, 186, 226], "doc": [36, 38, 40, 41, 42, 45, 46, 48, 52, 53, 78, 94, 133, 184, 190, 196, 201, 204, 207, 208, 213, 223, 225, 226], "en": [36, 38, 40, 42, 45, 46, 48, 52, 53, 230], "package_refer": [36, 38, 40, 42, 45, 46, 48, 52, 53], "loading_method": [36, 38, 40, 42, 45, 46, 48, 52, 53], "extra": [36, 41, 220, 224, 229, 230, 231], "still": [36, 41, 190, 229, 230, 231], "unless": 36, "concaten": [37, 67, 78, 114, 126, 133, 169, 172, 176], "sub": [37, 207], "unifi": [37, 122], "were": [37, 151, 156, 166, 224, 227, 230], "simplifi": [37, 164, 223, 229], "simultan": 37, "intern": [37, 190], "aggreg": 37, "transpar": 37, "index": [37, 39, 143, 148, 149, 150, 153, 162, 163, 169, 209, 220, 224, 226], "howev": [37, 131, 220], "constitu": 37, "might": [37, 223, 226], "larg": [37, 133, 155, 214, 223, 231], "comput": [37, 41, 79, 85, 95, 99, 104, 108, 134, 138, 143, 144, 148, 149, 164, 165, 167, 168, 178, 179, 197, 213, 226, 230, 231], "cumul": 37, "maintain": [37, 231], "indic": [37, 38, 39, 57, 143, 148, 149, 150, 151, 161, 166, 170, 178, 184, 201, 224], "deleg": 37, "retriev": [37, 41, 195], "lead": [37, 114, 173], "high": [37, 41, 221, 229], "scale": [37, 61, 62, 63, 72, 73, 74, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 115, 116, 117, 118, 127, 128, 134, 135, 136, 137, 155, 162, 165, 168, 180, 192, 229, 230, 231], "strategi": 37, "stream": [37, 196], "demand": 37, "deriv": [37, 144, 149, 150], "dataset1": 37, "mycustomdataset": 37, "params1": 37, "dataset2": 37, "params2": 37, "concat_dataset": 37, "total": [37, 153, 162, 166, 199, 218, 222, 226, 228, 229], "data_point": 37, "1500": 37, "element": [37, 226], "accomplish": [37, 45, 48, 52], "instruct_dataset": [37, 225], "vicgal": [37, 225], "gpt4": [37, 225], "alpacainstructtempl": [37, 48, 225], "samsum": [37, 49, 225], "summarizetempl": [37, 224, 225], "focus": [37, 227], "enhanc": [37, 151, 168, 231], "divers": 37, "machin": [37, 167, 193, 223, 226], "instructtempl": [38, 40, 225], "contribut": [38, 44, 47, 49, 50, 162, 166], "variabl": [38, 40, 48, 133, 188, 201, 204, 225, 231], "disabl": [38, 40, 42, 46, 48, 52, 53, 156, 213, 230], "recommend": [38, 40, 42, 44, 46, 48, 52, 53, 113, 204, 207, 224, 226, 231], "highest": [38, 40, 42, 44, 46, 48, 52, 53], "sequenc": [38, 39, 40, 42, 44, 46, 48, 52, 53, 67, 68, 71, 72, 78, 79, 84, 85, 94, 95, 98, 99, 104, 108, 114, 115, 117, 119, 121, 123, 126, 127, 129, 131, 133, 134, 138, 142, 143, 146, 148, 149, 151, 162, 163, 168, 169, 170, 174, 176, 178, 209, 224], "ds": [39, 50], "padding_idx": [39, 163, 169, 209], "max_pack": 39, "split_across_pack": [39, 52], "greedi": 39, "pack": [39, 43, 44, 45, 47, 48, 49, 50, 52, 53, 143, 148, 149, 150, 230], "done": [39, 159, 194, 203, 229, 230, 231], "outsid": [39, 213, 214, 229], "sampler": [39, 227], "part": [39, 167, 224, 231], "buffer": 39, "enough": [39, 224], "attent": [39, 57, 61, 62, 63, 68, 72, 73, 74, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 121, 127, 128, 129, 130, 134, 135, 136, 137, 138, 143, 146, 148, 149, 150, 159, 160, 178, 228, 229, 231], "lower": [39, 229], "triangular": 39, "cross": [39, 178], "attend": [39, 143, 149, 150, 178], "rel": [39, 143, 148, 149, 150, 164, 197, 229], "pad": [39, 133, 151, 162, 163, 166, 169, 170, 180, 182, 209, 225], "max": [39, 67, 78, 114, 126, 133, 142, 149, 151, 153, 174, 176, 223, 229], "wise": 39, "collat": [39, 209, 225], "made": [39, 45, 48, 52, 148, 226], "smaller": [39, 165, 226, 228, 229, 230, 231], "jam": 39, "vari": 39, "s1": [39, 78, 114, 173], "s2": [39, 78, 114, 173], "s3": 39, "s4": 39, "contamin": 39, "input_po": [39, 143, 146, 148, 149, 150], "matrix": 39, "causal": [39, 143, 149, 150], "continu": [39, 151, 204, 225], "increment": 39, "move": [39, 52, 149], "entir": [39, 52, 203, 224, 231], "avoid": [39, 52, 147, 151, 152, 165, 213, 223, 230, 231], "sentenc": [39, 52, 114], "message_transform": 41, "model_transform": [41, 47, 49, 50], "prompt_templ": [41, 47, 49, 50], "filter_fn": 41, "supervis": 41, "remot": 41, "At": [41, 149], "uniqu": [41, 78, 188], "extract": [41, 46, 175], "becaus": [41, 72, 146, 149, 151, 187, 223, 224, 230], "against": [41, 168, 215, 230, 231], "round": [41, 230], "involv": [41, 230], "incorpor": [41, 164, 225], "media": 41, "unit": [41, 203, 221], "row": [41, 143, 149, 150, 180, 224], "happen": 41, "ti": [41, 72, 134, 138], "agnost": [41, 225], "treat": [41, 151, 156, 190, 224], "final": [41, 61, 62, 63, 68, 72, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 127, 128, 134, 137, 138, 144, 149, 156, 159, 160, 226, 228, 229, 231], "modal": 41, "minimum": 41, "gear": 41, "whenev": [41, 229], "commun": [41, 224, 225, 226], "chatmltempl": 41, "filter": [41, 230], "prior": [41, 44, 45, 47, 48, 49, 50, 52, 53], "ref": [41, 130, 131, 208], "add_eo": [42, 52, 67, 78, 94, 114, 126, 133, 173, 174, 224], "freeform": [42, 52], "unstructur": [42, 52, 53], "corpu": [42, 46, 52, 53], "tabular": [42, 52], "txt": [42, 52, 133, 142, 205, 225, 227], "eo": [42, 52, 114, 126, 131, 173, 176, 224, 225], "yahma": [43, 48], "variant": [43, 47, 49], "version": [43, 72, 85, 99, 108, 115, 117, 127, 134, 143, 192, 215, 220, 224, 228, 230, 231], "page": [43, 53, 220, 221, 223, 227, 228], "tatsu": 44, "lab": 44, "codebas": [44, 47, 49, 226], "anyth": [44, 46, 51], "subset": [44, 46, 47, 49, 50, 51, 53, 72, 85, 99, 108, 115, 117, 127, 134, 157], "10": [44, 46, 47, 49, 50, 51, 53, 151, 169, 209, 226, 228, 230, 231], "alpaca_d": 44, "batch_siz": [44, 47, 49, 143, 146, 149, 150, 163, 164, 165, 167, 170, 226, 230], "conversation_styl": [45, 225], "chatdataset": [45, 224, 225], "friendli": [45, 48, 52, 192, 224], "check": [45, 54, 55, 56, 57, 149, 150, 151, 159, 194, 201, 215, 219, 224, 226, 227, 229], "huggingfaceh4": 45, "no_robot": 45, "chatmlformat": 45, "2096": [45, 48, 52], "packeddataset": [45, 47, 48, 49, 50, 52, 53, 225], "ccdv": 46, "cnn_dailymail": 46, "textcompletiondataset": [46, 52, 53, 225], "cnn": 46, "dailymail": 46, "articl": [46, 53], "highlight": [46, 231], "_transform": [47, 49, 151], "liweili": 47, "c4_200m": 47, "sftdataset": [47, 49, 50], "mirror": [47, 49], "llama_recip": [47, 49], "grammarerrorcorrectiontempl": [47, 49], "grammar_d": 47, "alpaca_clean": 48, "samsung": 49, "samsum_d": 49, "open": [50, 69, 70, 225, 226], "orca": 50, "slimorca": 50, "dedup": 50, "351": 50, "82": 50, "391": 50, "221": 50, "220": 50, "193": 50, "12": [50, 151, 169, 220, 230], "471": 50, "lvwerra": [51, 225], "stack": [51, 151, 214, 225], "exchang": [51, 225], "1024": [51, 225, 230], "preferencedataset": [51, 225], "stackexchangepair": 51, "allenai": [52, 225, 230], "c4": [52, 225, 230], "data_dir": [52, 225], "realnewslik": [52, 225], "wikitext_document_level": 53, "wikitext": [53, 230], "103": [53, 226], "wikipedia": 53, "clip": [54, 55, 56, 57, 151, 166], "max_num_til": [54, 55, 57, 151, 179], "tile": [54, 55, 56, 57, 151, 178, 179, 183], "patch": [54, 55, 56, 57, 151, 178], "document": [54, 55, 56, 57, 143, 165, 184, 195, 203, 223, 225], "vision_transform": [54, 55, 56, 57], "visiontransform": [54, 55, 56, 57], "divid": [54, 55, 56, 57, 151, 178, 179, 183], "dimension": [54, 55, 56, 57, 151], "aspect_ratio": [54, 55, 151], "bsz": [54, 55, 151, 192], "n_img": [54, 55, 151], "n_tile": [54, 55, 151], "n_token": [54, 55, 56, 151], "aspect": [54, 55, 221], "ratio": [54, 55, 164, 165, 166], "crop": [54, 55, 56, 57, 151, 183], "tile_s": [55, 56, 57, 151, 178, 179, 183], "patch_siz": [55, 56, 57, 151, 178], "local_token_positional_embed": 55, "_position_embed": [55, 151], "tokenpositionalembed": [55, 151], "gate": [55, 188, 223, 227], "global_token_positional_embed": 55, "advanc": [55, 56, 57, 151, 225], "40": [55, 56, 57, 133, 151, 178, 231], "400": [55, 56, 57, 151, 178, 183], "10x10": [55, 56, 57, 151, 178], "grid": [55, 56, 57, 151, 178], "k": [55, 143, 229], "th": 55, "cls_output_dim": [57, 151], "out_indic": [57, 151], "output_cls_project": 57, "in_channel": [57, 151], "transformerencoderlay": 57, "cl": [57, 151, 225], "head": [57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 148, 149, 188, 228], "intermedi": [57, 68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 151, 187, 211, 228, 231], "fourth": [57, 151], "determin": [57, 160, 180], "channel": [57, 151, 230], "code_llama2": [58, 59, 60, 61, 62, 63, 64, 65, 66, 223], "transformerdecod": [58, 59, 60, 61, 62, 63, 64, 65, 66, 79, 80, 81, 82, 83, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 120, 121, 122, 124, 125, 127, 128, 129, 130, 132, 134, 137, 138, 141, 192, 229], "w": [58, 59, 60, 69, 70, 80, 81, 82, 83, 96, 97, 105, 106, 107, 120, 122, 139, 140, 141, 151, 182, 204, 207, 208, 224, 226, 229, 231], "arxiv": [58, 59, 60, 64, 65, 66, 75, 76, 80, 81, 82, 83, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 143, 147, 148, 151, 161, 164, 165, 166, 167, 168, 178], "org": [58, 59, 60, 64, 65, 66, 75, 76, 78, 80, 81, 82, 83, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 133, 143, 147, 148, 151, 161, 164, 165, 166, 167, 168, 178, 184, 190, 196, 201, 207, 211, 213, 220], "pdf": [58, 59, 60, 161, 178], "2308": [58, 59, 60], "12950": [58, 59, 60], "lora_attn_modul": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 159, 160, 229, 231], "q_proj": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 143, 159, 160, 229, 230, 231], "k_proj": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 143, 159, 160, 229, 230, 231], "v_proj": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 143, 159, 160, 229, 230, 231], "output_proj": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 143, 159, 160, 229, 230, 231], "apply_lora_to_mlp": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 159, 160, 229], "apply_lora_to_output": [61, 62, 63, 64, 65, 66, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 137, 159, 160, 229], "lora_rank": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 229], "lora_alpha": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 229], "float": [61, 62, 63, 64, 65, 66, 68, 72, 73, 74, 75, 76, 79, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 99, 100, 101, 102, 103, 104, 108, 109, 110, 111, 112, 115, 116, 117, 118, 119, 121, 124, 125, 127, 128, 129, 132, 134, 135, 136, 137, 138, 143, 147, 153, 155, 161, 162, 164, 165, 166, 167, 168, 192, 197, 202, 204, 205, 206, 207, 208, 229, 230, 231], "16": [61, 62, 63, 64, 65, 66, 73, 74, 75, 76, 86, 87, 88, 89, 90, 91, 92, 93, 100, 101, 102, 103, 109, 110, 111, 112, 116, 118, 124, 125, 128, 132, 135, 136, 137, 151, 169, 229, 231], "lora_dropout": [61, 62, 63, 64, 65, 66, 72, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 108, 115, 117, 127, 134, 135, 136, 137], "05": [61, 62, 63, 64, 65, 66, 79, 85, 86, 87, 88, 89, 90, 91, 92, 93, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 135, 136, 137, 138], "quantize_bas": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 155, 231], "lora": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 155, 156, 159, 160, 185, 203, 219, 221, 224, 227, 228], "code_llama2_13b": 61, "tloen": [61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137], "8bb8579e403dc78e37fe81ffbb253c413007323f": [61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137], "l41": [61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137], "l43": [61, 62, 63, 73, 74, 86, 87, 88, 89, 100, 101, 109, 110, 128, 135, 136, 137], "linear": [61, 62, 63, 64, 65, 66, 72, 73, 74, 75, 76, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 108, 109, 110, 111, 112, 115, 116, 117, 118, 124, 125, 127, 128, 132, 134, 135, 136, 137, 149, 154, 155, 159, 160, 229, 230, 231], "mlp": [61, 62, 63, 68, 72, 73, 74, 79, 85, 86, 87, 88, 89, 95, 99, 100, 101, 104, 108, 109, 110, 115, 116, 117, 118, 119, 121, 127, 128, 129, 134, 135, 136, 137, 138, 149, 150, 159, 160, 228, 229], "low": [61, 62, 63, 72, 73, 74, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 115, 116, 117, 118, 127, 128, 134, 135, 136, 137, 155, 226, 229, 231], "approxim": [61, 62, 63, 72, 73, 74, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 115, 116, 117, 118, 127, 128, 134, 135, 136, 137, 155, 229], "factor": [61, 62, 63, 72, 73, 74, 85, 86, 87, 88, 89, 99, 100, 101, 108, 109, 110, 115, 116, 117, 118, 127, 128, 134, 135, 136, 137, 155, 161, 180, 226], "dropout": [61, 62, 63, 68, 72, 79, 85, 86, 87, 88, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 155, 229, 231], "probabl": [61, 62, 63, 72, 85, 86, 87, 88, 99, 108, 115, 117, 127, 134, 155, 164, 165, 166, 167, 168, 192, 226], "code_llama2_70b": 62, "code_llama2_7b": 63, "qlora": [64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 152, 219, 221, 228, 229], "per": [64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 146, 151, 152, 162, 164, 178, 179, 223, 230, 231], "paper": [64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 164, 165, 167, 168, 178, 229, 231], "ab": [64, 65, 66, 75, 76, 80, 81, 82, 83, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 143, 147, 148, 151, 164, 165, 166, 167, 168], "2305": [64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132, 143, 164, 167], "14314": [64, 65, 66, 75, 76, 90, 91, 92, 93, 102, 103, 111, 112, 124, 125, 132], "lora_code_llama2_13b": 64, "lora_code_llama2_70b": 65, "lora_code_llama2_7b": 66, "gemma": [67, 69, 70, 71, 72, 73, 74, 75, 76, 188], "sentencepiec": [67, 78, 114, 126, 173, 228], "pretrain": [67, 78, 94, 114, 126, 173, 174, 223, 224, 227, 229, 231], "spm_model": [67, 78, 114, 126, 173, 224], "tokenized_text": [67, 78, 94, 114, 126, 133, 173, 174], "add_bo": [67, 78, 94, 114, 126, 133, 173, 174, 224], "31587": [67, 78, 94, 114, 126, 173, 174], "29644": [67, 78, 94, 114, 126, 173, 174], "102": [67, 78, 94, 114, 126, 173, 174], "tokenizer_path": [67, 78, 114, 126], "separ": [67, 78, 114, 126, 176, 185, 224, 227, 228, 229, 231], "concat": [67, 78, 114, 126, 176], "1788": [67, 78, 114, 126, 176], "2643": [67, 78, 114, 126, 176], "13": [67, 78, 114, 126, 151, 169, 170, 176, 231], "1792": [67, 78, 114, 126, 176], "9508": [67, 78, 114, 126, 176], "465": [67, 78, 114, 126, 176], "22137": [67, 78, 114, 126, 176], "2933": [67, 78, 114, 126, 176], "join": [67, 78, 114, 126, 176], "attribut": [67, 78, 114, 126, 156, 168, 176, 191], "head_dim": [68, 72, 143, 146, 149], "intermediate_dim": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138], "attn_dropout": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 149], "norm_ep": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138], "1e": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 147], "06": [68, 72, 147, 229], "rope_bas": [68, 72, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138], "10000": [68, 72, 115, 117, 119, 121, 127, 129, 148], "norm_embed": [68, 72], "gemmatransformerdecod": [68, 69, 70, 72, 73, 74, 75, 76], "transformerdecoderlay": [68, 79, 95, 104, 119, 138, 149], "rm": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138], "norm": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 149, 150], "space": [68, 79, 95, 104, 119, 133, 138, 149], "slide": [68, 119, 130], "window": [68, 119, 130, 225], "vocabulari": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 133, 134, 138, 229], "queri": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 149, 150, 228], "mha": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 149], "dimens": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143, 146, 148, 149, 151, 155, 181, 228, 229, 231], "onto": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143], "scaled_dot_product_attent": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143], "epsilon": [68, 72, 79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 166], "rotari": [68, 72, 115, 117, 119, 121, 127, 129, 148, 228], "10_000": [68, 72, 115, 117, 119, 121, 129], "blog": [69, 70], "technolog": [69, 70], "develop": [69, 70, 231], "gemmatoken": 71, "gemma_2b": 73, "gemma_7b": 74, "lora_gemma_2b": 75, "lora_gemma_7b": 76, "card": [78, 94], "regist": [78, 94, 98, 126, 131, 142, 144, 152, 210, 231], "strongli": 78, "beforehand": 78, "html": [78, 133, 184, 190, 196, 201, 207, 211, 213, 219], "problem": [78, 114], "due": [78, 114, 173, 229, 231], "whitespac": [78, 114, 173], "slice": [78, 114], "gqa": [79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143], "mqa": [79, 85, 95, 99, 104, 108, 115, 117, 119, 121, 127, 129, 134, 138, 143], "kvcach": [79, 85, 95, 99, 104, 108, 127, 134, 138, 143, 149], "scale_hidden_dim_for_mlp": [79, 85, 95, 99, 104, 108, 134, 138], "2307": [80, 81, 82, 83], "09288": [80, 81, 82, 83], "classif": [83, 117, 121, 122, 188], "reward": [83, 89, 93, 118, 122, 125, 161, 162, 164, 165, 167, 168, 188], "llama2_70b": 87, "llama2_7b": [88, 229], "classifi": [89, 117, 121, 122, 225], "llama2_reward_7b": [89, 188], "lora_llama2_13b": 90, "lora_llama2_70b": 91, "lora_llama2_7b": [92, 229], "lora_llama2_reward_7b": 93, "special_token": [94, 126, 133, 174, 224], "tiktoken": [94, 174, 228], "left": [94, 126, 163, 229], "canon": [94, 98, 126, 131, 142], "tt_model": [94, 174], "token_id": [94, 114, 133, 171, 174], "truncate_at_eo": [94, 174], "skip_special_token": [94, 133, 174], "show": [94, 174, 178, 220, 223, 224, 229], "skip": [94, 143, 174], "tokenize_head": 94, "tokenize_end": 94, "header": [94, 224], "eom": 94, "wether": 94, "500000": [95, 99, 104, 108], "special_tokens_path": [98, 131, 142], "llama3token": [98, 224], "similarli": [98, 131, 142, 225, 230], "llama3_70b": 100, "llama3_8b": [101, 192, 228, 230], "lora_llama3_70b": 102, "lora_llama3_8b": 103, "llama3_1": [105, 106, 107, 108, 109, 110, 111, 112], "rtype": 105, "llama3_1_70b": 109, "llama3_1_8b": 110, "lora_llama3_1_70b": 111, "lora_llama3_1_8b": 112, "llama2chattempl": 113, "yet": [113, 224, 226], "trim_leading_whitespac": [114, 173], "unbatch": [114, 173], "bo": [114, 131, 173, 176, 224, 225], "trim": [114, 173], "num_class": [117, 121], "announc": 120, "ray2333": 122, "feedback": [122, 164], "mistraltoken": [123, 224], "lora_mistral_7b": 124, "lora_mistral_reward_7b": 125, "ignore_system_prompt": 126, "phi3_mini": [128, 188], "phi": [130, 131, 188], "128k": 130, "nor": 130, "phi3minitoken": 131, "tokenizer_config": 131, "spm": 131, "lm": [131, 166], "unk": 131, "augment": [131, 231], "endoftext": [131, 133], "phi3minisentencepiecebasetoken": 131, "lora_phi3_mini": 132, "merges_fil": [133, 142], "unk_token": 133, "bos_token": 133, "eos_token": 133, "pad_token": 133, "bpe_cache_s": 133, "151646": 133, "bpe": 133, "v4": [133, 153], "src": [133, 153], "tokenization_qwen2": 133, "word": [133, 134, 138, 165, 230], "utf": 133, "librari": [133, 164, 165, 167, 190, 194, 196, 213, 219, 221, 223, 225, 231], "stdtype": 133, "unknown": 133, "cach": [133, 143, 146, 148, 149, 150, 220, 223], "speed": [133, 174, 214, 228, 230, 231], "realli": [133, 226], "esp": 133, "chines": 133, "technic": [133, 227], "leak": 133, "appear": 133, "equal": [133, 178, 181, 183, 215], "assistant_for_gener": 133, "39": [133, 151], "385": 133, "78": 133, "675": 133, "2000": [133, 230], "remov": 133, "41": [133, 151], "tokenization_util": 133, "l541": 133, "l262": 133, "apply_chat_templ": 133, "1000000": [134, 138], "tie_word_embed": [134, 135, 136, 138, 139, 140], "tiedembeddingtransformerdecod": [134, 135, 136, 138, 139, 140], "qwen2transformerdecod": 134, "period": [134, 138], "rope": [134, 138, 143, 148], "qwen2_0_5b": 135, "qwen2_1_5b": 136, "qwen2_7b": 137, "qwen": [139, 140, 141], "qwen2token": 142, "pos_embed": [143, 229, 230], "kv_cach": 143, "introduc": [143, 147, 155, 168, 224, 225, 229, 230, 231], "13245v1": 143, "multihead": 143, "extrem": 143, "share": [143, 225, 226], "credit": 143, "lightn": 143, "lit": 143, "lit_gpt": 143, "v": [143, 149, 229], "q": [143, 229], "n_kv_head": 143, "rotarypositionalembed": [143, 229, 230], "seq_length": [143, 150, 192], "boolean": [143, 149, 150, 184], "softmax": [143, 149, 150], "j": [143, 149, 150], "seq_len": 143, "bigger": 143, "n_h": [143, 148], "num": [143, 148], "n_kv": 143, "kv": [143, 146, 149, 230], "emb": [143, 149], "h_d": [143, 148], "reset_cach": [143, 149, 150], "reset": [143, 146, 149, 150, 197], "setup_cach": [143, 149, 150], "dpython": [143, 146, 149, 150, 152, 212, 216], "gate_proj": 144, "down_proj": 144, "up_proj": 144, "silu": 144, "feed": [144, 150], "network": [144, 156, 229, 231], "fed": [144, 224], "multipli": 144, "subclass": [144, 190], "although": [144, 229, 230], "afterward": 144, "former": 144, "hook": [144, 152, 210, 231], "latter": 144, "layernorm": 145, "standalon": 146, "past": 146, "expand": 146, "k_val": 146, "v_val": 146, "assert": 146, "longer": [146, 225], "h": [146, 151, 182, 220, 223], "ep": 147, "root": [147, 207, 208], "squar": 147, "1910": 147, "07467": 147, "verfic": [147, 148], "small": [147, 226], "divis": [147, 183], "propos": 148, "2104": 148, "09864": 148, "l80": 148, "upto": 148, "init": [148, 197, 208, 231], "exceed": 148, "freq": 148, "recomput": 148, "geometr": 148, "progress": [148, 227], "rotat": 148, "angl": 148, "todo": 148, "effici": [148, 159, 195, 219, 221, 226, 227, 229, 230], "belong": [149, 191], "reduc": [149, 164, 221, 225, 229, 230, 231], "statement": 149, "improv": [149, 167, 174, 195, 228, 229], "readabl": [149, 226], "caches_are_en": 149, "arang": 149, "prompt_length": 149, "causal_mask": 149, "m_": 149, "seq": 149, "attn": [150, 229, 230, 231], "causalselfattent": [150, 229, 230], "sa_norm": 150, "mlp_norm": 150, "ff": 150, "cache_en": 150, "token_pos_embed": 151, "pre_tile_pos_emb": 151, "post_tile_pos_emb": 151, "cls_project": 151, "vit": 151, "11929": 151, "convolut": 151, "flatten": 151, "downscal": [151, 180, 181, 182], "800x400": 151, "400x400": 151, "clipimagetransform": 151, "broken": 151, "down": [151, 187, 225, 229, 231], "whole": 151, "num_til": [151, 183], "101": 151, "pool": 151, "tiledtokenpositionalembed": 151, "tilepositionalembed": 151, "tile_pos_emb": 151, "even": [151, 220, 223, 224, 225, 228, 229, 231], "8x8": 151, "14": [151, 169, 230, 231], "15": [151, 169, 195, 224, 226, 229, 231], "17": [151, 169, 229], "18": [151, 169, 228], "19": [151, 169, 231], "20": [151, 169, 170, 230], "21": 151, "22": 151, "23": [151, 153], "24": [151, 183, 227, 228], "25": [151, 226], "26": 151, "27": [151, 226], "28": [151, 226], "29": [151, 231], "30": [151, 170, 230], "31": [151, 228], "33": 151, "34": 151, "35": [151, 231], "36": 151, "37": 151, "38": [151, 226], "42": 151, "43": 151, "44": 151, "45": 151, "46": 151, "47": 151, "48": [151, 226, 231], "49": 151, "50": [151, 170, 183, 204, 226], "51": 151, "52": [151, 227], "53": 151, "54": 151, "55": [151, 227], "56": 151, "57": [151, 229, 231], "58": 151, "59": [151, 231], "60": 151, "61": [151, 226], "62": 151, "63": 151, "64": [151, 229], "num_patches_per_til": 151, "emb_dim": 151, "greater": [151, 215], "constain": 151, "anim": [151, 225], "max_n_img": 151, "n_channel": 151, "hidden_st": 151, "vision_util": 151, "tile_crop": 151, "num_channel": 151, "image_s": [151, 181, 182], "800": [151, 181, 182], "patch_grid_s": 151, "random": [151, 213, 227], "rand": [151, 180, 182, 183], "nch": 151, "tile_cropped_imag": 151, "batch_imag": 151, "unsqueez": 151, "batch_aspect_ratio": 151, "clip_vision_encod": 151, "common_util": 152, "bfloat16": [152, 212, 226, 227, 228, 229, 230], "offload_to_cpu": 152, "nf4": [152, 231], "restor": 152, "higher": [152, 165, 228, 230, 231], "offload": [152, 231], "increas": [152, 153, 164, 228, 229, 230], "peak": [152, 197, 202, 226, 228, 229, 231], "gpu": [152, 223, 226, 227, 228, 229, 230, 231], "_register_state_dict_hook": 152, "m": [152, 192, 224, 230], "mymodul": 152, "_after_": 152, "nf4tensor": [152, 231], "unquant": [152, 230, 231], "unus": 152, "num_warmup_step": 153, "num_training_step": 153, "num_cycl": [153, 214], "last_epoch": 153, "lambdalr": 153, "rate": [153, 221, 227], "schedul": [153, 214, 227], "linearli": 153, "decreas": [153, 225, 229, 230, 231], "cosin": 153, "l104": 153, "warmup": [153, 214], "phase": 153, "wave": 153, "half": 153, "lr_schedul": 153, "peft": [154, 155, 156, 157, 158, 159, 160, 185, 229, 231], "protocol": 154, "adapter_param": [154, 155, 156, 157, 158], "proj": 154, "in_dim": [154, 155, 229, 231], "out_dim": [154, 155, 229, 231], "bia": [154, 155, 229, 230, 231], "loralinear": [154, 229, 231], "alpha": [155, 229, 231], "use_bia": 155, "perturb": 155, "decomposit": [155, 229], "matric": [155, 203, 229, 231], "trainabl": [155, 158, 203, 229, 231], "mapsto": 155, "w_0x": 155, "r": [155, 229], "bax": 155, "lora_a": [155, 229, 231], "lora_b": [155, 229, 231], "temporarili": 156, "neural": [156, 229, 231], "dpo": [156, 164, 165, 167, 168, 169], "polici": [156, 162, 164, 165, 166, 167, 168, 184, 195, 203, 211], "caller": 156, "whose": [156, 204, 210], "yield": 156, "get_adapter_param": [158, 229], "base_miss": 159, "base_unexpect": 159, "lora_miss": 159, "lora_unexpect": 159, "validate_state_dict_for_lora": [159, 229], "unlik": [159, 165], "reli": [159, 176, 226, 228], "unexpect": 159, "strict": [159, 229], "pull": [159, 223], "120600": 159, "assertionerror": [159, 160, 169], "nonempti": 159, "full_model_state_dict_kei": 160, "lora_state_dict_kei": 160, "base_model_state_dict_kei": 160, "confirm": [160, 220], "lora_modul": 160, "complement": 160, "disjoint": 160, "overlap": 160, "rlhf": [161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 225], "gamma": [161, 167, 168], "lmbda": 161, "estim": [161, 162], "1506": 161, "02438": 161, "predict": [161, 162, 166, 192], "reponse_len": [161, 162], "receiv": [161, 224], "discount": 161, "gae": 161, "lambda": 161, "particip": [161, 178], "score": 162, "logprob": [162, 165, 168], "ref_logprob": 162, "kl_coeff": 162, "valid_score_idx": 162, "kl": 162, "coeffici": [162, 166], "response_len": 162, "total_reward": 162, "combin": [162, 179], "diverg": 162, "kl_reward": 162, "beta": [164, 168], "label_smooth": [164, 168], "18290": 164, "intuit": [164, 165, 167, 168], "dispref": 164, "dynam": [164, 230], "degener": 164, "occur": 164, "naiv": 164, "trl": [164, 165, 167, 168], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 164, "dpo_train": [164, 165, 167], "l844": 164, "retain": [164, 231], "2009": 164, "01325": 164, "align": [164, 224], "regular": [164, 165, 168, 230, 231], "baselin": [164, 166, 226, 229], "rather": 164, "overhead": [164, 230], "temperatur": [164, 165, 167, 168, 192, 226], "uncertainti": [164, 168], "policy_chosen_logp": [164, 165, 167, 168], "policy_rejected_logp": [164, 165, 167, 168], "reference_chosen_logp": [164, 165, 167], "reference_rejected_logp": [164, 165, 167], "chosen": [164, 165, 167, 168, 214, 225], "reject": [164, 165, 167, 168, 225], "chosen_reward": [164, 165, 167, 168], "rejected_reward": [164, 165, 167, 168], "tau": 165, "optimis": 165, "ipo": 165, "2310": 165, "12036": 165, "pi": 165, "pi_ref": 165, "regress": [165, 167], "gap": 165, "likelihood": 165, "he": 165, "weaker": 165, "regularis": 165, "toward": [165, 168], "4dce042a3863db1d375358e8c8092b874b02934b": [165, 167], "l1143": 165, "reciproc": 165, "larger": [165, 168, 181, 187, 226, 228], "value_clip_rang": 166, "value_coeff": 166, "proxim": 166, "1707": 166, "06347": 166, "eqn": 166, "vwxyzjn": 166, "ccc19538e817e98a60d3253242ac15e2a562cb49": 166, "lm_human_preference_detail": 166, "train_policy_acceler": 166, "l719": 166, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 166, "ppo2": 166, "l68": 166, "l75": 166, "pi_old_logprob": 166, "pi_logprob": 166, "phi_old_valu": 166, "phi_valu": 166, "padding_mask": [166, 170], "value_padding_mask": 166, "old": 166, "participag": 166, "five": 166, "policy_loss": 166, "value_loss": 166, "clipfrac": 166, "fraction": 166, "statist": 167, "rso": 167, "hing": 167, "2309": 167, "06657": 167, "logist": 167, "slic": 167, "10425": 167, "almost": [167, 229], "vector": [167, 224], "svm": 167, "counter": 167, "l1141": 167, "simpo": 168, "free": [168, 229], "2405": 168, "14734": 168, "averag": 168, "implicit": 168, "margin": 168, "bradlei": 168, "terri": 168, "win": 168, "lose": 168, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 168, "cpo_train": 168, "l603": 168, "pretti": [168, 226], "identitc": 168, "elimin": 168, "better": [168, 221, 224, 225, 226, 230], "kind": 168, "ipoloss": 168, "ignore_idx": [169, 209], "input_id": 169, "chosen_input_id": [169, 225], "rejected_input_id": [169, 225], "chosen_label": [169, 225], "rejected_label": [169, 225], "stop_token": [170, 192], "fill_valu": 170, "stop": [170, 192], "sequence_length": 170, "pad_id": 170, "been": [170, 195, 224, 230], "stop_token_id": 170, "869": 170, "eos_mask": 170, "truncated_sequ": 170, "light": 173, "sentencepieceprocessor": 173, "prefix": 173, "bos_id": [174, 176], "lightweight": [174, 224], "break": 174, "substr": 174, "repetit": 174, "identif": 174, "regex": 174, "chunk": 174, "present": [174, 187], "absent": 174, "tokenizer_json_path": 175, "heavili": 176, "beggin": 176, "runtimeerror": [176, 189, 194, 200], "satisfi": [176, 180, 226], "loos": 177, "image_token_id": 178, "laid": 178, "fig": 178, "flamingo": 178, "2204": 178, "14198": 178, "immedi": 178, "until": 178, "img1": 178, "img2": 178, "img3": 178, "dog": 178, "cat": 178, "text_seq_len": 178, "image_seq_len": 178, "resolut": [179, 180, 181, 182], "1x1": 179, "1x2": 179, "2x1": 179, "side": [179, 180, 181, 182], "height": [179, 180, 181, 182], "width": [179, 180, 181, 182, 230], "224": [179, 180, 181, 182], "896": 179, "448": [179, 180, 181, 182], "672": [179, 180], "possible_resolut": 180, "resize_to_max_canva": 180, "canva": 180, "resiz": [180, 181, 182], "distort": [180, 182], "select": 180, "smallest": 180, "upscal": [180, 181, 182], "2x": 180, "5x": 180, "canvas": 180, "condit": [180, 192, 201, 223, 225], "pick": 180, "lowest": [180, 229], "area": [180, 226], "minim": [180, 225, 227, 229, 230, 231], "200": [180, 183, 231], "300": [180, 181, 182, 183, 226], "scale_height": 180, "1200": 180, "3600": 180, "2400": 180, "scale_width": 180, "7467": 180, "4933": 180, "scaling_factor": 180, "upscaling_opt": 180, "selected_scal": 180, "150528": 180, "100352": 180, "optimal_canva": 180, "target_s": [181, 182], "max_siz": [181, 182], "inscrib": 181, "second": [181, 226, 229, 231], "exce": [181, 182], "1194": [181, 182], "1344": [181, 182], "stai": [181, 182], "600": [181, 182, 183], "500": [181, 182, 229], "1000": [181, 182, 184, 230], "488": [181, 182], "resampl": 182, "interpolationmod": 182, "torchvis": 182, "nearest": 182, "nearest_exact": 182, "bilinear": 182, "bicub": 182, "channel_s": 183, "4x6": 183, "2x3": 183, "datatyp": [184, 231], "denot": 184, "integ": [184, 209, 213], "auto_wrap_polici": [184, 195, 211], "submodul": [184, 203], "obei": 184, "contract": 184, "get_fsdp_polici": 184, "modules_to_wrap": [184, 195, 203], "min_num_param": 184, "my_fsdp_polici": 184, "recurs": [184, 203, 207], "isinst": [184, 225], "sum": [184, 229], "p": [184, 189, 229, 230, 231], "numel": [184, 229], "stabl": [184, 201, 207, 213, 220], "safe_seri": 185, "from_pretrain": 185, "0001_of_0003": 185, "0002_of_0003": 185, "preserv": [185, 231], "weight_map": [185, 226], "convert_weight": 185, "_model_typ": [185, 188], "intermediate_checkpoint": [185, 186, 187], "adapter_onli": [185, 186, 187], "_weight_map": 185, "shard": [186, 228], "wip": 186, "qualnam": 188, "boundari": 188, "distinguish": 188, "mistral_reward_7b": 188, "my_new_model": 188, "my_custom_state_dict_map": 188, "optim_map": 189, "bare": 189, "bone": 189, "distribut": [189, 193, 200, 201, 211, 213, 221, 223, 227, 228], "optim_dict": [189, 191, 210], "cfg_optim": 189, "ckpt": 189, "optim_ckpt": 189, "placeholder_optim_dict": 189, "optiminbackwardwrapp": 189, "get_optim_kei": 189, "arbitrari": [189, 229], "optim_ckpt_map": 189, "loadabl": 189, "argpars": 190, "argumentpars": 190, "builtin": 190, "said": 190, "noth": 190, "consult": 190, "info": [190, 227], "parse_known_arg": 190, "namespac": 190, "act": 190, "precid": 190, "parse_arg": 190, "too": [190, 228], "optimizerinbackwardwrapp": 191, "top": [191, 231], "named_paramet": 191, "max_generated_token": 192, "top_k": [192, 226], "custom_generate_next_token": 192, "prune": [192, 231], "compil": [192, 226, 228, 231], "generate_next_token": 192, "hi": [192, 224], "jeremi": 192, "float32": 194, "bf16": [194, 231], "inde": [194, 226], "kernel": 194, "isn": [194, 223], "hardwar": [194, 221, 225, 226, 229], "memory_efficient_fsdp_wrap": [195, 230], "maxim": [195, 203, 219, 221], "workload": [195, 230], "alongsid": 195, "ac": 195, "fullyshardeddataparallel": [195, 203], "fsdppolicytyp": [195, 203], "handler": 196, "reset_stat": 197, "track": [197, 204], "alloc": [197, 202, 203, 228, 231], "reserv": [197, 202, 224, 231], "stat": [197, 202, 231], "int4": [198, 230], "4w": 198, "recogn": 198, "int8dynactint4weightquant": [198, 230], "8da4w": [198, 230], "int8dynactint4weightqatquant": [198, 230], "qat": [198, 219], "mode": [198, 204, 226], "aka": 199, "master": 201, "port": [201, 223], "address": 201, "hold": [201, 227], "peak_memory_act": 202, "peak_memory_alloc": 202, "peak_memory_reserv": 202, "get_memory_stat": 202, "hierarch": 203, "requires_grad": [203, 229, 231], "api_kei": 204, "experiment_kei": 204, "onlin": [204, 224], "log_cod": 204, "comet": 204, "www": 204, "site": [204, 225, 226], "ml": 204, "team": 204, "compar": [204, 207, 215, 226, 228, 229, 230, 231], "sdk": 204, "uncategor": 204, "alphanumer": 204, "charact": 204, "get_or_cr": 204, "fresh": 204, "persist": 204, "hpo": 204, "sweep": 204, "server": 204, "offlin": 204, "auto": [204, 223], "creation": 204, "experimentconfig": 204, "project_nam": 204, "my_project": [204, 208], "my_workspac": 204, "my_metr": [204, 207, 208], "importerror": [204, 208], "termin": [204, 207, 208], "comet_api_kei": 204, "flush": [204, 205, 206, 207, 208], "ndarrai": [204, 205, 206, 207, 208], "scalar": [204, 205, 206, 207, 208], "record": [204, 205, 206, 207, 208, 214], "log_config": [204, 208], "payload": [204, 205, 206, 207, 208], "filenam": 205, "log_": 205, "unixtimestamp": 205, "thread": 205, "safe": 205, "organize_log": 207, "tensorboard": 207, "subdirectori": 207, "logdir": 207, "startup": 207, "tree": [207, 225, 226, 228], "tfevent": 207, "encount": 207, "frontend": 207, "organ": [207, 223], "accordingli": [207, 230], "my_log_dir": 207, "view": [207, 227], "entiti": 208, "bias": [208, 229, 231], "sent": 208, "usernam": 208, "my_ent": 208, "my_group": 208, "account": [208, 229, 231], "link": [208, 226, 228], "capecap": 208, "6053ofw0": 208, "torchtune_config_j67sb73v": 208, "longest": 209, "token_pair": 209, "soon": 210, "readi": [210, 219, 224, 230], "grad": 210, "acwrappolicytyp": 211, "author": [211, 221, 227, 231], "fsdp_adavnced_tutori": 211, "insid": 212, "contextmanag": 212, "debug_mod": 213, "pseudo": 213, "commonli": [213, 229, 231], "numpi": 213, "determinist": 213, "global": [213, 225], "warn": 213, "nondeterminist": 213, "cudnn": 213, "set_deterministic_debug_mod": 213, "profile_memori": 214, "with_stack": 214, "record_shap": 214, "with_flop": 214, "wait_step": 214, "warmup_step": 214, "active_step": 214, "profil": 214, "layout": 214, "trace": 214, "profileract": 214, "gradient_accumul": 214, "sensibl": 214, "default_schedul": 214, "reduct": [214, 229], "iter": [214, 216, 231], "scope": 214, "flop": 214, "wait": 214, "cycl": 214, "repeat": 214, "__version__": 215, "named_param": 216, "generated_examples_python": 217, "zip": 217, "galleri": [217, 222], "sphinx": 217, "000": [218, 222, 228], "execut": [218, 222], "generated_exampl": 218, "mem": [218, 222], "mb": [218, 222], "topic": 219, "gentl": 219, "introduct": 219, "first_finetune_tutori": 219, "workflow": [219, 225, 227, 229], "requisit": 220, "proper": [220, 227], "host": [220, 223, 227], "latest": [220, 227, 231], "And": [220, 226], "ls": [220, 223, 226, 227, 228], "welcom": [220, 223], "greatest": [220, 227], "contributor": 220, "cd": [220, 226], "commit": 220, "branch": 220, "url": 220, "whl": 220, "therebi": [220, 230, 231], "forc": 220, "reinstal": 220, "opt": [220, 227], "suffix": 220, "cu121": 220, "On": [221, 229], "pointer": 221, "emphas": 221, "simplic": 221, "component": 221, "prove": 221, "democrat": 221, "box": [221, 231], "zoo": 221, "varieti": [221, 229], "techniqu": [221, 226, 227, 228, 229, 230], "integr": [221, 226, 227, 228, 229, 230, 231], "excit": 221, "checkout": 221, "quickstart": 221, "attain": 221, "chekckpoint": 221, "embodi": 221, "philosophi": 221, "usabl": 221, "composit": 221, "hard": [221, 225], "outlin": 221, "unecessari": 221, "never": 221, "thoroughli": 221, "short": 223, "subcommand": 223, "anytim": 223, "symlink": 223, "wrote": 223, "readm": [223, 226, 228], "md": 223, "lot": [223, 226], "recent": 223, "releas": [223, 228], "agre": 223, "term": 223, "perman": 223, "eat": 223, "bandwith": 223, "storag": [223, 231], "00030": 223, "ootb": 223, "full_finetune_single_devic": [223, 225, 226, 227], "7b_full_low_memori": [223, 226, 227], "8b_full_single_devic": [223, 225], "mini_full_low_memori": 223, "7b_full": [223, 226, 227], "13b_full": [223, 226, 227], "70b_full": 223, "edit": 223, "clobber": 223, "destin": 223, "lora_finetune_distribut": [223, 228, 229], "torchrun": 223, "8b_lora_single_devic": [223, 224, 228], "launch": [223, 224, 227], "nproc": 223, "node": 223, "worker": 223, "nnode": [223, 229, 230], "minimum_nod": 223, "maximum_nod": 223, "fail": 223, "rdzv": 223, "rendezv": 223, "endpoint": 223, "8b_lora": [223, 228], "bypass": 223, "vice": 223, "versa": 223, "fancy_lora": 223, "8b_fancy_lora": 223, "sai": [223, 224, 227], "know": [224, 225, 226, 229], "intend": 224, "nice": 224, "meet": 224, "overhaul": 224, "begin_of_text": 224, "start_header_id": 224, "end_header_id": 224, "eot_id": 224, "untrain": 224, "accompani": 224, "who": 224, "influenti": 224, "hip": 224, "hop": 224, "artist": 224, "2pac": 224, "rakim": 224, "c": 224, "na": 224, "flavor": [224, 225], "msg": 224, "formatted_messag": [224, 225], "nyou": [224, 225], "nwho": 224, "why": [224, 227, 229], "518": 224, "25580": 224, "29962": 224, "3532": 224, "14816": 224, "29903": 224, "6778": 224, "_spm_model": 224, "piece_to_id": 224, "place": [224, 225], "manual": [224, 231], "529": 224, "29879": 224, "29958": 224, "nhere": 224, "128000": [224, 230], "128009": 224, "pure": 224, "That": 224, "won": 224, "mess": 224, "govern": 224, "prime": 224, "strictli": 224, "ask": 224, "untouch": 224, "though": 224, "robust": 224, "forum": 224, "panda": 224, "pd": 224, "df": 224, "read_csv": 224, "your_fil": 224, "nrow": 224, "tolist": 224, "iloc": 224, "gp": 224, "satellit": 224, "thing": [224, 231], "message_convert": 224, "input_msg": 224, "output_msg": 224, "But": [224, 226, 229], "mistralchatformat": 224, "custom_dataset": 224, "2048": 224, "honor": 224, "copi": [224, 226, 227, 228, 230, 231], "custom_8b_lora_single_devic": 224, "steer": 225, "wheel": 225, "publicli": 225, "great": [225, 226], "hood": [225, 231], "text_completion_dataset": [225, 230], "padded_col": 225, "upper": 225, "constraint": [225, 229], "slow": [225, 231], "signific": [225, 230], "speedup": [225, 226, 228], "my_data": 225, "fix": [225, 230], "goal": [225, 230], "respond": 225, "plant": 225, "miner": 225, "oak": 225, "copper": 225, "ore": 225, "eleph": 225, "customtempl": 225, "importlib": 225, "import_modul": 225, "mechan": 225, "search": 225, "often": [225, 229], "interpret": 225, "runtim": 225, "pythonpath": 225, "chat_dataset": 225, "quit": [225, 231], "customchatformat": 225, "concatdataset": 225, "drive": 225, "rajpurkar": 225, "io": 225, "squad": 225, "explor": 225, "few": [225, 228, 229, 231], "adjust": [225, 230], "chosen_messag": 225, "transformed_sampl": 225, "key_chosen": 225, "rejected_messag": 225, "key_reject": 225, "c_mask": 225, "np": 225, "cross_entropy_ignore_idx": 225, "r_mask": 225, "stack_exchanged_paired_dataset": 225, "had": 225, "stackexchangedpairedtempl": 225, "response_j": 225, "response_k": 225, "rl": 225, "favorit": [226, 229], "seemlessli": 226, "beyond": [226, 231], "connect": [226, 230], "amount": 226, "natur": 226, "export": 226, "mobil": 226, "phone": 226, "leverag": [226, 228, 231], "plai": 226, "freez": [226, 229], "percentag": 226, "learnabl": 226, "16gb": [226, 229], "rtx": 226, "3090": 226, "4090": 226, "hour": 226, "7b_qlora_single_devic": [226, 227, 231], "473": 226, "98": [226, 231], "gb": [226, 228, 229, 230, 231], "484": 226, "01": [226, 227], "fact": [226, 228, 229], "third": 226, "eleuther_ev": [226, 228, 230], "eleuther_evalu": [226, 228, 230], "lm_eval": [226, 228], "plan": 226, "custom_eval_config": [226, 228], "truthfulqa_mc2": [226, 228, 229], "measur": [226, 228], "propens": [226, 228], "shot": [226, 228, 230], "accuraci": [226, 228, 229, 230, 231], "324": 226, "loglikelihood": 226, "195": 226, "121": 226, "197": 226, "acc": [226, 230], "388": 226, "shown": [226, 230], "489": 226, "seem": 226, "custom_generation_config": [226, 228], "kick": 226, "interest": 226, "visit": 226, "bai": 226, "92": 226, "exploratorium": 226, "san": 226, "francisco": 226, "magazin": 226, "awesom": 226, "bridg": 226, "cool": 226, "96": [226, 231], "sec": [226, 228], "83": 226, "99": [226, 229], "72": 226, "littl": 226, "torchao": [226, 228, 230, 231], "int8_weight_onli": [226, 228], "int8_dynamic_activation_int8_weight": [226, 228], "ao": [226, 228], "quant_api": [226, 228], "quantize_": [226, 228], "int4_weight_onli": [226, 228], "previous": [226, 228, 229], "benefit": 226, "doesn": 226, "fast": 226, "clone": [226, 229, 230, 231], "assumpt": 226, "new_dir": 226, "output_dict": 226, "sd_1": 226, "sd_2": 226, "dump": 226, "convert_hf_checkpoint": 226, "checkpoint_path": 226, "justin": 226, "school": 226, "math": 226, "teacher": 226, "ws": 226, "94": [226, 228], "bandwidth": [226, 228], "1391": 226, "84": 226, "thats": 226, "seamlessli": 226, "authent": [226, 227], "hopefulli": 226, "gave": 226, "grant": 227, "minut": 227, "agreement": 227, "altern": 227, "hackabl": 227, "singularli": 227, "purpos": [227, 228], "depth": 227, "principl": 227, "boilerpl": 227, "substanti": [227, 229], "custom_config": 227, "replic": 227, "lorafinetunerecipesingledevic": 227, "lora_finetune_output": 227, "log_1713194212": 227, "3697006702423096": 227, "25880": [227, 231], "83it": 227, "monitor": 227, "tqdm": 227, "interv": 227, "e2": 227, "focu": 228, "128": [228, 229], "256": [228, 230], "theta": 228, "gain": 228, "basic": 228, "observ": [228, 230], "consum": [228, 231], "vram": [228, 229, 230], "overal": 228, "8b_qlora_single_devic": 228, "least": [228, 229, 230], "coupl": [228, 229, 231], "meta_model_0": [228, 230], "did": [228, 231], "122": 228, "sarah": 228, "busi": 228, "mum": 228, "young": 228, "children": 228, "live": 228, "north": 228, "east": 228, "england": 228, "135": 228, "88": 228, "138": 228, "346": 228, "09": 228, "139": 228, "broader": 228, "teach": 229, "straight": 229, "jump": 229, "unfamiliar": 229, "oppos": [229, 231], "momentum": 229, "relat": 229, "aghajanyan": 229, "et": 229, "al": 229, "hypothes": 229, "intrins": 229, "four": 229, "eight": 229, "practic": 229, "blue": 229, "rememb": 229, "approx": 229, "15m": 229, "8192": [229, 230], "65k": 229, "frozen_out": [229, 231], "lora_out": [229, 231], "omit": 229, "base_model": 229, "lora_model": 229, "lora_llama_2_7b": [229, 231], "alon": 229, "bit": [229, 230, 231], "in_featur": [229, 230], "out_featur": [229, 230], "inplac": 229, "feel": 229, "validate_missing_and_unexpected_for_lora": 229, "peft_util": 229, "set_trainable_param": 229, "fetch": 229, "lora_param": 229, "total_param": 229, "trainable_param": 229, "2f": 229, "6742609920": 229, "4194304": 229, "7b_lora": 229, "my_model_checkpoint_path": [229, 230, 231], "tokenizer_checkpoint": [229, 230, 231], "my_tokenizer_checkpoint_path": [229, 230, 231], "factori": 229, "benefici": 229, "impact": 229, "minor": 229, "good": 229, "lora_experiment_1": 229, "smooth": [229, 231], "curv": [229, 231], "ran": 229, "footprint": [229, 230], "commod": 229, "cogniz": 229, "ax": 229, "parallel": 229, "truthfulqa": 229, "475": 229, "87": 229, "508": 229, "86": 229, "504": 229, "04": 229, "514": 229, "absolut": 229, "4gb": 229, "tradeoff": 229, "potenti": 229, "awar": 230, "incur": [230, 231], "degrad": [230, 231], "perplex": 230, "simul": 230, "ultim": 230, "ptq": 230, "fake": 230, "kept": 230, "cast": 230, "nois": 230, "henc": 230, "x_q": 230, "int8": 230, "zp": 230, "x_float": 230, "qmin": 230, "qmax": 230, "clamp": 230, "x_fq": 230, "dequant": 230, "insert": 230, "proce": 230, "prepared_model": 230, "swap": 230, "int8dynactint4weightqatlinear": 230, "int8dynactint4weightlinear": 230, "train_loop": 230, "converted_model": 230, "demonstr": 230, "recov": 230, "modif": 230, "8b_qat_ful": 230, "custom_8b_qat_ful": 230, "fake_quant_after_n_step": 230, "issu": 230, "futur": 230, "empir": 230, "led": 230, "presum": 230, "80gb": 230, "qat_distribut": 230, "op": 230, "mutat": 230, "5gb": 230, "custom_quant": 230, "groupsiz": 230, "poorli": 230, "custom_eleuther_evalu": 230, "fullmodeltorchtunecheckpoint": 230, "hellaswag": 230, "max_seq_length": 230, "my_eleuther_evalu": 230, "stderr": 230, "word_perplex": 230, "9148": 230, "byte_perplex": 230, "5357": 230, "bits_per_byt": 230, "6189": 230, "5687": 230, "0049": 230, "acc_norm": 230, "7536": 230, "0043": 230, "portion": [230, 231], "drop": 230, "74": 230, "048": 230, "190": 230, "7735": 230, "5598": 230, "6413": 230, "5481": 230, "0050": 230, "7390": 230, "0044": 230, "7251": 230, "4994": 230, "5844": 230, "5740": 230, "7610": 230, "outperform": 230, "importantli": 230, "characterist": 230, "187": 230, "958": 230, "halv": 230, "int4weightonlyquant": 230, "motiv": 230, "constrain": 230, "edg": 230, "smartphon": 230, "executorch": 230, "xnnpack": 230, "export_llama": 230, "use_sdpa_with_kv_cach": 230, "qmode": 230, "group_siz": 230, "get_bos_id": 230, "get_eos_id": 230, "128001": 230, "output_nam": 230, "llama3_8da4w": 230, "pte": 230, "881": 230, "oneplu": 230, "709": 230, "tok": 230, "815": 230, "316": 230, "364": 230, "highli": 231, "vanilla": 231, "held": 231, "therefor": 231, "bespok": 231, "normalfloat": 231, "8x": 231, "vast": 231, "major": 231, "normatfloat": 231, "doubl": 231, "themselv": 231, "deepdiv": 231, "idea": 231, "distinct": 231, "de": 231, "counterpart": 231, "set_default_devic": 231, "qlora_linear": 231, "memory_alloc": 231, "177": 231, "152": 231, "del": 231, "empty_cach": 231, "lora_linear": 231, "081": 231, "344": 231, "qlora_llama2_7b": 231, "qlora_model": 231, "essenti": 231, "reparametrize_as_dtype_state_dict_post_hook": 231, "slower": 231, "149": 231, "9157477021217346": 231, "02": 231, "08": 231, "15it": 231, "nightli": 231, "hundr": 231, "228": 231, "8158286809921265": 231, "95it": 231, "exercis": 231, "linear_nf4": 231, "to_nf4": 231, "linear_weight": 231, "autograd": 231, "incom": 231}, "objects": {"torchtune.config": [[11, 0, 1, "", "instantiate"], [12, 0, 1, "", "log_config"], [13, 0, 1, "", "parse"], [14, 0, 1, "", "validate"]], "torchtune.data": [[15, 1, 1, "", "AlpacaInstructTemplate"], [16, 1, 1, "", "ChatFormat"], [17, 1, 1, "", "ChatMLFormat"], [18, 1, 1, "", "ChatMLTemplate"], [19, 3, 1, "", "GrammarErrorCorrectionTemplate"], [20, 1, 1, "", "InputOutputToMessages"], [21, 1, 1, "", "InstructTemplate"], [22, 1, 1, "", "JSONToMessages"], [23, 1, 1, "", "Llama2ChatFormat"], [24, 1, 1, "", "Message"], [25, 1, 1, "", "MistralChatFormat"], [26, 1, 1, "", "PromptTemplate"], [27, 1, 1, "", "PromptTemplateInterface"], [28, 3, 1, "", "Role"], [29, 1, 1, "", "ShareGPTToMessages"], [30, 1, 1, "", "StackExchangedPairedTemplate"], [31, 3, 1, "", "SummarizeTemplate"], [32, 0, 1, "", "get_openai_messages"], [33, 0, 1, "", "get_sharegpt_messages"], [34, 0, 1, "", "truncate"], [35, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[15, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[16, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[21, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[23, 2, 1, "", "format"]], "torchtune.data.Message": [[24, 4, 1, "", "contains_media"], [24, 2, 1, "", "from_dict"], [24, 4, 1, "", "text_content"]], "torchtune.data.MistralChatFormat": [[25, 2, 1, "", "format"]], "torchtune.data.StackExchangedPairedTemplate": [[30, 2, 1, "", "format"]], "torchtune.datasets": [[36, 1, 1, "", "ChatDataset"], [37, 1, 1, "", "ConcatDataset"], [38, 1, 1, "", "InstructDataset"], [39, 1, 1, "", "PackedDataset"], [40, 1, 1, "", "PreferenceDataset"], [41, 1, 1, "", "SFTDataset"], [42, 1, 1, "", "TextCompletionDataset"], [43, 0, 1, "", "alpaca_cleaned_dataset"], [44, 0, 1, "", "alpaca_dataset"], [45, 0, 1, "", "chat_dataset"], [46, 0, 1, "", "cnn_dailymail_articles_dataset"], [47, 0, 1, "", "grammar_dataset"], [48, 0, 1, "", "instruct_dataset"], [49, 0, 1, "", "samsum_dataset"], [50, 0, 1, "", "slimorca_dataset"], [51, 0, 1, "", "stack_exchanged_paired_dataset"], [52, 0, 1, "", "text_completion_dataset"], [53, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[54, 1, 1, "", "TilePositionalEmbedding"], [55, 1, 1, "", "TiledTokenPositionalEmbedding"], [56, 1, 1, "", "TokenPositionalEmbedding"], [57, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[54, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[55, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[56, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[58, 0, 1, "", "code_llama2_13b"], [59, 0, 1, "", "code_llama2_70b"], [60, 0, 1, "", "code_llama2_7b"], [61, 0, 1, "", "lora_code_llama2_13b"], [62, 0, 1, "", "lora_code_llama2_70b"], [63, 0, 1, "", "lora_code_llama2_7b"], [64, 0, 1, "", "qlora_code_llama2_13b"], [65, 0, 1, "", "qlora_code_llama2_70b"], [66, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[67, 1, 1, "", "GemmaTokenizer"], [68, 0, 1, "", "gemma"], [69, 0, 1, "", "gemma_2b"], [70, 0, 1, "", "gemma_7b"], [71, 0, 1, "", "gemma_tokenizer"], [72, 0, 1, "", "lora_gemma"], [73, 0, 1, "", "lora_gemma_2b"], [74, 0, 1, "", "lora_gemma_7b"], [75, 0, 1, "", "qlora_gemma_2b"], [76, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[67, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[77, 1, 1, "", "Llama2ChatTemplate"], [78, 1, 1, "", "Llama2Tokenizer"], [79, 0, 1, "", "llama2"], [80, 0, 1, "", "llama2_13b"], [81, 0, 1, "", "llama2_70b"], [82, 0, 1, "", "llama2_7b"], [83, 0, 1, "", "llama2_reward_7b"], [84, 0, 1, "", "llama2_tokenizer"], [85, 0, 1, "", "lora_llama2"], [86, 0, 1, "", "lora_llama2_13b"], [87, 0, 1, "", "lora_llama2_70b"], [88, 0, 1, "", "lora_llama2_7b"], [89, 0, 1, "", "lora_llama2_reward_7b"], [90, 0, 1, "", "qlora_llama2_13b"], [91, 0, 1, "", "qlora_llama2_70b"], [92, 0, 1, "", "qlora_llama2_7b"], [93, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[78, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[94, 1, 1, "", "Llama3Tokenizer"], [95, 0, 1, "", "llama3"], [96, 0, 1, "", "llama3_70b"], [97, 0, 1, "", "llama3_8b"], [98, 0, 1, "", "llama3_tokenizer"], [99, 0, 1, "", "lora_llama3"], [100, 0, 1, "", "lora_llama3_70b"], [101, 0, 1, "", "lora_llama3_8b"], [102, 0, 1, "", "qlora_llama3_70b"], [103, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[94, 2, 1, "", "decode"], [94, 2, 1, "", "tokenize_message"], [94, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[104, 0, 1, "", "llama3_1"], [105, 0, 1, "", "llama3_1_405b"], [106, 0, 1, "", "llama3_1_70b"], [107, 0, 1, "", "llama3_1_8b"], [108, 0, 1, "", "lora_llama3_1"], [109, 0, 1, "", "lora_llama3_1_70b"], [110, 0, 1, "", "lora_llama3_1_8b"], [111, 0, 1, "", "qlora_llama3_1_70b"], [112, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[113, 1, 1, "", "MistralChatTemplate"], [114, 1, 1, "", "MistralTokenizer"], [115, 0, 1, "", "lora_mistral"], [116, 0, 1, "", "lora_mistral_7b"], [117, 0, 1, "", "lora_mistral_classifier"], [118, 0, 1, "", "lora_mistral_reward_7b"], [119, 0, 1, "", "mistral"], [120, 0, 1, "", "mistral_7b"], [121, 0, 1, "", "mistral_classifier"], [122, 0, 1, "", "mistral_reward_7b"], [123, 0, 1, "", "mistral_tokenizer"], [124, 0, 1, "", "qlora_mistral_7b"], [125, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[114, 2, 1, "", "decode"], [114, 2, 1, "", "encode"], [114, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[126, 1, 1, "", "Phi3MiniTokenizer"], [127, 0, 1, "", "lora_phi3"], [128, 0, 1, "", "lora_phi3_mini"], [129, 0, 1, "", "phi3"], [130, 0, 1, "", "phi3_mini"], [131, 0, 1, "", "phi3_mini_tokenizer"], [132, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[126, 2, 1, "", "decode"], [126, 2, 1, "", "tokenize_messages"]], "torchtune.models.qwen2": [[133, 1, 1, "", "Qwen2Tokenizer"], [134, 0, 1, "", "lora_qwen2"], [135, 0, 1, "", "lora_qwen2_0_5b"], [136, 0, 1, "", "lora_qwen2_1_5b"], [137, 0, 1, "", "lora_qwen2_7b"], [138, 0, 1, "", "qwen2"], [139, 0, 1, "", "qwen2_0_5b"], [140, 0, 1, "", "qwen2_1_5b"], [141, 0, 1, "", "qwen2_7b"], [142, 0, 1, "", "qwen2_tokenizer"]], "torchtune.models.qwen2.Qwen2Tokenizer": [[133, 2, 1, "", "decode"], [133, 2, 1, "", "encode"], [133, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[143, 1, 1, "", "CausalSelfAttention"], [144, 1, 1, "", "FeedForward"], [145, 1, 1, "", "Fp32LayerNorm"], [146, 1, 1, "", "KVCache"], [147, 1, 1, "", "RMSNorm"], [148, 1, 1, "", "RotaryPositionalEmbeddings"], [149, 1, 1, "", "TransformerDecoder"], [150, 1, 1, "", "TransformerDecoderLayer"], [151, 1, 1, "", "VisionTransformer"], [153, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[143, 2, 1, "", "forward"], [143, 2, 1, "", "reset_cache"], [143, 2, 1, "", "setup_cache"]], "torchtune.modules.FeedForward": [[144, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[145, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[146, 2, 1, "", "reset"], [146, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[147, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[148, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[149, 2, 1, "", "caches_are_enabled"], [149, 2, 1, "", "forward"], [149, 2, 1, "", "reset_caches"], [149, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[150, 4, 1, "", "cache_enabled"], [150, 2, 1, "", "forward"], [150, 2, 1, "", "reset_cache"], [150, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[151, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[152, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[154, 1, 1, "", "AdapterModule"], [155, 1, 1, "", "LoRALinear"], [156, 0, 1, "", "disable_adapter"], [157, 0, 1, "", "get_adapter_params"], [158, 0, 1, "", "set_trainable_params"], [159, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [160, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[154, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[155, 2, 1, "", "adapter_params"], [155, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[161, 0, 1, "", "estimate_advantages"], [162, 0, 1, "", "get_rewards_ppo"], [163, 0, 1, "", "left_padded_collate"], [169, 0, 1, "", "padded_collate_dpo"], [170, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.rlhf.loss": [[164, 1, 1, "", "DPOLoss"], [165, 1, 1, "", "IPOLoss"], [166, 1, 1, "", "PPOLoss"], [167, 1, 1, "", "RSOLoss"], [168, 1, 1, "", "SimPOLoss"]], "torchtune.modules.rlhf.loss.DPOLoss": [[164, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.IPOLoss": [[165, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.PPOLoss": [[166, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.RSOLoss": [[167, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.SimPOLoss": [[168, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[171, 1, 1, "", "BaseTokenizer"], [172, 1, 1, "", "ModelTokenizer"], [173, 1, 1, "", "SentencePieceBaseTokenizer"], [174, 1, 1, "", "TikTokenBaseTokenizer"], [175, 0, 1, "", "parse_hf_tokenizer_json"], [176, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[171, 2, 1, "", "decode"], [171, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[172, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[173, 2, 1, "", "decode"], [173, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[174, 2, 1, "", "decode"], [174, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[177, 1, 1, "", "Transform"], [178, 1, 1, "", "VisionCrossAttentionMask"], [179, 0, 1, "", "find_supported_resolutions"], [180, 0, 1, "", "get_canvas_best_fit"], [181, 0, 1, "", "get_inscribed_size"], [182, 0, 1, "", "resize_with_pad"], [183, 0, 1, "", "tile_crop"]], "torchtune.utils": [[184, 3, 1, "", "FSDPPolicyType"], [185, 1, 1, "", "FullModelHFCheckpointer"], [186, 1, 1, "", "FullModelMetaCheckpointer"], [187, 1, 1, "", "FullModelTorchTuneCheckpointer"], [188, 1, 1, "", "ModelType"], [189, 1, 1, "", "OptimizerInBackwardWrapper"], [190, 1, 1, "", "TuneRecipeArgumentParser"], [191, 0, 1, "", "create_optim_in_bwd_wrapper"], [192, 0, 1, "", "generate"], [193, 0, 1, "", "get_device"], [194, 0, 1, "", "get_dtype"], [195, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [196, 0, 1, "", "get_logger"], [197, 0, 1, "", "get_memory_stats"], [198, 0, 1, "", "get_quantizer_mode"], [199, 0, 1, "", "get_world_size_and_rank"], [200, 0, 1, "", "init_distributed"], [201, 0, 1, "", "is_distributed"], [202, 0, 1, "", "log_memory_stats"], [203, 0, 1, "", "lora_fsdp_wrap_policy"], [209, 0, 1, "", "padded_collate"], [210, 0, 1, "", "register_optim_in_bwd_hooks"], [211, 0, 1, "", "set_activation_checkpointing"], [212, 0, 1, "", "set_default_dtype"], [213, 0, 1, "", "set_seed"], [214, 0, 1, "", "setup_torch_profiler"], [215, 0, 1, "", "torch_version_ge"], [216, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.utils.FullModelHFCheckpointer": [[185, 2, 1, "", "load_checkpoint"], [185, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[186, 2, 1, "", "load_checkpoint"], [186, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelTorchTuneCheckpointer": [[187, 2, 1, "", "load_checkpoint"], [187, 2, 1, "", "save_checkpoint"]], "torchtune.utils.OptimizerInBackwardWrapper": [[189, 2, 1, "", "get_optim_key"], [189, 2, 1, "", "load_state_dict"], [189, 2, 1, "", "state_dict"]], "torchtune.utils.TuneRecipeArgumentParser": [[190, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[204, 1, 1, "", "CometLogger"], [205, 1, 1, "", "DiskLogger"], [206, 1, 1, "", "StdoutLogger"], [207, 1, 1, "", "TensorBoardLogger"], [208, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.CometLogger": [[204, 2, 1, "", "close"], [204, 2, 1, "", "log"], [204, 2, 1, "", "log_config"], [204, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.DiskLogger": [[205, 2, 1, "", "close"], [205, 2, 1, "", "log"], [205, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[206, 2, 1, "", "close"], [206, 2, 1, "", "log"], [206, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[207, 2, 1, "", "close"], [207, 2, 1, "", "log"], [207, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[208, 2, 1, "", "close"], [208, 2, 1, "", "log"], [208, 2, 1, "", "log_config"], [208, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 19, 28, 31, 184, 219, 221, 223, 226, 228, 229, 230, 231], "config": [0, 8, 9, 223, 227], "data": [1, 5, 19, 28, 31, 224], "text": [1, 225, 228], "templat": [1, 224, 225], "type": 1, "convert": 1, "messag": [1, 24], "transform": [1, 4, 177], "helper": 1, "function": 1, "dataset": [2, 224, 225], "exampl": 2, "gener": [2, 192, 226, 228], "builder": 2, "class": [2, 9], "model": [3, 4, 10, 223, 226, 227, 228, 229, 230], "llama3": [3, 95, 224, 228, 230], "1": 3, "llama2": [3, 79, 224, 226, 229, 231], "code": 3, "llama": 3, "qwen": 3, "2": 3, "phi": 3, "3": 3, "mistral": [3, 119], "gemma": [3, 68], "clip": 3, "modul": 4, "compon": [4, 8], "build": [4, 220, 231], "block": 4, "base": 4, "token": [4, 224], "util": [4, 5, 184], "peft": 4, "vision": 4, "reinforc": 4, "learn": 4, "from": [4, 224, 231], "human": 4, "feedback": 4, "rlhf": 4, "loss": 4, "checkpoint": [5, 6, 10, 226], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 225, 229, 231], "manag": 5, "perform": [5, 229], "profil": 5, "metric": [5, 7, 10], "log": [5, 7, 10], "miscellan": 5, "overview": [6, 221, 226], "format": [6, 225], "handl": 6, "differ": 6, "hfcheckpoint": 6, "metacheckpoint": 6, "torchtunecheckpoint": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 226, 229, 231], "put": [6, 231], "thi": 6, "all": [6, 8, 231], "togeth": [6, 231], "comet": 7, "logger": [7, 10], "about": 8, "where": 8, "do": 8, "paramet": 8, "live": 8, "write": 8, "configur": [8, 225], "us": [8, 9, 224, 226, 231], "instanti": [8, 11], "referenc": 8, "other": [8, 226], "field": 8, "interpol": 8, "valid": [8, 14, 223], "your": [8, 9, 226, 227], "best": 8, "practic": 8, "airtight": 8, "public": 8, "api": 8, "onli": 8, "command": 8, "line": 8, "overrid": 8, "remov": 8, "what": [9, 221, 229, 230, 231], "ar": 9, "recip": [9, 223, 227, 229, 230], "script": 9, "run": [9, 223, 226], "cli": [9, 223], "pars": [9, 13], "weight": 10, "bias": 10, "w": 10, "b": 10, "log_config": 12, "alpacainstructtempl": 15, "chatformat": 16, "chatmlformat": 17, "chatmltempl": 18, "grammarerrorcorrectiontempl": 19, "inputoutputtomessag": 20, "instructtempl": 21, "jsontomessag": 22, "llama2chatformat": 23, "mistralchatformat": 25, "prompttempl": 26, "prompttemplateinterfac": 27, "role": 28, "sharegpttomessag": 29, "stackexchangedpairedtempl": 30, "summarizetempl": 31, "get_openai_messag": 32, "get_sharegpt_messag": 33, "truncat": 34, "validate_messag": 35, "chatdataset": 36, "concatdataset": 37, "instructdataset": 38, "packeddataset": 39, "preferencedataset": 40, "sftdataset": 41, "textcompletiondataset": 42, "alpaca_cleaned_dataset": 43, "alpaca_dataset": 44, "chat_dataset": 45, "cnn_dailymail_articles_dataset": 46, "grammar_dataset": 47, "instruct_dataset": 48, "samsum_dataset": 49, "slimorca_dataset": 50, "stack_exchanged_paired_dataset": 51, "text_completion_dataset": 52, "wikitext_dataset": 53, "tilepositionalembed": 54, "tiledtokenpositionalembed": 55, "tokenpositionalembed": 56, "clip_vision_encod": 57, "code_llama2_13b": 58, "code_llama2_70b": 59, "code_llama2_7b": 60, "lora_code_llama2_13b": 61, "lora_code_llama2_70b": 62, "lora_code_llama2_7b": 63, "qlora_code_llama2_13b": 64, "qlora_code_llama2_70b": 65, "qlora_code_llama2_7b": 66, "gemmatoken": 67, "gemma_2b": 69, "gemma_7b": 70, "gemma_token": 71, "lora_gemma": 72, "lora_gemma_2b": 73, "lora_gemma_7b": 74, "qlora_gemma_2b": 75, "qlora_gemma_7b": 76, "llama2chattempl": 77, "llama2token": 78, "llama2_13b": 80, "llama2_70b": 81, "llama2_7b": 82, "llama2_reward_7b": 83, "llama2_token": 84, "lora_llama2": 85, "lora_llama2_13b": 86, "lora_llama2_70b": 87, "lora_llama2_7b": 88, "lora_llama2_reward_7b": 89, "qlora_llama2_13b": 90, "qlora_llama2_70b": 91, "qlora_llama2_7b": 92, "qlora_llama2_reward_7b": 93, "llama3token": 94, "llama3_70b": 96, "llama3_8b": 97, "llama3_token": 98, "lora_llama3": 99, "lora_llama3_70b": 100, "lora_llama3_8b": 101, "qlora_llama3_70b": 102, "qlora_llama3_8b": 103, "llama3_1": 104, "llama3_1_405b": 105, "llama3_1_70b": 106, "llama3_1_8b": 107, "lora_llama3_1": 108, "lora_llama3_1_70b": 109, "lora_llama3_1_8b": 110, "qlora_llama3_1_70b": 111, "qlora_llama3_1_8b": 112, "mistralchattempl": 113, "mistraltoken": 114, "lora_mistr": 115, "lora_mistral_7b": 116, "lora_mistral_classifi": 117, "lora_mistral_reward_7b": 118, "mistral_7b": 120, "mistral_classifi": 121, "mistral_reward_7b": 122, "mistral_token": 123, "qlora_mistral_7b": 124, "qlora_mistral_reward_7b": 125, "phi3minitoken": 126, "lora_phi3": 127, "lora_phi3_mini": 128, "phi3": 129, "phi3_mini": 130, "phi3_mini_token": 131, "qlora_phi3_mini": 132, "qwen2token": 133, "lora_qwen2": 134, "lora_qwen2_0_5b": 135, "lora_qwen2_1_5b": 136, "lora_qwen2_7b": 137, "qwen2": 138, "qwen2_0_5b": 139, "qwen2_1_5b": 140, "qwen2_7b": 141, "qwen2_token": 142, "causalselfattent": 143, "todo": [143, 150], "feedforward": 144, "fp32layernorm": 145, "kvcach": 146, "rmsnorm": 147, "rotarypositionalembed": 148, "transformerdecod": 149, "transformerdecoderlay": 150, "visiontransform": 151, "reparametrize_as_dtype_state_dict_post_hook": 152, "get_cosine_schedule_with_warmup": 153, "adaptermodul": 154, "loralinear": 155, "disable_adapt": 156, "get_adapter_param": 157, "set_trainable_param": 158, "validate_missing_and_unexpected_for_lora": 159, "validate_state_dict_for_lora": 160, "estimate_advantag": 161, "get_rewards_ppo": 162, "left_padded_col": 163, "dpoloss": 164, "ipoloss": 165, "ppoloss": 166, "rsoloss": 167, "simpoloss": 168, "padded_collate_dpo": 169, "truncate_sequence_at_first_stop_token": 170, "basetoken": 171, "modeltoken": 172, "sentencepiecebasetoken": 173, "tiktokenbasetoken": 174, "parse_hf_tokenizer_json": 175, "tokenize_messages_no_special_token": 176, "visioncrossattentionmask": 178, "find_supported_resolut": 179, "get_canvas_best_fit": 180, "get_inscribed_s": 181, "resize_with_pad": 182, "tile_crop": 183, "fsdppolicytyp": 184, "fullmodelhfcheckpoint": 185, "fullmodelmetacheckpoint": 186, "fullmodeltorchtunecheckpoint": 187, "modeltyp": 188, "optimizerinbackwardwrapp": 189, "tunerecipeargumentpars": 190, "create_optim_in_bwd_wrapp": 191, "get_devic": 193, "get_dtyp": 194, "get_full_finetune_fsdp_wrap_polici": 195, "get_logg": 196, "get_memory_stat": 197, "get_quantizer_mod": 198, "get_world_size_and_rank": 199, "init_distribut": 200, "is_distribut": 201, "log_memory_stat": 202, "lora_fsdp_wrap_polici": 203, "cometlogg": 204, "disklogg": 205, "stdoutlogg": 206, "tensorboardlogg": 207, "wandblogg": 208, "padded_col": 209, "register_optim_in_bwd_hook": 210, "set_activation_checkpoint": 211, "set_default_dtyp": 212, "set_se": 213, "setup_torch_profil": 214, "torch_version_g": 215, "validate_expected_param_dtyp": 216, "comput": [218, 222], "time": [218, 222], "welcom": 219, "document": 219, "get": [219, 223, 228], "start": [219, 223], "tutori": 219, "instal": 220, "instruct": [220, 225, 228], "via": [220, 228], "pypi": 220, "git": 220, "clone": 220, "nightli": 220, "kei": 221, "concept": 221, "design": 221, "principl": 221, "download": [223, 226, 227], "list": 223, "built": [223, 225], "copi": 223, "fine": [224, 225, 227, 228], "tune": [224, 225, 227, 228], "chat": [224, 225], "chang": 224, "prompt": 224, "special": 224, "when": 224, "should": 224, "i": 224, "custom": [224, 225], "hug": [225, 226], "face": [225, 226], "set": 225, "max": 225, "sequenc": 225, "length": 225, "sampl": 225, "pack": 225, "unstructur": 225, "corpu": 225, "multipl": 225, "local": 225, "remot": 225, "fulli": 225, "end": 226, "workflow": 226, "7b": 226, "finetun": [226, 229, 230, 231], "evalu": [226, 228, 230], "eleutherai": [226, 228], "s": [226, 228], "eval": [226, 228], "har": [226, 228], "speed": 226, "up": 226, "quantiz": [226, 228, 230], "librari": 226, "upload": 226, "hub": 226, "first": 227, "llm": 227, "select": 227, "modifi": 227, "train": 227, "next": 227, "step": 227, "meta": 228, "8b": 228, "access": 228, "our": 228, "faster": 228, "how": 229, "doe": 229, "work": 229, "appli": [229, 230], "trade": 229, "off": 229, "qat": 230, "lower": 230, "devic": 230, "option": 230, "qlora": 231, "save": 231, "deep": 231, "dive": 231}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})