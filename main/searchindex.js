Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_training", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Message", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.left_pad_sequence", "generated/torchtune.data.padded_collate", "generated/torchtune.data.padded_collate_dpo", "generated/torchtune.data.padded_collate_sft", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.llava_instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.the_cauldron_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.Qwen2Tokenizer", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.MultiHeadAttention", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TanhGate", "generated/torchtune.modules.TiedEmbeddingTransformerDecoder", "generated/torchtune.modules.TransformerCrossAttentionLayer", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerSelfAttentionLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss", "generated/torchtune.modules.model_fusion.DeepFusionModel", "generated/torchtune.modules.model_fusion.FusionEmbedding", "generated/torchtune.modules.model_fusion.FusionLayer", "generated/torchtune.modules.model_fusion.register_fusion_module", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.loss.DPOLoss", "generated/torchtune.modules.rlhf.loss.IPOLoss", "generated/torchtune.modules.rlhf.loss.PPOLoss", "generated/torchtune.modules.rlhf.loss.RSOLoss", "generated/torchtune.modules.rlhf.loss.SimPOLoss", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.training.FSDPPolicyType", "generated/torchtune.training.FullModelHFCheckpointer", "generated/torchtune.training.FullModelMetaCheckpointer", "generated/torchtune.training.FullModelTorchTuneCheckpointer", "generated/torchtune.training.ModelType", "generated/torchtune.training.OptimizerInBackwardWrapper", "generated/torchtune.training.apply_selective_activation_checkpointing", "generated/torchtune.training.create_optim_in_bwd_wrapper", "generated/torchtune.training.get_dtype", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.training.get_memory_stats", "generated/torchtune.training.get_quantizer_mode", "generated/torchtune.training.get_unmasked_sequence_lengths", "generated/torchtune.training.get_world_size_and_rank", "generated/torchtune.training.init_distributed", "generated/torchtune.training.is_distributed", "generated/torchtune.training.log_memory_stats", "generated/torchtune.training.lora_fsdp_wrap_policy", "generated/torchtune.training.metric_logging.CometLogger", "generated/torchtune.training.metric_logging.DiskLogger", "generated/torchtune.training.metric_logging.StdoutLogger", "generated/torchtune.training.metric_logging.TensorBoardLogger", "generated/torchtune.training.metric_logging.WandBLogger", "generated/torchtune.training.register_optim_in_bwd_hooks", "generated/torchtune.training.set_activation_checkpointing", "generated/torchtune.training.set_default_dtype", "generated/torchtune.training.set_seed", "generated/torchtune.training.setup_torch_profiler", "generated/torchtune.training.update_state_dict_for_classifier", "generated/torchtune.training.validate_expected_param_dtype", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.torch_version_ge", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "recipes/lora_finetune_single_device", "recipes/qat_distributed", "recipes/recipes_overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/memory_optimizations", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_training.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.left_pad_sequence.rst", "generated/torchtune.data.padded_collate.rst", "generated/torchtune.data.padded_collate_dpo.rst", "generated/torchtune.data.padded_collate_sft.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.llava_instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.the_cauldron_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.Qwen2Tokenizer.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.MultiHeadAttention.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TanhGate.rst", "generated/torchtune.modules.TiedEmbeddingTransformerDecoder.rst", "generated/torchtune.modules.TransformerCrossAttentionLayer.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerSelfAttentionLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss.rst", "generated/torchtune.modules.model_fusion.DeepFusionModel.rst", "generated/torchtune.modules.model_fusion.FusionEmbedding.rst", "generated/torchtune.modules.model_fusion.FusionLayer.rst", "generated/torchtune.modules.model_fusion.register_fusion_module.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.loss.DPOLoss.rst", "generated/torchtune.modules.rlhf.loss.IPOLoss.rst", "generated/torchtune.modules.rlhf.loss.PPOLoss.rst", "generated/torchtune.modules.rlhf.loss.RSOLoss.rst", "generated/torchtune.modules.rlhf.loss.SimPOLoss.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.training.FSDPPolicyType.rst", "generated/torchtune.training.FullModelHFCheckpointer.rst", "generated/torchtune.training.FullModelMetaCheckpointer.rst", "generated/torchtune.training.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.training.ModelType.rst", "generated/torchtune.training.OptimizerInBackwardWrapper.rst", "generated/torchtune.training.apply_selective_activation_checkpointing.rst", "generated/torchtune.training.create_optim_in_bwd_wrapper.rst", "generated/torchtune.training.get_dtype.rst", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.training.get_memory_stats.rst", "generated/torchtune.training.get_quantizer_mode.rst", "generated/torchtune.training.get_unmasked_sequence_lengths.rst", "generated/torchtune.training.get_world_size_and_rank.rst", "generated/torchtune.training.init_distributed.rst", "generated/torchtune.training.is_distributed.rst", "generated/torchtune.training.log_memory_stats.rst", "generated/torchtune.training.lora_fsdp_wrap_policy.rst", "generated/torchtune.training.metric_logging.CometLogger.rst", "generated/torchtune.training.metric_logging.DiskLogger.rst", "generated/torchtune.training.metric_logging.StdoutLogger.rst", "generated/torchtune.training.metric_logging.TensorBoardLogger.rst", "generated/torchtune.training.metric_logging.WandBLogger.rst", "generated/torchtune.training.register_optim_in_bwd_hooks.rst", "generated/torchtune.training.set_activation_checkpointing.rst", "generated/torchtune.training.set_default_dtype.rst", "generated/torchtune.training.set_seed.rst", "generated/torchtune.training.setup_torch_profiler.rst", "generated/torchtune.training.update_state_dict_for_classifier.rst", "generated/torchtune.training.validate_expected_param_dtype.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "recipes/lora_finetune_single_device.rst", "recipes/qat_distributed.rst", "recipes/recipes_overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/memory_optimizations.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.training", "torchtune.utils", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "ChatFormat", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Message", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "left_pad_sequence", "padded_collate", "padded_collate_dpo", "padded_collate_sft", "truncate", "validate_messages", "torchtune.datasets.ChatDataset", "ConcatDataset", "torchtune.datasets.InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "llava_instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "the_cauldron_dataset", "wikitext_dataset", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralChatTemplate", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "Qwen2Tokenizer", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "FeedForward", "Fp32LayerNorm", "KVCache", "MultiHeadAttention", "RMSNorm", "RotaryPositionalEmbeddings", "TanhGate", "TiedEmbeddingTransformerDecoder", "TransformerCrossAttentionLayer", "TransformerDecoder", "TransformerSelfAttentionLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "CEWithChunkedOutputLoss", "DeepFusionModel", "FusionEmbedding", "FusionLayer", "register_fusion_module", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "torchtune.training.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "apply_selective_activation_checkpointing", "create_optim_in_bwd_wrapper", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_memory_stats", "get_quantizer_mode", "get_unmasked_sequence_lengths", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "update_state_dict_for_classifier", "validate_expected_param_dtype", "generate", "get_device", "get_logger", "torch_version_ge", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "LoRA Single Device Finetuning", "Distributed Quantization-Aware Training (QAT)", "Recipes Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Memory Optimization Overview", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 3, 17, 18, 20, 21, 22, 28, 40, 41, 43, 46, 49, 50, 51, 52, 53, 55, 56, 98, 117, 126, 133, 134, 135, 143, 144, 145, 225, 228, 229, 232, 233, 236, 238, 240, 241], "prompt": [1, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 38, 40, 42, 43, 46, 47, 49, 50, 51, 52, 53, 54, 56, 71, 75, 81, 82, 88, 98, 102, 117, 118, 127, 130, 135, 137, 146, 154, 156, 162, 186, 219, 234, 235, 237], "chat": [1, 2, 16, 17, 22, 28, 30, 31, 38, 43, 47, 81, 82, 135, 228], "includ": [1, 7, 9, 10, 16, 21, 24, 25, 43, 61, 72, 82, 83, 99, 108, 123, 135, 142, 154, 156, 167, 181, 190, 191, 227, 230, 232, 233, 234, 235, 236, 237, 238, 241], "some": [1, 7, 9, 17, 121, 163, 169, 170, 225, 227, 228, 229, 232, 233, 234, 235, 236, 238, 239, 240, 241], "specif": [1, 4, 9, 10, 12, 42, 43, 51, 56, 71, 82, 98, 118, 130, 137, 182, 198, 229, 233, 234, 235, 239, 240, 241], "format": [1, 2, 5, 16, 21, 23, 30, 32, 38, 40, 42, 43, 46, 47, 50, 71, 81, 82, 98, 117, 118, 130, 137, 182, 190, 191, 192, 193, 232, 233, 235, 236, 237, 238, 239], "differ": [1, 9, 11, 34, 39, 40, 47, 50, 58, 59, 60, 118, 154, 158, 175, 183, 193, 218, 227, 228, 229, 232, 233, 235, 237, 238, 239, 240, 241], "dataset": [1, 9, 18, 20, 21, 22, 23, 28, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 175, 176, 227, 230, 236, 237, 240], "model": [1, 2, 7, 8, 9, 10, 12, 17, 18, 20, 22, 23, 28, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 150, 151, 152, 154, 156, 157, 158, 159, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 181, 182, 185, 186, 187, 190, 191, 192, 193, 195, 196, 198, 199, 206, 207, 212, 213, 217, 219, 225, 227, 228, 229, 230, 233, 234, 241], "from": [1, 2, 3, 7, 8, 9, 10, 11, 12, 18, 21, 22, 23, 28, 31, 32, 33, 38, 39, 40, 41, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 73, 74, 81, 84, 85, 86, 87, 102, 118, 124, 126, 135, 137, 143, 144, 145, 146, 147, 150, 154, 155, 156, 157, 158, 160, 161, 164, 165, 166, 169, 172, 175, 176, 178, 179, 183, 185, 188, 190, 191, 192, 194, 196, 207, 210, 211, 212, 217, 219, 224, 226, 229, 231, 232, 234, 235, 236, 237, 238, 239, 240], "common": [1, 2, 4, 9, 186, 232, 233, 234, 237, 238, 239, 240], "json": [1, 7, 22, 28, 30, 31, 38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 102, 135, 137, 146, 185, 190, 232, 234, 235, 240], "schema": 1, "convers": [1, 7, 16, 18, 28, 30, 31, 37, 38, 42, 43, 47, 51, 53, 190, 192, 193, 227, 233, 234, 235, 238, 239, 241], "list": [1, 7, 9, 16, 18, 23, 24, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 47, 48, 51, 56, 57, 61, 65, 66, 67, 68, 69, 70, 71, 75, 76, 77, 78, 79, 80, 82, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 127, 128, 129, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141, 154, 156, 158, 161, 162, 163, 164, 166, 167, 171, 172, 181, 182, 183, 184, 186, 188, 190, 191, 192, 207, 219, 221, 233, 234, 235, 236, 237, 239, 240], "us": [1, 2, 3, 4, 7, 8, 11, 12, 14, 16, 17, 20, 21, 23, 24, 30, 31, 33, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 71, 81, 82, 83, 89, 98, 99, 102, 103, 108, 112, 118, 130, 131, 135, 137, 138, 142, 146, 147, 150, 151, 152, 154, 156, 157, 158, 159, 161, 162, 163, 165, 168, 171, 173, 174, 175, 177, 179, 183, 184, 188, 189, 190, 191, 193, 194, 197, 198, 199, 201, 206, 207, 208, 209, 210, 211, 215, 217, 219, 220, 225, 226, 227, 228, 229, 230, 232, 234, 236, 237, 238, 239, 240], "collect": [1, 9, 219, 236], "sampl": [1, 8, 11, 16, 18, 20, 21, 22, 23, 28, 30, 31, 38, 40, 41, 42, 43, 44, 49, 51, 52, 53, 55, 56, 150, 152, 154, 156, 157, 158, 162, 178, 187, 188, 219, 233, 235, 239], "batch": [1, 10, 33, 34, 35, 41, 46, 49, 51, 52, 56, 59, 149, 150, 152, 154, 155, 156, 157, 158, 162, 164, 173, 174, 175, 176, 178, 179, 201, 216, 227, 234, 236, 237, 238, 239], "handl": [1, 9, 14, 39, 43, 82, 118, 183, 184, 233, 235, 238, 239, 241], "ani": [1, 4, 7, 9, 10, 12, 14, 15, 18, 21, 22, 24, 28, 30, 31, 33, 36, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 60, 82, 118, 148, 159, 162, 164, 169, 170, 171, 172, 181, 182, 183, 186, 190, 191, 192, 194, 203, 206, 207, 215, 218, 219, 232, 233, 234, 236, 238, 239, 240], "pad": [1, 32, 33, 34, 35, 41, 137, 158, 174, 177, 180, 188, 201, 234], "miscellan": 1, "modifi": [1, 9, 10, 11, 159, 227, 235, 237, 238, 239, 240, 241], "For": [2, 5, 7, 9, 10, 18, 22, 23, 24, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 72, 76, 82, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 154, 156, 158, 161, 163, 165, 190, 196, 200, 207, 211, 213, 215, 226, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "detail": [2, 7, 38, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 82, 125, 149, 158, 161, 177, 189, 198, 206, 215, 228, 229, 232, 234, 235, 236, 237, 238, 240, 241], "usag": [2, 159, 161, 193, 194, 216, 226, 228, 232, 234, 235, 236, 237, 239, 240, 241], "guid": [2, 8, 9, 11, 18, 20, 22, 28, 49, 50, 51, 52, 53, 56, 179, 207, 227, 233, 234, 236, 238], "pleas": [2, 5, 16, 19, 21, 26, 29, 30, 31, 38, 40, 58, 59, 60, 61, 68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 158, 161, 189, 198, 206, 213, 226, 235, 237, 241], "see": [2, 5, 7, 8, 11, 19, 26, 29, 33, 38, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 68, 69, 70, 79, 80, 81, 82, 94, 95, 96, 97, 106, 107, 115, 116, 117, 125, 128, 129, 136, 137, 149, 158, 166, 189, 193, 198, 206, 207, 211, 213, 215, 221, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "our": [2, 7, 10, 227, 228, 229, 230, 233, 234, 235, 236, 238, 239, 240, 241], "tutori": [2, 7, 82, 213, 227, 228, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241], "support": [2, 3, 7, 8, 10, 11, 12, 23, 38, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 53, 56, 57, 76, 89, 103, 112, 117, 119, 121, 131, 134, 135, 138, 148, 150, 158, 163, 164, 167, 178, 191, 192, 194, 197, 199, 200, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "sever": [2, 239], "wide": [2, 150], "help": [2, 7, 81, 154, 156, 158, 162, 190, 207, 225, 226, 227, 230, 232, 233, 234, 235, 236, 239, 240, 241], "quickli": [2, 9, 24, 44, 228, 233, 234, 239], "bootstrap": 2, "your": [2, 5, 8, 11, 12, 24, 38, 44, 47, 50, 59, 60, 61, 82, 158, 163, 207, 210, 211, 217, 225, 226, 227, 228, 229, 230, 232, 233, 234, 237, 238, 239, 240, 241], "fine": [2, 7, 8, 10, 11, 23, 41, 42, 43, 55, 217, 225, 227, 228, 229, 230, 235, 238, 240], "tune": [2, 3, 7, 8, 9, 10, 11, 14, 23, 41, 42, 43, 55, 217, 225, 226, 227, 228, 229, 230, 232, 235, 238, 240, 241], "also": [2, 7, 8, 9, 10, 11, 12, 39, 47, 50, 55, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 135, 138, 142, 150, 154, 156, 167, 179, 198, 199, 206, 207, 211, 217, 220, 226, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "like": [2, 4, 7, 8, 9, 10, 11, 38, 135, 158, 161, 163, 192, 226, 228, 232, 233, 234, 235, 236, 238, 239, 240], "These": [2, 4, 7, 9, 10, 12, 41, 42, 158, 188, 228, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241], "ar": [2, 4, 7, 8, 9, 11, 12, 16, 21, 24, 25, 30, 31, 32, 33, 34, 37, 40, 41, 42, 43, 46, 47, 50, 51, 59, 65, 66, 67, 68, 69, 70, 71, 76, 77, 78, 79, 80, 81, 82, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 136, 137, 138, 139, 140, 141, 154, 155, 156, 157, 158, 162, 163, 164, 167, 168, 171, 172, 174, 188, 189, 190, 191, 193, 194, 196, 197, 199, 204, 206, 216, 217, 219, 226, 227, 228, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "especi": [2, 227, 232, 235, 239], "specifi": [2, 7, 9, 10, 12, 18, 20, 22, 28, 47, 49, 50, 51, 52, 53, 56, 75, 83, 88, 89, 99, 102, 103, 108, 112, 127, 135, 138, 142, 146, 150, 154, 156, 157, 162, 189, 198, 200, 206, 211, 213, 216, 219, 229, 230, 232, 233, 234, 235, 236, 237, 239, 240, 241], "yaml": [2, 9, 10, 12, 13, 14, 39, 47, 50, 55, 211, 227, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241], "config": [2, 7, 8, 11, 12, 13, 14, 15, 39, 47, 50, 55, 150, 171, 190, 194, 207, 211, 216, 227, 228, 229, 230, 233, 234, 235, 237, 238, 239, 240, 241], "represent": [2, 238, 240, 241], "abov": [2, 3, 7, 159, 204, 226, 229, 235, 237, 238, 239, 240, 241], "all": [3, 4, 10, 15, 23, 24, 32, 33, 39, 41, 42, 43, 61, 102, 135, 137, 146, 147, 150, 154, 156, 158, 159, 162, 163, 164, 165, 168, 187, 190, 194, 196, 204, 212, 218, 223, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240], "famili": [3, 7, 10, 46, 48, 51, 53, 54, 56, 57, 193, 227, 232, 237], "request": [3, 197, 234, 235], "access": [3, 7, 9, 10, 39, 190, 196, 228, 229, 232, 234, 235, 236], "hug": [3, 7, 17, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 102, 135, 146, 160, 185, 227, 232, 236, 237], "face": [3, 7, 17, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 102, 135, 146, 160, 185, 227, 232, 236, 237], "To": [3, 7, 9, 10, 11, 41, 51, 158, 164, 190, 217, 226, 227, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "download": [3, 7, 51, 223, 226, 228, 229, 233, 234, 237, 238, 240, 241], "8b": [3, 101, 105, 107, 111, 114, 116, 132, 228, 229, 232, 233, 240], "meta": [3, 7, 81, 82, 98, 152, 190, 191, 228, 229, 232, 233, 235, 236], "hf": [3, 7, 130, 175, 176, 178, 190, 232, 233, 235, 236, 237], "token": [3, 7, 9, 10, 23, 33, 35, 36, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 71, 72, 75, 76, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 119, 121, 123, 125, 127, 130, 131, 133, 135, 137, 138, 142, 146, 150, 152, 154, 155, 156, 157, 158, 161, 162, 163, 164, 174, 177, 180, 181, 182, 183, 184, 185, 186, 188, 198, 201, 219, 228, 232, 234, 235, 236, 237, 238, 239, 240, 241], "hf_token": [3, 229], "70b": [3, 63, 66, 69, 85, 91, 95, 100, 104, 106, 110, 113, 115, 237], "ignor": [3, 7, 55, 130, 147, 150, 195, 217, 228, 229, 232], "pattern": [3, 184, 228, 229, 232], "origin": [3, 7, 45, 46, 159, 163, 164, 167, 228, 229, 233, 235, 237, 238, 239, 240, 241], "consolid": [3, 7, 228, 229], "weight": [3, 7, 10, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 150, 154, 159, 166, 167, 171, 175, 183, 190, 191, 192, 193, 200, 211, 217, 225, 229, 232, 233, 235, 236, 237, 238, 239, 240, 241], "you": [3, 7, 8, 9, 10, 11, 12, 21, 23, 24, 38, 40, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 81, 98, 158, 161, 164, 165, 193, 207, 210, 211, 217, 219, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "can": [3, 4, 7, 8, 9, 10, 11, 12, 15, 18, 20, 22, 23, 24, 25, 28, 39, 40, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 82, 118, 151, 152, 154, 155, 156, 158, 161, 162, 164, 165, 168, 183, 184, 189, 190, 193, 195, 198, 206, 207, 210, 211, 213, 216, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "instead": [3, 7, 10, 30, 31, 32, 41, 42, 55, 61, 137, 147, 149, 158, 167, 179, 232, 237, 238, 239, 240], "405b": [3, 109], "The": [3, 7, 8, 9, 10, 11, 14, 15, 16, 17, 21, 23, 30, 31, 33, 37, 38, 39, 40, 41, 42, 43, 47, 50, 51, 54, 56, 58, 59, 60, 61, 65, 66, 67, 71, 76, 77, 78, 82, 89, 90, 91, 92, 93, 98, 103, 104, 105, 112, 113, 114, 118, 119, 121, 130, 131, 132, 137, 138, 139, 140, 141, 148, 151, 152, 153, 158, 159, 160, 161, 162, 163, 164, 168, 173, 175, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 188, 189, 190, 192, 197, 200, 207, 211, 214, 216, 220, 221, 222, 226, 227, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "reus": [3, 227], "llama3_token": [3, 51, 56, 219, 233, 237], "builder": [3, 7, 45, 47, 48, 50, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 77, 78, 79, 80, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 104, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 120, 122, 124, 126, 128, 129, 132, 134, 136, 139, 140, 141, 143, 144, 145, 233, 234, 239, 241], "class": [3, 9, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 38, 39, 40, 41, 42, 43, 44, 51, 56, 58, 59, 60, 61, 71, 75, 81, 82, 87, 88, 98, 102, 117, 118, 121, 125, 126, 127, 130, 135, 137, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 166, 167, 169, 170, 175, 176, 177, 178, 179, 181, 182, 183, 184, 187, 188, 190, 191, 192, 193, 194, 207, 208, 209, 210, 211, 230, 233, 234, 236, 238, 239, 241], "7b": [3, 7, 40, 48, 57, 64, 67, 70, 74, 78, 86, 87, 92, 93, 96, 97, 120, 122, 124, 126, 129, 141, 145, 190, 191, 233, 236, 237, 238, 241], "13b": [3, 7, 62, 65, 68, 84, 90, 94], "codellama": 3, "size": [3, 7, 10, 12, 32, 46, 49, 51, 52, 56, 59, 60, 61, 137, 149, 150, 151, 152, 154, 155, 156, 157, 158, 161, 162, 163, 164, 173, 174, 188, 201, 202, 204, 227, 229, 232, 234, 235, 236, 237, 238, 239, 240], "0": [3, 7, 10, 32, 33, 34, 35, 41, 47, 50, 61, 65, 66, 67, 68, 69, 70, 71, 72, 76, 82, 83, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 103, 108, 112, 118, 119, 121, 123, 125, 130, 131, 133, 137, 138, 139, 140, 141, 142, 143, 144, 150, 154, 156, 158, 160, 163, 167, 175, 176, 177, 178, 179, 180, 186, 200, 201, 207, 210, 211, 215, 219, 222, 224, 229, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241], "5b": [3, 139, 140, 143, 144, 239], "qwen2": [3, 137, 138, 139, 140, 141, 143, 144, 145, 146, 193, 239], "exampl": [3, 7, 8, 9, 10, 11, 12, 14, 18, 22, 24, 32, 33, 34, 35, 39, 40, 41, 43, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 61, 71, 82, 98, 118, 130, 137, 150, 158, 161, 162, 163, 164, 165, 166, 168, 175, 176, 178, 179, 180, 183, 184, 186, 189, 190, 191, 193, 194, 200, 201, 207, 210, 211, 214, 217, 219, 222, 223, 224, 226, 228, 229, 231, 232, 233, 234, 235, 237, 238, 239, 240, 241], "output": [3, 7, 20, 21, 32, 39, 40, 42, 43, 46, 49, 50, 52, 53, 61, 65, 66, 67, 72, 76, 83, 87, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 126, 131, 132, 138, 141, 142, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 167, 170, 171, 172, 188, 192, 198, 209, 216, 217, 219, 226, 228, 229, 232, 233, 234, 235, 236, 237, 238, 239, 241], "dir": [3, 7, 211, 226, 228, 229, 232, 235, 236, 237, 240], "tmp": [3, 9, 194, 228, 229, 233, 236], "none": [3, 10, 11, 13, 15, 18, 20, 21, 22, 28, 36, 37, 40, 41, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 71, 75, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 127, 130, 135, 137, 146, 147, 149, 150, 152, 154, 155, 156, 157, 158, 162, 164, 168, 170, 171, 172, 173, 174, 177, 183, 186, 188, 190, 191, 192, 193, 195, 197, 200, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 218, 219, 220, 221, 232, 233, 234, 235, 240], "mini": [3, 130, 132, 133, 134, 135, 136], "4k": [3, 133, 134, 135], "microsoft": [3, 134, 135], "ai": [3, 42, 43, 124, 150, 211, 233, 237], "v0": [3, 117], "mistralai": [3, 232], "2b": [3, 73, 77], "googl": [3, 73, 74], "gguf": 3, "vision": [3, 43, 61, 163, 188], "compon": [3, 7, 10, 15, 34, 42, 43, 51, 56, 227, 230, 234, 236, 238, 241], "multimod": [3, 23, 43, 51, 56, 162, 226], "encod": [3, 4, 43, 61, 71, 82, 98, 118, 130, 137, 154, 155, 156, 162, 163, 164, 165, 175, 179, 181, 183, 184, 186, 188, 233], "perform": [4, 7, 41, 82, 147, 158, 161, 168, 179, 187, 219, 227, 228, 229, 233, 235, 237, 240, 241], "direct": [4, 10, 34, 65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132, 175, 226], "text": [4, 23, 24, 25, 38, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 82, 98, 118, 130, 137, 163, 164, 181, 183, 184, 186, 188, 233, 235, 240], "id": [4, 7, 33, 34, 35, 38, 40, 41, 48, 51, 56, 57, 82, 98, 118, 130, 137, 150, 152, 154, 156, 157, 162, 181, 182, 183, 184, 185, 186, 188, 190, 192, 207, 219, 233, 234, 235], "decod": [4, 47, 50, 72, 76, 83, 89, 98, 99, 103, 108, 112, 118, 119, 121, 123, 125, 130, 131, 133, 137, 138, 142, 150, 154, 155, 156, 162, 164, 165, 181, 183, 184, 219, 233], "typic": [4, 9, 18, 22, 28, 41, 42, 43, 44, 55, 135, 165, 175, 179, 234, 239, 240, 241], "byte": [4, 137, 184, 239, 241], "pair": [4, 9, 34, 35, 54, 176, 184, 234], "underli": [4, 118, 183, 239, 241], "helper": 4, "method": [4, 7, 9, 10, 11, 14, 38, 40, 42, 44, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 137, 159, 161, 162, 165, 166, 169, 171, 181, 182, 194, 200, 226, 227, 234, 238, 241], "two": [4, 7, 9, 20, 37, 51, 56, 59, 158, 163, 165, 180, 188, 227, 228, 229, 235, 236, 237, 238, 239, 240, 241], "pre": [4, 41, 42, 43, 44, 51, 55, 56, 81, 82, 158, 162, 164, 165, 228, 229, 230, 233, 234], "train": [4, 7, 8, 9, 10, 11, 18, 20, 38, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 81, 82, 98, 118, 130, 137, 148, 150, 152, 154, 156, 157, 159, 160, 161, 162, 163, 164, 165, 175, 179, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 225, 227, 228, 230, 232, 233, 234, 235, 237, 238, 239, 240, 241], "function": [4, 7, 9, 10, 12, 14, 32, 33, 34, 38, 40, 47, 50, 59, 60, 61, 147, 150, 158, 159, 168, 171, 172, 175, 177, 189, 190, 202, 215, 217, 219, 220, 227, 233, 234, 241], "preprocess": [4, 41, 158], "imag": [4, 23, 43, 51, 56, 58, 59, 60, 61, 158, 163, 188, 238], "algorithm": [4, 173, 179, 215], "ppo": [4, 173, 174, 175, 177], "offer": 5, "allow": [5, 39, 164, 171, 210, 229, 232, 239, 240, 241], "seamless": 5, "transit": 5, "between": [5, 7, 42, 47, 137, 154, 155, 156, 162, 174, 176, 177, 179, 190, 193, 207, 234, 235, 237, 238, 240, 241], "interoper": [5, 7, 10, 227, 235, 241], "rest": [5, 233, 239, 241], "ecosystem": [5, 7, 10, 227, 235, 237, 241], "comprehens": [5, 239], "overview": [5, 9, 11, 162, 225, 228, 229, 236, 238, 241], "deep": [5, 7, 8, 9, 10, 11, 164, 165, 227, 230, 236, 237, 239], "dive": [5, 7, 8, 9, 10, 11, 227, 230, 236, 237, 239], "util": [5, 7, 9, 10, 12, 32, 33, 195, 210, 212, 213, 219, 220, 221, 222, 227, 235, 236, 239, 241], "work": [5, 7, 10, 163, 164, 227, 229, 232, 235, 237, 239, 241], "set": [5, 7, 8, 9, 10, 11, 18, 22, 23, 28, 40, 41, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 83, 89, 98, 99, 103, 108, 112, 119, 121, 123, 125, 130, 131, 133, 137, 138, 142, 150, 152, 154, 155, 156, 168, 170, 189, 198, 204, 206, 207, 213, 214, 215, 216, 220, 227, 230, 232, 233, 235, 236, 237, 238, 239, 240], "enabl": [5, 8, 9, 10, 11, 39, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 90, 91, 92, 93, 94, 95, 96, 97, 104, 105, 106, 107, 113, 114, 115, 116, 120, 122, 128, 129, 132, 136, 139, 140, 141, 143, 144, 150, 164, 167, 215, 216, 228, 229, 237, 238, 239, 241], "consumpt": [5, 39, 228, 239], "dure": [5, 7, 40, 41, 46, 47, 49, 50, 52, 53, 149, 150, 152, 154, 156, 157, 158, 159, 162, 163, 179, 188, 199, 228, 229, 233, 235, 237, 238, 239, 240, 241], "variou": [5, 21, 230], "provid": [5, 7, 9, 10, 12, 17, 18, 20, 22, 28, 33, 36, 39, 40, 41, 61, 150, 154, 156, 158, 162, 168, 175, 192, 198, 207, 211, 216, 220, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 239], "debug": [5, 7, 9, 10, 207, 232], "finetun": [5, 7, 9, 10, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141, 162, 225, 227, 229, 236, 237, 239], "job": [5, 11, 215, 236], "walk": [7, 10, 210, 227, 233, 234, 235, 236, 240, 241], "through": [7, 8, 9, 10, 11, 42, 61, 147, 158, 164, 168, 227, 228, 229, 230, 232, 233, 234, 235, 236, 239, 240, 241], "design": [7, 10, 179], "behavior": [7, 206, 233, 234], "associ": [7, 9, 10, 61, 72, 83, 99, 108, 123, 142, 207, 219, 235, 238], "what": [7, 8, 9, 11, 42, 43, 47, 50, 81, 117, 158, 225, 230, 233, 234, 235, 236, 237, 239], "cover": [7, 8, 9, 10, 11, 233, 235, 241], "how": [7, 8, 9, 10, 11, 98, 158, 189, 207, 213, 225, 228, 229, 232, 233, 234, 235, 236, 237, 239, 240, 241], "we": [7, 8, 9, 10, 11, 33, 40, 41, 42, 43, 47, 48, 57, 82, 118, 137, 149, 150, 152, 154, 156, 158, 161, 162, 167, 175, 179, 190, 191, 192, 197, 200, 206, 212, 217, 219, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "them": [7, 9, 39, 40, 71, 82, 118, 130, 147, 158, 159, 164, 186, 229, 232, 233, 234, 235, 238, 239, 240, 241], "scenario": [7, 39, 71, 82, 98, 118, 130, 137], "full": [7, 9, 10, 19, 26, 29, 42, 57, 68, 69, 70, 71, 79, 80, 82, 94, 95, 96, 97, 106, 107, 115, 116, 118, 128, 129, 130, 136, 162, 171, 172, 186, 195, 227, 232, 234, 235, 237, 238, 239, 240], "compos": [7, 158], "which": [7, 9, 10, 33, 39, 40, 41, 44, 46, 47, 49, 50, 52, 53, 55, 65, 66, 67, 75, 76, 77, 78, 88, 89, 90, 91, 92, 93, 98, 102, 103, 104, 105, 112, 113, 114, 117, 118, 119, 120, 121, 122, 127, 131, 132, 135, 138, 139, 140, 141, 150, 152, 154, 156, 157, 158, 160, 162, 164, 171, 172, 183, 190, 191, 192, 194, 197, 208, 211, 213, 217, 227, 228, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241], "plug": [7, 239], "recip": [7, 8, 9, 11, 12, 13, 14, 147, 154, 156, 171, 190, 191, 192, 227, 228, 229, 233, 234, 235, 237, 239, 241], "evalu": [7, 10, 225, 227, 229, 230, 236, 238, 241], "gener": [7, 10, 33, 38, 40, 41, 48, 55, 82, 98, 118, 168, 173, 207, 214, 215, 216, 223, 225, 229, 233, 234, 238, 239, 240, 241], "each": [7, 10, 16, 21, 24, 25, 33, 34, 39, 41, 42, 43, 58, 59, 60, 61, 65, 66, 67, 71, 76, 77, 78, 82, 89, 90, 91, 92, 93, 103, 104, 105, 112, 113, 114, 118, 119, 120, 121, 122, 130, 131, 132, 138, 139, 140, 141, 150, 152, 154, 156, 157, 158, 161, 162, 164, 171, 172, 173, 174, 175, 176, 178, 179, 186, 188, 201, 215, 216, 227, 229, 230, 232, 234, 235, 236, 238, 239, 240], "make": [7, 8, 9, 10, 11, 157, 158, 227, 232, 233, 235, 236, 237, 238, 239, 240, 241], "easi": [7, 10, 227, 234, 238, 239], "understand": [7, 9, 10, 164, 225, 227, 228, 233, 234, 238, 239, 241], "extend": [7, 10, 227], "befor": [7, 24, 37, 40, 41, 58, 59, 61, 72, 76, 82, 150, 154, 155, 156, 157, 158, 161, 162, 164, 167, 184, 190, 207, 229, 232, 235, 239, 240], "let": [7, 9, 11, 232, 233, 234, 235, 236, 237, 238, 239, 241], "s": [7, 9, 10, 11, 12, 14, 16, 17, 22, 28, 30, 31, 37, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 71, 81, 82, 89, 90, 91, 92, 93, 98, 103, 104, 105, 112, 113, 114, 117, 118, 119, 120, 121, 122, 130, 131, 132, 135, 137, 138, 141, 142, 149, 150, 152, 154, 156, 157, 158, 159, 162, 165, 166, 169, 171, 172, 175, 176, 178, 179, 180, 184, 189, 190, 191, 194, 198, 199, 201, 206, 207, 210, 213, 214, 217, 220, 227, 232, 233, 234, 236, 238, 239, 240, 241], "defin": [7, 9, 10, 24, 38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 147, 150, 154, 155, 156, 162, 166, 167, 169, 174, 234, 236, 238], "concept": [7, 230, 235, 236, 239], "In": [7, 9, 10, 38, 59, 60, 61, 152, 158, 167, 176, 189, 206, 210, 211, 229, 233, 235, 237, 238, 239, 240, 241], "ll": [7, 9, 10, 200, 219, 227, 229, 233, 234, 235, 236, 237, 239, 240, 241], "talk": 7, "about": [7, 10, 51, 56, 158, 175, 179, 207, 211, 227, 228, 229, 230, 232, 233, 235, 236, 237, 238, 240, 241], "take": [7, 9, 10, 12, 34, 42, 43, 51, 56, 147, 149, 158, 159, 164, 190, 192, 220, 229, 233, 234, 235, 236, 237, 238, 241], "close": [7, 10, 207, 208, 209, 210, 211, 238], "look": [7, 9, 10, 196, 210, 226, 233, 234, 235, 236, 237, 238, 240], "veri": [7, 39, 154, 156, 162, 232, 235, 239], "simpli": [7, 9, 22, 41, 43, 175, 176, 232, 233, 234, 235, 237, 239, 241], "dictat": 7, "state_dict": [7, 159, 163, 164, 171, 190, 191, 192, 193, 194, 217, 238, 241], "store": [7, 42, 43, 207, 208, 211, 238, 239, 241], "file": [7, 8, 9, 10, 11, 12, 13, 14, 38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 82, 98, 102, 118, 130, 135, 137, 146, 183, 184, 185, 190, 191, 192, 208, 211, 216, 224, 227, 229, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "disk": [7, 44, 208], "string": [7, 20, 23, 24, 47, 48, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 166, 181, 183, 184, 186, 195, 197, 200, 207, 220, 232, 234, 239], "kei": [7, 9, 11, 18, 20, 22, 28, 30, 33, 34, 38, 40, 42, 43, 46, 47, 49, 50, 51, 52, 53, 54, 56, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 154, 155, 156, 157, 162, 164, 170, 171, 172, 179, 190, 192, 194, 207, 216, 232, 235, 236, 238, 239, 241], "identifi": [7, 207], "state": [7, 10, 158, 159, 169, 170, 171, 172, 173, 175, 190, 191, 192, 194, 196, 217, 228, 235, 237, 238, 239, 241], "dict": [7, 9, 10, 11, 12, 18, 20, 21, 22, 23, 24, 28, 30, 31, 33, 34, 35, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 75, 88, 98, 102, 127, 130, 135, 137, 146, 155, 157, 159, 162, 164, 169, 170, 171, 172, 181, 182, 184, 185, 187, 190, 191, 192, 194, 196, 199, 203, 205, 207, 212, 217, 234], "If": [7, 9, 15, 18, 20, 21, 22, 23, 28, 30, 33, 36, 37, 38, 40, 46, 47, 49, 50, 51, 52, 53, 55, 56, 61, 75, 83, 88, 89, 98, 99, 102, 103, 108, 112, 127, 130, 135, 137, 138, 142, 146, 150, 152, 154, 156, 157, 158, 159, 161, 162, 167, 172, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 203, 207, 210, 211, 215, 216, 218, 219, 220, 226, 232, 233, 234, 235, 236, 237, 238, 239, 240], "don": [7, 9, 10, 211, 215, 232, 233, 234, 235, 236, 239, 241], "t": [7, 9, 10, 32, 161, 164, 176, 197, 211, 215, 230, 232, 233, 234, 235, 236, 239, 241], "match": [7, 40, 130, 172, 207, 217, 226, 232, 234, 235, 237, 238], "up": [7, 8, 10, 11, 40, 41, 48, 57, 137, 184, 188, 196, 207, 216, 228, 229, 230, 232, 233, 234, 236, 237, 238, 239, 241], "exactli": [7, 172, 240], "those": [7, 193, 235, 237, 238], "definit": [7, 238], "either": [7, 33, 42, 43, 172, 190, 207, 213, 219, 226, 232, 238, 240, 241], "run": [7, 8, 9, 11, 14, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 137, 138, 142, 147, 149, 154, 156, 159, 161, 190, 191, 192, 194, 195, 196, 204, 207, 210, 211, 212, 226, 227, 228, 229, 233, 234, 236, 237, 238, 239, 240, 241], "explicit": 7, "error": [7, 9, 19, 37, 137, 149, 190, 215, 232], "load": [7, 10, 38, 39, 40, 41, 42, 43, 44, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 162, 171, 190, 191, 192, 194, 210, 217, 233, 234, 235, 237, 238], "rais": [7, 12, 15, 18, 20, 22, 28, 30, 33, 37, 40, 46, 47, 49, 50, 51, 52, 53, 55, 56, 61, 130, 137, 138, 149, 150, 154, 155, 156, 158, 171, 172, 186, 190, 191, 192, 194, 197, 199, 203, 207, 211, 215, 217, 218], "an": [7, 8, 9, 10, 11, 12, 37, 39, 40, 44, 47, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 89, 103, 112, 119, 121, 125, 131, 137, 138, 139, 140, 143, 144, 149, 150, 154, 156, 158, 162, 163, 164, 165, 166, 168, 169, 170, 175, 188, 189, 190, 191, 192, 194, 195, 198, 207, 211, 216, 220, 227, 229, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "except": [7, 23, 117, 186, 234], "wors": [7, 239], "silent": [7, 147], "succe": 7, "infer": [7, 38, 43, 72, 81, 82, 123, 149, 150, 152, 154, 156, 157, 162, 188, 220, 225, 229, 230, 233, 235, 236, 237, 240, 241], "expect": [7, 9, 12, 18, 20, 21, 22, 28, 38, 40, 42, 43, 46, 47, 49, 50, 51, 52, 53, 54, 56, 71, 82, 98, 118, 130, 137, 152, 162, 172, 194, 207, 211, 218, 233, 234, 238, 240], "addit": [7, 9, 10, 12, 38, 40, 42, 43, 44, 46, 47, 48, 50, 51, 55, 56, 57, 82, 117, 163, 164, 171, 175, 189, 190, 191, 192, 197, 198, 203, 206, 207, 208, 210, 211, 213, 227, 233, 236, 238, 239], "line": [7, 8, 10, 230, 232, 234, 236, 237], "need": [7, 8, 9, 10, 11, 21, 24, 38, 41, 43, 147, 150, 154, 156, 158, 162, 163, 179, 206, 207, 210, 211, 212, 226, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 241], "shape": [7, 33, 58, 59, 60, 61, 149, 150, 152, 154, 155, 156, 157, 158, 161, 162, 163, 164, 167, 173, 174, 175, 176, 177, 178, 179, 180, 188, 201, 216, 217, 219], "valu": [7, 9, 18, 20, 22, 28, 31, 32, 33, 34, 46, 47, 49, 50, 52, 53, 54, 62, 63, 64, 72, 73, 74, 76, 83, 84, 85, 86, 87, 89, 98, 99, 100, 101, 103, 108, 109, 110, 111, 112, 119, 121, 123, 124, 125, 126, 131, 133, 138, 142, 143, 144, 145, 149, 150, 151, 154, 155, 156, 157, 160, 162, 164, 171, 173, 174, 177, 180, 190, 193, 194, 201, 207, 208, 209, 210, 211, 215, 219, 229, 232, 234, 236, 237, 238, 239, 240], "popular": [7, 162, 227, 234, 235], "llama2": [7, 9, 10, 12, 38, 40, 48, 57, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 118, 130, 137, 147, 156, 157, 193, 225, 227, 228, 232, 236, 237, 239, 240], "offici": [7, 81, 137, 233, 236, 237], "implement": [7, 10, 38, 40, 42, 44, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 71, 82, 118, 130, 147, 151, 152, 153, 158, 160, 166, 167, 175, 176, 177, 178, 179, 181, 182, 190, 200, 210, 227, 229, 234, 238, 240, 241], "when": [7, 9, 10, 14, 39, 41, 42, 43, 44, 55, 82, 137, 150, 152, 154, 156, 157, 158, 159, 160, 161, 162, 163, 171, 174, 198, 210, 212, 217, 219, 228, 232, 235, 237, 238, 239, 240, 241], "llama": [7, 38, 81, 82, 98, 151, 152, 190, 191, 228, 229, 232, 233, 235, 236, 237, 238], "websit": 7, "get": [7, 8, 9, 10, 11, 38, 71, 82, 98, 118, 130, 137, 197, 199, 202, 207, 221, 226, 227, 228, 229, 233, 234, 235, 236, 238, 239, 240], "singl": [7, 9, 12, 16, 18, 20, 21, 22, 28, 30, 31, 33, 39, 41, 42, 43, 44, 47, 55, 59, 60, 61, 75, 87, 88, 98, 102, 126, 127, 135, 137, 150, 158, 162, 171, 190, 191, 192, 193, 194, 196, 230, 232, 233, 234, 235, 236, 237, 238, 239, 241], "pth": [7, 228, 229, 235], "inspect": [7, 235, 238, 241], "content": [7, 16, 18, 22, 23, 24, 25, 28, 30, 31, 38, 42, 43, 47, 71, 82, 98, 118, 130, 137, 186, 233, 234], "easili": [7, 9, 227, 234, 238, 240, 241], "torch": [7, 9, 32, 33, 34, 35, 58, 59, 60, 61, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 167, 173, 174, 175, 176, 177, 178, 179, 180, 192, 194, 196, 197, 199, 200, 201, 203, 204, 210, 212, 213, 214, 215, 216, 217, 218, 219, 220, 222, 226, 229, 235, 236, 237, 238, 239, 241], "import": [7, 9, 12, 47, 50, 51, 55, 56, 158, 175, 207, 210, 211, 233, 234, 235, 236, 237, 238, 240, 241], "00": [7, 47, 50, 224, 228, 229, 231, 236], "mmap": [7, 235], "true": [7, 9, 23, 30, 31, 32, 39, 40, 41, 44, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 61, 68, 69, 70, 71, 72, 76, 79, 80, 82, 94, 95, 96, 97, 98, 106, 107, 115, 116, 118, 128, 129, 130, 136, 137, 150, 154, 155, 156, 157, 159, 162, 164, 168, 173, 177, 180, 183, 184, 186, 188, 189, 190, 191, 192, 198, 199, 201, 203, 204, 207, 210, 216, 222, 228, 232, 233, 234, 235, 237, 238, 239, 240, 241], "weights_onli": [7, 192], "map_loc": [7, 235], "cpu": [7, 10, 159, 197, 216, 226, 232, 235, 241], "tensor": [7, 32, 33, 34, 35, 58, 59, 60, 61, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 167, 173, 174, 175, 176, 177, 178, 179, 180, 190, 201, 207, 208, 209, 210, 211, 214, 217, 219, 238, 239, 241], "item": 7, "print": [7, 11, 39, 46, 49, 51, 52, 53, 56, 71, 82, 98, 118, 130, 137, 158, 183, 184, 186, 219, 222, 233, 234, 236, 238, 240, 241], "f": [7, 11, 46, 49, 51, 52, 56, 233, 235, 238, 241], "tok_embed": [7, 154, 156, 162, 163], "32000": [7, 12, 238], "4096": [7, 12, 40, 48, 57, 150, 152, 234, 238, 240], "len": [7, 39, 46, 49, 51, 52, 56, 154, 156, 158, 162], "292": 7, "contain": [7, 18, 20, 23, 30, 33, 34, 35, 41, 42, 43, 44, 47, 55, 71, 82, 98, 102, 118, 130, 135, 137, 146, 149, 150, 152, 154, 156, 157, 162, 166, 169, 170, 171, 173, 180, 184, 186, 190, 191, 192, 194, 196, 199, 205, 210, 216, 217, 233, 235, 237, 238], "input": [7, 20, 21, 33, 34, 35, 38, 40, 41, 42, 43, 46, 48, 49, 50, 51, 52, 53, 56, 57, 58, 59, 60, 61, 75, 88, 98, 102, 118, 127, 130, 135, 138, 142, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 162, 163, 164, 167, 183, 184, 188, 190, 192, 215, 218, 233, 234, 238, 241], "embed": [7, 58, 59, 60, 61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 151, 152, 154, 155, 156, 158, 162, 163, 164, 165, 198, 233, 237, 239, 240], "tabl": [7, 163, 233, 235, 237, 239, 241], "call": [7, 12, 23, 24, 43, 51, 56, 117, 147, 150, 154, 156, 158, 159, 171, 207, 208, 209, 210, 211, 212, 216, 217, 233, 234, 238, 241], "layer": [7, 10, 61, 65, 66, 67, 68, 69, 70, 72, 76, 77, 78, 79, 80, 83, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 125, 126, 128, 129, 131, 132, 133, 136, 138, 139, 140, 141, 142, 150, 153, 154, 155, 156, 157, 158, 162, 164, 165, 167, 171, 172, 189, 195, 198, 227, 228, 237, 238, 239, 240, 241], "have": [7, 9, 12, 20, 23, 42, 47, 59, 60, 61, 149, 150, 158, 161, 166, 172, 179, 188, 192, 194, 198, 210, 218, 226, 233, 234, 235, 236, 237, 238, 239, 240, 241], "dim": [7, 147, 150, 151, 152, 154, 156, 161, 162], "most": [7, 9, 23, 24, 233, 236, 238, 239, 241], "within": [7, 9, 12, 38, 41, 58, 76, 89, 103, 112, 119, 121, 131, 138, 147, 158, 210, 215, 216, 219, 232, 234, 238, 241], "hub": [7, 42, 43, 232, 234, 236], "default": [7, 9, 17, 18, 20, 22, 23, 28, 30, 31, 32, 34, 35, 36, 38, 40, 41, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 150, 151, 152, 154, 155, 156, 157, 159, 160, 162, 167, 171, 173, 174, 175, 179, 183, 184, 186, 188, 190, 191, 192, 197, 202, 206, 207, 208, 211, 214, 215, 216, 219, 226, 229, 232, 233, 234, 235, 237, 238, 239, 240, 241], "everi": [7, 10, 49, 52, 53, 58, 59, 60, 147, 158, 210, 216, 226, 232, 239, 241], "2": [7, 11, 32, 33, 34, 35, 37, 41, 53, 58, 59, 71, 82, 98, 117, 118, 130, 137, 150, 158, 176, 177, 179, 180, 183, 184, 186, 190, 191, 200, 201, 214, 215, 216, 222, 229, 233, 235, 236, 237, 238, 239, 240], "repo": [7, 51, 190, 191, 193, 232, 235], "first": [7, 9, 12, 37, 41, 51, 61, 149, 154, 156, 158, 162, 180, 190, 225, 227, 228, 233, 234, 235, 237, 238, 240, 241], "big": 7, "split": [7, 38, 39, 40, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 137, 184, 233, 234, 235, 240], "across": [7, 10, 39, 190, 210, 215, 235, 237, 240], "bin": [7, 232, 235], "correctli": [7, 10, 15, 171, 190, 226, 230, 233, 236, 241], "piec": 7, "one": [7, 10, 18, 20, 22, 28, 33, 37, 47, 56, 71, 82, 118, 130, 137, 147, 158, 161, 186, 192, 207, 233, 234, 235, 236, 237, 239, 241], "pytorch_model": [7, 235], "00001": [7, 232], "00002": [7, 232], "embed_token": 7, "241": 7, "Not": [7, 230], "onli": [7, 8, 11, 23, 41, 42, 43, 48, 61, 76, 89, 103, 112, 117, 119, 121, 131, 138, 150, 154, 156, 158, 161, 167, 169, 171, 183, 190, 191, 192, 194, 197, 198, 199, 200, 206, 219, 232, 234, 235, 236, 238, 239, 240, 241], "doe": [7, 30, 38, 41, 55, 72, 82, 117, 123, 134, 150, 154, 156, 157, 162, 166, 186, 190, 192, 194, 217, 232, 233, 235, 240], "fewer": [7, 150], "sinc": [7, 9, 12, 42, 43, 147, 161, 190, 192, 228, 233, 235, 237, 239, 240], "mismatch": 7, "name": [7, 8, 9, 11, 13, 18, 20, 21, 22, 28, 40, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 166, 170, 172, 184, 190, 191, 192, 193, 194, 196, 207, 208, 209, 210, 211, 217, 218, 219, 220, 232, 233, 235, 237, 239, 240], "caus": [7, 118, 183], "try": [7, 9, 233, 235, 236, 237, 241], "same": [7, 9, 24, 32, 58, 59, 65, 66, 67, 71, 77, 78, 82, 90, 91, 92, 93, 104, 105, 113, 114, 118, 130, 132, 139, 140, 141, 149, 155, 157, 158, 162, 164, 177, 179, 180, 186, 194, 198, 211, 217, 229, 232, 233, 235, 237, 238, 239, 240, 241], "As": [7, 9, 10, 11, 167, 227, 235, 239, 241], "re": [7, 9, 164, 179, 192, 227, 228, 229, 230, 233, 235, 236, 238, 239], "care": [7, 147, 190, 192, 235, 237, 238], "end": [7, 10, 23, 44, 55, 98, 118, 137, 184, 186, 225, 227, 228, 233, 237, 238, 240], "number": [7, 10, 38, 40, 41, 48, 57, 58, 59, 61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 154, 156, 158, 160, 188, 190, 191, 192, 195, 202, 215, 216, 219, 232, 236, 238, 239], "just": [7, 227, 228, 229, 232, 233, 234, 236, 237, 238, 239, 240], "save": [7, 10, 11, 154, 156, 159, 161, 190, 191, 192, 194, 198, 206, 211, 225, 229, 232, 233, 234, 235, 237, 238, 239, 240], "less": [7, 235, 236, 237, 239, 241], "prone": 7, "manag": [7, 39, 168, 207, 214, 233], "invari": 7, "accept": [7, 9, 189, 234, 236, 239, 241], "multipl": [7, 9, 10, 18, 22, 23, 28, 34, 38, 39, 43, 150, 154, 156, 157, 158, 162, 167, 207, 208, 209, 210, 211, 216, 230, 236, 237], "sourc": [7, 9, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 233, 234, 235, 240], "worri": [7, 230, 233, 236], "explicitli": [7, 166, 227, 238], "convert": [7, 18, 20, 22, 28, 30, 31, 35, 38, 42, 43, 47, 51, 56, 137, 190, 233, 235, 240, 241], "time": [7, 47, 50, 71, 72, 82, 118, 123, 130, 161, 173, 186, 208, 210, 216, 229, 232, 233, 234, 235, 237, 241], "produc": [7, 194, 229, 240, 241], "back": [7, 37, 168, 190, 230, 234, 238, 241], "form": [7, 9, 10, 37, 42, 43, 232], "One": [7, 240], "advantag": [7, 173, 177, 229, 238], "being": [7, 43, 190, 191, 192, 196, 220, 240, 241], "should": [7, 9, 10, 16, 18, 20, 21, 22, 23, 24, 28, 30, 31, 33, 41, 46, 49, 50, 51, 52, 53, 54, 55, 56, 65, 66, 67, 76, 77, 78, 81, 83, 89, 90, 91, 92, 93, 98, 99, 103, 104, 105, 108, 112, 113, 114, 117, 119, 120, 121, 122, 123, 125, 131, 132, 133, 137, 138, 139, 140, 141, 142, 147, 150, 154, 156, 158, 166, 171, 172, 173, 177, 189, 205, 207, 208, 209, 210, 211, 226, 227, 234, 235, 236, 237, 238, 239, 240, 241], "abl": [7, 10, 235, 236, 240], "post": [7, 158, 212, 216, 229, 235, 237, 240, 241], "tool": [7, 23, 24, 43, 117, 207, 234, 235, 236], "quantiz": [7, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 167, 192, 200, 225, 226, 228, 230, 236, 241], "eval": [7, 225, 227, 240], "without": [7, 9, 11, 150, 171, 226, 227, 229, 233, 235, 238, 239, 240], "code": [7, 10, 62, 63, 64, 65, 66, 67, 68, 69, 70, 154, 156, 207, 223, 227, 234, 236], "chang": [7, 8, 9, 11, 18, 20, 50, 54, 56, 192, 226, 232, 235, 236, 237, 238, 239, 240, 241], "OR": [7, 30], "script": [7, 11, 230, 232, 234, 235, 236, 237], "wai": [7, 9, 38, 42, 43, 171, 232, 233, 234, 235, 236, 237], "surround": [7, 10, 227], "load_checkpoint": [7, 10, 190, 191, 192, 193], "save_checkpoint": [7, 10, 11, 190, 191, 192], "map": [7, 18, 20, 21, 22, 24, 28, 30, 31, 33, 38, 39, 40, 41, 46, 49, 50, 51, 52, 53, 54, 56, 75, 88, 98, 102, 127, 130, 135, 146, 170, 184, 185, 190, 194, 196, 207, 208, 209, 210, 211, 212, 216, 233, 234, 235, 238], "appli": [7, 10, 33, 38, 40, 42, 43, 46, 51, 56, 65, 66, 67, 68, 69, 70, 72, 76, 77, 78, 79, 80, 82, 83, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 128, 129, 131, 132, 136, 138, 139, 140, 141, 142, 150, 151, 152, 154, 155, 156, 157, 162, 171, 172, 213, 227, 228, 239, 241], "permut": 7, "certain": [7, 9, 216, 230, 233], "ensur": [7, 9, 15, 37, 42, 43, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 190, 192, 197, 227, 234, 236], "behav": 7, "further": [7, 158, 179, 229, 232, 234, 238, 239, 240, 241], "illustr": [7, 51, 56, 237], "whilst": [7, 228, 239], "other": [7, 10, 12, 20, 24, 39, 176, 192, 198, 216, 228, 229, 234, 236, 237, 238, 239, 240], "phi3": [7, 130, 131, 132, 134, 135, 136, 193, 232], "own": [7, 24, 206, 215, 232, 233, 234, 235, 237, 238], "found": [7, 8, 9, 11, 151, 152, 190, 191, 192, 229, 232, 238, 241], "folder": [7, 233], "three": [7, 10, 71, 82, 98, 118, 130, 137, 175, 176, 178, 179, 230, 236], "read": [7, 190, 191, 192, 227, 239], "write": [7, 10, 190, 191, 192, 208, 233, 234, 236], "compat": [7, 190, 192, 239, 240], "transform": [7, 10, 18, 20, 22, 38, 40, 42, 43, 46, 47, 49, 51, 52, 53, 56, 61, 65, 66, 67, 72, 76, 77, 78, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 125, 131, 132, 133, 137, 138, 139, 140, 141, 142, 154, 155, 156, 157, 158, 160, 164, 188, 213, 238, 239, 240], "framework": [7, 10, 227], "mention": [7, 235, 241], "assum": [7, 21, 32, 33, 40, 51, 75, 88, 102, 127, 135, 146, 150, 152, 154, 156, 157, 160, 162, 163, 169, 184, 194, 196, 197, 233, 235, 238], "checkpoint_dir": [7, 9, 190, 191, 192, 235, 237, 240], "necessari": [7, 42, 43, 207, 208, 209, 210, 211, 233, 238], "easiest": [7, 235, 236], "sure": [7, 9, 233, 235, 236, 237, 238, 239, 240, 241], "everyth": [7, 10, 227, 230, 236], "follow": [7, 10, 22, 23, 24, 28, 30, 31, 38, 41, 42, 43, 50, 137, 150, 155, 160, 177, 188, 192, 193, 194, 204, 211, 216, 225, 226, 229, 232, 234, 235, 236, 237, 238, 239, 240, 241], "flow": [7, 38, 40, 41, 240, 241], "By": [7, 137, 229, 232, 238, 239, 240, 241], "safetensor": [7, 190, 232], "output_dir": [7, 9, 190, 191, 192, 216, 235, 237, 238, 240, 241], "here": [7, 8, 9, 11, 17, 49, 51, 56, 151, 152, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "argument": [7, 9, 12, 19, 21, 26, 29, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 150, 189, 198, 203, 207, 208, 210, 211, 213, 232, 233, 234, 238, 240], "snippet": 7, "explain": [7, 239], "setup": [7, 9, 10, 150, 154, 155, 156, 157, 162, 164, 195, 216, 232, 234, 235, 238, 241], "_component_": [7, 8, 9, 11, 12, 39, 47, 50, 55, 216, 229, 233, 234, 235, 237, 238, 239, 240], "fullmodelhfcheckpoint": [7, 235], "directori": [7, 9, 51, 190, 191, 192, 208, 210, 211, 216, 232, 233, 234, 235, 236, 237], "sort": [7, 190, 192], "so": [7, 9, 41, 51, 158, 190, 226, 227, 233, 235, 236, 237, 238, 239, 240, 241], "order": [7, 8, 10, 190, 192, 210, 211, 236], "matter": [7, 190, 192, 232, 238], "checkpoint_fil": [7, 9, 11, 190, 191, 192, 235, 237, 238, 240, 241], "restart": [7, 232], "previou": [7, 41, 190, 191, 192], "more": [7, 9, 10, 24, 38, 40, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 82, 137, 149, 152, 158, 161, 165, 171, 189, 192, 207, 211, 213, 215, 227, 228, 229, 230, 232, 234, 235, 236, 237, 238, 239, 240, 241], "next": [7, 41, 55, 61, 158, 188, 219, 228, 237, 241], "section": [7, 10, 199, 225, 235, 237, 239, 241], "recipe_checkpoint": [7, 190, 191, 192, 240], "null": [7, 9, 240], "usual": [7, 152, 180, 190, 201, 211, 232, 235, 238, 239], "model_typ": [7, 190, 191, 192, 235, 237, 240], "resume_from_checkpoint": [7, 190, 191, 192], "fals": [7, 9, 18, 20, 22, 23, 28, 30, 31, 32, 38, 39, 40, 41, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 136, 137, 138, 139, 140, 141, 142, 150, 154, 156, 157, 162, 163, 164, 167, 168, 171, 180, 183, 190, 191, 192, 204, 216, 217, 232, 233, 234, 235, 237, 238, 240, 241], "requir": [7, 9, 33, 34, 39, 42, 43, 44, 51, 55, 56, 71, 82, 98, 118, 130, 137, 163, 190, 192, 194, 200, 203, 204, 206, 207, 210, 211, 215, 216, 226, 229, 232, 233, 234, 236, 239, 240, 241], "param": [7, 10, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141, 165, 167, 169, 170, 172, 190, 238, 240, 241], "directli": [7, 9, 10, 12, 42, 43, 47, 50, 51, 55, 175, 189, 190, 232, 235, 236, 237, 238, 239, 240, 241], "out": [7, 9, 10, 40, 46, 47, 49, 50, 52, 53, 188, 190, 191, 201, 225, 227, 228, 229, 230, 232, 233, 235, 236, 237, 238, 239, 241], "case": [7, 10, 11, 23, 24, 59, 60, 61, 158, 190, 194, 197, 200, 206, 208, 213, 227, 232, 233, 234, 235, 237, 238, 239, 241], "discrep": [7, 190], "along": [7, 238], "github": [7, 12, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 137, 139, 140, 141, 150, 151, 152, 160, 161, 171, 175, 176, 177, 178, 179, 226, 234, 235, 237], "repositori": [7, 38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 81, 228, 229, 235, 236], "fullmodelmetacheckpoint": [7, 237, 240], "current": [7, 41, 72, 76, 89, 103, 112, 119, 121, 123, 131, 134, 138, 149, 150, 152, 154, 156, 157, 162, 177, 191, 192, 198, 200, 202, 208, 210, 212, 215, 229, 230, 234, 236, 237, 239, 240], "test": [7, 9, 10, 227, 228, 229, 233, 239], "complet": [7, 10, 41, 48, 55, 135, 176, 233, 234, 235, 236, 237, 239], "written": [7, 9, 10, 190, 191, 207, 208, 209, 210, 211, 227], "begin": [7, 41, 55, 82, 118, 137, 158, 184, 233, 237, 241], "partit": [7, 190, 241], "ha": [7, 50, 82, 118, 154, 155, 156, 158, 161, 162, 166, 168, 169, 172, 180, 192, 194, 217, 218, 233, 234, 235, 236, 237, 238, 239, 241], "standard": [7, 19, 30, 42, 43, 47, 71, 82, 83, 89, 98, 99, 103, 108, 112, 118, 119, 121, 123, 125, 130, 131, 133, 137, 138, 142, 150, 209, 227, 233, 235, 237], "key_1": [7, 192], "weight_1": 7, "key_2": 7, "weight_2": 7, "mid": 7, "chekpoint": 7, "middl": [7, 164, 235, 239], "inform": [7, 137, 207, 211, 213, 227, 230, 232, 235, 236], "subsequ": [7, 10, 158, 188], "recipe_st": [7, 190, 191, 192], "pt": [7, 11, 190, 191, 192, 235, 237, 240], "epoch": [7, 10, 11, 160, 190, 191, 192, 232, 233, 235, 236, 237, 240], "optim": [7, 9, 10, 34, 39, 42, 72, 82, 123, 134, 160, 175, 177, 178, 179, 192, 194, 196, 199, 212, 216, 228, 229, 230, 233, 235, 236, 237, 238, 241], "etc": [7, 10, 190, 199, 236], "prevent": [7, 41, 175, 232, 239], "flood": 7, "overwritten": 7, "note": [7, 9, 21, 76, 137, 154, 156, 162, 166, 194, 212, 215, 217, 228, 229, 233, 234, 235, 238, 239, 240, 241], "updat": [7, 9, 10, 24, 149, 150, 175, 177, 187, 194, 216, 226, 233, 235, 236, 237, 238, 239, 240, 241], "hf_model_0001_0": [7, 235], "hf_model_0002_0": [7, 235], "both": [7, 39, 164, 165, 172, 232, 235, 238, 240, 241], "adapt": [7, 65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132, 162, 164, 166, 167, 168, 169, 170, 190, 191, 192, 206, 228, 233, 235, 238, 241], "merg": [7, 12, 13, 137, 146, 190, 235, 237, 241], "would": [7, 9, 11, 24, 41, 154, 156, 158, 162, 176, 226, 233, 234, 235, 238, 239, 241], "addition": [7, 179, 183, 184, 215, 234, 238], "option": [7, 9, 10, 18, 20, 21, 22, 28, 36, 38, 40, 41, 42, 43, 44, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 65, 66, 67, 71, 75, 76, 77, 78, 82, 83, 88, 89, 90, 91, 92, 93, 98, 99, 102, 103, 104, 105, 108, 112, 113, 114, 118, 119, 120, 121, 122, 127, 130, 131, 132, 135, 137, 138, 139, 140, 141, 142, 146, 147, 150, 152, 154, 155, 156, 157, 158, 159, 162, 171, 172, 173, 174, 177, 181, 183, 186, 188, 190, 191, 192, 195, 197, 200, 207, 208, 211, 215, 216, 219, 220, 221, 226, 227, 232, 234, 235], "save_adapter_weights_onli": 7, "choos": [7, 47, 238], "primari": [7, 9, 10, 42, 43, 230, 236], "want": [7, 9, 10, 11, 12, 38, 42, 43, 165, 219, 226, 230, 232, 233, 234, 235, 236, 237, 238, 239], "resum": [7, 10, 160, 190, 191, 192, 241], "initi": [7, 10, 14, 39, 41, 62, 63, 64, 73, 74, 84, 85, 86, 87, 100, 101, 109, 110, 111, 124, 126, 143, 144, 145, 175, 194, 203, 204, 217, 229, 236, 238, 241], "frozen": [7, 163, 175, 238, 239, 241], "base": [7, 12, 23, 24, 40, 65, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 79, 80, 82, 83, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 125, 128, 129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 152, 160, 167, 168, 170, 171, 172, 174, 175, 176, 178, 179, 190, 198, 206, 208, 217, 220, 225, 233, 235, 236, 237, 238, 239, 241], "well": [7, 9, 10, 227, 232, 234, 235, 237, 239, 241], "learnt": [7, 233, 235], "someth": [7, 10, 11, 233, 235, 240], "NOT": [7, 72, 123], "refer": [7, 9, 10, 151, 152, 158, 161, 168, 174, 175, 176, 177, 178, 179, 207, 227, 238, 239, 240], "adapter_checkpoint": [7, 190, 191, 192], "adapter_0": [7, 235], "now": [7, 194, 196, 229, 233, 234, 235, 236, 237, 238, 240, 241], "knowledg": 7, "creat": [7, 9, 12, 22, 24, 41, 43, 47, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 77, 78, 79, 80, 84, 85, 86, 87, 90, 91, 92, 93, 94, 95, 96, 97, 100, 101, 104, 105, 106, 107, 109, 110, 111, 113, 114, 115, 116, 120, 122, 124, 126, 128, 129, 132, 134, 136, 139, 140, 141, 143, 144, 145, 149, 158, 160, 189, 190, 191, 192, 196, 207, 208, 210, 232, 233, 234, 235, 241], "simpl": [7, 10, 158, 179, 225, 234, 236, 238, 240, 241], "forward": [7, 10, 58, 59, 60, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 163, 164, 167, 175, 176, 177, 178, 179, 199, 216, 237, 238, 239, 241], "modeltyp": [7, 190, 191, 192], "llama2_13b": [7, 90], "right": [7, 33, 190, 235, 237, 238], "pytorch_fil": 7, "00003": 7, "torchtune_sd": 7, "load_state_dict": [7, 162, 163, 164, 171, 194, 217, 238], "successfulli": [7, 232, 236], "vocab": [7, 12, 137, 146, 154, 156, 162, 163, 237], "70": [7, 100], "x": [7, 32, 58, 59, 60, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 162, 163, 164, 167, 201, 214, 219, 238, 240, 241], "randint": 7, "1": [7, 10, 32, 33, 34, 35, 41, 53, 58, 59, 71, 82, 83, 89, 98, 99, 103, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 123, 125, 130, 131, 133, 137, 138, 139, 140, 142, 143, 144, 150, 154, 156, 158, 160, 161, 175, 176, 177, 178, 179, 183, 184, 186, 191, 193, 201, 204, 207, 210, 211, 214, 215, 219, 228, 232, 233, 235, 236, 238, 239, 240, 241], "no_grad": 7, "6": [7, 32, 33, 34, 35, 41, 72, 76, 151, 158, 201, 229, 240, 241], "3989": 7, "9": [7, 32, 33, 34, 158, 201, 235, 240, 241], "0531": 7, "3": [7, 32, 33, 34, 35, 41, 61, 98, 117, 132, 134, 135, 137, 158, 193, 200, 201, 214, 221, 228, 229, 232, 233, 235, 236, 237, 240, 241], "2375": 7, "5": [7, 9, 32, 33, 34, 35, 158, 160, 175, 179, 180, 235, 236, 237, 239], "2822": 7, "4": [7, 9, 32, 33, 34, 35, 61, 150, 158, 200, 201, 222, 227, 229, 232, 234, 235, 237, 238, 239, 240, 241], "4872": 7, "7469": 7, "8": [7, 32, 33, 34, 46, 49, 51, 52, 56, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 90, 91, 92, 93, 94, 95, 96, 97, 104, 105, 106, 107, 108, 113, 114, 115, 116, 120, 122, 128, 129, 132, 136, 137, 139, 140, 141, 158, 161, 235, 238, 239, 240, 241], "6737": 7, "11": [7, 32, 33, 34, 158, 235, 240, 241], "0023": 7, "8235": 7, "6819": 7, "2424": 7, "0109": 7, "6915": 7, "7": [7, 32, 33, 34, 35, 158, 177, 188], "3618": 7, "1628": 7, "8594": 7, "5857": 7, "1151": 7, "7808": 7, "2322": 7, "8850": 7, "9604": 7, "7624": 7, "6040": 7, "3159": 7, "5849": 7, "8039": 7, "9322": 7, "2010": [7, 158], "6824": 7, "8929": 7, "8465": 7, "3794": 7, "3500": 7, "6145": 7, "5931": 7, "do": [7, 8, 10, 23, 33, 40, 51, 137, 171, 186, 207, 211, 217, 232, 233, 234, 235, 236, 237, 238, 239, 240], "find": [7, 8, 10, 11, 175, 230, 232, 235, 236, 238, 239], "hope": 7, "deeper": [7, 228, 236, 239], "insight": [7, 229, 235], "happi": [7, 235], "thi": [8, 9, 10, 11, 12, 16, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 71, 72, 76, 82, 83, 89, 98, 99, 103, 108, 112, 117, 118, 119, 121, 123, 125, 130, 131, 133, 134, 135, 137, 138, 142, 147, 150, 152, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 168, 171, 172, 174, 175, 176, 177, 179, 183, 184, 186, 188, 189, 190, 191, 192, 194, 197, 199, 201, 204, 206, 207, 208, 210, 211, 212, 213, 215, 217, 219, 220, 225, 226, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "torchtun": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 226, 228, 229, 230, 233, 234, 236, 239], "start": [8, 10, 11, 186, 193, 207, 226, 227, 233, 234, 235, 236, 239, 240], "cometlogg": 8, "checkpoint": [8, 9, 10, 159, 162, 164, 184, 190, 191, 192, 193, 194, 195, 211, 213, 217, 227, 228, 229, 232, 237, 238, 240, 241], "workspac": [8, 11, 207], "seen": [8, 11, 238, 241], "screenshot": [8, 11], "below": [8, 11, 33, 152, 189, 234, 237, 238, 241], "instal": [8, 9, 11, 204, 207, 210, 211, 225, 232, 234, 235, 236, 237, 238, 239, 240, 241], "comet_ml": [8, 207], "packag": [8, 11, 207, 210, 211, 226, 234], "featur": [8, 10, 11, 226, 227, 228, 229, 230, 235, 236, 239], "via": [8, 9, 11, 42, 47, 50, 55, 167, 190, 238, 241], "pip": [8, 11, 207, 210, 211, 226, 235, 237, 239], "login": [8, 11, 207, 211, 232, 235], "data": [8, 16, 17, 18, 20, 21, 22, 23, 24, 25, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 81, 82, 118, 137, 146, 158, 175, 178, 187, 199, 207, 208, 209, 210, 211, 228, 229, 234, 235, 239, 240, 241], "command": [8, 10, 11, 226, 229, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241], "add": [8, 9, 11, 38, 41, 44, 55, 71, 82, 98, 117, 118, 130, 137, 158, 165, 184, 186, 192, 193, 234, 235, 237, 238, 241], "built": [8, 9, 11, 54, 226, 230, 233, 236, 241], "metric_logg": [8, 9, 10, 11], "metric_log": [8, 9, 11, 207, 208, 209, 210, 211], "project": [8, 11, 61, 65, 66, 67, 72, 76, 83, 87, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 126, 131, 132, 138, 141, 142, 147, 150, 154, 158, 162, 165, 171, 172, 193, 198, 207, 211, 225, 233, 238, 239, 241], "experiment_nam": [8, 207], "my": [8, 219, 232, 233, 234, 235, 237], "experi": [8, 9, 207, 211, 225, 227, 233, 237, 238], "automat": [8, 9, 11, 12, 46, 47, 232, 235, 241], "grab": [8, 11, 237], "hyperparamet": [8, 179, 194, 227, 236, 238, 241], "tab": [8, 11], "actual": [8, 9, 11, 18, 20, 38, 42, 43, 46, 49, 50, 52, 54, 56, 71, 82, 98, 118, 130, 137, 229, 233, 240], "asset": 8, "artifact": [8, 11, 216], "click": [8, 11], "after": [8, 10, 24, 43, 51, 56, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 149, 150, 151, 153, 154, 156, 157, 162, 164, 171, 180, 206, 207, 208, 209, 210, 211, 229, 233, 235, 237, 240, 241], "pars": [9, 12, 13, 185, 230, 233, 236], "effect": [9, 179, 239, 240], "cli": [9, 11, 13, 14, 226, 228, 235, 236, 239], "prerequisit": [9, 233, 234, 235, 236, 237, 238, 240, 241], "Be": [9, 233, 235, 236, 237, 238, 239, 240, 241], "familiar": [9, 233, 235, 236, 237, 238, 240, 241], "fundament": [9, 240], "There": [9, 37, 59, 233, 236, 237, 238, 239], "entri": [9, 10, 33, 230, 236, 239], "point": [9, 10, 30, 31, 47, 186, 230, 234, 235, 236, 237, 238, 240, 241], "locat": [9, 232, 234, 237, 238, 240, 241], "thei": [9, 10, 39, 61, 154, 156, 158, 164, 172, 198, 232, 233, 234, 238, 239, 240], "truth": [9, 161, 235, 237], "reproduc": [9, 207], "overridden": [9, 147, 216], "quick": 9, "experiment": 9, "serv": [9, 18, 20, 22, 28, 49, 50, 51, 52, 53, 56, 186, 189, 234, 238], "particular": [9, 38, 39, 71, 82, 98, 118, 130, 137, 189, 228, 234, 238, 241], "seed": [9, 10, 11, 215, 236, 240], "shuffl": [9, 41, 240], "devic": [9, 10, 171, 194, 197, 199, 220, 230, 232, 233, 235, 236, 237, 238, 239, 241], "cuda": [9, 197, 199, 216, 220, 226, 235, 241], "dtype": [9, 10, 149, 150, 154, 155, 156, 157, 159, 162, 164, 197, 214, 218, 235, 239, 240, 241], "fp32": [9, 161, 239, 240, 241], "enable_fsdp": 9, "mani": [9, 41, 228, 229, 234, 235], "object": [9, 12, 13, 16, 61, 82, 118, 137, 150, 175, 179, 189, 200, 233], "keyword": [9, 12, 38, 40, 42, 43, 44, 46, 47, 48, 50, 51, 55, 56, 57, 159, 233, 234], "loss": [9, 10, 23, 24, 40, 42, 43, 46, 47, 49, 50, 52, 53, 161, 175, 176, 177, 178, 179, 236, 238, 241], "subfield": 9, "dotpath": [9, 75, 88, 102, 127, 135, 146, 234], "wish": [9, 217, 234], "exact": [9, 12, 235], "path": [9, 10, 11, 12, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 183, 184, 185, 190, 191, 192, 216, 232, 233, 234, 235, 237, 238], "normal": [9, 38, 41, 71, 82, 98, 118, 130, 137, 148, 150, 151, 154, 155, 156, 157, 161, 183, 233, 234, 238, 240, 241], "python": [9, 137, 207, 211, 215, 221, 223, 232, 234, 235, 240], "alpaca_dataset": [9, 45, 234], "custom": [9, 10, 16, 21, 24, 38, 40, 42, 43, 47, 50, 51, 55, 56, 75, 88, 102, 127, 135, 146, 213, 227, 228, 229, 230, 232, 236, 237, 238, 239], "train_on_input": [9, 18, 20, 22, 28, 30, 31, 38, 39, 40, 45, 46, 47, 49, 50, 51, 52, 53, 54, 56, 233, 234], "onc": [9, 24, 168, 235, 236, 237, 238, 241], "ve": [9, 149, 229, 232, 233, 234, 235, 237, 238, 239], "instanc": [9, 12, 39, 40, 89, 103, 112, 119, 121, 131, 138, 139, 140, 143, 144, 147, 159, 169, 170, 238], "cfg": [9, 10, 13, 14, 15], "under": [9, 216, 234, 239, 241], "preced": [9, 12, 232, 237, 238], "throw": 9, "notic": [9, 58, 59, 60, 158, 233, 234, 238], "miss": [9, 171, 172, 216, 238], "posit": [9, 12, 41, 58, 59, 60, 61, 72, 76, 119, 121, 123, 125, 131, 133, 149, 150, 152, 154, 155, 156, 157, 158, 162, 163, 237], "anoth": [9, 43, 207, 235, 239], "def": [9, 10, 11, 14, 51, 56, 189, 193, 233, 234, 238, 241], "dictconfig": [9, 10, 12, 13, 14, 15, 207, 211, 216], "arg": [9, 12, 25, 60, 148, 154, 156, 159, 164, 166, 181, 182, 187, 209, 216, 229, 240], "tupl": [9, 12, 24, 34, 60, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 149, 158, 159, 173, 174, 175, 176, 177, 178, 179, 180, 182, 186, 189, 202, 216, 217, 218], "kwarg": [9, 12, 25, 146, 148, 155, 157, 159, 164, 166, 181, 182, 187, 203, 207, 208, 209, 210, 211, 213, 216, 234], "str": [9, 12, 13, 18, 20, 21, 22, 23, 24, 28, 30, 31, 33, 34, 35, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 159, 163, 164, 166, 167, 169, 170, 171, 172, 181, 182, 183, 184, 185, 190, 191, 192, 193, 194, 195, 197, 199, 200, 203, 205, 207, 208, 209, 210, 211, 215, 216, 217, 218, 220, 221, 222, 233, 234, 239], "mean": [9, 150, 151, 154, 155, 156, 157, 162, 173, 206, 232, 233, 234, 236, 238, 240], "pass": [9, 12, 23, 24, 38, 39, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 147, 150, 154, 156, 159, 162, 168, 172, 177, 184, 189, 192, 197, 198, 199, 203, 206, 207, 210, 211, 213, 216, 232, 233, 234, 238, 240, 241], "d": [9, 23, 51, 149, 150, 154, 156, 162, 230, 232, 233, 238, 240], "llama2_token": [9, 233, 235], "llama2token": [9, 88], "modeltoken": [9, 23, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 186, 233, 234], "bool": [9, 18, 20, 22, 23, 28, 30, 31, 32, 38, 40, 41, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 61, 65, 66, 67, 68, 69, 70, 71, 72, 76, 77, 78, 79, 80, 82, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 128, 129, 130, 131, 132, 136, 137, 138, 139, 140, 141, 142, 150, 154, 155, 156, 157, 159, 162, 164, 167, 171, 172, 173, 180, 182, 183, 184, 186, 189, 190, 191, 192, 198, 199, 203, 204, 206, 207, 210, 213, 216, 217, 222, 233, 239, 241], "max_seq_len": [9, 12, 33, 36, 38, 40, 41, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 71, 72, 75, 76, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 119, 121, 123, 125, 127, 130, 131, 133, 135, 137, 138, 142, 146, 149, 150, 152, 154, 156, 186, 188, 233, 234, 240], "int": [9, 11, 33, 34, 35, 36, 38, 40, 41, 48, 51, 56, 57, 58, 59, 60, 61, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 77, 78, 79, 80, 82, 83, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 102, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 118, 119, 120, 121, 122, 123, 125, 127, 128, 129, 130, 131, 132, 133, 135, 136, 137, 138, 139, 140, 141, 142, 146, 149, 150, 151, 152, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 167, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 195, 198, 202, 206, 207, 208, 209, 210, 211, 213, 215, 216, 219, 232, 233, 234, 238, 239, 241], "512": [9, 61, 234, 241], "instructdataset": [9, 234], "alreadi": [9, 18, 22, 28, 49, 51, 52, 53, 56, 150, 162, 193, 203, 226, 232, 234, 235, 238], "overwrit": [9, 192, 217, 226, 232], "duplic": [9, 10, 227, 232], "sometim": 9, "than": [9, 37, 149, 150, 154, 156, 158, 175, 189, 192, 193, 218, 222, 233, 234, 235, 236, 237, 238, 239, 241], "resolv": [9, 13, 236], "alpaca": [9, 39, 45, 46, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141, 234], "disklogg": 9, "log_dir": [9, 208, 210, 211], "conveni": [9, 10, 232], "verifi": [9, 197, 198, 220, 233, 236, 238], "properli": [9, 171, 204, 232], "wa": [9, 59, 60, 61, 158, 171, 233, 238, 240, 241], "cp": [9, 226, 232, 233, 235, 236, 237, 240], "7b_lora_single_devic": [9, 235, 236, 238, 241], "my_config": [9, 232], "discuss": [9, 235, 236, 237, 238], "guidelin": 9, "while": [9, 10, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141, 147, 163, 227, 229, 235, 240, 241], "mai": [9, 11, 47, 158, 163, 198, 217, 228, 229, 233, 234, 236, 238, 239], "tempt": 9, "put": [9, 10, 230, 236, 238, 240], "much": [9, 163, 179, 235, 237, 238, 240, 241], "give": [9, 234, 238, 239], "maximum": [9, 33, 36, 38, 40, 41, 48, 57, 58, 59, 61, 72, 75, 76, 83, 88, 89, 98, 99, 102, 103, 108, 112, 119, 121, 123, 125, 127, 131, 133, 135, 138, 142, 149, 150, 152, 154, 156, 188, 232], "flexibl": [9, 39, 234, 239], "switch": 9, "encourag": [9, 179, 238, 239], "clariti": 9, "significantli": [9, 175, 228, 229, 239], "easier": [9, 235, 236], "dont": 9, "slimorca_dataset": 9, "privat": 9, "expos": [9, 10, 192, 230, 233, 236], "parent": [9, 232], "modul": [9, 12, 51, 56, 58, 59, 60, 61, 121, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 193, 195, 196, 198, 206, 212, 213, 215, 234, 236, 238, 241], "__init__": [9, 10, 51, 56, 238, 241], "py": [9, 12, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 137, 139, 140, 141, 149, 150, 151, 152, 160, 175, 176, 177, 178, 179, 232, 235, 237], "guarante": 9, "stabil": [9, 161, 227, 229, 239, 240, 241], "underscor": 9, "_alpaca": 9, "itself": 9, "k1": [9, 10], "v1": [9, 10, 57], "k2": [9, 10], "v2": [9, 10, 207, 234], "lora": [9, 65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 167, 168, 171, 172, 190, 206, 225, 227, 230, 233, 236, 237], "lora_finetune_single_devic": [9, 228, 232, 233, 235, 236, 237, 238, 239, 241], "home": 9, "my_model_checkpoint": 9, "file_1": 9, "file_2": 9, "my_tokenizer_path": 9, "assign": [9, 42, 43], "nest": 9, "dot": 9, "notat": [9, 150, 152, 154, 156, 162, 173, 174, 201], "flag": [9, 10, 23, 40, 46, 47, 49, 50, 52, 53, 189, 192, 198, 232, 239, 241], "bitsandbyt": [9, 239], "pagedadamw8bit": [9, 239], "delet": 9, "foreach": [9, 38], "pytorch": [9, 10, 82, 154, 156, 159, 161, 171, 189, 204, 210, 213, 215, 216, 225, 226, 227, 229, 235, 237, 238, 239, 240, 241], "llama3": [9, 38, 51, 56, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 131, 161, 193, 198, 219, 225, 228, 229, 232, 234, 235, 239], "8b_full": [9, 232, 234], "adamw": [9, 238, 239], "lr": [9, 160, 239], "2e": [9, 239], "fuse": [9, 162, 163, 164, 212, 240], "nproc_per_nod": [9, 229, 234, 237, 238, 240], "full_finetune_distribut": [9, 232, 234, 235, 236], "core": [10, 42, 43, 227, 230, 234, 236, 241], "i": [10, 23, 81, 117, 150, 154, 155, 156, 157, 158, 159, 162, 170, 194, 219, 234, 235, 237, 239, 240, 241], "structur": [10, 16, 22, 25, 28, 30, 31, 38, 47, 71, 82, 98, 102, 118, 130, 135, 137, 146, 188, 233, 234, 235, 240], "new": [10, 22, 28, 30, 31, 46, 48, 49, 51, 52, 53, 124, 149, 163, 164, 193, 207, 208, 210, 233, 235, 236, 237, 238, 241], "user": [10, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 37, 38, 42, 43, 47, 50, 71, 75, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 119, 121, 123, 125, 127, 130, 131, 133, 135, 138, 142, 146, 150, 186, 230, 233, 234, 236, 240], "thought": [10, 227, 230, 236, 241], "target": [10, 179, 227], "pipelin": [10, 227, 229], "llm": [10, 162, 164, 225, 226, 227, 228, 230, 234, 235, 237, 238], "eg": [10, 154, 156, 162, 190, 227], "meaning": [10, 227, 235], "fsdp": [10, 189, 194, 198, 206, 227, 230, 236, 237], "activ": [10, 61, 147, 195, 199, 205, 213, 216, 227, 228, 229, 240, 241], "gradient": [10, 206, 212, 216, 227, 228, 229, 235, 237, 238, 241], "accumul": [10, 212, 216, 227, 228, 229], "mix": [10, 148, 232, 234, 235, 239], "precis": [10, 148, 159, 197, 227, 228, 229, 236, 241], "given": [10, 12, 21, 33, 37, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 137, 167, 168, 174, 176, 181, 182, 197, 200, 206, 212, 219, 220, 222, 227, 238], "complex": 10, "becom": [10, 158, 176, 226, 234], "harder": 10, "anticip": 10, "architectur": [10, 81, 117, 156, 158, 162, 164, 193, 232, 234], "methodolog": 10, "reason": [10, 219, 235, 239, 240], "possibl": [10, 41, 232, 234, 239], "trade": [10, 239], "off": [10, 24, 82, 118, 228, 229, 235, 240], "memori": [10, 39, 40, 41, 44, 48, 55, 57, 137, 154, 156, 159, 161, 171, 198, 199, 205, 206, 216, 225, 227, 228, 229, 230, 235, 236, 237, 240], "vs": [10, 176, 236], "qualiti": [10, 235, 238, 240], "believ": 10, "best": [10, 229, 233, 239], "suit": [10, 236, 239], "b": [10, 32, 33, 149, 150, 152, 154, 156, 157, 162, 167, 173, 174, 179, 201, 211, 238, 241], "fit": [10, 38, 40, 41, 48, 55, 57, 158, 175, 176, 234], "solut": [10, 176], "result": [10, 51, 61, 71, 82, 118, 130, 158, 161, 186, 188, 216, 228, 229, 235, 237, 238, 239, 240, 241], "meant": [10, 159, 194], "depend": [10, 11, 190, 216, 232, 234, 235, 238, 241], "level": [10, 42, 43, 137, 161, 187, 196, 206, 221, 227, 241], "expertis": 10, "routin": 10, "yourself": [10, 232, 237, 238], "exist": [10, 164, 207, 226, 232, 235, 236, 237, 241], "ad": [10, 24, 58, 59, 60, 118, 125, 158, 162, 163, 165, 183, 192, 193, 233, 234, 238, 239, 240, 241], "ones": 10, "modular": [10, 227], "build": [10, 55, 61, 72, 83, 99, 108, 123, 125, 142, 227, 237, 238], "block": [10, 41, 65, 66, 67, 72, 76, 77, 78, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 131, 132, 138, 139, 140, 141, 142, 171, 172, 227], "wandb": [10, 11, 211, 236], "log": [10, 13, 175, 176, 177, 178, 179, 199, 205, 207, 208, 209, 210, 211, 221, 235, 236, 237, 238, 239, 241], "fulli": [10, 39], "nativ": [10, 225, 227, 238, 240, 241], "correct": [10, 19, 49, 151, 152, 154, 156, 220, 227, 233, 234], "numer": [10, 56, 227, 229, 240], "pariti": [10, 227], "verif": 10, "extens": [10, 192, 227], "comparison": [10, 238, 241], "benchmark": [10, 215, 227, 235, 237, 238, 240], "limit": [10, 194, 234, 239, 240], "hidden": [10, 61, 147, 158], "behind": 10, "100": [10, 34, 35, 40, 46, 47, 49, 50, 52, 53, 161, 163, 219, 238, 241], "prefer": [10, 34, 42, 54, 175, 176, 177, 178, 179, 227, 232, 234, 239], "over": [10, 23, 43, 160, 175, 176, 227, 229, 232, 235, 238, 239, 241], "unnecessari": 10, "abstract": [10, 16, 21, 181, 182, 227, 236, 241], "No": [10, 192, 227], "inherit": [10, 227, 234], "go": [10, 61, 71, 81, 82, 117, 118, 130, 158, 186, 227, 234, 235, 236, 239, 241], "upon": [10, 39, 237], "figur": [10, 238, 241], "spectrum": 10, "decid": 10, "interact": [10, 225, 230, 236], "avail": [10, 57, 162, 164, 197, 204, 220, 227, 232, 235, 237, 238], "paradigm": [10, 137, 228, 230, 239], "consist": [10, 18, 22, 28, 51, 56, 57, 230, 236], "configur": [10, 40, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 76, 89, 98, 103, 112, 119, 130, 131, 138, 157, 207, 227, 228, 229, 230, 233, 236, 237, 238, 239, 240, 241], "paramet": [10, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 28, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 188, 189, 190, 191, 192, 194, 195, 196, 197, 198, 199, 200, 201, 203, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 225, 227, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241], "overrid": [10, 13, 14, 18, 22, 28, 49, 51, 52, 53, 56, 217, 230, 232, 235, 236, 237, 241], "togeth": [10, 41, 161, 211, 230, 236, 238, 239, 240], "valid": [10, 37, 171, 172, 174, 217, 218, 226, 230, 235, 236], "environ": [10, 204, 207, 220, 226, 230, 232, 234, 235, 236, 240], "logic": [10, 43, 182, 193, 227, 230, 236, 238], "api": [10, 11, 19, 26, 29, 30, 42, 43, 46, 51, 56, 68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 171, 207, 226, 230, 232, 233, 236, 237, 241], "closer": [10, 238], "monolith": [10, 227], "trainer": [10, 175, 176, 178, 179], "A": [10, 11, 18, 19, 22, 26, 28, 29, 30, 31, 33, 34, 35, 39, 41, 61, 71, 82, 118, 130, 137, 146, 150, 154, 155, 156, 157, 158, 159, 162, 167, 171, 173, 174, 175, 176, 177, 178, 179, 180, 183, 184, 186, 188, 189, 193, 194, 199, 200, 205, 206, 224, 225, 231, 232, 233, 238, 239, 240, 241], "wrapper": [10, 148, 183, 184, 194, 196, 232, 238], "around": [10, 38, 71, 82, 98, 118, 130, 137, 148, 183, 184, 199, 232, 233, 235, 238, 239, 240, 241], "extern": [10, 234], "primarili": [10, 39, 238], "eleutherai": [10, 57, 227, 238, 240], "har": [10, 227, 238, 240], "control": [10, 23, 40, 46, 47, 49, 50, 52, 53, 164, 168, 176, 207, 215, 229, 235, 239], "multi": [10, 38, 150, 171, 237], "stage": [10, 158], "distil": 10, "oper": [10, 137, 158, 168, 187, 215, 240], "turn": [10, 18, 22, 23, 28, 37, 38, 98, 233, 239], "dataload": [10, 41, 46, 49, 51, 52, 56], "applic": [10, 190, 191, 211], "clean": [10, 11, 45], "process": [10, 11, 42, 43, 44, 51, 55, 56, 61, 137, 158, 159, 202, 203, 215, 234, 236, 240, 241], "group": [10, 150, 202, 203, 207, 208, 209, 210, 211, 232, 237, 240], "init_process_group": [10, 203], "backend": [10, 232, 240], "gloo": 10, "els": [10, 211, 227, 241], "nccl": 10, "fullfinetunerecipedistribut": 10, "cleanup": 10, "stuff": 10, "carri": [10, 43], "relev": [10, 155, 157, 232, 235, 238, 239], "interfac": [10, 16, 21, 24, 25, 39, 166, 187, 234], "metric": [10, 236, 239, 240], "logger": [10, 205, 207, 208, 209, 210, 211, 221, 236], "self": [10, 11, 41, 51, 56, 65, 66, 67, 72, 76, 77, 78, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 125, 131, 132, 133, 138, 139, 140, 141, 142, 150, 154, 155, 156, 157, 161, 164, 166, 171, 172, 190, 193, 194, 234, 238, 241], "_devic": 10, "get_devic": 10, "_dtype": 10, "get_dtyp": 10, "ckpt_dict": 10, "wrap": [10, 164, 189, 195, 198, 206, 213, 233], "_model": [10, 194], "_setup_model": 10, "_token": [10, 234], "_setup_token": 10, "_optim": 10, "_setup_optim": 10, "_loss_fn": 10, "_setup_loss": 10, "_sampler": 10, "_dataload": 10, "_setup_data": 10, "backward": [10, 194, 196, 212, 216, 241], "zero_grad": 10, "curr_epoch": 10, "rang": [10, 163, 175, 177, 179, 215, 232, 237, 240], "epochs_run": [10, 11], "total_epoch": [10, 11], "idx": [10, 41], "enumer": 10, "_autocast": 10, "logit": [10, 161, 201, 219], "label": [10, 33, 34, 35, 38, 40, 41, 48, 53, 57, 161, 175, 179], "global_step": 10, "_log_every_n_step": 10, "_metric_logg": 10, "log_dict": [10, 207, 208, 209, 210, 211], "step": [10, 41, 42, 43, 51, 56, 154, 156, 160, 162, 173, 196, 207, 208, 209, 210, 211, 212, 216, 225, 228, 229, 235, 238, 240, 241], "learn": [10, 39, 160, 163, 164, 165, 176, 227, 228, 229, 230, 233, 234, 236, 237, 238, 239, 240, 241], "decor": [10, 14], "recipe_main": [10, 14], "fullfinetunerecip": 10, "wandblogg": [11, 238, 241], "Then": [11, 168, 236, 239], "tip": 11, "straggler": 11, "background": 11, "crash": 11, "otherwis": [11, 32, 33, 59, 60, 61, 158, 204, 207, 233, 240], "exit": [11, 226, 232], "resourc": [11, 207, 208, 209, 210, 211, 240], "kill": 11, "ps": 11, "aux": 11, "grep": 11, "awk": 11, "xarg": 11, "desir": [11, 38, 42, 43, 214, 233, 239], "suggest": 11, "approach": [11, 39, 234], "full_finetun": 11, "joinpath": 11, "_checkpoint": [11, 235], "_output_dir": [11, 190, 191, 192], "torchtune_model_": 11, "with_suffix": 11, "wandb_at": 11, "type": [11, 12, 14, 23, 30, 31, 32, 33, 34, 35, 36, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 103, 104, 105, 108, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 163, 164, 167, 169, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 213, 214, 215, 216, 218, 219, 220, 221, 222, 229, 230, 234, 235, 238, 239, 240, 241], "descript": [11, 232], "whatev": 11, "metadata": [11, 240], "seed_kei": 11, "epochs_kei": 11, "total_epochs_kei": 11, "max_steps_kei": 11, "max_steps_per_epoch": [11, 240], "add_fil": 11, "log_artifact": 11, "field": [12, 20, 21, 23, 30, 31, 38, 41, 42, 43, 46, 51, 56, 205, 234], "hydra": 12, "facebook": 12, "research": 12, "http": [12, 38, 40, 44, 48, 51, 55, 57, 62, 63, 64, 65, 66, 67, 68, 69, 70, 73, 74, 76, 77, 78, 79, 80, 82, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 103, 104, 105, 106, 107, 113, 114, 115, 116, 119, 120, 121, 122, 124, 126, 128, 129, 131, 132, 134, 135, 136, 137, 139, 140, 141, 143, 144, 145, 150, 151, 152, 158, 160, 161, 171, 173, 175, 176, 177, 178, 179, 188, 189, 190, 191, 204, 207, 210, 211, 213, 215, 221, 226, 234, 235, 237], "com": [12, 65, 66, 67, 77, 78, 82, 90, 91, 92, 93, 98, 104, 105, 113, 114, 132, 137, 139, 140, 141, 150, 151, 152, 160, 161, 171, 175, 176, 177, 178, 179, 207, 226, 235, 237], "facebookresearch": [12, 151], "blob": [12, 65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 135, 137, 139, 140, 141, 150, 151, 152, 160, 175, 176, 177, 178, 179], "main": [12, 14, 82, 135, 150, 151, 152, 226, 229, 235, 237], "_intern": 12, "_instantiate2": 12, "l148": 12, "omegaconf": 12, "num_lay": [12, 61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 154, 156, 158, 162, 164], "32": [12, 158, 162, 164, 207, 237, 238, 239, 240, 241], "num_head": [12, 61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 152, 154, 156], "num_kv_head": [12, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150], "vocab_s": [12, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 161, 163], "must": [12, 24, 39, 51, 56, 150, 166, 207, 241], "return": [12, 14, 16, 21, 23, 24, 30, 31, 32, 33, 34, 35, 36, 38, 40, 41, 42, 43, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 71, 72, 73, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 160, 161, 162, 163, 164, 166, 167, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 214, 215, 216, 219, 220, 221, 222, 233, 234, 238, 241], "nn": [12, 32, 33, 61, 147, 149, 150, 154, 155, 156, 157, 158, 159, 162, 163, 164, 165, 166, 168, 169, 170, 189, 195, 196, 206, 212, 213, 217, 218, 238, 241], "parsed_yaml": 12, "embed_dim": [12, 58, 59, 60, 61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 152, 155, 157, 158, 163, 164, 217, 238], "valueerror": [12, 18, 20, 22, 28, 30, 33, 37, 40, 46, 47, 49, 50, 51, 52, 53, 55, 56, 130, 138, 150, 154, 156, 158, 190, 191, 192, 197, 199, 215, 218], "recipe_nam": 13, "rank": [13, 65, 66, 67, 76, 77, 78, 89, 90, 91, 92, 93, 103, 104, 105, 112, 113, 114, 119, 120, 121, 122, 131, 132, 138, 139, 140, 141, 167, 202, 204, 215, 228, 236, 238, 241], "zero": [13, 149, 151, 235, 237, 240], "displai": 13, "callabl": [14, 38, 40, 42, 43, 44, 51, 55, 56, 61, 154, 156, 168, 189, 198, 200, 206, 213, 219], "With": [14, 235, 238, 240, 241], "my_recip": 14, "foo": 14, "bar": [14, 227, 236, 239], "instanti": [15, 24, 62, 63, 64, 65, 66, 67, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 112, 113, 114, 119, 120, 121, 122, 123, 124, 125, 126, 127, 131, 132, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 145, 146, 194], "configerror": 15, "cannot": [15, 137, 192, 237], "deprec": [16, 21, 30, 31, 38, 40], "remov": [16, 21, 30, 31, 38, 40, 137, 239], "futur": [16, 21, 30, 31, 38, 40, 229, 240], "releas": [16, 21, 30, 31, 38, 40, 232, 237], "prompttempl": [16, 19, 21, 26, 29, 71, 82, 98, 118, 130, 137], "tag": [16, 24, 38, 71, 75, 81, 82, 88, 98, 102, 117, 118, 127, 130, 135, 137, 146, 207, 208, 209, 210, 211, 233], "system": [16, 17, 18, 20, 22, 23, 24, 25, 27, 28, 30, 31, 37, 38, 42, 43, 49, 50, 51, 52, 53, 56, 71, 75, 81, 82, 88, 102, 117, 118, 127, 130, 135, 146, 186, 233, 234], "assist": [16, 17, 18, 20, 22, 23, 24, 25, 27, 28, 30, 31, 37, 38, 42, 43, 47, 71, 75, 81, 82, 88, 98, 102, 118, 127, 130, 135, 146, 186, 219, 233, 234], "role": [16, 18, 22, 23, 24, 25, 28, 30, 31, 38, 42, 43, 47, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 186, 233, 234], "prepend": [16, 18, 20, 22, 24, 25, 28, 49, 50, 51, 52, 53, 56, 75, 82, 88, 98, 102, 118, 127, 135, 146, 183], "append": [16, 24, 25, 75, 88, 98, 102, 118, 127, 130, 135, 146, 154, 156, 162, 183, 207, 226, 234], "messag": [16, 17, 18, 20, 22, 24, 25, 28, 30, 31, 37, 38, 42, 43, 46, 47, 49, 50, 51, 52, 53, 56, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 182, 186, 226, 232, 233, 234], "classmethod": [16, 21, 23, 234], "accord": [16, 51, 56, 117, 233], "openai": [17, 30, 177, 234], "markup": 17, "languag": [17, 137, 163, 164, 167, 175, 217, 219, 238, 239], "It": [17, 23, 24, 42, 43, 47, 49, 50, 51, 52, 54, 56, 117, 158, 162, 175, 179, 207, 232, 233, 234, 241], "templat": [17, 19, 21, 24, 25, 26, 29, 38, 39, 40, 42, 43, 46, 49, 52, 71, 75, 81, 82, 88, 98, 102, 117, 118, 127, 130, 135, 137, 146], "im_start": 17, "context": [17, 134, 168, 214, 216, 234, 239], "im_end": 17, "goe": [17, 168], "respons": [17, 18, 20, 22, 23, 28, 42, 43, 49, 50, 51, 52, 53, 56, 71, 82, 118, 130, 173, 174, 175, 176, 178, 179, 186, 234, 235, 236, 237], "column_map": [18, 20, 21, 22, 28, 39, 40, 45, 46, 49, 50, 51, 52, 53, 54, 56, 234], "new_system_prompt": [18, 20, 22, 28, 49, 50, 51, 52, 53, 56], "chosen": [18, 42, 54, 175, 176, 178, 179, 216, 234], "reject": [18, 42, 54, 175, 176, 178, 179, 234], "column": [18, 20, 21, 22, 28, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 150, 154, 156, 157, 162, 233, 234, 240], "q1": [18, 42, 47], "a1": [18, 42, 47], "a2": [18, 42], "whether": [18, 20, 22, 23, 28, 30, 31, 33, 38, 40, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 72, 76, 77, 78, 89, 90, 91, 92, 93, 98, 103, 104, 105, 112, 113, 114, 118, 119, 120, 121, 122, 130, 131, 132, 137, 138, 139, 140, 141, 142, 159, 167, 171, 172, 183, 184, 189, 197, 199, 207, 217, 233, 234], "keep": [18, 20, 22, 28, 50, 54, 56, 163, 235, 238, 239], "present": [18, 22, 28, 49, 51, 52, 53, 56, 184, 192, 217], "functool": [19, 26, 29, 189], "partial": [19, 26, 29, 189], "_prompt_templ": [19, 26, 29, 82, 118, 137], "english": 19, "n": [19, 24, 26, 29, 71, 82, 118, 130, 150, 158, 186, 224, 231, 232, 233, 234, 240], "ncorrect": 19, "grammar": [19, 49, 234], "task": [19, 26, 29, 39, 48, 71, 82, 98, 118, 130, 137, 228, 233, 234, 235, 237, 238, 239, 240, 241], "user_messag": [19, 26, 29, 233], "assistant_messag": [19, 26, 29, 233], "equival": [20, 30, 31, 59, 176, 178, 179], "respect": [20, 39, 81, 170, 216, 233, 234], "placehold": [21, 40, 234], "ident": [21, 22, 32, 33, 40, 41, 51, 117, 176, 235, 240], "alwai": [21, 176, 207, 217, 239], "dataclass": [22, 233], "remain": [22, 28, 30, 31, 160, 238, 239], "unmask": [22, 28, 30, 31], "liter": [23, 24, 27, 65, 66, 67, 68, 69, 70, 75, 76, 77, 78, 79, 80, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 102, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 127, 128, 129, 131, 132, 135, 136, 138, 139, 140, 141, 146, 171, 172], "ipython": [23, 24, 27, 42, 43, 75, 88, 102, 127, 135, 146], "union": [23, 33, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 75, 88, 102, 127, 135, 138, 142, 146, 154, 156, 162, 172, 195, 207, 208, 209, 210, 211, 213, 215], "mask": [23, 24, 40, 41, 43, 46, 47, 49, 50, 51, 52, 53, 56, 71, 82, 98, 118, 130, 137, 150, 154, 155, 156, 157, 162, 173, 177, 182, 186, 188, 201, 233, 234], "eot": [23, 98], "repres": [23, 34, 58, 59, 158, 195, 233, 239, 240], "individu": [23, 41, 162, 199, 211, 213, 233, 234], "interleav": [23, 188], "tokenize_messag": [23, 38, 40, 42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 71, 82, 98, 118, 130, 137, 182, 186, 233, 234], "attach": 23, "appropri": [23, 39, 81, 160, 163, 190, 234, 241], "special": [23, 38, 71, 82, 98, 102, 118, 130, 135, 137, 146, 158, 163, 181, 182, 184, 185, 186, 188, 194, 234], "writer": 23, "human": [23, 28, 31, 47, 81, 175, 177, 178, 233], "dictionari": [23, 24, 33, 34, 35, 41, 42, 43, 75, 88, 102, 127, 135, 146, 199, 205, 207, 208, 209, 210, 211, 235], "hello": [23, 71, 82, 98, 118, 130, 137, 183, 184, 233, 235, 237], "world": [23, 71, 82, 98, 118, 130, 137, 183, 184, 202, 204, 235], "calcul": [23, 24, 150, 154, 155, 156, 157, 158, 162, 173, 174, 177, 237], "correspond": [23, 34, 166, 169, 173, 177, 197, 229, 236, 237, 239, 240], "where": [23, 24, 32, 34, 38, 46, 59, 82, 87, 118, 126, 150, 154, 156, 158, 161, 162, 167, 173, 175, 176, 177, 180, 183, 188, 198, 201, 206, 234, 239], "hand": 23, "consecut": [23, 37, 188], "e": [23, 38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 137, 150, 158, 159, 162, 166, 170, 188, 190, 194, 199, 216, 226, 229, 235, 237, 238, 239, 240, 241], "last": [23, 36, 41, 55, 98, 160, 174, 234], "properti": [23, 155, 157, 164, 238, 239], "contains_media": 23, "non": [23, 137, 172, 174], "from_dict": [23, 233], "construct": [23, 137, 188, 230, 238], "text_cont": [23, 233], "achiev": [24, 212, 229, 235, 237, 238, 240, 241], "prepend_tag": 24, "append_tag": 24, "thu": [24, 42, 43, 176, 239, 240], "consid": [24, 39, 42, 43, 59, 60, 61, 158, 239], "come": [24, 37, 166, 230, 238, 239], "question": [26, 50, 233, 234, 235, 237], "nanswer": 26, "answer": [26, 50, 233, 235, 237], "alia": [27, 189], "adher": [28, 30, 31], "sharegpt": [28, 31, 47], "gpt": [28, 31, 47, 137, 150, 235], "summar": [29, 52, 233, 234, 239], "dialogu": [29, 52, 233], "nsummari": [29, 233], "summari": [29, 39, 52, 158, 199, 234], "jsontomessag": [30, 47], "transformed_sampl": [30, 31, 234], "could": [30, 238], "sharegpttomessag": [31, 47, 53], "sequenc": [32, 33, 34, 35, 40, 41, 44, 48, 51, 55, 56, 57, 71, 72, 75, 76, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 119, 121, 123, 125, 127, 130, 131, 133, 135, 137, 138, 142, 146, 149, 150, 152, 154, 156, 158, 162, 174, 179, 180, 184, 186, 188, 201, 233], "batch_first": 32, "padding_valu": 32, "float": [32, 65, 66, 67, 68, 69, 70, 72, 76, 77, 78, 79, 80, 83, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 103, 104, 105, 106, 107, 108, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 125, 128, 129, 131, 132, 133, 136, 138, 139, 140, 141, 142, 150, 151, 160, 167, 173, 174, 175, 176, 177, 178, 179, 199, 205, 207, 208, 209, 210, 211, 219, 238, 239, 240, 241], "rnn": [32, 33], "pad_sequ": [32, 33], "variabl": [32, 40, 193, 204, 207, 234, 241], "length": [32, 33, 35, 36, 37, 39, 40, 41, 48, 57, 71, 72, 75, 76, 82, 83, 88, 89, 98, 99, 102, 103, 108, 112, 118, 119, 121, 123, 125, 127, 130, 131, 133, 134, 135, 137, 138, 142, 146, 149, 150, 152, 154, 156, 161, 162, 173, 174, 184, 186, 188, 191, 201, 207, 239], "left": [32, 33, 98, 130, 238], "longest": [32, 35], "trail": 32, "dimens": [32, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 152, 154, 156, 158, 163, 167, 237, 238, 239, 241], "element": [32, 33, 39, 201, 235], "c": [32, 33, 51, 233], "10": [32, 33, 34, 35, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 158, 163, 235, 237, 239, 240, 241], "12": [32, 33, 34, 53, 158, 226, 240], "pad_direct": 33, "keys_to_pad": 33, "padding_idx": [33, 34, 35, 41], "collat": [33, 35, 41, 234], "left_pad_sequ": 33, "subset": [33, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 76, 89, 103, 112, 119, 121, 131, 138, 169], "integ": [33, 35, 163, 189, 195, 215], "per": [33, 68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 149, 158, 159, 174, 175, 188, 232, 239, 240, 241], "batch_siz": [33, 46, 49, 51, 52, 56, 149, 150, 154, 155, 156, 157, 161, 162, 163, 164, 175, 176, 178, 180, 235, 239, 240], "empti": [33, 37, 232], "ignore_idx": [34, 35], "dpo": [34, 42, 168, 175, 176, 178, 179], "input_id": [34, 201], "chosen_input_id": [34, 234], "chosen_label": [34, 234], "rejected_input_id": [34, 234], "rejected_label": [34, 234], "index": [34, 35, 39, 41, 150, 152, 154, 156, 157, 160, 162, 174, 226, 227, 233, 235, 236], "concaten": [34, 39, 71, 82, 118, 130, 137, 182, 186], "13": [34, 71, 82, 118, 130, 158, 180, 186, 241], "14": [34, 158, 240, 241], "15": [34, 158, 198, 233, 235, 238, 241], "16": [34, 65, 66, 67, 68, 69, 70, 77, 78, 79, 80, 90, 91, 92, 93, 94, 95, 96, 97, 104, 105, 106, 107, 113, 114, 115, 116, 120, 122, 128, 129, 132, 136, 139, 140, 141, 158, 238, 241], "17": [34, 158, 238], "18": [34, 158, 237], "19": [34, 158, 241], "20": [34, 158, 180, 240], "token_pair": 35, "padded_col": [35, 234], "eos_id": [36, 137, 184, 186], "replac": [36, 40, 46, 47, 49, 50, 52, 53, 137, 154, 159, 163, 217, 238], "forth": [37, 234], "shorter": 37, "min": [37, 238], "invalid": 37, "sftdataset": [38, 40, 42, 45, 46, 47, 49, 50, 51, 52, 53, 56], "chat_dataset": [38, 234], "multiturn": [38, 233], "convert_to_messag": [38, 233], "prepar": [38, 233, 240], "truncat": [38, 40, 41, 48, 55, 57, 71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146, 180, 184, 186, 234], "local": [38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 102, 135, 146, 207, 211, 215, 226, 232, 233, 235, 236], "g": [38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 137, 150, 158, 162, 166, 188, 190, 199, 216, 229, 237, 238, 239, 240, 241], "csv": [38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 233, 234], "filepath": [38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57], "data_fil": [38, 40, 42, 43, 44, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 233, 234], "load_dataset": [38, 40, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 233, 234], "huggingfac": [38, 40, 44, 48, 55, 57, 126, 134, 135, 137, 143, 144, 145, 160, 175, 176, 178, 179, 190, 191, 232, 235], "co": [38, 40, 44, 48, 55, 57, 126, 134, 135, 143, 144, 145, 190, 191, 235], "doc": [38, 40, 43, 44, 48, 55, 57, 82, 98, 137, 189, 204, 207, 210, 211, 215, 221, 232, 234, 235], "en": [38, 40, 44, 48, 55, 57, 240], "package_refer": [38, 40, 44, 48, 55, 57], "loading_method": [38, 40, 44, 48, 55, 57], "chat_format": [38, 233, 234], "chatformat": [38, 234], "inst": [38, 71, 81, 82, 98, 117, 118, 130, 137, 233, 234], "mistral": [38, 71, 82, 98, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 130, 137, 193, 232, 233, 235, 236], "extra": [38, 71, 82, 98, 118, 130, 137, 162, 226, 233, 238, 240, 241], "still": [38, 71, 82, 98, 118, 130, 137, 161, 163, 164, 228, 238, 240, 241], "unless": 38, "load_dataset_kwarg": [38, 40, 42, 43, 44, 45, 46, 47, 48, 50, 51, 55, 56, 57], "sub": [39, 210], "unifi": [39, 126], "were": [39, 158, 168, 177, 233, 236, 240], "simplifi": [39, 175, 232, 238], "simultan": 39, "intern": 39, "aggreg": 39, "transpar": 39, "howev": [39, 135, 226, 228, 239], "constitu": 39, "might": [39, 163, 165, 232, 235, 239], "larg": [39, 137, 161, 167, 216, 232, 239, 241], "comput": [39, 42, 43, 83, 89, 99, 103, 108, 112, 138, 142, 147, 150, 152, 154, 156, 161, 162, 175, 176, 178, 179, 188, 199, 215, 229, 235, 239, 240, 241], "cumul": 39, "maintain": [39, 164, 228, 239, 241], "indic": [39, 40, 41, 61, 150, 152, 154, 156, 157, 158, 162, 163, 173, 177, 180, 188, 189, 201, 204, 233], "deleg": 39, "retriev": [39, 42, 43, 198], "lead": [39, 118, 183, 229], "high": [39, 42, 43, 227, 238, 239], "scale": [39, 65, 66, 67, 76, 77, 78, 89, 90, 91, 92, 93, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 131, 132, 138, 139, 140, 141, 153, 155, 157, 167, 174, 176, 179, 219, 228, 238, 239, 240, 241], "strategi": [39, 229], "stream": [39, 221], "demand": 39, "deriv": [39, 147, 156, 157], "dataset1": 39, "mycustomdataset": 39, "params1": 39, "dataset2": 39, "params2": 39, "concat_dataset": 39, "total": [39, 160, 174, 177, 202, 224, 231, 235, 237, 238, 239], "data_point": 39, "1500": 39, "accomplish": [39, 47, 50, 55], "instruct_dataset": [39, 40, 234], "vicgal": [39, 234], "gpt4": [39, 234], "alpacainstructtempl": [39, 234], "samsum": [39, 52, 234], "summarizetempl": [39, 52, 233, 234], "focus": [39, 230, 236, 239], "enhanc": [39, 158, 179, 239, 241], "divers": 39, "machin": [39, 178, 220, 232, 235], "instructtempl": [40, 234], "contribut": [40, 46, 47, 49, 50, 52, 53, 174, 177, 230], "disabl": [40, 48, 57, 168, 215, 240], "recommend": [40, 47, 48, 49, 52, 54, 57, 117, 161, 207, 210, 233, 235, 239, 241], "highest": [40, 48, 57], "ds": [41, 53], "max_pack": 41, "split_across_pack": [41, 55], "greedi": 41, "pack": [41, 45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 150, 152, 154, 156, 157, 162, 240], "done": [41, 171, 197, 206, 217, 238, 240, 241], "outsid": [41, 215, 216, 238], "sampler": [41, 236], "part": [41, 163, 178, 233, 241], "style": [41, 45, 46, 47, 53, 164, 241], "buffer": [41, 239], "long": [41, 137, 184, 233, 234, 238], "enough": [41, 233], "attent": [41, 61, 65, 66, 67, 72, 76, 77, 78, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 125, 131, 132, 133, 134, 138, 139, 140, 141, 142, 149, 150, 152, 154, 155, 156, 157, 162, 164, 171, 172, 188, 237, 238, 239, 241], "lower": [41, 228, 229, 238], "triangular": 41, "cross": [41, 155, 161, 162, 164, 188], "attend": [41, 150, 154, 155, 156, 157, 162, 188], "rel": [41, 150, 152, 154, 156, 157, 162, 175, 199, 238], "its": [41, 81, 117, 121, 150, 152, 154, 156, 157, 162, 164, 176, 212, 215, 232, 233, 234, 235, 237, 238, 239], "max": [41, 71, 82, 118, 130, 137, 146, 154, 156, 158, 160, 162, 184, 186, 232, 238], "wise": 41, "made": [41, 47, 50, 55, 152, 235], "smaller": [41, 163, 176, 235, 237, 238, 239, 240, 241], "jam": 41, "vari": 41, "s1": [41, 82, 118, 183], "s2": [41, 82, 118, 183], "s3": 41, "s4": 41, "contamin": 41, "input_po": [41, 149, 150, 152, 154, 156, 157, 162], "matrix": [41, 154, 155, 156, 162], "causal": [41, 150, 154, 156, 157, 162], "continu": [41, 158, 207, 230, 234], "increment": 41, "move": [41, 55, 154, 156], "entir": [41, 55, 161, 165, 206, 233, 241], "avoid": [41, 55, 151, 158, 159, 176, 215, 232, 240, 241], "sentenc": [41, 55, 118], "message_transform": [42, 43], "techniqu": [42, 227, 228, 230, 235, 236, 237, 238, 239, 240], "rlhf": [42, 173, 174, 175, 176, 177, 178, 179, 180, 234], "remot": [42, 43], "separ": [42, 71, 82, 118, 130, 164, 186, 190, 233, 236, 237, 238, 241], "repons": 42, "At": [42, 43, 154, 156, 162], "uniqu": [42, 43, 82, 193], "extract": [42, 43, 48, 185], "becaus": [42, 43, 76, 149, 154, 156, 158, 162, 192, 232, 233, 240], "against": [42, 43, 179, 222, 240, 241], "unit": [42, 43, 206, 227], "row": [42, 43, 150, 154, 156, 157, 162, 233], "final": [42, 43, 65, 66, 67, 72, 76, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 131, 132, 138, 141, 142, 147, 154, 156, 162, 168, 171, 172, 235, 237, 238, 239, 241], "model_transform": [42, 43, 49, 51, 52, 53, 56], "ref": [42, 43, 46, 51, 56, 134, 135, 211], "filter_fn": [43, 44, 55], "supervis": 43, "round": [43, 240], "involv": [43, 240], "incorpor": [43, 175, 234], "media": 43, "happen": [43, 161], "ti": [43, 76, 138, 142, 154, 239], "agnost": [43, 234], "treat": [43, 158, 168, 233], "modal": [43, 164], "minimum": [43, 51, 56], "filter": [43, 44, 55, 240], "prior": [43, 44, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 217], "add_eo": [44, 55, 71, 82, 98, 118, 130, 137, 183, 184, 233], "freeform": [44, 55], "unstructur": [44, 55, 57], "corpu": [44, 48, 55, 57], "tabular": [44, 55], "txt": [44, 55, 137, 146, 208, 234, 236], "eo": [44, 55, 118, 130, 135, 183, 186, 233, 234], "yahma": 45, "packeddataset": [45, 46, 47, 49, 50, 51, 52, 53, 55, 56, 57, 234], "variant": [45, 49, 52], "version": [45, 76, 89, 103, 112, 119, 121, 131, 138, 150, 219, 222, 226, 233, 237, 240, 241], "page": [45, 57, 226, 227, 232, 236, 237, 239], "tatsu": 46, "lab": 46, "codebas": [46, 235], "independ": 46, "alpacatomessag": 46, "alpaca_d": 46, "conversation_column": 47, "conversation_styl": [47, 234], "altern": [47, 50, 236, 239], "friendli": [47, 50, 55, 219, 233], "similar": [47, 48, 51, 54, 55, 56, 57, 171, 175, 234, 235, 237, 238, 241], "toward": [47, 176, 179], "my_dataset": [47, 50], "london": [47, 50], "am": [47, 50, 81, 117, 233, 234, 235, 237], "ccdv": 48, "cnn_dailymail": 48, "textcompletiondataset": [48, 55, 57, 234], "cnn": 48, "dailymail": 48, "articl": [48, 57], "highlight": [48, 241], "anyth": 48, "liweili": 49, "c4_200m": 49, "grammarerrorcorrectiontempl": 49, "conjunct": [49, 52, 54], "inputoutputtomessag": [49, 52], "grammar_d": 49, "liuhaotian": 51, "llava": 51, "150k": 51, "llava_instruct_150k": 51, "coco": 51, "2017": 51, "visit": [51, 235], "cocodataset": 51, "org": [51, 62, 63, 64, 65, 66, 68, 69, 70, 76, 77, 78, 79, 80, 82, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 103, 104, 105, 106, 107, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 137, 150, 151, 152, 158, 173, 175, 176, 177, 178, 179, 188, 189, 204, 210, 213, 215, 221, 226], "wget": 51, "zip": [51, 223], "train2017": 51, "unzip": 51, "minim": [51, 56, 234, 236, 238, 240, 241], "purpos": [51, 56, 236, 237], "clip": [51, 56, 58, 59, 60, 61, 158, 177], "clipimagetransform": [51, 56, 158], "mymodeltransform": [51, 56], "tokenizer_path": [51, 56, 71, 82, 118, 130], "image_transform": [51, 56], "__call__": [51, 56], "align": [51, 56, 175, 233], "llava_instruct_d": 51, "samsung": 52, "samsum_d": 52, "open": [53, 73, 74, 234, 235], "orca": 53, "slimorca": 53, "dedup": 53, "351": 53, "82": 53, "391": 53, "221": 53, "220": 53, "193": 53, "471": 53, "lvwerra": [54, 234], "stack": [54, 158, 216, 234], "exchang": [54, 234], "preferencedataset": [54, 234], "questionanswertempl": 54, "allenai": [55, 234, 240], "c4": [55, 234, 240], "data_dir": [55, 234], "realnewslik": [55, 234], "huggingfacem4": 56, "the_cauldron": 56, "cauldron": 56, "card": [56, 82, 98], "cauldron_d": 56, "ai2d": 56, "wikitext_document_level": 57, "wikitext": [57, 240], "103": [57, 235], "wikipedia": 57, "max_num_til": [58, 59, 61, 158], "tile": [58, 59, 60, 61, 158, 188], "patch": [58, 59, 60, 61, 158, 188], "check": [58, 59, 60, 61, 154, 155, 156, 157, 158, 162, 164, 171, 197, 204, 222, 225, 227, 228, 229, 230, 233, 235, 236, 238, 239], "document": [58, 59, 60, 61, 150, 176, 189, 198, 206, 228, 230, 232, 234, 239], "vision_transform": [58, 59, 60, 61], "visiontransform": [58, 59, 60, 61], "divid": [58, 59, 60, 61, 158, 188], "dimension": [58, 59, 60, 61, 158], "aspect_ratio": [58, 59, 158], "bsz": [58, 59, 158, 161, 219], "n_img": [58, 59, 158], "n_tile": [58, 59, 158], "n_token": [58, 59, 60, 158], "aspect": [58, 59, 227], "ratio": [58, 59, 175, 176, 177], "crop": [58, 59, 60, 61, 158], "tile_s": [59, 60, 61, 158, 188], "patch_siz": [59, 60, 61, 158, 188], "local_token_positional_embed": 59, "_position_embed": [59, 158], "tokenpositionalembed": [59, 158], "gate": [59, 153, 193, 228, 229, 232, 236], "global_token_positional_embed": 59, "advanc": [59, 60, 61, 158, 234], "40": [59, 60, 61, 137, 158, 188, 239, 241], "400": [59, 60, 61, 158, 188], "10x10": [59, 60, 61, 158, 188], "grid": [59, 60, 61, 158, 188], "k": [59, 150, 238], "th": 59, "silu": [61, 147], "cls_output_dim": [61, 158], "attn_bia": 61, "out_indic": [61, 158], "output_cls_project": 61, "in_channel": [61, 158], "intermediate_act": 61, "transformerencoderlay": 61, "cl": [61, 158, 234], "head": [61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 152, 154, 156, 162, 165, 193, 237], "mlp": [61, 65, 66, 67, 72, 76, 77, 78, 83, 89, 90, 91, 92, 93, 99, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 123, 125, 131, 132, 133, 138, 139, 140, 141, 142, 154, 155, 156, 157, 171, 172, 237, 238, 239], "boolean": [61, 150, 154, 155, 156, 157, 162, 164, 189, 201], "bia": [61, 166, 167, 217, 238, 240, 241], "intermedi": [61, 72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 158, 192, 213, 237, 241], "fourth": [61, 158], "determin": [61, 172], "channel": [61, 158, 240], "assertionerror": [61, 154, 155, 156, 171, 172, 217], "divis": [61, 151], "code_llama2": [62, 63, 64, 65, 66, 67, 68, 69, 70, 232], "transformerdecod": [62, 63, 64, 65, 66, 67, 68, 69, 70, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 131, 132, 133, 134, 136, 138, 141, 142, 145, 154, 162, 163, 164, 219, 238], "w": [62, 63, 64, 73, 74, 84, 85, 86, 87, 100, 101, 109, 110, 111, 124, 126, 143, 144, 145, 158, 207, 210, 211, 233, 235, 238, 241], "arxiv": [62, 63, 64, 65, 66, 68, 69, 70, 76, 77, 78, 79, 80, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 103, 104, 105, 106, 107, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 150, 151, 152, 158, 173, 175, 176, 177, 178, 179, 188], "pdf": [62, 63, 64, 173, 188], "2308": [62, 63, 64], "12950": [62, 63, 64], "lora_attn_modul": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 171, 172, 228, 238, 239, 241], "q_proj": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 150, 171, 172, 228, 238, 239, 240, 241], "k_proj": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 150, 171, 172, 228, 238, 239, 240, 241], "v_proj": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 150, 171, 172, 228, 238, 239, 240, 241], "output_proj": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 150, 171, 172, 238, 239, 240, 241], "apply_lora_to_mlp": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 171, 172, 228, 238, 239], "apply_lora_to_output": [65, 66, 67, 68, 69, 70, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 141, 171, 172, 238, 239], "lora_rank": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 228, 238, 239], "lora_alpha": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 228, 238, 239], "lora_dropout": [65, 66, 67, 68, 69, 70, 76, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 112, 119, 121, 131, 138, 139, 140, 141, 239], "05": [65, 66, 67, 68, 69, 70, 83, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 139, 140, 141, 142], "use_dora": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141], "quantize_bas": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 167, 241], "code_llama2_13b": 65, "tloen": [65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141], "8bb8579e403dc78e37fe81ffbb253c413007323f": [65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141], "l41": [65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141], "l43": [65, 66, 67, 77, 78, 90, 91, 92, 93, 104, 105, 113, 114, 132, 139, 140, 141], "linear": [65, 66, 67, 68, 69, 70, 76, 77, 78, 79, 80, 89, 90, 91, 92, 93, 94, 95, 96, 97, 103, 104, 105, 106, 107, 112, 113, 114, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 138, 139, 140, 141, 156, 166, 167, 171, 172, 238, 239, 240, 241], "low": [65, 66, 67, 76, 77, 78, 89, 90, 91, 92, 93, 103, 104, 105, 112, 113, 114, 119, 120, 121, 122, 131, 132, 138, 139, 140, 141, 167, 228, 235, 238, 241], "approxim": [65, 66, 67, 76, 77, 78, 89, 90, 91, 92, 93, 103, 104, 105, 112, 113, 114, 119, 120, 121, 122, 131, 132, 138, 139, 140, 141, 167, 238], "factor": [65, 66, 67, 76, 77, 78, 89, 90, 91, 92, 93, 103, 104, 105, 108, 112, 113, 114, 119, 120, 121, 122, 131, 132, 138, 139, 140, 141, 167, 173, 235], "dropout": [65, 67, 72, 76, 83, 89, 92, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 167, 238, 239, 241], "probabl": [65, 67, 76, 89, 92, 103, 112, 119, 121, 131, 138, 167, 175, 176, 177, 178, 179, 219, 235], "decompos": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132], "magnitud": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132, 239], "introduc": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132, 150, 151, 164, 167, 179, 229, 233, 234, 238, 239, 240, 241], "dora": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132], "ab": [65, 66, 68, 69, 70, 76, 77, 78, 79, 80, 84, 85, 86, 87, 89, 90, 91, 92, 94, 95, 96, 97, 103, 104, 105, 106, 107, 115, 116, 119, 120, 121, 122, 128, 129, 131, 132, 136, 150, 151, 152, 158, 175, 176, 177, 178, 179], "2402": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132], "09353": [65, 66, 76, 77, 78, 89, 90, 91, 92, 103, 104, 105, 119, 120, 121, 122, 131, 132], "code_llama2_70b": 66, "code_llama2_7b": 67, "qlora": [68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 159, 225, 227, 228, 237, 238], "paper": [68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 175, 176, 178, 179, 188, 238, 241], "2305": [68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136, 150, 175, 178], "14314": [68, 69, 70, 79, 80, 94, 95, 96, 97, 106, 107, 115, 116, 128, 129, 136], "lora_code_llama2_13b": 68, "lora_code_llama2_70b": 69, "lora_code_llama2_7b": 70, "gemma": [71, 73, 74, 75, 76, 77, 78, 79, 80, 193, 239], "prompt_templ": [71, 75, 82, 88, 98, 102, 118, 127, 130, 135, 137, 146], "sentencepiec": [71, 82, 118, 130, 183, 237], "pretrain": [71, 82, 98, 118, 130, 162, 164, 165, 183, 184, 232, 233, 236, 238, 241], "gear": [71, 82, 98, 118, 130, 137], "whenev": [71, 82, 98, 118, 130, 137, 238], "commun": [71, 82, 98, 118, 130, 137, 233, 234, 235], "chatmltempl": [71, 82, 98, 118, 130, 137, 146], "spm_model": [71, 82, 118, 130, 183, 233], "tokenized_text": [71, 82, 98, 118, 130, 137, 183, 184], "add_bo": [71, 82, 98, 118, 130, 137, 183, 184, 233], "31587": [71, 82, 98, 118, 130, 183, 184], "29644": [71, 82, 98, 118, 130, 183, 184], "102": [71, 82, 98, 118, 130, 183, 184], "concat": [71, 82, 118, 130, 186], "1788": [71, 82, 118, 130, 186], "2643": [71, 82, 118, 130, 186], "1792": [71, 82, 118, 130, 186], "9508": [71, 82, 118, 130, 186], "465": [71, 82, 118, 130, 186], "22137": [71, 82, 118, 130, 186], "2933": [71, 82, 118, 130, 186], "join": [71, 82, 118, 130, 186], "attribut": [71, 82, 118, 130, 168, 179, 186, 196], "head_dim": [72, 76, 149, 150, 154, 156], "intermediate_dim": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142], "attn_dropout": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 154, 156], "norm_ep": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142], "1e": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 151], "06": [72, 76, 151, 238], "rope_bas": [72, 76, 83, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142], "10000": [72, 76, 83, 119, 121, 123, 125, 131, 133, 152], "norm_embed": [72, 76], "gemmatransformerdecod": [72, 73, 74, 76, 77, 78, 79, 80], "transformerselfattentionlay": [72, 83, 99, 108, 123, 142, 155, 162, 164], "rm": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142], "norm": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 154, 156, 157], "space": [72, 83, 99, 108, 123, 137, 142, 154, 156, 165, 239], "slide": [72, 123, 134], "window": [72, 123, 134, 234], "vocabulari": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 137, 138, 142, 161, 238, 239], "queri": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 149, 150, 154, 156, 157, 162, 237, 239], "mha": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 154, 156], "onto": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150, 165], "scaled_dot_product_attent": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150], "epsilon": [72, 76, 83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 177], "rotari": [72, 76, 83, 119, 121, 123, 125, 131, 133, 152, 237], "10_000": [72, 76, 119, 121, 123, 125, 133], "blog": [73, 74], "technolog": [73, 74], "develop": [73, 74, 241], "gemmatoken": 75, "_templatetyp": [75, 88, 102, 127, 135, 146], "prompttemplateinterfac": [75, 88, 102, 127, 135, 146], "gemma_2b": 77, "gemma_7b": 78, "lora_gemma_2b": 79, "lora_gemma_7b": 80, "taken": [81, 238, 241], "sy": [81, 82, 233, 234], "honest": [81, 233, 234], "pari": [81, 117, 234], "capit": [81, 117, 234], "franc": [81, 117, 234], "known": [81, 82, 117, 118, 200, 234, 240], "stun": [81, 117, 234], "llama2chattempl": [82, 88, 117, 146], "describ": [82, 98, 213, 234], "regist": [82, 98, 102, 130, 135, 146, 147, 159, 212, 241], "html": [82, 137, 189, 204, 210, 213, 215, 221, 225], "problem": [82, 118], "due": [82, 118, 183, 228, 238, 239, 241], "whitespac": [82, 118, 183], "slice": [82, 118], "gqa": [83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150], "mqa": [83, 89, 99, 103, 108, 112, 119, 121, 123, 125, 131, 133, 138, 142, 150], "kvcach": [83, 89, 99, 103, 108, 112, 131, 138, 142, 150, 154, 156], "scale_hidden_dim_for_mlp": [83, 89, 99, 103, 108, 112, 138, 142], "2307": [84, 85, 86, 87], "09288": [84, 85, 86, 87], "classif": [87, 121, 125, 126, 193], "reward": [87, 93, 97, 122, 126, 129, 173, 174, 175, 176, 178, 179, 193], "llama2_70b": 91, "llama2_7b": [92, 238], "classifi": [93, 121, 125, 126, 217, 234, 239], "llama2_reward_7b": [93, 193], "lora_llama2_13b": 94, "lora_llama2_70b": 95, "lora_llama2_7b": [96, 238], "lora_llama2_reward_7b": 97, "special_token": [98, 130, 137, 184, 233], "tiktoken": [98, 184, 237], "canon": [98, 102, 130, 135, 146], "tt_model": [98, 184], "token_id": [98, 118, 137, 181, 184], "truncate_at_eo": [98, 184], "skip_special_token": [98, 137, 184], "show": [98, 184, 188, 226, 228, 229, 232, 233, 238], "skip": [98, 150, 184], "add_start_token": 98, "add_end_token": 98, "header": [98, 233], "eom": 98, "seq": [98, 154, 156, 162], "500000": [99, 103, 108, 112], "special_tokens_path": [102, 135, 146], "llama3token": [102, 233], "similarli": [102, 135, 146, 234, 240], "llama3_70b": 104, "llama3_8b": [105, 162, 219, 237, 240], "lora_llama3_70b": 106, "lora_llama3_8b": 107, "scale_factor": 108, "rope": [108, 138, 142, 150, 152], "llama3_1": [109, 110, 111, 112, 113, 114, 115, 116, 228], "rtype": 109, "llama3_1_70b": 113, "llama3_1_8b": 114, "lora_llama3_1_70b": 115, "lora_llama3_1_8b": 116, "yet": [117, 233, 235], "mistralchattempl": [118, 127], "trim_leading_whitespac": [118, 183], "unbatch": [118, 183], "bo": [118, 135, 183, 186, 233, 234], "trim": [118, 183], "num_class": [121, 125, 217], "announc": 124, "ray2333": 126, "feedback": [126, 175], "mistraltoken": [127, 233], "lora_mistral_7b": 128, "lora_mistral_reward_7b": 129, "ignore_system_prompt": 130, "phi3_mini": [132, 193], "phi": [134, 135, 193], "128k": 134, "nor": 134, "phi3minitoken": 135, "tokenizer_config": 135, "spm": 135, "lm": [135, 177], "unk": 135, "augment": [135, 241], "endoftext": [135, 137], "phi3minisentencepiecebasetoken": 135, "lora_phi3_mini": 136, "merges_fil": [137, 146], "unk_token": 137, "bos_token": 137, "eos_token": 137, "pad_token": 137, "bpe_cache_s": 137, "151646": 137, "bpe": 137, "v4": [137, 160], "src": [137, 160], "tokenization_qwen2": 137, "word": [137, 138, 142, 176, 240], "utf": 137, "librari": [137, 175, 176, 178, 197, 215, 221, 225, 226, 227, 232, 234, 239, 241], "stdtype": 137, "unknown": 137, "cach": [137, 149, 150, 152, 154, 155, 156, 157, 162, 164, 226, 232], "speed": [137, 184, 216, 237, 239, 240, 241], "realli": [137, 235], "esp": 137, "chines": 137, "technic": [137, 230, 236], "leak": 137, "appear": [137, 239], "equal": [137, 188, 222], "39": [137, 158], "385": 137, "78": 137, "675": 137, "2000": [137, 240], "41": [137, 158], "tokenization_util": 137, "l541": 137, "l262": 137, "wether": 137, "runtimeerror": [137, 186, 194, 197, 203], "1000000": [138, 142], "tie_word_embed": [138, 139, 140, 142, 143, 144], "tiedembeddingtransformerdecod": [138, 139, 140, 142, 143, 144], "qwen2transformerdecod": 138, "period": [138, 142], "qwen2_0_5b": 139, "qwen2_1_5b": 140, "qwen2_7b": 141, "qwen": [143, 144, 145], "qwen2token": 146, "gate_proj": 147, "down_proj": 147, "up_proj": 147, "feed": [147, 155, 157], "network": [147, 168, 238, 241], "fed": [147, 233], "multipli": [147, 239], "subclass": 147, "although": [147, 238, 240], "afterward": 147, "former": 147, "hook": [147, 159, 212, 241], "latter": 147, "layernorm": 148, "standalon": 149, "kv": [149, 150, 154, 156, 162, 240], "past": 149, "expand": 149, "dpython": [149, 150, 154, 155, 156, 157, 159, 162, 164, 214, 218], "reset": [149, 150, 154, 155, 156, 157, 162, 164, 199], "k_val": 149, "v_val": 149, "assert": 149, "longer": [149, 234, 239], "h": [149, 158, 161, 226, 232], "pos_embed": [150, 155, 238, 240], "q_norm": 150, "k_norm": 150, "kv_cach": 150, "is_caus": 150, "13245v1": 150, "multihead": 150, "extrem": 150, "share": [150, 234, 235], "credit": 150, "lightn": 150, "lit": 150, "lit_gpt": 150, "v": [150, 154, 156, 162, 238], "q": [150, 238], "n_kv_head": 150, "rotarypositionalembed": [150, 238, 240], "rmsnorm": 150, "vice": [150, 232], "versa": [150, 232], "y": 150, "s_x": 150, "second": [150, 163, 235, 238, 239, 241], "s_y": 150, "seq_length": [150, 155, 157, 163, 164, 219], "softmax": [150, 154, 156, 157, 162], "j": [150, 154, 155, 156, 157, 162], "n_h": [150, 152], "num": [150, 152], "n_kv": 150, "emb": [150, 154, 155, 156, 162], "h_d": [150, 152], "reset_cach": [150, 154, 155, 156, 157, 162, 164], "setup_cach": [150, 154, 155, 156, 157, 162, 164], "ep": 151, "root": [151, 210, 211], "squar": 151, "1910": 151, "07467": 151, "verfic": [151, 152], "small": [151, 235], "propos": [152, 239], "2104": 152, "09864": 152, "l80": 152, "upto": 152, "init": [152, 199, 211, 241], "exceed": 152, "freq": 152, "recomput": [152, 239], "geometr": 152, "progress": [152, 236, 239], "rotat": 152, "angl": 152, "todo": 152, "effici": [152, 171, 198, 225, 227, 228, 235, 236, 238, 240], "basic": [153, 237], "learnabl": [153, 162, 164, 235], "output_hidden_st": [154, 156, 162], "belong": [154, 156, 196], "reduc": [154, 156, 175, 227, 228, 229, 234, 238, 239, 240, 241], "statement": [154, 156], "improv": [154, 156, 178, 184, 198, 229, 237, 238, 239], "readabl": [154, 156, 235], "caches_are_en": [154, 156, 162], "encoder_input": [154, 155, 156, 162], "encoder_mask": [154, 155, 156, 162], "s_e": [154, 156, 162], "d_e": [154, 156, 162], "relat": [154, 155, 156, 162, 238], "arang": [154, 156, 162], "prompt_length": [154, 156, 162], "seq_len": [154, 156], "bigger": [154, 156], "mode": [154, 156, 195, 200, 207, 235], "m_": [154, 156, 162], "set_num_output_chunk": [154, 156], "num_output_chunk": [154, 156, 161], "combin": [154, 156, 162, 164, 165, 174], "cewithchunkedoutputloss": [154, 156], "attn": [155, 157, 238, 240, 241], "multiheadattent": [155, 157, 238, 240], "ca_norm": 155, "mlp_norm": [155, 157], "ca_scal": 155, "mlp_scale": [155, 157], "convent": 155, "ff": [155, 157], "cache_en": [155, 157, 164], "token_sequ": 155, "embed_sequ": 155, "sa_norm": 157, "sa_scal": 157, "token_pos_embed": 158, "pre_tile_pos_emb": 158, "post_tile_pos_emb": 158, "cls_project": 158, "vit": 158, "11929": 158, "convolut": 158, "flatten": 158, "downscal": 158, "800x400": 158, "400x400": 158, "_transform": 158, "broken": [158, 164], "down": [158, 192, 234, 238, 239, 241], "whole": 158, "num_til": 158, "101": 158, "pool": 158, "tiledtokenpositionalembed": 158, "tilepositionalembed": 158, "tile_pos_emb": 158, "even": [158, 217, 226, 232, 233, 234, 237, 238, 239, 241], "8x8": 158, "21": 158, "22": 158, "23": [158, 160], "24": [158, 236, 237], "25": [158, 235], "26": 158, "27": [158, 235], "28": [158, 235], "29": [158, 241], "30": [158, 180, 240], "31": [158, 237], "33": 158, "34": 158, "35": [158, 241], "36": 158, "37": 158, "38": [158, 235], "42": 158, "43": 158, "44": 158, "45": 158, "46": 158, "47": 158, "48": [158, 235, 241], "49": 158, "50": [158, 180, 207, 235], "51": 158, "52": [158, 236], "53": 158, "54": 158, "55": [158, 236], "56": 158, "57": [158, 238, 241], "58": 158, "59": [158, 241], "60": 158, "61": [158, 235], "62": 158, "63": 158, "64": [158, 228, 238], "num_patches_per_til": 158, "emb_dim": 158, "greater": [158, 222], "constain": 158, "anim": [158, 234], "max_n_img": 158, "n_channel": 158, "hidden_st": 158, "vision_util": 158, "tile_crop": 158, "num_channel": 158, "image_s": 158, "800": 158, "patch_grid_s": 158, "random": [158, 215, 236], "rand": 158, "nch": 158, "tile_cropped_imag": 158, "batch_imag": 158, "unsqueez": 158, "batch_aspect_ratio": 158, "clip_vision_encod": 158, "common_util": 159, "bfloat16": [159, 214, 235, 236, 237, 238, 239, 240], "offload_to_cpu": 159, "nf4": [159, 239, 241], "restor": 159, "higher": [159, 176, 228, 237, 239, 240, 241], "offload": [159, 241], "increas": [159, 160, 175, 228, 237, 238, 239, 240], "peak": [159, 199, 205, 235, 237, 238, 241], "gpu": [159, 229, 232, 235, 236, 237, 238, 240, 241], "_register_state_dict_hook": 159, "m": [159, 219, 233, 240], "mymodul": 159, "_after_": 159, "nf4tensor": [159, 241], "unquant": [159, 240, 241], "unus": 159, "num_warmup_step": 160, "num_training_step": 160, "num_cycl": [160, 216], "last_epoch": 160, "lambdalr": 160, "rate": [160, 227, 236, 239], "schedul": [160, 216, 236, 239], "linearli": 160, "decreas": [160, 234, 238, 239, 240, 241], "cosin": 160, "l104": 160, "warmup": [160, 216], "phase": 160, "wave": 160, "half": [160, 239], "lr_schedul": 160, "ignore_index": 161, "ce": 161, "chunk": [161, 184], "upcast": 161, "bf16": [161, 197, 239, 241], "better": [161, 179, 227, 233, 234, 235, 239, 240], "accuraci": [161, 228, 229, 235, 237, 238, 239, 240, 241], "doubl": [161, 241], "therefor": [161, 241], "num_token": 161, "entropi": 161, "consider": 161, "compil": [161, 219, 235, 237, 239, 241], "compute_cross_entropi": 161, "gain": [161, 229, 237], "won": [161, 233], "realiz": 161, "pull": [161, 171, 228, 229, 232], "1390": 161, "ground": [161, 239], "loss_fn": 161, "chunkedcrossentropyloss": 161, "output_chunk": 161, "num_chunk": 161, "model_fus": [162, 163, 164, 165], "deepfus": 162, "evolut": 162, "signatur": 162, "interchang": 162, "fusion_param": [162, 163, 164, 165], "fusionembed": 162, "fusionlay": 162, "fusion_lay": [162, 164], "transformercrossattentionlay": [162, 164], "clip_vit_224": [162, 165], "projection_head": [162, 165], "feedforward": [162, 165], "register_fusion_modul": 162, "sequenti": [162, 165], "flamingo": [162, 164, 188], "Or": 162, "strict": [162, 163, 164, 171, 238], "fusion_vocab_s": 163, "fusion": [163, 164, 165], "necessit": 163, "rout": 163, "drop": [163, 240], "128": [163, 228, 237, 238, 239], "fusion_first": 164, "visual": 164, "few": [164, 234, 237, 238, 241], "shot": [164, 235, 237, 240], "infus": 164, "interpret": [164, 234], "enocd": 164, "isn": [164, 197, 232], "trainabl": [164, 167, 170, 206, 238, 239, 241], "fused_lay": 164, "insert": [164, 240], "mark": 165, "earli": 165, "peft": [166, 167, 168, 169, 170, 171, 172, 190, 228, 238, 241], "protocol": 166, "adapter_param": [166, 167, 168, 169, 170], "proj": 166, "in_dim": [166, 167, 238, 239, 241], "out_dim": [166, 167, 238, 239, 241], "loralinear": [166, 238, 241], "alpha": [167, 238, 239, 241], "use_bia": 167, "perturb": 167, "decomposit": [167, 238, 239], "matric": [167, 238, 241], "mapsto": 167, "w_0x": 167, "r": [167, 238], "bax": 167, "lora_a": [167, 238, 241], "lora_b": [167, 238, 241], "temporarili": 168, "neural": [168, 238, 241], "polici": [168, 174, 175, 176, 177, 178, 179, 189, 198, 206, 213, 228], "caller": 168, "whose": [168, 207, 212], "yield": 168, "get_adapter_param": [170, 238], "base_miss": 171, "base_unexpect": 171, "lora_miss": 171, "lora_unexpect": 171, "validate_state_dict_for_lora": [171, 238], "unlik": [171, 176], "reli": [171, 186, 235, 237], "unexpect": 171, "120600": 171, "nonempti": 171, "full_model_state_dict_kei": 172, "lora_state_dict_kei": 172, "base_model_state_dict_kei": 172, "confirm": [172, 226], "lora_modul": 172, "complement": 172, "disjoint": 172, "overlap": 172, "gamma": [173, 178, 179], "lmbda": 173, "estim": [173, 174], "1506": 173, "02438": 173, "predict": [173, 174, 177, 219, 228], "reponse_len": [173, 174], "receiv": [173, 233], "discount": 173, "gae": 173, "lambda": 173, "particip": [173, 188], "score": 174, "logprob": [174, 176, 179], "ref_logprob": 174, "kl_coeff": 174, "valid_score_idx": 174, "kl": 174, "coeffici": [174, 177], "response_len": 174, "total_reward": 174, "diverg": 174, "kl_reward": 174, "beta": [175, 179], "label_smooth": [175, 179], "18290": 175, "intuit": [175, 176, 178, 179], "dispref": 175, "dynam": [175, 240], "degener": 175, "occur": [175, 229], "naiv": 175, "trl": [175, 176, 178, 179], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 175, "dpo_train": [175, 176, 178], "l844": 175, "retain": [175, 239, 241], "2009": 175, "01325": 175, "regular": [175, 176, 179, 239, 240, 241], "baselin": [175, 177, 235, 238], "rather": [175, 239], "overhead": [175, 229, 239, 240], "temperatur": [175, 176, 178, 179, 219, 235], "uncertainti": [175, 179], "policy_chosen_logp": [175, 176, 178, 179], "policy_rejected_logp": [175, 176, 178, 179], "reference_chosen_logp": [175, 176, 178], "reference_rejected_logp": [175, 176, 178], "chosen_reward": [175, 176, 178, 179], "rejected_reward": [175, 176, 178, 179], "tau": 176, "optimis": 176, "ipo": 176, "2310": 176, "12036": 176, "pi": 176, "pi_ref": 176, "regress": [176, 178], "gap": 176, "likelihood": 176, "he": 176, "weaker": 176, "regularis": 176, "4dce042a3863db1d375358e8c8092b874b02934b": [176, 178], "l1143": 176, "reciproc": 176, "larger": [176, 179, 192, 235, 237, 239], "value_clip_rang": 177, "value_coeff": 177, "proxim": 177, "1707": 177, "06347": 177, "eqn": 177, "vwxyzjn": 177, "ccc19538e817e98a60d3253242ac15e2a562cb49": 177, "lm_human_preference_detail": 177, "train_policy_acceler": 177, "l719": 177, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 177, "ppo2": 177, "l68": 177, "l75": 177, "pi_old_logprob": 177, "pi_logprob": 177, "phi_old_valu": 177, "phi_valu": 177, "padding_mask": [177, 180], "value_padding_mask": 177, "old": 177, "participag": 177, "five": 177, "policy_loss": 177, "value_loss": 177, "clipfrac": 177, "fraction": 177, "statist": [178, 239], "rso": 178, "hing": 178, "2309": 178, "06657": 178, "logist": 178, "slic": 178, "10425": 178, "almost": [178, 238], "vector": [178, 233], "svm": 178, "counter": 178, "l1141": 178, "simpo": 179, "free": [179, 238], "2405": 179, "14734": 179, "averag": 179, "implicit": 179, "margin": 179, "bradlei": 179, "terri": 179, "win": 179, "lose": 179, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 179, "cpo_train": 179, "l603": 179, "pretti": [179, 235], "identitc": 179, "elimin": 179, "kind": 179, "ipoloss": 179, "stop_token": [180, 219], "fill_valu": 180, "stop": [180, 219], "sequence_length": 180, "pad_id": 180, "been": [180, 198, 233, 240], "stop_token_id": 180, "869": 180, "eos_mask": 180, "truncated_sequ": 180, "light": 183, "sentencepieceprocessor": 183, "prefix": [183, 239], "bos_id": [184, 186], "lightweight": [184, 233], "break": 184, "substr": 184, "repetit": 184, "identif": 184, "regex": 184, "absent": 184, "tokenizer_json_path": 185, "heavili": 186, "beggin": 186, "satisfi": [186, 235], "loos": 187, "image_token_id": 188, "encoder_max_seq_len": 188, "laid": 188, "fig": 188, "2204": 188, "14198": 188, "immedi": [188, 239], "until": [188, 239], "img1": 188, "img2": 188, "img3": 188, "dog": 188, "cat": 188, "text_seq_len": 188, "image_seq_len": 188, "datatyp": [189, 239, 241], "denot": 189, "auto_wrap_polici": [189, 198, 213], "submodul": [189, 206], "obei": 189, "contract": 189, "get_fsdp_polici": 189, "modules_to_wrap": [189, 198, 206], "min_num_param": 189, "my_fsdp_polici": 189, "recurs": [189, 206, 210], "isinst": [189, 234], "sum": [189, 238], "p": [189, 194, 238, 240, 241], "numel": [189, 238], "1000": [189, 240], "stabl": [189, 204, 210, 215, 226, 239], "safe_seri": 190, "from_pretrain": 190, "0001_of_0003": 190, "0002_of_0003": 190, "preserv": [190, 241], "weight_map": [190, 235], "convert_weight": 190, "_model_typ": [190, 193], "intermediate_checkpoint": [190, 191, 192], "adapter_onli": [190, 191, 192], "_weight_map": 190, "shard": [191, 237], "wip": 191, "qualnam": 193, "boundari": 193, "distinguish": 193, "mistral_reward_7b": 193, "my_new_model": 193, "my_custom_state_dict_map": 193, "optim_map": 194, "bare": 194, "bone": 194, "distribut": [194, 203, 204, 213, 215, 220, 227, 230, 232, 236, 237, 239], "optim_dict": [194, 196, 212], "cfg_optim": 194, "ckpt": 194, "optim_ckpt": 194, "placeholder_optim_dict": 194, "optiminbackwardwrapp": 194, "get_optim_kei": 194, "arbitrari": [194, 238, 239], "optim_ckpt_map": 194, "loadabl": 194, "ac_mod": 195, "ac_opt": 195, "select": 195, "op": [195, 240], "ac": [195, 198], "optimizerinbackwardwrapp": 196, "top": [196, 239, 241], "named_paramet": [196, 217], "float32": 197, "inde": [197, 235], "kernel": 197, "hardwar": [197, 227, 234, 235, 238, 239], "memory_efficient_fsdp_wrap": [198, 240], "maxim": [198, 206, 225, 227], "workload": [198, 229, 240], "alongsid": 198, "fullyshardeddataparallel": [198, 206], "fsdppolicytyp": [198, 206], "reset_stat": 199, "track": [199, 207], "alloc": [199, 205, 206, 237, 241], "reserv": [199, 205, 233, 241], "stat": [199, 205, 241], "int4": [200, 240], "4w": 200, "recogn": 200, "int8dynactint4weightquant": [200, 229, 240], "8da4w": [200, 240], "int8dynactint4weightqatquant": [200, 229, 240], "qat": [200, 225], "exclud": 201, "get_last_unmasked_token_idx": 201, "aka": 202, "condit": [204, 219, 232, 234], "master": 204, "port": [204, 232], "address": [204, 239], "hold": [204, 236], "peak_memory_act": 205, "peak_memory_alloc": 205, "peak_memory_reserv": 205, "get_memory_stat": 205, "hierarch": 206, "api_kei": 207, "experiment_kei": 207, "onlin": [207, 233], "log_cod": 207, "comet": 207, "www": 207, "site": [207, 234, 235], "ml": 207, "team": 207, "compar": [207, 210, 222, 235, 237, 238, 240, 241], "sdk": 207, "uncategor": 207, "alphanumer": 207, "charact": 207, "get_or_cr": 207, "fresh": 207, "persist": 207, "hpo": 207, "sweep": 207, "server": 207, "offlin": 207, "auto": [207, 232], "creation": 207, "experimentconfig": 207, "project_nam": 207, "my_project": [207, 211], "my_workspac": 207, "my_metr": [207, 210, 211], "importerror": [207, 211], "termin": [207, 210, 211], "comet_api_kei": 207, "flush": [207, 208, 209, 210, 211], "ndarrai": [207, 208, 209, 210, 211], "scalar": [207, 208, 209, 210, 211], "record": [207, 208, 209, 210, 211, 216], "log_config": [207, 211], "payload": [207, 208, 209, 210, 211], "filenam": 208, "log_": 208, "unixtimestamp": 208, "thread": 208, "safe": 208, "organize_log": 210, "tensorboard": 210, "subdirectori": 210, "logdir": 210, "startup": 210, "tree": [210, 234, 235, 237], "tfevent": 210, "encount": 210, "frontend": 210, "organ": [210, 232], "accordingli": [210, 240], "my_log_dir": 210, "view": 210, "entiti": 211, "bias": [211, 238, 241], "sent": 211, "usernam": 211, "my_ent": 211, "my_group": 211, "account": [211, 238, 241], "link": [211, 235, 237], "capecap": 211, "6053ofw0": 211, "torchtune_config_j67sb73v": 211, "soon": [212, 230, 239], "readi": [212, 225, 233, 240], "grad": 212, "acwrappolicytyp": 213, "author": [213, 227, 236, 239, 241], "fsdp_adavnced_tutori": 213, "insid": 214, "contextmanag": 214, "debug_mod": 215, "pseudo": 215, "commonli": [215, 238, 239, 241], "numpi": 215, "determinist": 215, "global": [215, 234, 239], "warn": 215, "nondeterminist": 215, "cudnn": 215, "set_deterministic_debug_mod": 215, "profile_memori": 216, "with_stack": 216, "record_shap": 216, "with_flop": 216, "wait_step": 216, "warmup_step": 216, "active_step": 216, "profil": 216, "layout": 216, "trace": 216, "profileract": 216, "gradient_accumul": 216, "sensibl": 216, "default_schedul": 216, "reduct": [216, 229, 238], "iter": [216, 217, 218, 241], "scope": 216, "flop": 216, "wait": 216, "cycl": 216, "repeat": [216, 239], "model_named_paramet": 217, "force_overrid": 217, "behaviour": 217, "concret": [217, 239], "vocab_dim": 217, "randomli": 217, "place": [217, 233, 234, 239], "named_param": 218, "max_generated_token": 219, "top_k": [219, 235], "custom_generate_next_token": 219, "prune": [219, 241], "generate_next_token": 219, "hi": [219, 233], "jeremi": 219, "handler": 221, "__version__": 222, "generated_examples_python": 223, "galleri": [223, 231], "sphinx": 223, "000": [224, 231, 237], "execut": [224, 231], "generated_exampl": 224, "mem": [224, 231], "mb": [224, 231], "topic": 225, "gentl": 225, "introduct": 225, "first_finetune_tutori": 225, "workflow": [225, 228, 234, 236, 238], "proper": [226, 236], "host": [226, 232, 236, 239], "torchvis": 226, "torchao": [226, 229, 235, 237, 240, 241], "latest": [226, 228, 229, 236, 239, 241], "url": 226, "whl": 226, "cu121": 226, "And": [226, 235], "ls": [226, 232, 235, 236, 237], "welcom": [226, 232], "greatest": [226, 236], "contributor": 226, "cd": [226, 235], "commit": 226, "branch": 226, "therebi": [226, 239, 240, 241], "forc": 226, "reinstal": 226, "opt": [226, 236], "suffix": 226, "On": [227, 238], "pointer": 227, "emphas": 227, "simplic": 227, "component": 227, "prove": 227, "democrat": 227, "box": [227, 228, 229, 241], "zoo": 227, "varieti": [227, 230, 238], "integr": [227, 235, 236, 237, 238, 240, 241], "excit": 227, "checkout": 227, "quickstart": 227, "attain": 227, "eager": 227, "embodi": 227, "philosophi": 227, "usabl": 227, "composit": 227, "hard": [227, 234], "outlin": 227, "unecessari": 227, "never": 227, "thoroughli": 227, "competit": 228, "grant": [228, 229, 236], "interest": [228, 229, 230, 235], "8b_lora_single_devic": [228, 232, 233, 237, 239], "adjust": [228, 229, 234, 239, 240], "imapct": 228, "aggress": 228, "tradeoff": [228, 238], "slower": [228, 239, 241], "lever": [228, 229], "too": [228, 229, 237], "action": [228, 229], "degrad": [229, 239, 240, 241], "simul": [229, 239, 240], "compromis": 229, "blogpost": [229, 239], "qat_distribut": [229, 240], "8b_qat_ful": [229, 240], "least": [229, 237, 238, 240], "vram": [229, 237, 238, 240], "80gb": [229, 240], "delai": 229, "fake": [229, 240], "empir": [229, 240], "potenti": [229, 238, 239], "fake_quant_after_n_step": [229, 240], "idea": [229, 241], "roughli": 229, "total_step": 229, "plan": [229, 235], "un": 229, "groupsiz": [229, 240], "256": [229, 237, 240], "hackabl": [230, 236], "singularli": [230, 236], "funetun": 230, "awar": [230, 239, 240], "favourit": 230, "love": 230, "tracker": 230, "issu": [230, 240], "short": 232, "subcommand": 232, "anytim": 232, "symlink": 232, "wrote": 232, "readm": [232, 235, 237], "md": 232, "lot": [232, 235, 239], "recent": 232, "agre": 232, "term": [232, 239], "perman": 232, "eat": 232, "bandwith": 232, "storag": [232, 241], "00030": 232, "ootb": 232, "full_finetune_single_devic": [232, 234, 235, 236], "7b_full_low_memori": [232, 235, 236], "8b_full_single_devic": [232, 234], "mini_full_low_memori": 232, "7b_full": [232, 235, 236], "13b_full": [232, 235, 236], "70b_full": 232, "edit": 232, "clobber": 232, "destin": 232, "lora_finetune_distribut": [232, 237, 238], "torchrun": 232, "launch": [232, 233, 236], "nproc": 232, "node": 232, "worker": 232, "nnode": [232, 238, 240], "minimum_nod": 232, "maximum_nod": 232, "fail": 232, "rdzv": 232, "rendezv": 232, "endpoint": 232, "8b_lora": [232, 237], "bypass": 232, "fancy_lora": 232, "8b_fancy_lora": 232, "sai": [232, 233, 236], "know": [233, 234, 235, 238], "intend": 233, "nice": 233, "meet": 233, "overhaul": 233, "begin_of_text": 233, "start_header_id": 233, "end_header_id": 233, "eot_id": 233, "untrain": 233, "accompani": 233, "who": 233, "influenti": 233, "hip": 233, "hop": 233, "artist": 233, "2pac": 233, "rakim": 233, "na": 233, "llama2chatformat": [233, 234], "flavor": [233, 234], "msg": 233, "formatted_messag": [233, 234], "nyou": [233, 234], "nwho": 233, "why": [233, 236, 238], "518": 233, "25580": 233, "29962": 233, "3532": 233, "14816": 233, "29903": 233, "6778": 233, "_spm_model": 233, "piece_to_id": 233, "manual": [233, 241], "529": 233, "29879": 233, "29958": 233, "nhere": 233, "128000": [233, 240], "128009": 233, "pure": 233, "That": 233, "mess": 233, "govern": 233, "prime": 233, "strictli": 233, "ask": [233, 239], "untouch": 233, "though": 233, "robust": 233, "forum": 233, "panda": 233, "pd": 233, "df": 233, "read_csv": 233, "your_fil": 233, "nrow": 233, "tolist": 233, "iloc": 233, "gp": 233, "satellit": 233, "thing": [233, 239, 241], "message_convert": 233, "input_msg": 233, "output_msg": 233, "But": [233, 235, 238], "mistralchatformat": 233, "chatdataset": [233, 234], "custom_dataset": 233, "2048": 233, "honor": 233, "copi": [233, 235, 236, 237, 240, 241], "custom_8b_lora_single_devic": 233, "steer": 234, "wheel": 234, "publicli": 234, "great": [234, 235, 239], "hood": [234, 241], "text_completion_dataset": [234, 240], "upper": 234, "constraint": [234, 238], "slow": [234, 239, 241], "signific": [234, 239, 240], "speedup": [234, 235, 237], "my_data": 234, "fix": [234, 240], "goal": [234, 240], "respond": 234, "plant": 234, "miner": 234, "oak": 234, "copper": 234, "ore": 234, "eleph": 234, "customtempl": 234, "importlib": 234, "import_modul": 234, "mechan": 234, "search": 234, "often": [234, 238, 239], "runtim": 234, "pythonpath": 234, "quit": [234, 239, 241], "customchatformat": 234, "concatdataset": 234, "drive": 234, "rajpurkar": 234, "io": 234, "squad": 234, "explor": 234, "chosen_messag": 234, "key_chosen": 234, "rejected_messag": 234, "key_reject": 234, "c_mask": 234, "np": 234, "cross_entropy_ignore_idx": 234, "r_mask": 234, "stack_exchanged_paired_dataset": 234, "had": 234, "1024": [234, 240], "stackexchangedpairedtempl": 234, "response_j": 234, "response_k": 234, "rl": 234, "favorit": [235, 238], "seemlessli": 235, "beyond": [235, 239, 241], "connect": [235, 240], "amount": 235, "natur": 235, "export": 235, "mobil": 235, "phone": 235, "leverag": [235, 237, 241], "plai": [235, 239], "freez": [235, 238], "percentag": 235, "16gb": [235, 238], "rtx": 235, "3090": 235, "4090": 235, "hour": 235, "7b_qlora_single_devic": [235, 236, 241], "473": 235, "98": [235, 241], "gb": [235, 237, 238, 240, 241], "484": 235, "01": [235, 236], "fact": [235, 237, 238], "third": 235, "eleuther_ev": [235, 237, 240], "eleuther_evalu": [235, 237, 240], "lm_eval": [235, 237], "custom_eval_config": [235, 237], "truthfulqa_mc2": [235, 237, 238], "measur": [235, 237], "propens": [235, 237], "324": 235, "loglikelihood": 235, "195": 235, "121": 235, "197": 235, "acc": [235, 240], "388": 235, "shown": [235, 240], "489": 235, "seem": 235, "custom_generation_config": [235, 237], "kick": 235, "300": 235, "bai": 235, "area": 235, "92": 235, "exploratorium": 235, "san": 235, "francisco": 235, "magazin": 235, "awesom": 235, "bridg": 235, "cool": 235, "96": [235, 241], "sec": [235, 237], "83": 235, "99": [235, 238], "72": 235, "littl": 235, "int8_weight_onli": [235, 237], "int8_dynamic_activation_int8_weight": [235, 237], "ao": [235, 237], "quant_api": [235, 237], "quantize_": [235, 237], "int4_weight_onli": [235, 237], "previous": [235, 237, 238], "benefit": 235, "doesn": 235, "fast": 235, "clone": [235, 238, 240, 241], "assumpt": 235, "new_dir": 235, "output_dict": 235, "sd_1": 235, "sd_2": 235, "dump": 235, "convert_hf_checkpoint": 235, "checkpoint_path": 235, "justin": 235, "school": 235, "math": 235, "teacher": 235, "ws": 235, "94": [235, 237], "bandwidth": [235, 237], "1391": 235, "84": 235, "thats": 235, "seamlessli": 235, "authent": [235, 236], "hopefulli": 235, "gave": 235, "minut": 236, "agreement": 236, "depth": 236, "principl": 236, "boilerpl": 236, "substanti": [236, 238], "custom_config": 236, "replic": 236, "info": 236, "lorafinetunerecipesingledevic": 236, "lora_finetune_output": 236, "log_1713194212": 236, "3697006702423096": 236, "25880": [236, 241], "83it": 236, "monitor": 236, "tqdm": 236, "interv": 236, "e2": 236, "focu": 237, "theta": 237, "observ": [237, 240], "consum": [237, 241], "overal": 237, "8b_qlora_single_devic": [237, 239], "coupl": [237, 238, 241], "meta_model_0": [237, 240], "did": [237, 241], "122": 237, "sarah": 237, "busi": 237, "mum": 237, "young": 237, "children": 237, "live": 237, "north": 237, "east": 237, "england": 237, "135": 237, "88": 237, "138": 237, "346": 237, "09": 237, "139": 237, "broader": 237, "teach": 238, "straight": 238, "jump": 238, "unfamiliar": 238, "oppos": [238, 241], "momentum": [238, 239], "aghajanyan": 238, "et": 238, "al": 238, "hypothes": 238, "intrins": 238, "four": 238, "eight": 238, "practic": 238, "blue": 238, "rememb": 238, "approx": 238, "15m": 238, "8192": [238, 240], "65k": 238, "requires_grad": [238, 241], "frozen_out": [238, 241], "lora_out": [238, 241], "omit": [238, 239], "base_model": 238, "lora_model": 238, "lora_llama_2_7b": [238, 241], "alon": 238, "bit": [238, 239, 240, 241], "in_featur": [238, 240], "out_featur": [238, 240], "inplac": 238, "feel": 238, "validate_missing_and_unexpected_for_lora": 238, "peft_util": 238, "set_trainable_param": 238, "fetch": 238, "lora_param": 238, "total_param": 238, "trainable_param": 238, "2f": 238, "6742609920": 238, "4194304": 238, "7b_lora": 238, "my_model_checkpoint_path": [238, 240, 241], "tokenizer_checkpoint": [238, 240, 241], "my_tokenizer_checkpoint_path": [238, 240, 241], "factori": 238, "benefici": 238, "impact": [238, 239], "minor": 238, "good": [238, 239], "lora_experiment_1": 238, "smooth": [238, 241], "curv": [238, 241], "500": 238, "ran": 238, "footprint": [238, 240], "commod": 238, "cogniz": 238, "ax": 238, "parallel": 238, "truthfulqa": 238, "475": 238, "87": 238, "508": 238, "86": 238, "504": 238, "04": 238, "514": 238, "lowest": 238, "absolut": 238, "4gb": 238, "salman": 239, "mohammadi": 239, "brief": 239, "glossari": 239, "leav": 239, "struggl": 239, "constrain": [239, 240], "particularli": 239, "gradient_accumulation_step": 239, "throughput": 239, "cost": 239, "sebastian": 239, "raschka": 239, "fp16": 239, "sound": 239, "quot": 239, "aliv": 239, "region": 239, "enable_activation_checkpoint": 239, "total_batch_s": 239, "count": 239, "suppos": 239, "log_every_n_step": 239, "translat": 239, "frequent": 239, "slowli": 239, "num_devic": 239, "adamw8bit": 239, "pagedadamw": 239, "modern": 239, "converg": 239, "stateless": 239, "stochast": 239, "descent": 239, "sacrif": 239, "optimizer_in_bwd": 239, "greatli": 239, "lora_": 239, "lora_llama3": 239, "aim": 239, "_lora": 239, "firstli": 239, "secondli": 239, "affect": 239, "fashion": 239, "jointli": 239, "sens": 239, "novel": 239, "normalfloat": [239, 241], "8x": [239, 241], "worth": 239, "cast": [239, 240], "incur": [239, 240, 241], "penalti": 239, "demonstr": [239, 240], "qlora_": 239, "qlora_llama3": 239, "_qlora": 239, "perplex": 240, "ultim": 240, "ptq": 240, "kept": 240, "width": 240, "nois": 240, "henc": 240, "x_q": 240, "int8": 240, "zp": 240, "x_float": 240, "qmin": 240, "qmax": 240, "clamp": 240, "x_fq": 240, "dequant": 240, "proce": 240, "prepared_model": 240, "swap": 240, "int8dynactint4weightqatlinear": 240, "int8dynactint4weightlinear": 240, "train_loop": 240, "converted_model": 240, "qat_distributed_recipe_label": 240, "recov": 240, "modif": 240, "custom_8b_qat_ful": 240, "led": 240, "presum": 240, "mutat": 240, "5gb": 240, "custom_quant": 240, "poorli": 240, "custom_eleuther_evalu": 240, "fullmodeltorchtunecheckpoint": 240, "hellaswag": 240, "max_seq_length": 240, "my_eleuther_evalu": 240, "stderr": 240, "word_perplex": 240, "9148": 240, "byte_perplex": 240, "5357": 240, "bits_per_byt": 240, "6189": 240, "5687": 240, "0049": 240, "acc_norm": 240, "7536": 240, "0043": 240, "portion": [240, 241], "74": 240, "048": 240, "190": 240, "7735": 240, "5598": 240, "6413": 240, "5481": 240, "0050": 240, "7390": 240, "0044": 240, "7251": 240, "4994": 240, "5844": 240, "5740": 240, "7610": 240, "outperform": 240, "importantli": 240, "characterist": 240, "187": 240, "958": 240, "halv": 240, "int4weightonlyquant": 240, "motiv": 240, "edg": 240, "smartphon": 240, "executorch": 240, "xnnpack": 240, "export_llama": 240, "use_sdpa_with_kv_cach": 240, "qmode": 240, "group_siz": 240, "get_bos_id": 240, "get_eos_id": 240, "128001": 240, "output_nam": 240, "llama3_8da4w": 240, "pte": 240, "881": 240, "oneplu": 240, "709": 240, "tok": 240, "815": 240, "316": 240, "364": 240, "highli": 241, "vanilla": 241, "held": 241, "bespok": 241, "vast": 241, "major": 241, "normatfloat": 241, "themselv": 241, "deepdiv": 241, "distinct": 241, "de": 241, "counterpart": 241, "set_default_devic": 241, "qlora_linear": 241, "memory_alloc": 241, "177": 241, "152": 241, "del": 241, "empty_cach": 241, "lora_linear": 241, "081": 241, "344": 241, "qlora_llama2_7b": 241, "qlora_model": 241, "essenti": 241, "reparametrize_as_dtype_state_dict_post_hook": 241, "149": 241, "9157477021217346": 241, "02": 241, "08": 241, "15it": 241, "nightli": 241, "200": 241, "hundr": 241, "228": 241, "8158286809921265": 241, "95it": 241, "exercis": 241, "linear_nf4": 241, "to_nf4": 241, "linear_weight": 241, "autograd": 241, "incom": 241}, "objects": {"torchtune.config": [[12, 0, 1, "", "instantiate"], [13, 0, 1, "", "log_config"], [14, 0, 1, "", "parse"], [15, 0, 1, "", "validate"]], "torchtune.data": [[16, 1, 1, "", "ChatFormat"], [17, 1, 1, "", "ChatMLTemplate"], [18, 1, 1, "", "ChosenRejectedToMessages"], [19, 3, 1, "", "GrammarErrorCorrectionTemplate"], [20, 1, 1, "", "InputOutputToMessages"], [21, 1, 1, "", "InstructTemplate"], [22, 1, 1, "", "JSONToMessages"], [23, 1, 1, "", "Message"], [24, 1, 1, "", "PromptTemplate"], [25, 1, 1, "", "PromptTemplateInterface"], [26, 3, 1, "", "QuestionAnswerTemplate"], [27, 3, 1, "", "Role"], [28, 1, 1, "", "ShareGPTToMessages"], [29, 3, 1, "", "SummarizeTemplate"], [30, 0, 1, "", "get_openai_messages"], [31, 0, 1, "", "get_sharegpt_messages"], [32, 0, 1, "", "left_pad_sequence"], [33, 0, 1, "", "padded_collate"], [34, 0, 1, "", "padded_collate_dpo"], [35, 0, 1, "", "padded_collate_sft"], [36, 0, 1, "", "truncate"], [37, 0, 1, "", "validate_messages"]], "torchtune.data.ChatFormat": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[21, 2, 1, "", "format"]], "torchtune.data.Message": [[23, 4, 1, "", "contains_media"], [23, 2, 1, "", "from_dict"], [23, 4, 1, "", "text_content"]], "torchtune.datasets": [[38, 3, 1, "", "ChatDataset"], [39, 1, 1, "", "ConcatDataset"], [40, 3, 1, "", "InstructDataset"], [41, 1, 1, "", "PackedDataset"], [42, 1, 1, "", "PreferenceDataset"], [43, 1, 1, "", "SFTDataset"], [44, 1, 1, "", "TextCompletionDataset"], [45, 0, 1, "", "alpaca_cleaned_dataset"], [46, 0, 1, "", "alpaca_dataset"], [47, 0, 1, "", "chat_dataset"], [48, 0, 1, "", "cnn_dailymail_articles_dataset"], [49, 0, 1, "", "grammar_dataset"], [50, 0, 1, "", "instruct_dataset"], [51, 0, 1, "", "llava_instruct_dataset"], [52, 0, 1, "", "samsum_dataset"], [53, 0, 1, "", "slimorca_dataset"], [54, 0, 1, "", "stack_exchange_paired_dataset"], [55, 0, 1, "", "text_completion_dataset"], [56, 0, 1, "", "the_cauldron_dataset"], [57, 0, 1, "", "wikitext_dataset"]], "torchtune.models.clip": [[58, 1, 1, "", "TilePositionalEmbedding"], [59, 1, 1, "", "TiledTokenPositionalEmbedding"], [60, 1, 1, "", "TokenPositionalEmbedding"], [61, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[58, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[59, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[60, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[62, 0, 1, "", "code_llama2_13b"], [63, 0, 1, "", "code_llama2_70b"], [64, 0, 1, "", "code_llama2_7b"], [65, 0, 1, "", "lora_code_llama2_13b"], [66, 0, 1, "", "lora_code_llama2_70b"], [67, 0, 1, "", "lora_code_llama2_7b"], [68, 0, 1, "", "qlora_code_llama2_13b"], [69, 0, 1, "", "qlora_code_llama2_70b"], [70, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[71, 1, 1, "", "GemmaTokenizer"], [72, 0, 1, "", "gemma"], [73, 0, 1, "", "gemma_2b"], [74, 0, 1, "", "gemma_7b"], [75, 0, 1, "", "gemma_tokenizer"], [76, 0, 1, "", "lora_gemma"], [77, 0, 1, "", "lora_gemma_2b"], [78, 0, 1, "", "lora_gemma_7b"], [79, 0, 1, "", "qlora_gemma_2b"], [80, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[71, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[81, 1, 1, "", "Llama2ChatTemplate"], [82, 1, 1, "", "Llama2Tokenizer"], [83, 0, 1, "", "llama2"], [84, 0, 1, "", "llama2_13b"], [85, 0, 1, "", "llama2_70b"], [86, 0, 1, "", "llama2_7b"], [87, 0, 1, "", "llama2_reward_7b"], [88, 0, 1, "", "llama2_tokenizer"], [89, 0, 1, "", "lora_llama2"], [90, 0, 1, "", "lora_llama2_13b"], [91, 0, 1, "", "lora_llama2_70b"], [92, 0, 1, "", "lora_llama2_7b"], [93, 0, 1, "", "lora_llama2_reward_7b"], [94, 0, 1, "", "qlora_llama2_13b"], [95, 0, 1, "", "qlora_llama2_70b"], [96, 0, 1, "", "qlora_llama2_7b"], [97, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[82, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[98, 1, 1, "", "Llama3Tokenizer"], [99, 0, 1, "", "llama3"], [100, 0, 1, "", "llama3_70b"], [101, 0, 1, "", "llama3_8b"], [102, 0, 1, "", "llama3_tokenizer"], [103, 0, 1, "", "lora_llama3"], [104, 0, 1, "", "lora_llama3_70b"], [105, 0, 1, "", "lora_llama3_8b"], [106, 0, 1, "", "qlora_llama3_70b"], [107, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[98, 2, 1, "", "decode"], [98, 2, 1, "", "tokenize_message"], [98, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[108, 0, 1, "", "llama3_1"], [109, 0, 1, "", "llama3_1_405b"], [110, 0, 1, "", "llama3_1_70b"], [111, 0, 1, "", "llama3_1_8b"], [112, 0, 1, "", "lora_llama3_1"], [113, 0, 1, "", "lora_llama3_1_70b"], [114, 0, 1, "", "lora_llama3_1_8b"], [115, 0, 1, "", "qlora_llama3_1_70b"], [116, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[117, 1, 1, "", "MistralChatTemplate"], [118, 1, 1, "", "MistralTokenizer"], [119, 0, 1, "", "lora_mistral"], [120, 0, 1, "", "lora_mistral_7b"], [121, 0, 1, "", "lora_mistral_classifier"], [122, 0, 1, "", "lora_mistral_reward_7b"], [123, 0, 1, "", "mistral"], [124, 0, 1, "", "mistral_7b"], [125, 0, 1, "", "mistral_classifier"], [126, 0, 1, "", "mistral_reward_7b"], [127, 0, 1, "", "mistral_tokenizer"], [128, 0, 1, "", "qlora_mistral_7b"], [129, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[118, 2, 1, "", "decode"], [118, 2, 1, "", "encode"], [118, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[130, 1, 1, "", "Phi3MiniTokenizer"], [131, 0, 1, "", "lora_phi3"], [132, 0, 1, "", "lora_phi3_mini"], [133, 0, 1, "", "phi3"], [134, 0, 1, "", "phi3_mini"], [135, 0, 1, "", "phi3_mini_tokenizer"], [136, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[130, 2, 1, "", "decode"], [130, 2, 1, "", "tokenize_messages"]], "torchtune.models.qwen2": [[137, 1, 1, "", "Qwen2Tokenizer"], [138, 0, 1, "", "lora_qwen2"], [139, 0, 1, "", "lora_qwen2_0_5b"], [140, 0, 1, "", "lora_qwen2_1_5b"], [141, 0, 1, "", "lora_qwen2_7b"], [142, 0, 1, "", "qwen2"], [143, 0, 1, "", "qwen2_0_5b"], [144, 0, 1, "", "qwen2_1_5b"], [145, 0, 1, "", "qwen2_7b"], [146, 0, 1, "", "qwen2_tokenizer"]], "torchtune.models.qwen2.Qwen2Tokenizer": [[137, 2, 1, "", "decode"], [137, 2, 1, "", "encode"], [137, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[147, 1, 1, "", "FeedForward"], [148, 1, 1, "", "Fp32LayerNorm"], [149, 1, 1, "", "KVCache"], [150, 1, 1, "", "MultiHeadAttention"], [151, 1, 1, "", "RMSNorm"], [152, 1, 1, "", "RotaryPositionalEmbeddings"], [153, 1, 1, "", "TanhGate"], [154, 1, 1, "", "TiedEmbeddingTransformerDecoder"], [155, 1, 1, "", "TransformerCrossAttentionLayer"], [156, 1, 1, "", "TransformerDecoder"], [157, 1, 1, "", "TransformerSelfAttentionLayer"], [158, 1, 1, "", "VisionTransformer"], [160, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.FeedForward": [[147, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[148, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[149, 2, 1, "", "reset"], [149, 2, 1, "", "update"]], "torchtune.modules.MultiHeadAttention": [[150, 2, 1, "", "forward"], [150, 2, 1, "", "reset_cache"], [150, 2, 1, "", "setup_cache"]], "torchtune.modules.RMSNorm": [[151, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[152, 2, 1, "", "forward"]], "torchtune.modules.TanhGate": [[153, 2, 1, "", "forward"]], "torchtune.modules.TiedEmbeddingTransformerDecoder": [[154, 2, 1, "", "caches_are_enabled"], [154, 2, 1, "", "forward"], [154, 2, 1, "", "reset_caches"], [154, 2, 1, "", "set_num_output_chunks"], [154, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerCrossAttentionLayer": [[155, 4, 1, "", "cache_enabled"], [155, 2, 1, "", "forward"], [155, 2, 1, "", "reset_cache"], [155, 2, 1, "", "setup_cache"]], "torchtune.modules.TransformerDecoder": [[156, 2, 1, "", "caches_are_enabled"], [156, 2, 1, "", "forward"], [156, 2, 1, "", "reset_caches"], [156, 2, 1, "", "set_num_output_chunks"], [156, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerSelfAttentionLayer": [[157, 4, 1, "", "cache_enabled"], [157, 2, 1, "", "forward"], [157, 2, 1, "", "reset_cache"], [157, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[158, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[159, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[161, 1, 1, "", "CEWithChunkedOutputLoss"]], "torchtune.modules.loss.CEWithChunkedOutputLoss": [[161, 2, 1, "", "compute_cross_entropy"], [161, 2, 1, "", "forward"]], "torchtune.modules.model_fusion": [[162, 1, 1, "", "DeepFusionModel"], [163, 1, 1, "", "FusionEmbedding"], [164, 1, 1, "", "FusionLayer"], [165, 0, 1, "", "register_fusion_module"]], "torchtune.modules.model_fusion.DeepFusionModel": [[162, 2, 1, "", "caches_are_enabled"], [162, 2, 1, "", "forward"], [162, 2, 1, "", "reset_caches"], [162, 2, 1, "", "setup_caches"]], "torchtune.modules.model_fusion.FusionEmbedding": [[163, 2, 1, "", "forward"], [163, 2, 1, "", "fusion_params"]], "torchtune.modules.model_fusion.FusionLayer": [[164, 4, 1, "", "cache_enabled"], [164, 2, 1, "", "forward"], [164, 2, 1, "", "fusion_params"], [164, 2, 1, "", "reset_cache"], [164, 2, 1, "", "setup_cache"]], "torchtune.modules.peft": [[166, 1, 1, "", "AdapterModule"], [167, 1, 1, "", "LoRALinear"], [168, 0, 1, "", "disable_adapter"], [169, 0, 1, "", "get_adapter_params"], [170, 0, 1, "", "set_trainable_params"], [171, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [172, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[166, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[167, 2, 1, "", "adapter_params"], [167, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[173, 0, 1, "", "estimate_advantages"], [174, 0, 1, "", "get_rewards_ppo"], [180, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.rlhf.loss": [[175, 1, 1, "", "DPOLoss"], [176, 1, 1, "", "IPOLoss"], [177, 1, 1, "", "PPOLoss"], [178, 1, 1, "", "RSOLoss"], [179, 1, 1, "", "SimPOLoss"]], "torchtune.modules.rlhf.loss.DPOLoss": [[175, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.IPOLoss": [[176, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.PPOLoss": [[177, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.RSOLoss": [[178, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.SimPOLoss": [[179, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[181, 1, 1, "", "BaseTokenizer"], [182, 1, 1, "", "ModelTokenizer"], [183, 1, 1, "", "SentencePieceBaseTokenizer"], [184, 1, 1, "", "TikTokenBaseTokenizer"], [185, 0, 1, "", "parse_hf_tokenizer_json"], [186, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[181, 2, 1, "", "decode"], [181, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[182, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[183, 2, 1, "", "decode"], [183, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[184, 2, 1, "", "decode"], [184, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[187, 1, 1, "", "Transform"], [188, 1, 1, "", "VisionCrossAttentionMask"]], "torchtune.training": [[189, 3, 1, "", "FSDPPolicyType"], [190, 1, 1, "", "FullModelHFCheckpointer"], [191, 1, 1, "", "FullModelMetaCheckpointer"], [192, 1, 1, "", "FullModelTorchTuneCheckpointer"], [193, 1, 1, "", "ModelType"], [194, 1, 1, "", "OptimizerInBackwardWrapper"], [195, 0, 1, "", "apply_selective_activation_checkpointing"], [196, 0, 1, "", "create_optim_in_bwd_wrapper"], [197, 0, 1, "", "get_dtype"], [198, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [199, 0, 1, "", "get_memory_stats"], [200, 0, 1, "", "get_quantizer_mode"], [201, 0, 1, "", "get_unmasked_sequence_lengths"], [202, 0, 1, "", "get_world_size_and_rank"], [203, 0, 1, "", "init_distributed"], [204, 0, 1, "", "is_distributed"], [205, 0, 1, "", "log_memory_stats"], [206, 0, 1, "", "lora_fsdp_wrap_policy"], [212, 0, 1, "", "register_optim_in_bwd_hooks"], [213, 0, 1, "", "set_activation_checkpointing"], [214, 0, 1, "", "set_default_dtype"], [215, 0, 1, "", "set_seed"], [216, 0, 1, "", "setup_torch_profiler"], [217, 0, 1, "", "update_state_dict_for_classifier"], [218, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.training.FullModelHFCheckpointer": [[190, 2, 1, "", "load_checkpoint"], [190, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelMetaCheckpointer": [[191, 2, 1, "", "load_checkpoint"], [191, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelTorchTuneCheckpointer": [[192, 2, 1, "", "load_checkpoint"], [192, 2, 1, "", "save_checkpoint"]], "torchtune.training.OptimizerInBackwardWrapper": [[194, 2, 1, "", "get_optim_key"], [194, 2, 1, "", "load_state_dict"], [194, 2, 1, "", "state_dict"]], "torchtune.training.metric_logging": [[207, 1, 1, "", "CometLogger"], [208, 1, 1, "", "DiskLogger"], [209, 1, 1, "", "StdoutLogger"], [210, 1, 1, "", "TensorBoardLogger"], [211, 1, 1, "", "WandBLogger"]], "torchtune.training.metric_logging.CometLogger": [[207, 2, 1, "", "close"], [207, 2, 1, "", "log"], [207, 2, 1, "", "log_config"], [207, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.DiskLogger": [[208, 2, 1, "", "close"], [208, 2, 1, "", "log"], [208, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.StdoutLogger": [[209, 2, 1, "", "close"], [209, 2, 1, "", "log"], [209, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.TensorBoardLogger": [[210, 2, 1, "", "close"], [210, 2, 1, "", "log"], [210, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.WandBLogger": [[211, 2, 1, "", "close"], [211, 2, 1, "", "log"], [211, 2, 1, "", "log_config"], [211, 2, 1, "", "log_dict"]], "torchtune.utils": [[219, 0, 1, "", "generate"], [220, 0, 1, "", "get_device"], [221, 0, 1, "", "get_logger"], [222, 0, 1, "", "torch_version_ge"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 7, 19, 26, 27, 29, 38, 40, 189, 225, 227, 232, 235, 237, 238, 240, 241], "config": [0, 9, 10, 232, 236], "data": [1, 19, 26, 27, 29, 233], "text": [1, 2, 234, 237], "templat": [1, 233, 234], "type": 1, "convert": 1, "messag": [1, 23], "transform": [1, 4, 187], "collat": 1, "helper": 1, "function": 1, "dataset": [2, 38, 40, 233, 234], "onli": [2, 9], "multimod": 2, "gener": [2, 219, 235, 237], "builder": 2, "class": [2, 10], "model": [3, 4, 11, 232, 235, 236, 237, 238, 239, 240], "llama3": [3, 99, 233, 237, 240], "1": 3, "llama2": [3, 83, 233, 235, 238, 241], "code": 3, "llama": 3, "qwen": 3, "2": 3, "phi": 3, "3": 3, "mistral": [3, 123], "gemma": [3, 72], "clip": 3, "modul": 4, "compon": [4, 9, 239], "build": [4, 226, 241], "block": 4, "base": 4, "token": [4, 233], "util": [4, 6], "peft": [4, 239], "fusion": 4, "vision": 4, "reinforc": 4, "learn": 4, "from": [4, 233, 241], "human": 4, "feedback": 4, "rlhf": 4, "loss": 4, "train": [5, 189, 229, 236], "checkpoint": [5, 7, 11, 235, 239], "reduc": 5, "precis": [5, 239], "distribut": [5, 229], "memori": [5, 234, 238, 239, 241], "manag": 5, "metric": [5, 8, 11], "log": [5, 8, 11], "perform": [5, 238], "profil": 5, "miscellan": [5, 6], "overview": [7, 227, 230, 235, 239], "format": [7, 234], "handl": 7, "differ": 7, "hfcheckpoint": 7, "metacheckpoint": 7, "torchtunecheckpoint": 7, "intermedi": 7, "vs": 7, "final": 7, "lora": [7, 228, 235, 238, 239, 241], "put": [7, 241], "thi": 7, "all": [7, 9, 241], "togeth": [7, 241], "comet": 8, "logger": [8, 11], "about": 9, "where": 9, "do": 9, "paramet": [9, 239], "live": 9, "write": 9, "configur": [9, 234], "us": [9, 10, 233, 235, 241], "instanti": [9, 12], "referenc": 9, "other": [9, 235], "field": 9, "interpol": 9, "valid": [9, 15, 232], "your": [9, 10, 235, 236], "best": 9, "practic": 9, "airtight": 9, "public": 9, "api": 9, "command": 9, "line": 9, "overrid": 9, "remov": 9, "what": [10, 227, 238, 240, 241], "ar": 10, "recip": [10, 230, 232, 236, 238, 240], "script": 10, "run": [10, 232, 235], "cli": [10, 232], "pars": [10, 14], "weight": 11, "bias": 11, "w": 11, "b": 11, "log_config": 13, "chatformat": 16, "chatmltempl": 17, "chosenrejectedtomessag": 18, "grammarerrorcorrectiontempl": 19, "inputoutputtomessag": 20, "instructtempl": 21, "jsontomessag": 22, "prompttempl": 24, "prompttemplateinterfac": 25, "questionanswertempl": 26, "role": 27, "sharegpttomessag": 28, "summarizetempl": 29, "get_openai_messag": 30, "get_sharegpt_messag": 31, "left_pad_sequ": 32, "padded_col": 33, "padded_collate_dpo": 34, "padded_collate_sft": 35, "truncat": 36, "validate_messag": 37, "chatdataset": 38, "concatdataset": 39, "instructdataset": 40, "packeddataset": 41, "preferencedataset": 42, "sftdataset": 43, "textcompletiondataset": 44, "alpaca_cleaned_dataset": 45, "alpaca_dataset": 46, "chat_dataset": 47, "cnn_dailymail_articles_dataset": 48, "grammar_dataset": 49, "instruct_dataset": 50, "llava_instruct_dataset": 51, "samsum_dataset": 52, "slimorca_dataset": 53, "stack_exchange_paired_dataset": 54, "text_completion_dataset": 55, "the_cauldron_dataset": 56, "wikitext_dataset": 57, "tilepositionalembed": 58, "tiledtokenpositionalembed": 59, "tokenpositionalembed": 60, "clip_vision_encod": 61, "code_llama2_13b": 62, "code_llama2_70b": 63, "code_llama2_7b": 64, "lora_code_llama2_13b": 65, "lora_code_llama2_70b": 66, "lora_code_llama2_7b": 67, "qlora_code_llama2_13b": 68, "qlora_code_llama2_70b": 69, "qlora_code_llama2_7b": 70, "gemmatoken": 71, "gemma_2b": 73, "gemma_7b": 74, "gemma_token": 75, "lora_gemma": 76, "lora_gemma_2b": 77, "lora_gemma_7b": 78, "qlora_gemma_2b": 79, "qlora_gemma_7b": 80, "llama2chattempl": 81, "llama2token": 82, "llama2_13b": 84, "llama2_70b": 85, "llama2_7b": 86, "llama2_reward_7b": 87, "llama2_token": 88, "lora_llama2": 89, "lora_llama2_13b": 90, "lora_llama2_70b": 91, "lora_llama2_7b": 92, "lora_llama2_reward_7b": 93, "qlora_llama2_13b": 94, "qlora_llama2_70b": 95, "qlora_llama2_7b": 96, "qlora_llama2_reward_7b": 97, "llama3token": 98, "llama3_70b": 100, "llama3_8b": 101, "llama3_token": 102, "lora_llama3": 103, "lora_llama3_70b": 104, "lora_llama3_8b": 105, "qlora_llama3_70b": 106, "qlora_llama3_8b": 107, "llama3_1": 108, "llama3_1_405b": 109, "llama3_1_70b": 110, "llama3_1_8b": 111, "lora_llama3_1": 112, "lora_llama3_1_70b": 113, "lora_llama3_1_8b": 114, "qlora_llama3_1_70b": 115, "qlora_llama3_1_8b": 116, "mistralchattempl": 117, "mistraltoken": 118, "lora_mistr": 119, "lora_mistral_7b": 120, "lora_mistral_classifi": 121, "lora_mistral_reward_7b": 122, "mistral_7b": 124, "mistral_classifi": 125, "mistral_reward_7b": 126, "mistral_token": 127, "qlora_mistral_7b": 128, "qlora_mistral_reward_7b": 129, "phi3minitoken": 130, "lora_phi3": 131, "lora_phi3_mini": 132, "phi3": 133, "phi3_mini": 134, "phi3_mini_token": 135, "qlora_phi3_mini": 136, "qwen2token": 137, "lora_qwen2": 138, "lora_qwen2_0_5b": 139, "lora_qwen2_1_5b": 140, "lora_qwen2_7b": 141, "qwen2": 142, "qwen2_0_5b": 143, "qwen2_1_5b": 144, "qwen2_7b": 145, "qwen2_token": 146, "feedforward": 147, "fp32layernorm": 148, "kvcach": 149, "multiheadattent": 150, "todo": [150, 157], "rmsnorm": 151, "rotarypositionalembed": 152, "tanhgat": 153, "tiedembeddingtransformerdecod": 154, "transformercrossattentionlay": 155, "transformerdecod": 156, "transformerselfattentionlay": 157, "visiontransform": 158, "reparametrize_as_dtype_state_dict_post_hook": 159, "get_cosine_schedule_with_warmup": 160, "cewithchunkedoutputloss": 161, "deepfusionmodel": 162, "fusionembed": 163, "fusionlay": 164, "register_fusion_modul": 165, "adaptermodul": 166, "loralinear": 167, "disable_adapt": 168, "get_adapter_param": 169, "set_trainable_param": 170, "validate_missing_and_unexpected_for_lora": 171, "validate_state_dict_for_lora": 172, "estimate_advantag": 173, "get_rewards_ppo": 174, "dpoloss": 175, "ipoloss": 176, "ppoloss": 177, "rsoloss": 178, "simpoloss": 179, "truncate_sequence_at_first_stop_token": 180, "basetoken": 181, "modeltoken": 182, "sentencepiecebasetoken": 183, "tiktokenbasetoken": 184, "parse_hf_tokenizer_json": 185, "tokenize_messages_no_special_token": 186, "visioncrossattentionmask": 188, "fsdppolicytyp": 189, "fullmodelhfcheckpoint": 190, "fullmodelmetacheckpoint": 191, "fullmodeltorchtunecheckpoint": 192, "modeltyp": 193, "optimizerinbackwardwrapp": 194, "apply_selective_activation_checkpoint": 195, "create_optim_in_bwd_wrapp": 196, "get_dtyp": 197, "get_full_finetune_fsdp_wrap_polici": 198, "get_memory_stat": 199, "get_quantizer_mod": 200, "get_unmasked_sequence_length": 201, "get_world_size_and_rank": 202, "init_distribut": 203, "is_distribut": 204, "log_memory_stat": 205, "lora_fsdp_wrap_polici": 206, "cometlogg": 207, "disklogg": 208, "stdoutlogg": 209, "tensorboardlogg": 210, "wandblogg": 211, "register_optim_in_bwd_hook": 212, "set_activation_checkpoint": 213, "set_default_dtyp": 214, "set_se": 215, "setup_torch_profil": 216, "update_state_dict_for_classifi": 217, "validate_expected_param_dtyp": 218, "get_devic": 220, "get_logg": 221, "torch_version_g": 222, "comput": [224, 231], "time": [224, 231], "welcom": 225, "document": 225, "get": [225, 232, 237], "start": [225, 232], "tutori": 225, "instal": 226, "instruct": [226, 234, 237], "pre": 226, "requisit": 226, "via": [226, 237], "pypi": 226, "git": 226, "clone": 226, "nightli": 226, "kei": 227, "concept": 227, "design": 227, "principl": 227, "singl": 228, "devic": [228, 240], "finetun": [228, 230, 235, 238, 240, 241], "quantiz": [229, 235, 237, 239, 240], "awar": 229, "qat": [229, 240], "supervis": 230, "download": [232, 235, 236], "list": 232, "built": [232, 234], "copi": 232, "fine": [233, 234, 236, 237, 239], "tune": [233, 234, 236, 237, 239], "chat": [233, 234], "chang": 233, "prompt": 233, "special": 233, "when": 233, "should": 233, "i": 233, "custom": [233, 234], "hug": [234, 235], "face": [234, 235], "set": 234, "max": 234, "sequenc": 234, "length": 234, "sampl": 234, "pack": 234, "unstructur": 234, "corpu": 234, "multipl": 234, "local": 234, "remot": 234, "fulli": 234, "end": 235, "workflow": 235, "7b": 235, "evalu": [235, 237, 240], "eleutherai": [235, 237], "s": [235, 237], "eval": [235, 237], "har": [235, 237], "speed": 235, "up": 235, "librari": 235, "upload": 235, "hub": 235, "first": 236, "llm": 236, "select": 236, "modifi": 236, "next": 236, "step": [236, 239], "meta": 237, "8b": 237, "access": 237, "our": 237, "faster": 237, "how": 238, "doe": 238, "work": 238, "appli": [238, 240], "trade": 238, "off": 238, "optim": 239, "activ": 239, "gradient": 239, "accumul": 239, "lower": [239, 240], "fuse": 239, "backward": 239, "pass": 239, "effici": 239, "low": 239, "rank": 239, "adapt": 239, "qlora": [239, 241], "option": 240, "save": 241, "deep": 241, "dive": 241}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})