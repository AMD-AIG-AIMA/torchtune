Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.Message", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.sharegpt_to_llama2_messages", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FSDPPolicyType", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.generate", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.sharegpt_to_llama2_messages.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FSDPPolicyType.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.generate.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "Message", "MistralChatFormat", "SummarizeTemplate", "sharegpt_to_llama2_messages", "validate_messages", "ChatDataset", "ConcatDataset", "InstructDataset", "PackedDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "grammar_dataset", "instruct_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_70b", "llama3_8b", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "lora_phi3_mini", "phi3_mini", "qlora_phi3_mini", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "torchtune.utils.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "generate", "get_device", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"support": [2, 6, 8, 9, 10, 20, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 53, 55, 65, 72, 76, 81, 94, 96, 97, 98, 99, 100, 101, 102], "sever": [2, 97], "wide": [2, 97], "us": [2, 4, 6, 9, 10, 11, 15, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 55, 56, 58, 59, 60, 61, 62, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 82, 83, 84, 85, 89, 92, 93, 94, 97, 99, 100, 101], "help": [2, 6, 18, 60, 71, 73, 92, 93, 94, 96, 97, 98, 99, 100, 102], "quickli": [2, 7, 96, 97], "bootstrap": [2, 97], "your": [2, 5, 9, 10, 24, 84, 85, 92, 93, 94, 96, 97, 100, 101, 102], "fine": [2, 6, 8, 9, 27, 92, 94, 98, 101], "tune": [2, 3, 6, 7, 8, 9, 11, 27, 92, 93, 94, 98, 101, 102], "also": [2, 6, 7, 8, 9, 10, 30, 55, 60, 65, 75, 77, 85, 93, 96, 97, 98, 99, 100, 101, 102], "common": [2, 4, 7, 96, 97, 100, 101], "format": [2, 5, 13, 14, 15, 16, 17, 18, 20, 21, 22, 24, 26, 28, 29, 30, 32, 34, 71, 72, 96, 98, 99, 100, 101], "like": [2, 6, 7, 8, 9, 24, 93, 96, 97, 98, 99, 101], "chat": [2, 14, 15, 18, 19, 22, 24, 30, 34], "model": [2, 6, 7, 8, 10, 15, 20, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 64, 65, 66, 67, 71, 72, 74, 77, 87, 88, 92, 94, 96, 97, 102], "instruct": [2, 3, 13, 15, 17, 19, 20, 26, 27, 28, 29, 32, 53, 92, 96, 99, 101, 102], "These": [2, 4, 6, 7, 8, 10, 27, 73, 96, 97, 98, 99, 100, 101, 102], "ar": [2, 4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 23, 26, 27, 28, 29, 30, 31, 32, 33, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 60, 65, 70, 71, 72, 74, 76, 93, 94, 96, 97, 98, 99, 100, 101, 102], "especi": [2, 94, 98], "specifi": [2, 6, 7, 8, 10, 30, 55, 60, 61, 70, 74, 77, 85, 88, 96, 97, 98, 99, 100, 102], "from": [2, 3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 50, 56, 60, 61, 63, 64, 66, 68, 71, 72, 73, 74, 84, 85, 91, 93, 95, 97, 98, 99, 100, 101], "yaml": [2, 7, 8, 10, 11, 30, 32, 73, 85, 94, 96, 98, 99, 100, 101, 102], "config": [2, 6, 9, 10, 11, 12, 30, 32, 55, 71, 73, 85, 94, 96, 97, 98, 100, 101, 102], "represent": [2, 101, 102], "abov": [2, 6, 62, 93, 98, 100, 101, 102], "all": [3, 4, 8, 12, 24, 25, 27, 30, 55, 56, 60, 62, 69, 71, 73, 90, 92, 94, 95, 96, 98, 99, 100, 101], "famili": [3, 8, 28, 29, 34, 94, 100], "download": [3, 6, 90, 93, 96, 100, 101, 102], "meta": [3, 6, 18, 71, 72, 96, 98, 99], "llama": [3, 6, 18, 24, 30, 58, 59, 71, 72, 96, 98, 99, 100, 101], "8b": [3, 45, 46, 47, 52, 96], "hf": [3, 6, 71, 96, 98, 99, 100], "token": [3, 6, 7, 8, 19, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 55, 59, 60, 61, 68, 69, 74, 77, 97, 98, 99, 100, 101, 102], "access_token": 3, "pre": [3, 18, 27, 93, 96], "train": [3, 5, 6, 8, 9, 18, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 55, 59, 60, 61, 62, 63, 71, 72, 76, 87, 92, 94, 96, 97, 98, 100, 101, 102], "can": [3, 4, 6, 7, 8, 9, 10, 12, 19, 24, 25, 26, 28, 29, 30, 32, 58, 59, 68, 70, 71, 73, 77, 84, 85, 88, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102], "hug": [3, 6, 24, 26, 28, 29, 30, 31, 32, 33, 34, 63, 94, 97, 99, 100], "face": [3, 6, 24, 26, 28, 29, 30, 31, 32, 33, 34, 63, 94, 97, 99, 100], "hub": [3, 6, 99], "follow": [3, 6, 8, 22, 24, 27, 55, 63, 85, 92, 93, 97, 98, 99, 100, 101, 102], "command": [3, 8, 9, 73, 93, 96, 98, 99, 100, 101, 102], "2": [3, 6, 9, 23, 27, 34, 55, 68, 71, 72, 86, 89, 96, 98, 99, 100, 101], "7b": [3, 6, 26, 28, 29, 32, 38, 41, 43, 49, 50, 71, 72, 96, 99, 100, 101, 102], "mini": [3, 52, 53, 54], "microsoft": [3, 53], "4k": [3, 53], "hf_token": 3, "ignor": [3, 6, 55, 56], "pattern": [3, 69], "ai": [3, 50, 55, 85, 96, 100], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 27, 34, 55, 60, 63, 68, 69, 72, 74, 84, 85, 86, 89, 96, 98, 99, 100, 101, 102], "googl": [3, 35], "2b": [3, 35], "offer": 5, "allow": [5, 25, 84, 102], "seamless": 5, "transit": 5, "between": [5, 6, 71, 98, 100, 101, 102], "interoper": [5, 6, 8, 94, 98, 102], "rest": [5, 96, 102], "ecosystem": [5, 6, 8, 94, 98, 100, 102], "For": [5, 6, 7, 8, 24, 25, 26, 27, 28, 29, 30, 32, 55, 60, 73, 85, 88, 89, 93, 96, 97, 98, 99, 100, 101, 102], "comprehens": 5, "overview": [5, 7, 9, 99, 101, 102], "pleas": [5, 42, 43, 48, 51, 54, 70, 77, 88, 93, 102], "see": [5, 6, 9, 18, 20, 30, 34, 42, 43, 48, 51, 54, 57, 64, 70, 73, 77, 78, 85, 87, 88, 89, 93, 94, 96, 97, 98, 99, 100, 101, 102], "deep": [5, 6, 7, 8, 9, 94, 99, 100], "dive": [5, 6, 7, 8, 9, 94, 99, 100], "enabl": [5, 7, 8, 9, 25, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 65, 87, 89, 100, 101, 102], "work": [5, 6, 8, 73, 94, 98, 100, 102], "set": [5, 6, 7, 8, 9, 26, 27, 28, 29, 31, 32, 33, 34, 59, 60, 67, 70, 75, 77, 88, 89, 94, 96, 97, 98, 99, 100, 101], "consumpt": [5, 25], "dure": [5, 6, 25, 26, 27, 28, 29, 31, 33, 55, 57, 59, 60, 61, 62, 96, 98, 100, 101, 102], "provid": [5, 6, 7, 8, 10, 15, 20, 24, 25, 26, 27, 34, 60, 73, 77, 85, 94, 96, 97, 98, 99, 100], "debug": [5, 6, 7, 8], "finetun": [5, 6, 7, 8, 39, 40, 41, 46, 47, 52, 81, 92, 94, 99, 100], "job": [5, 9, 89, 99], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 94, 99, 100], "walk": [6, 8, 84, 94, 96, 97, 98, 99, 102], "you": [6, 7, 8, 9, 10, 17, 18, 24, 26, 28, 29, 32, 73, 74, 84, 85, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102], "through": [6, 7, 8, 9, 56, 94, 96, 97, 98, 99, 102], "design": [6, 8], "behavior": [6, 96, 97], "associ": [6, 7, 8, 74, 98, 101], "util": [6, 7, 8, 9, 10, 25, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 94, 98, 99, 100, 102], "what": [6, 7, 9, 18, 20, 31, 33, 92, 96, 97, 98, 99, 100], "cover": [6, 7, 8, 9, 96, 98, 102], "how": [6, 7, 8, 9, 70, 88, 92, 96, 97, 98, 99, 100, 102], "we": [6, 7, 8, 9, 26, 27, 28, 29, 32, 55, 57, 59, 60, 65, 68, 71, 72, 74, 76, 94, 96, 97, 98, 99, 100, 101, 102], "them": [6, 7, 24, 25, 26, 32, 56, 62, 68, 96, 97, 98, 101, 102], "scenario": [6, 25], "full": [6, 7, 8, 42, 43, 48, 51, 54, 68, 94, 100, 101], "compos": 6, "compon": [6, 8, 12, 87, 94, 97, 99, 101, 102], "which": [6, 8, 25, 26, 27, 28, 29, 31, 33, 39, 40, 41, 46, 47, 49, 52, 55, 59, 60, 61, 63, 68, 71, 72, 76, 82, 85, 88, 94, 96, 97, 98, 99, 100, 101, 102], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 21, 22, 24, 26, 30, 32, 62, 66, 67, 68, 71, 72, 74, 89, 96, 97, 98, 99, 100, 101], "recip": [6, 7, 9, 10, 11, 56, 71, 72, 94, 96, 97, 98, 100, 102], "evalu": [6, 8, 92, 94, 99, 101, 102], "gener": [6, 8, 13, 16, 21, 24, 26, 27, 34, 68, 89, 90, 92, 96, 97, 101, 102], "each": [6, 8, 14, 17, 25, 27, 39, 40, 41, 46, 47, 49, 52, 55, 59, 60, 61, 68, 69, 89, 94, 97, 98, 99, 100, 101], "make": [6, 7, 8, 9, 55, 61, 94, 98, 99, 100, 101, 102], "easi": [6, 8, 94, 101], "understand": [6, 7, 8, 92, 94, 96, 97, 101, 102], "extend": [6, 8, 94], "befor": [6, 23, 26, 27, 55, 60, 61, 65, 71, 98], "let": [6, 7, 9, 96, 97, 98, 99, 100, 101, 102], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 20, 23, 24, 26, 28, 29, 30, 31, 32, 33, 34, 39, 40, 41, 46, 47, 49, 52, 55, 57, 59, 60, 61, 62, 64, 66, 69, 70, 71, 72, 75, 77, 84, 87, 88, 94, 96, 97, 99, 101, 102], "defin": [6, 7, 8, 56, 64, 65, 66, 99, 101], "some": [6, 7, 15, 66, 67, 92, 94, 96, 98, 99, 101, 102], "concept": [6, 98, 99], "In": [6, 7, 8, 24, 59, 65, 70, 84, 85, 96, 98, 100, 101, 102], "ll": [6, 7, 8, 69, 74, 94, 96, 97, 98, 99, 100, 102], "talk": 6, "about": [6, 8, 71, 85, 94, 96, 98, 99, 100, 101, 102], "take": [6, 7, 8, 10, 56, 57, 62, 71, 73, 75, 96, 97, 98, 99, 100, 101, 102], "close": [6, 8, 82, 83, 84, 85, 101], "look": [6, 7, 8, 84, 93, 96, 97, 98, 99, 100, 101], "veri": [6, 25, 60, 98], "simpli": [6, 7, 27, 96, 97, 98, 100, 102], "dictat": 6, "state_dict": [6, 62, 71, 72, 101, 102], "store": [6, 25, 82, 85, 101, 102], "file": [6, 7, 8, 9, 10, 11, 68, 69, 71, 72, 73, 82, 85, 87, 91, 94, 95, 96, 97, 98, 99, 100, 101, 102], "disk": [6, 82], "weight": [6, 8, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 62, 64, 65, 71, 72, 85, 92, 96, 98, 99, 100, 101, 102], "string": [6, 24, 26, 28, 29, 30, 31, 32, 33, 34, 64, 68, 69, 75, 76], "kei": [6, 7, 9, 24, 26, 32, 55, 57, 60, 61, 67, 71, 98, 99, 101, 102], "identifi": 6, "state": [6, 8, 62, 66, 67, 71, 72, 98, 100, 101, 102], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 19, 21, 22, 24, 26, 30, 32, 62, 66, 67, 71, 72, 80, 86], "If": [6, 7, 12, 13, 16, 17, 20, 21, 23, 24, 26, 28, 29, 31, 32, 33, 34, 55, 59, 60, 61, 62, 65, 71, 72, 74, 75, 76, 77, 80, 84, 85, 89, 93, 96, 97, 98, 99, 100, 101], "don": [6, 7, 8, 85, 89, 96, 98, 99, 100, 102], "t": [6, 7, 8, 69, 76, 85, 89, 96, 98, 99, 100, 102], "match": [6, 24, 26, 32, 93, 98, 100, 101], "up": [6, 8, 9, 26, 27, 28, 29, 32, 96, 97, 99, 100, 101, 102], "exactli": 6, "those": [6, 101], "definit": [6, 101], "either": [6, 71, 74, 88, 101, 102], "run": [6, 7, 9, 11, 56, 57, 60, 62, 71, 72, 84, 85, 93, 94, 96, 99, 100, 101, 102], "explicit": 6, "error": [6, 7, 23, 71, 89], "load": [6, 8, 24, 25, 26, 27, 71, 72, 73, 84, 96, 98, 100, 101], "rais": [6, 10, 12, 20, 23, 30, 34, 55, 57, 60, 71, 72, 76, 80, 85, 89], "an": [6, 7, 8, 9, 10, 13, 19, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 55, 60, 64, 66, 67, 70, 71, 72, 77, 85, 94, 96, 97, 98, 99, 100, 101, 102], "except": [6, 19, 20, 97], "wors": 6, "silent": [6, 56], "succe": 6, "infer": [6, 18, 24, 55, 57, 59, 60, 61, 92, 96, 98, 99, 100, 102], "expect": [6, 7, 10, 13, 16, 17, 21, 24, 26, 30, 32, 59, 85, 96, 97, 101], "addit": [6, 7, 8, 10, 24, 26, 30, 32, 70, 71, 72, 76, 77, 80, 82, 84, 85, 88, 94, 96, 99, 101], "line": [6, 8, 73, 99, 100], "need": [6, 7, 8, 9, 17, 24, 27, 34, 55, 56, 60, 84, 85, 93, 96, 97, 98, 99, 100, 101, 102], "shape": [6, 55, 57, 59, 60, 61, 65, 74], "valu": [6, 7, 22, 34, 35, 36, 37, 38, 44, 45, 50, 55, 57, 58, 60, 61, 63, 71, 73, 74, 82, 83, 84, 85, 89, 99, 100, 101], "two": [6, 7, 23, 94, 98, 99, 100, 101, 102], "popular": [6, 94, 97, 98], "llama2": [6, 7, 8, 10, 18, 22, 24, 26, 28, 29, 32, 34, 36, 37, 38, 39, 40, 41, 42, 43, 56, 60, 61, 68, 92, 94, 99, 100], "offici": [6, 18, 96, 99, 100], "implement": [6, 8, 24, 26, 28, 29, 30, 31, 32, 33, 34, 56, 58, 59, 63, 64, 65, 71, 84, 94, 101, 102], "when": [6, 7, 8, 11, 19, 25, 27, 55, 59, 60, 61, 62, 63, 74, 77, 84, 98, 100, 101, 102], "websit": 6, "get": [6, 7, 8, 9, 24, 68, 76, 78, 79, 93, 94, 96, 97, 98, 99, 101], "access": [6, 7, 8, 25, 71, 98, 99], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 27, 55, 71, 72, 96, 97, 98, 99, 100, 101, 102], "pth": [6, 98, 100], "inspect": [6, 98, 101, 102], "content": [6, 19, 22, 24, 68, 96, 97], "easili": [6, 7, 94, 97, 101, 102], "torch": [6, 25, 57, 60, 62, 63, 74, 75, 76, 80, 87, 88, 89, 98, 99, 100, 101, 102], "import": [6, 7, 10, 30, 84, 85, 96, 97, 98, 99, 101, 102], "consolid": [6, 100], "00": [6, 91, 95, 99, 100], "mmap": [6, 98], "true": [6, 7, 19, 26, 27, 28, 29, 30, 31, 33, 42, 43, 48, 51, 54, 55, 60, 61, 62, 68, 69, 70, 71, 72, 77, 80, 84, 96, 97, 98, 100, 101, 102], "weights_onli": 6, "map_loc": [6, 98], "cpu": [6, 8, 62, 76, 93, 98, 102], "tensor": [6, 55, 56, 57, 58, 59, 60, 61, 62, 65, 71, 74, 82, 83, 84, 85, 86, 101, 102], "item": 6, "print": [6, 9, 25, 28, 29, 31, 33, 34, 68, 74, 96, 97, 99, 101, 102], "f": [6, 9, 28, 29, 31, 33, 96, 98, 101, 102], "tok_embed": [6, 60], "size": [6, 8, 10, 28, 29, 31, 33, 55, 57, 58, 59, 60, 61, 79, 94, 97, 98, 99, 100, 101], "32000": [6, 10, 101], "4096": [6, 10, 26, 28, 29, 32, 55, 59, 101], "len": [6, 25, 28, 29, 31, 33, 60], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 23, 24, 25, 26, 27, 31, 33, 34, 39, 40, 41, 46, 47, 52, 58, 59, 62, 63, 68, 69, 70, 71, 73, 75, 76, 78, 85, 87, 89, 93, 94, 96, 97, 98, 99, 100, 101, 102], "contain": [6, 19, 25, 27, 55, 57, 59, 60, 61, 64, 66, 67, 68, 69, 71, 72, 73, 84, 86, 96, 98, 100, 101], "includ": [6, 7, 8, 14, 17, 65, 71, 72, 73, 94, 96, 98, 99, 100, 101, 102], "input": [6, 13, 14, 17, 24, 26, 27, 28, 29, 30, 31, 32, 34, 55, 56, 58, 59, 60, 61, 65, 68, 71, 86, 89, 96, 97, 101, 102], "embed": [6, 55, 57, 58, 59, 60, 77, 96, 100], "tabl": [6, 96, 102], "call": [6, 10, 19, 56, 62, 73, 82, 83, 84, 85, 96, 101, 102], "layer": [6, 8, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 60, 61, 65, 70, 77, 94, 100, 101, 102], "have": [6, 7, 10, 55, 57, 64, 73, 77, 84, 87, 93, 96, 97, 98, 99, 100, 101, 102], "dim": [6, 55, 56, 58, 59, 60, 61], "most": [6, 7, 69, 96, 99, 101, 102], "within": [6, 7, 10, 24, 27, 34, 56, 74, 84, 89, 98, 100, 101, 102], "default": [6, 7, 15, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 55, 56, 58, 59, 60, 61, 62, 63, 65, 68, 69, 71, 72, 73, 74, 76, 82, 85, 86, 87, 89, 93, 98, 100, 101, 102], "everi": [6, 8, 56, 84, 93, 102], "repo": [6, 71, 72, 98], "first": [6, 7, 10, 23, 27, 57, 60, 69, 71, 73, 92, 94, 96, 98, 100, 101, 102], "big": [6, 98], "split": [6, 27, 96, 97, 98], "across": [6, 8, 25, 71, 84, 89, 98, 100], "bin": [6, 98], "To": [6, 7, 8, 9, 27, 71, 93, 94, 96, 97, 98, 99, 100, 101, 102], "correctli": [6, 8, 12, 71, 93, 96, 99, 102], "piec": 6, "one": [6, 8, 23, 56, 68, 96, 97, 98, 99, 100, 102], "pytorch_model": [6, 98], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 19, 27, 65, 66, 68, 72, 73, 74, 76, 77, 98, 99, 100, 101, 102], "doe": [6, 20, 24, 27, 53, 55, 60, 61, 64, 71, 73, 96, 98], "fewer": [6, 55], "sinc": [6, 7, 10, 56, 71, 96, 98, 100], "instead": [6, 8, 27, 30, 32, 56, 57, 65, 98, 100, 101], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 21, 24, 26, 30, 32, 34, 64, 67, 69, 71, 72, 73, 74, 75, 82, 83, 84, 85, 96, 98, 100], "caus": [6, 68], "try": [6, 7, 96, 98, 99, 100, 102], "same": [6, 7, 39, 40, 41, 46, 47, 52, 55, 57, 61, 68, 73, 77, 85, 96, 98, 100, 101, 102], "As": [6, 7, 8, 9, 65, 94, 98, 100, 102], "re": [6, 7, 69, 94, 96, 98, 99, 100, 101], "care": [6, 56, 71, 98, 100, 101], "end": [6, 8, 19, 25, 69, 92, 94, 96, 100, 101], "number": [6, 8, 24, 26, 27, 28, 29, 30, 32, 34, 55, 57, 60, 63, 71, 72, 74, 79, 89, 99, 101], "just": [6, 13, 94, 96, 97, 99, 100, 101], "save": [6, 8, 9, 62, 71, 72, 77, 85, 92, 96, 98, 100, 101], "less": [6, 34, 98, 99, 100, 102], "prone": 6, "manag": [6, 25, 87, 96], "invari": 6, "accept": [6, 7, 34, 68, 70, 99, 102], "multipl": [6, 7, 8, 19, 24, 25, 55, 60, 61, 65, 82, 83, 84, 85, 97, 99, 100], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 96, 97, 98], "worri": [6, 96, 99], "explicitli": [6, 64, 94, 101], "convert": [6, 22, 24, 71, 86, 96, 98, 102], "time": [6, 68, 82, 84, 96, 98, 100, 102], "produc": [6, 102], "back": [6, 23, 71, 101, 102], "origin": [6, 28, 29, 62, 65, 96, 97, 98, 100, 101, 102], "form": [6, 7, 8, 23], "One": [6, 98], "advantag": [6, 101], "being": [6, 71, 72, 75, 102], "should": [6, 7, 8, 14, 17, 18, 19, 20, 22, 27, 30, 32, 39, 40, 41, 46, 47, 49, 52, 55, 56, 64, 70, 73, 82, 83, 84, 85, 93, 94, 97, 98, 99, 100, 101, 102], "abl": [6, 8, 97, 98, 99, 100], "post": [6, 102], "tool": [6, 98, 99], "quantiz": [6, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 65, 92, 99, 102], "eval": [6, 92, 94], "without": [6, 7, 9, 93, 94, 96, 97, 98, 101], "code": [6, 8, 60, 90, 94, 97, 99], "chang": [6, 7, 9, 13, 93, 97, 98, 99, 100, 101, 102], "OR": 6, "convers": [6, 14, 15, 18, 20, 22, 23, 24, 30, 34, 71, 94, 96, 97, 98, 100, 101, 102], "script": [6, 9, 98, 99, 100], "wai": [6, 7, 24, 96, 97, 98, 99, 100], "surround": [6, 8, 94], "load_checkpoint": [6, 8, 71, 72], "save_checkpoint": [6, 8, 9, 71, 72], "method": [6, 7, 8, 9, 11, 24, 26, 28, 29, 30, 31, 32, 33, 34, 62, 64, 66, 73, 93, 94, 98, 100, 101, 102], "convertor": 6, "avail": [6, 8, 73, 75, 76, 94, 98, 100, 101], "here": [6, 7, 9, 15, 31, 58, 59, 96, 97, 98, 99, 100, 101, 102], "three": [6, 8, 99], "hfcheckpoint": 6, "read": [6, 71, 72, 94], "write": [6, 8, 71, 72, 82, 96, 97, 99], "compat": [6, 71], "transform": [6, 8, 24, 26, 39, 40, 41, 46, 47, 49, 52, 60, 61, 63, 88, 101], "framework": [6, 8, 94], "mention": [6, 98, 102], "assum": [6, 13, 16, 17, 21, 26, 32, 55, 59, 60, 61, 63, 66, 69, 76, 98, 101], "checkpoint_dir": [6, 7, 71, 72, 98, 100], "necessari": [6, 34, 82, 83, 84, 85, 96, 101], "json": [6, 71, 87, 98], "easiest": [6, 98, 99], "sure": [6, 7, 98, 99, 100, 101, 102], "everyth": [6, 8, 73, 94, 99], "flow": [6, 24, 26, 27, 102], "By": [6, 100, 101, 102], "safetensor": 6, "output": [6, 17, 28, 29, 31, 34, 39, 40, 41, 46, 47, 49, 52, 55, 56, 58, 59, 60, 61, 65, 67, 74, 77, 83, 87, 93, 96, 97, 98, 99, 100, 101, 102], "dir": [6, 85, 93, 98, 99, 100], "output_dir": [6, 7, 71, 72, 87, 98, 100, 101, 102], "argument": [6, 7, 10, 17, 24, 26, 30, 32, 34, 42, 43, 48, 51, 54, 55, 70, 73, 77, 80, 82, 84, 85, 88, 96, 100, 101], "snippet": [6, 97], "explain": 6, "setup": [6, 7, 8, 60, 88, 98, 101, 102], "_component_": [6, 7, 9, 10, 30, 96, 97, 98, 100, 101], "fullmodelhfcheckpoint": [6, 98], "directori": [6, 7, 71, 72, 82, 84, 85, 98, 99, 100], "sort": [6, 71], "id": [6, 24, 26, 27, 28, 29, 30, 32, 34, 55, 59, 60, 61, 68, 69, 71, 74, 86, 96, 98], "so": [6, 7, 27, 71, 73, 93, 94, 96, 98, 99, 100, 101, 102], "order": [6, 8, 71, 84, 85, 99], "matter": [6, 71, 101], "checkpoint_fil": [6, 7, 9, 71, 72, 98, 100, 101, 102], "restart": 6, "previou": [6, 27, 71, 72], "more": [6, 7, 8, 30, 34, 57, 59, 70, 73, 85, 87, 88, 89, 94, 97, 98, 99, 100, 101, 102], "next": [6, 27, 74, 100, 102], "section": [6, 8, 92, 98, 100, 102], "recipe_checkpoint": [6, 71, 72], "null": [6, 7], "usual": [6, 59, 71, 85, 98, 101], "model_typ": [6, 71, 72, 98, 100], "resume_from_checkpoint": [6, 71, 72], "fals": [6, 7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 60, 61, 65, 68, 69, 71, 72, 87, 96, 97, 98, 100, 101, 102], "requir": [6, 7, 25, 34, 71, 84, 85, 89, 93, 96, 97, 99, 102], "param": [6, 8, 39, 40, 41, 46, 47, 52, 65, 66, 67, 71, 101, 102], "directli": [6, 7, 8, 10, 30, 32, 70, 71, 97, 98, 99, 100, 101, 102], "ensur": [6, 7, 12, 23, 34, 55, 71, 76, 94, 99], "out": [6, 7, 8, 24, 26, 28, 29, 30, 31, 33, 71, 72, 92, 94, 96, 98, 99, 100, 101, 102], "case": [6, 8, 9, 19, 55, 71, 76, 82, 88, 94, 96, 97, 98, 100, 101, 102], "discrep": [6, 71], "along": [6, 100, 101], "detail": [6, 30, 34, 57, 70, 77, 87, 89, 98, 99, 100, 101, 102], "found": [6, 7, 9, 58, 59, 101, 102], "metacheckpoint": 6, "github": [6, 10, 39, 40, 41, 46, 47, 52, 55, 58, 59, 63, 93, 99], "repositori": [6, 18, 98, 99], "fullmodelmetacheckpoint": [6, 100], "torchtunecheckpoint": 6, "perform": [6, 27, 56, 74, 94, 96, 98, 100, 102], "current": [6, 27, 53, 55, 57, 59, 60, 61, 72, 77, 79, 82, 84, 89, 98, 99, 100], "test": [6, 7, 8, 94, 96], "complet": [6, 8, 27, 96, 97, 98, 99, 100], "written": [6, 7, 8, 71, 72, 82, 83, 84, 85, 94], "begin": [6, 27, 68, 69, 96, 100, 102], "partit": [6, 102], "ha": [6, 64, 66, 68, 97, 98, 99, 100, 101, 102], "standard": [6, 83, 94, 96, 98, 100], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 98], "inform": [6, 19, 85, 88, 94, 98, 99, 100], "subsequ": [6, 8], "recipe_st": [6, 71, 72], "pt": [6, 9, 71, 72, 98, 100], "epoch": [6, 8, 9, 63, 71, 72, 96, 98, 99, 100], "optim": [6, 7, 8, 25, 53, 63, 96, 98, 99, 100, 101, 102], "etc": [6, 8, 71, 99], "prevent": [6, 27], "flood": 6, "overwritten": 6, "note": [6, 7, 17, 19, 60, 64, 68, 71, 87, 89, 96, 97, 98, 101, 102], "updat": [6, 7, 8, 57, 93, 96, 98, 99, 100, 101, 102], "hf_model_0001_0": [6, 98], "hf_model_0002_0": [6, 98], "both": [6, 25, 98, 101, 102], "adapt": [6, 64, 65, 66, 67, 71, 72, 96, 98, 101, 102], "merg": [6, 10, 71, 98, 100, 102], "would": [6, 7, 9, 27, 60, 93, 96, 97, 98, 101, 102], "our": [6, 8, 94, 96, 97, 98, 99, 101, 102], "tutori": [6, 88, 94, 96, 97, 98, 99, 100, 101, 102], "primari": [6, 7, 8, 99], "want": [6, 7, 8, 9, 10, 24, 74, 93, 96, 98, 99, 100, 101], "resum": [6, 8, 63, 71, 72, 102], "initi": [6, 8, 11, 25, 27, 35, 36, 37, 38, 44, 45, 50, 80, 99, 101, 102], "frozen": [6, 101, 102], "base": [6, 10, 26, 34, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 59, 63, 65, 67, 71, 73, 77, 82, 92, 96, 98, 99, 100, 101, 102], "well": [6, 7, 8, 94, 97, 98, 100, 102], "learnt": [6, 96, 98], "someth": [6, 8, 9, 96, 98], "NOT": 6, "refer": [6, 7, 8, 58, 59, 94, 101], "adapter_checkpoint": [6, 71, 72], "adapter_0": [6, 98], "now": [6, 68, 96, 97, 98, 99, 100, 101, 102], "knowledg": 6, "creat": [6, 7, 10, 27, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 57, 63, 70, 71, 72, 82, 84, 96, 97, 98, 100, 102], "simpl": [6, 8, 92, 99, 101, 102], "forward": [6, 8, 55, 56, 58, 59, 60, 61, 65, 100, 101, 102], "13b": [6, 36, 39, 42], "modeltyp": [6, 71, 72], "llama2_13b": [6, 39], "right": [6, 71, 98, 100, 101], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 101], "successfulli": [6, 99], "vocab": [6, 10, 60, 100], "70": [6, 44], "x": [6, 55, 56, 58, 59, 60, 61, 65, 74, 101, 102], "randint": 6, "0": [6, 8, 27, 39, 40, 41, 42, 43, 55, 60, 63, 65, 68, 74, 84, 85, 86, 89, 91, 95, 96, 98, 99, 100, 101, 102], "no_grad": 6, "6": [6, 27, 58, 86, 98, 102], "3989": 6, "9": [6, 98, 102], "0531": 6, "3": [6, 27, 52, 53, 69, 73, 78, 86, 96, 98, 99, 100, 102], "2375": 6, "5": [6, 63, 86, 87, 98, 99, 100], "2822": 6, "4": [6, 34, 55, 86, 94, 98, 100, 101, 102], "4872": 6, "7469": 6, "8": [6, 28, 29, 31, 33, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 98, 101, 102], "6737": 6, "11": [6, 98, 100, 102], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 86], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 24, 26, 32, 34, 85, 96, 97, 98, 99, 100, 101], "find": [6, 8, 9, 98, 99, 101], "list": [6, 7, 14, 15, 18, 20, 22, 23, 24, 25, 26, 28, 29, 30, 32, 34, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 64, 65, 68, 69, 71, 72, 73, 74, 78, 81, 86, 96, 97, 99, 100], "builder": [6, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 96, 97, 102], "hope": 6, "deeper": [6, 99], "insight": [6, 98], "happi": [6, 98], "thi": [7, 8, 9, 10, 19, 24, 25, 26, 27, 28, 29, 30, 32, 34, 53, 55, 56, 59, 60, 61, 62, 63, 64, 68, 70, 71, 72, 73, 74, 75, 76, 82, 84, 85, 87, 88, 89, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102], "guid": [7, 9, 94, 96, 97, 99, 101], "pars": [7, 10, 69, 73, 96, 99], "effect": 7, "cli": [7, 9, 11, 93, 98, 99], "prerequisit": [7, 96, 97, 98, 99, 100, 101, 102], "Be": [7, 96, 98, 99, 100, 101, 102], "familiar": [7, 96, 98, 99, 100, 101, 102], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 93, 96, 97, 99], "instal": [7, 9, 84, 85, 92, 98, 99, 100, 101, 102], "fundament": 7, "There": [7, 14, 23, 96, 98, 99, 100, 101], "entri": [7, 8, 99], "point": [7, 8, 22, 97, 98, 99, 100, 101, 102], "locat": [7, 100, 101, 102], "thei": [7, 8, 19, 25, 60, 73, 96, 97, 101], "truth": [7, 98, 100], "reproduc": 7, "overridden": [7, 56, 73], "quick": [7, 25], "experiment": 7, "modifi": [7, 8, 9, 62, 94, 98, 100, 101, 102], "serv": [7, 70, 97, 101], "particular": [7, 24, 25, 34, 70, 97, 101, 102], "seed": [7, 8, 9, 89, 99], "shuffl": [7, 27], "devic": [7, 8, 75, 76, 96, 98, 99, 100, 101], "cuda": [7, 75, 76, 93, 98, 102], "dtype": [7, 8, 57, 60, 62, 76, 81, 98, 102], "fp32": [7, 102], "enable_fsdp": 7, "mani": [7, 27, 97, 98], "object": [7, 10, 14, 15, 18, 20, 55, 70, 96, 97], "keyword": [7, 10, 24, 26, 30, 32, 34, 62, 96], "loss": [7, 8, 26, 28, 29, 31, 33, 99, 101, 102], "function": [7, 8, 10, 11, 24, 55, 56, 62, 70, 74, 75, 79, 89, 94, 96, 97, 102], "exampl": [7, 8, 9, 10, 11, 15, 18, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 55, 64, 68, 70, 71, 72, 74, 84, 85, 86, 90, 91, 93, 95, 96, 97, 98, 100, 101, 102], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 98], "path": [7, 8, 9, 10, 24, 26, 28, 29, 30, 31, 32, 33, 34, 68, 69, 71, 72, 73, 87, 96, 98, 100, 101], "normal": [7, 24, 27, 58, 60, 61, 68, 96, 97, 101, 102], "python": [7, 69, 73, 78, 85, 89, 90, 98], "alpaca_dataset": [7, 28, 97], "custom": [7, 8, 24, 26, 30, 32, 88, 94, 98, 99, 100, 101], "train_on_input": [7, 22, 24, 26, 28, 29, 30, 31, 32, 33, 34, 97], "onc": [7, 98, 99, 100, 101, 102], "ve": [7, 57, 69, 96, 97, 98, 100, 101], "instanc": [7, 10, 25, 56, 62, 66, 67, 101], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 30, 98, 102], "under": [7, 98, 100, 102], "preced": [7, 10, 97, 100, 101], "actual": [7, 9, 24, 96], "throw": 7, "notic": [7, 96, 97, 101], "miss": [7, 101], "posit": [7, 10, 27, 55, 57, 59, 60, 61, 100], "anoth": [7, 98], "handl": [7, 11, 19, 25, 68, 96, 98, 101, 102], "def": [7, 8, 9, 11, 70, 96, 97, 101, 102], "dictconfig": [7, 8, 10, 11, 12, 85], "arg": [7, 10, 60, 62, 64, 73, 83], "tupl": [7, 10, 25, 34, 57, 62, 68, 69, 70, 73, 79, 86], "kwarg": [7, 10, 62, 64, 73, 80, 82, 83, 84, 85, 88], "str": [7, 10, 13, 16, 17, 19, 21, 22, 24, 26, 28, 29, 30, 31, 32, 33, 34, 62, 64, 65, 66, 67, 68, 69, 71, 72, 73, 75, 76, 78, 81, 82, 83, 84, 85, 86, 87, 89, 96], "mean": [7, 55, 58, 60, 61, 96, 99, 101], "pass": [7, 10, 24, 25, 26, 30, 32, 55, 56, 62, 70, 76, 77, 80, 84, 85, 88, 96, 101, 102], "add": [7, 9, 24, 27, 69, 73, 97, 98, 100, 101, 102], "d": [7, 19, 55, 57, 60, 61, 69, 96, 97, 101], "llama2_token": [7, 98], "tmp": [7, 96, 99, 100], "option": [7, 8, 13, 16, 17, 21, 24, 26, 27, 30, 32, 34, 39, 40, 41, 46, 47, 49, 52, 55, 59, 60, 61, 62, 68, 69, 71, 72, 74, 75, 76, 78, 82, 85, 87, 89, 93, 94, 98], "bool": [7, 19, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 62, 65, 68, 69, 70, 71, 72, 77, 80, 84, 87, 88, 102], "max_seq_len": [7, 10, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 55, 57, 59, 60, 68, 69, 96, 97], "int": [7, 9, 24, 25, 26, 27, 28, 29, 30, 32, 34, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 57, 58, 59, 60, 63, 65, 68, 69, 70, 71, 72, 74, 77, 79, 82, 83, 84, 85, 86, 88, 89, 96, 97, 101, 102], "512": [7, 28, 29, 97, 102], "instructdataset": [7, 28, 29, 31, 32, 33, 97], "alreadi": [7, 80, 93, 98, 101], "overwrit": [7, 93], "duplic": [7, 8, 94], "sometim": 7, "than": [7, 23, 34, 55, 57, 70, 96, 98, 99, 100, 101, 102], "resolv": [7, 99], "alpaca": [7, 13, 28, 29, 39, 40, 41, 46, 47, 52, 97], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 82, 83, 84, 85], "disklogg": 7, "log_dir": [7, 82, 84, 85], "conveni": [7, 8], "verifi": [7, 75, 76, 77, 96, 99, 101], "properli": 7, "experi": [7, 85, 92, 94, 96, 100, 101], "wa": [7, 96, 98, 100, 101, 102], "cp": [7, 93, 96, 98, 99, 100], "7b_lora_single_devic": [7, 98, 99, 101, 102], "my_config": 7, "discuss": [7, 99, 101], "guidelin": 7, "while": [7, 8, 39, 40, 41, 46, 47, 52, 56, 94, 98, 102], "mai": [7, 9, 77, 87, 96, 97, 99, 101], "tempt": 7, "put": [7, 8, 99, 101], "much": [7, 98, 100, 101, 102], "give": [7, 101], "maximum": [7, 24, 26, 27, 28, 29, 30, 32, 34, 55, 57, 59, 60, 69], "flexibl": [7, 25, 97], "switch": 7, "encourag": [7, 101], "clariti": 7, "significantli": 7, "easier": [7, 98, 99], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 27, 102], "expos": [7, 8, 96, 97, 99], "parent": 7, "modul": [7, 10, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 77, 88, 89, 96, 99, 101, 102], "__init__": [7, 8, 101, 102], "py": [7, 10, 39, 40, 41, 46, 47, 52, 55, 57, 58, 59, 63, 98, 100], "guarante": 7, "stabil": [7, 94, 102], "underscor": 7, "_alpaca": 7, "collect": [7, 74, 99], "differ": [7, 9, 24, 25, 26, 68, 94, 96, 98, 100, 101, 102], "itself": 7, "via": [7, 9, 30, 65, 101, 102], "pair": [7, 86, 97], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 96, 98, 99, 100, 101, 102], "checkpoint": [7, 8, 62, 69, 71, 72, 85, 88, 94, 100, 101, 102], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 27, 30, 32, 55, 56, 57, 58, 59, 60, 61, 64, 65, 66, 67, 68, 69, 71, 72, 73, 82, 83, 84, 85, 96, 97, 99, 101, 102], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 55, 59, 60, 61], "core": [8, 94, 97, 99, 102], "i": [8, 18, 20, 55, 60, 61, 62, 67, 69, 74, 97, 98, 100, 102], "structur": [8, 14, 15, 18, 20, 24, 96, 97, 98], "new": [8, 50, 82, 84, 96, 98, 99, 100, 101, 102], "user": [8, 14, 15, 18, 19, 20, 22, 23, 24, 55, 68, 96, 97, 99], "thought": [8, 94, 99, 102], "target": [8, 94], "pipelin": [8, 94], "llm": [8, 92, 94, 97, 98, 101], "eg": [8, 60, 71, 94], "meaning": [8, 94, 98], "featur": [8, 9, 93, 94, 98, 99], "fsdp": [8, 70, 77, 94, 99, 100], "activ": [8, 56, 88, 94, 102], "gradient": [8, 94, 98, 100, 101, 102], "accumul": [8, 94], "mix": [8, 98], "precis": [8, 62, 76, 94, 99, 102], "appli": [8, 24, 26, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 58, 59, 60, 61, 88, 94, 102], "given": [8, 10, 17, 23, 65, 74, 75, 76, 94, 101], "complex": 8, "becom": [8, 93, 97], "harder": 8, "anticip": 8, "architectur": [8, 18, 20, 60, 97], "methodolog": 8, "reason": [8, 74, 98], "possibl": [8, 24, 27, 30, 97], "trade": 8, "off": [8, 68, 98], "memori": [8, 25, 26, 27, 28, 29, 32, 62, 77, 92, 94, 97, 98, 99, 100], "vs": [8, 99], "qualiti": [8, 98, 101], "believ": 8, "best": [8, 96], "suit": [8, 99], "specif": [8, 10, 77, 96, 97, 98, 102], "b": [8, 55, 57, 59, 60, 61, 65, 85, 101, 102], "fit": [8, 24, 26, 27, 28, 29, 32], "solut": 8, "result": [8, 68, 98, 100, 101, 102], "meant": [8, 62], "depend": [8, 9, 13, 98, 101, 102], "level": [8, 78, 94, 102], "expertis": 8, "routin": 8, "yourself": [8, 100, 101], "exist": [8, 93, 97, 98, 99, 100, 102], "ad": [8, 68, 96, 101, 102], "ones": 8, "modular": [8, 94], "build": [8, 30, 32, 94, 100, 101], "block": [8, 27, 39, 40, 41, 46, 47, 49, 52, 94], "wandb": [8, 9, 85, 99], "log": [8, 78, 82, 83, 84, 85, 98, 99, 100, 102], "fulli": [8, 25], "nativ": [8, 92, 94, 101, 102], "pytorch": [8, 60, 62, 70, 84, 87, 88, 89, 92, 93, 94, 100, 101, 102], "correct": [8, 16, 31, 58, 59, 60, 75, 94, 96, 97], "numer": [8, 94], "pariti": [8, 94], "verif": 8, "extens": [8, 94], "comparison": [8, 101, 102], "benchmark": [8, 89, 94, 98, 100, 101], "limit": 8, "hidden": [8, 56], "behind": 8, "100": [8, 26, 28, 29, 31, 33, 34, 74, 86, 87, 101, 102], "flag": [8, 26, 28, 29, 31, 33, 70, 77, 102], "prefer": [8, 94, 97], "over": [8, 63, 73, 94, 97, 98, 100, 101, 102], "unnecessari": 8, "abstract": [8, 14, 17, 94, 99, 102], "No": [8, 94], "inherit": [8, 73, 94], "go": [8, 18, 20, 68, 94, 97, 98, 99, 102], "upon": [8, 25, 100], "figur": [8, 101, 102], "spectrum": 8, "decid": 8, "interact": [8, 92, 99], "start": [8, 9, 25, 69, 93, 94, 96, 97, 98, 99], "paradigm": 8, "consist": [8, 99], "configur": [8, 26, 28, 29, 30, 31, 32, 33, 34, 61, 94, 96, 99, 100, 101, 102], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 75, 76, 77, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 92, 94, 96, 97, 98, 99, 100, 101, 102], "overrid": [8, 11, 98, 99, 100, 102], "togeth": [8, 27, 85, 99, 101], "valid": [8, 23, 93, 98, 99], "environ": [8, 93, 98, 99], "logic": [8, 71, 94, 99, 101], "api": [8, 9, 42, 43, 48, 51, 54, 96, 98, 99, 100, 102], "closer": [8, 101], "monolith": [8, 94], "trainer": [8, 79], "A": [8, 9, 22, 25, 27, 55, 60, 61, 62, 65, 68, 69, 70, 71, 73, 86, 91, 92, 95, 96, 98, 101, 102], "wrapper": [8, 68, 69, 101], "around": [8, 24, 68, 69, 87, 96, 98, 101, 102], "extern": 8, "primarili": [8, 25, 101], "eleutherai": [8, 94, 101], "har": [8, 94, 101], "control": [8, 26, 28, 29, 31, 33, 89, 98], "multi": [8, 24, 55, 100], "stage": 8, "distil": 8, "oper": [8, 25, 87, 89], "turn": [8, 19, 23, 24, 69, 96], "dataload": [8, 27, 28, 29, 31, 33], "applic": [8, 55, 71, 72, 85], "clean": [8, 9, 28], "after": [8, 55, 57, 58, 60, 61, 82, 83, 84, 85, 96, 102], "process": [8, 9, 62, 89, 99, 102], "group": [8, 55, 82, 83, 84, 85, 100], "init_process_group": [8, 80], "backend": 8, "gloo": 8, "els": [8, 73, 85, 94, 102], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 25, 73, 77, 97, 99, 100, 101], "stuff": 8, "carri": 8, "relev": [8, 19, 98, 101], "interfac": [8, 14, 17, 25, 64], "metric": [8, 99], "logger": [8, 78, 82, 83, 84, 85, 99], "self": [8, 9, 27, 39, 40, 41, 46, 47, 49, 52, 55, 60, 61, 64, 97, 101, 102], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 70, 77, 87, 88, 96], "_model": 8, "_setup_model": 8, "_token": [8, 97], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 102], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 89, 100], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": [8, 27], "batch": [8, 27, 28, 29, 31, 33, 55, 57, 59, 60, 61, 68, 86, 94, 97, 99, 100, 101], "enumer": 8, "_autocast": 8, "logit": [8, 74], "label": [8, 24, 26, 27, 28, 29, 30, 32, 34, 86], "global_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 82, 83, 84, 85], "step": [8, 27, 60, 63, 69, 82, 83, 84, 85, 87, 92, 98, 101, 102], "learn": [8, 25, 63, 94, 96, 97, 99, 100, 101, 102], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 20, 21, 23, 24, 26, 27, 30, 32, 34, 55, 57, 59, 60, 61, 67, 68, 69, 71, 72, 74, 75, 76, 78, 82, 83, 84, 85, 88, 89, 96, 98], "fullfinetunerecip": 8, "direct": [8, 93], "wandblogg": [9, 101, 102], "workspac": 9, "seen": [9, 101, 102], "screenshot": 9, "below": [9, 59, 70, 97, 100, 101, 102], "packag": [9, 84, 85, 93], "pip": [9, 84, 85, 93, 98, 100], "Then": [9, 99], "login": [9, 85, 98], "built": [9, 93, 96, 97, 99, 102], "project": [9, 39, 40, 41, 46, 47, 49, 52, 55, 56, 77, 85, 92, 101, 102], "grab": [9, 100], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": [9, 96], "exit": [9, 93], "resourc": [9, 82, 83, 84, 85], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 32, 34, 55, 59, 60, 61, 74, 96, 97, 98], "desir": [9, 24, 96], "suggest": 9, "approach": [9, 25, 97], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 98], "_output_dir": [9, 71, 72], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 19, 22, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 53, 55, 57, 58, 59, 60, 61, 62, 65, 66, 68, 69, 70, 71, 72, 74, 76, 77, 78, 79, 80, 87, 88, 98, 101, 102], "descript": [9, 30, 34], "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 19, 22, 24, 27, 28, 29, 31, 33], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 24, 26, 30, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 46, 47, 48, 50, 51, 52, 53, 54, 55, 58, 59, 63, 70, 71, 72, 73, 78, 84, 85, 87, 88, 89, 93, 98], "com": [10, 39, 40, 41, 46, 47, 52, 55, 58, 59, 63, 93], "facebookresearch": [10, 58, 59], "blob": [10, 39, 40, 41, 46, 47, 52, 55, 58, 59, 63], "main": [10, 11, 55, 58, 59, 93, 98, 100], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 60], "32": [10, 100, 101, 102], "num_head": [10, 55, 57, 59, 60], "num_kv_head": [10, 55, 57], "vocab_s": 10, "must": [10, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 64, 69, 73, 102], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 53, 55, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 74, 75, 76, 77, 78, 79, 80, 81, 86, 87, 89, 96, 97, 101, 102], "nn": [10, 55, 56, 57, 60, 61, 62, 64, 66, 67, 70, 88, 101, 102], "parsed_yaml": 10, "embed_dim": [10, 55, 59, 61, 101], "valueerror": [10, 20, 23, 30, 34, 55, 57, 60, 71, 72, 76, 89], "callabl": [11, 24, 26, 60, 70, 74, 77, 88], "With": [11, 98, 101, 102], "my_recip": 11, "foo": 11, "bar": [11, 94, 99], "instanti": [12, 35, 36, 37, 38, 39, 40, 41, 44, 45, 46, 47, 49, 50, 52, 53], "configerror": 12, "cannot": [12, 100], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33, 34, 82, 83, 84, 85, 97, 98, 102], "prompt": [13, 14, 16, 17, 18, 20, 21, 22, 24, 26, 28, 29, 30, 31, 32, 33, 34, 60, 68, 74, 97, 98, 100], "templat": [13, 14, 16, 17, 21, 24, 26, 28, 29, 31, 32, 33, 34], "style": [13, 27, 28, 29, 30, 34, 102], "slightli": 13, "classmethod": [13, 14, 15, 16, 17, 18, 19, 20, 21], "map": [13, 16, 17, 21, 22, 24, 25, 26, 27, 32, 67, 71, 82, 83, 84, 85, 96, 98, 101], "column_map": [13, 16, 17, 21, 24, 26, 32, 97], "placehold": [13, 14, 16, 17, 21, 24, 26, 32], "column": [13, 16, 17, 21, 24, 26, 32, 55, 60, 61, 96], "ident": [13, 16, 17, 20, 21, 26, 27, 32, 98], "role": [14, 19, 22, 24, 68, 96, 97], "system": [14, 15, 18, 19, 20, 22, 23, 24, 68, 96, 97], "assist": [14, 15, 18, 19, 22, 23, 24, 68, 74, 96, 97], "messag": [14, 15, 18, 20, 22, 23, 24, 30, 68, 69, 93, 96, 97], "accord": [14, 20, 96], "openai": 15, "markup": 15, "languag": [15, 65, 74, 101], "It": [15, 20, 96, 97, 102], "huggingfac": [15, 24, 26, 30, 32, 53, 63, 71, 72, 97, 98], "im_start": 15, "context": [15, 53, 87, 97], "im_end": 15, "goe": 15, "respons": [15, 68, 97, 98, 99, 100], "appropri": [15, 18, 20, 25, 63, 97, 102], "tag": [15, 18, 20, 24, 69, 82, 83, 84, 85, 96], "grammar": [16, 31, 97], "sentenc": [16, 27], "alwai": [17, 73], "human": [18, 22, 96], "taken": [18, 101, 102], "inst": [18, 20, 24, 96, 97], "sy": [18, 96, 97], "respect": [18, 25, 67, 96, 97], "honest": [18, 96, 97], "am": [18, 20, 96, 97, 98, 100], "pari": [18, 20, 97], "capit": [18, 20, 97], "franc": [18, 20, 97], "known": [18, 20, 68, 97], "its": [18, 20, 27, 55, 59, 60, 61, 89, 97, 98, 100, 101], "stun": [18, 20, 97], "liter": [19, 39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54], "mask": [19, 26, 27, 28, 29, 31, 33, 55, 60, 61, 68, 69, 96, 97], "ipython": 19, "eot": 19, "dataclass": [19, 96], "repres": [19, 96], "individu": [19, 27, 85, 88, 96, 97], "tiktoken": [19, 69, 100], "special": [19, 24, 69], "variabl": [19, 24, 25, 26, 32, 102], "writer": 19, "whether": [19, 22, 24, 26, 28, 29, 30, 31, 32, 33, 34, 39, 40, 41, 46, 47, 49, 52, 62, 65, 68, 69, 70, 76, 96], "correspond": [19, 64, 66, 76, 99, 100], "consecut": [19, 23], "from_dict": [19, 96], "construct": [19, 101], "dictionari": [19, 27, 82, 83, 84, 85, 98], "mistral": [20, 24, 34, 49, 50, 51, 96, 98, 99], "llama2chatformat": [20, 96, 97], "summar": [21, 33, 96, 97], "task": [21, 25, 96, 97, 98, 100, 101, 102], "dialogu": [21, 33, 96], "dialog": 21, "adher": 22, "sharegpt": [22, 30, 97], "gpt": [22, 55, 98], "remain": [22, 63, 101], "unmask": 22, "forth": 23, "come": [23, 64, 101], "empti": 23, "shorter": 23, "length": [23, 25, 26, 27, 28, 29, 32, 34, 53, 55, 57, 59, 60, 61, 68, 69, 72, 86], "min": [23, 101], "invalid": 23, "convert_to_messag": [24, 96], "chat_format": [24, 30, 34, 96, 97], "chatformat": [24, 30, 97], "load_dataset_kwarg": [24, 26, 30, 32], "multiturn": [24, 96], "foreach": 24, "prepar": [24, 96], "truncat": [24, 26, 27, 32, 34, 68, 69], "encod": [24, 26, 28, 29, 30, 31, 32, 33, 34, 68, 69, 96], "decod": [24, 26, 28, 29, 30, 31, 32, 33, 34, 60, 68, 69, 74, 96], "anyth": [24, 26, 28, 29, 30, 31, 32, 33, 34], "load_dataset": [24, 26, 28, 29, 30, 31, 32, 33, 34, 96], "co": [24, 26, 30, 32, 53, 71, 72, 98], "doc": [24, 26, 30, 32, 70, 73, 78, 84, 85, 87, 89, 98], "en": [24, 26, 30, 32], "package_refer": [24, 26, 30, 32], "loading_method": [24, 26, 30, 32], "text": [24, 27, 68, 69, 96, 97, 98], "extra": [24, 93, 101, 102], "still": [24, 73, 101, 102], "llama3": [24, 34, 44, 45, 46, 47, 48, 74, 77, 92], "where": [24, 25, 28, 29, 31, 33, 55, 60, 65, 68, 97], "unless": 24, "check": [24, 30, 60, 76, 92, 96, 98, 99, 101], "concaten": [25, 68, 97], "sub": [25, 84], "unifi": 25, "were": [25, 96, 99], "simplifi": [25, 101], "simultan": 25, "intern": [25, 73], "aggreg": 25, "transpar": 25, "index": [25, 55, 59, 60, 61, 63, 86, 93, 96, 98], "howev": [25, 93], "constitu": 25, "might": [25, 98], "larg": [25, 65, 102], "comput": [25, 55, 56, 59, 60, 89, 98, 102], "cumul": 25, "maintain": [25, 102], "indic": [25, 27, 55, 59, 60, 61, 70, 96, 97], "deleg": 25, "retriev": [25, 77], "lead": [25, 68], "high": [25, 94, 101], "scale": [25, 39, 40, 41, 46, 47, 49, 52, 65, 74, 101, 102], "consid": 25, "strategi": 25, "stream": [25, 78], "demand": 25, "deriv": [25, 56, 60, 61], "_dataset": 25, "_len": 25, "total": [25, 63, 79, 91, 95, 98, 100, 101], "combin": [25, 97], "_index": 25, "lookup": 25, "dataset1": 25, "mycustomdataset": 25, "params1": 25, "dataset2": 25, "params2": 25, "concat_dataset": 25, "data_point": 25, "1500": 25, "element": [25, 69, 98], "focus": [25, 99], "enhanc": [25, 102], "divers": 25, "machin": [25, 75, 98], "instructtempl": [26, 97], "contribut": [26, 28, 29, 31, 33], "replac": [26, 28, 29, 31, 33, 62, 101], "disabl": [26, 32, 89], "recommend": [26, 28, 29, 32, 84, 96, 98, 102], "highest": [26, 28, 29, 32], "sequenc": [26, 27, 28, 29, 32, 34, 55, 57, 59, 60, 61, 68, 69, 86, 96], "ds": [27, 34], "max_pack": 27, "split_across_pack": 27, "greedi": 27, "pack": [27, 28, 29, 30, 31, 32, 33, 34, 55, 59, 60, 61], "done": [27, 76, 101, 102], "preprocess": 27, "outsid": [27, 89, 98, 100, 101], "sampler": [27, 99], "part": [27, 96, 102], "buffer": 27, "long": [27, 96, 101], "enough": [27, 96], "attent": [27, 39, 40, 41, 46, 47, 49, 52, 53, 55, 57, 59, 60, 61, 100, 101, 102], "lower": [27, 101], "triangular": 27, "cross": 27, "attend": [27, 55, 60, 61], "rel": [27, 55, 59, 60, 61, 101], "pad": [27, 74, 86], "max": [27, 34, 60, 63, 68, 101], "wise": 27, "collat": [27, 86], "made": [27, 30, 32, 59, 98], "smaller": [27, 98, 100, 101, 102], "jam": 27, "vari": 27, "s1": [27, 68], "s2": [27, 68], "s3": 27, "s4": 27, "contamin": 27, "input_po": [27, 55, 57, 59, 60, 61], "matrix": 27, "causal": [27, 55, 60, 61], "continu": 27, "increment": 27, "last": [27, 63], "move": [27, 60], "entir": [27, 96, 102], "avoid": [27, 58, 62, 89, 102], "yahma": 28, "codebas": [28, 29, 31, 33, 98], "prior": [28, 29, 30, 31, 32, 33, 34], "alpaca_d": [28, 29], "batch_siz": [28, 29, 31, 33, 55, 57, 60, 61, 98], "tatsu": [29, 97], "lab": [29, 97], "conversation_styl": [30, 97], "chatdataset": [30, 34, 96], "friendli": [30, 32, 74, 96], "huggingfaceh4": 30, "no_robot": 30, "chatmlformat": 30, "2096": 30, "accomplish": 30, "liweili": 31, "c4_200m": 31, "variant": [31, 33], "mirror": [31, 33], "llama_recip": [31, 33], "grammar_d": 31, "samsum": [33, 97], "summari": [33, 97], "samsum_d": 33, "open": [34, 35, 97, 98], "orca": [34, 97], "slimorca": [34, 97], "dedup": [34, 97], "1024": [34, 97], "prescrib": 34, "least": [34, 100, 101], "though": [34, 96], "10": [34, 86, 98, 100, 102], "351": 34, "82": [34, 98], "391": 34, "221": 34, "220": 34, "193": 34, "12": [34, 93], "471": 34, "gemma": 35, "gemmatransformerdecod": 35, "w": [35, 36, 37, 38, 44, 45, 50, 84, 85, 96, 98, 101, 102], "blog": 35, "technolog": 35, "develop": [35, 102], "transformerdecod": [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 74, 101], "arxiv": [36, 37, 38, 42, 43, 48, 51, 54, 55, 58, 59], "org": [36, 37, 38, 42, 43, 48, 51, 54, 55, 58, 59, 70, 73, 78, 84, 87, 88, 89, 93], "ab": [36, 37, 38, 42, 43, 48, 51, 54, 59], "2307": [36, 37, 38], "09288": [36, 37, 38], "70b": [37, 40, 44, 46, 100], "lora_attn_modul": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101, 102], "q_proj": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 101, 102], "k_proj": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 101, 102], "v_proj": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 101, 102], "output_proj": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 101, 102], "apply_lora_to_mlp": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101], "apply_lora_to_output": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101], "lora_rank": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101], "lora_alpha": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101], "float": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 55, 58, 63, 65, 74, 82, 83, 84, 85, 101, 102], "16": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 101, 102], "lora_dropout": [39, 40, 41, 42, 43], "05": [39, 40, 41, 42, 43], "quantize_bas": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 65, 102], "lora": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 65, 92, 94, 96, 99, 100], "tloen": [39, 40, 41, 46, 47, 52], "8bb8579e403dc78e37fe81ffbb253c413007323f": [39, 40, 41, 46, 47, 52], "l41": [39, 40, 41, 46, 47, 52], "l43": [39, 40, 41, 46, 47, 52], "linear": [39, 40, 41, 42, 43, 46, 47, 48, 49, 51, 52, 54, 60, 64, 65, 101, 102], "mlp": [39, 40, 41, 46, 47, 49, 52, 60, 61, 100, 101], "final": [39, 40, 41, 46, 47, 49, 52, 56, 60, 69, 98, 100, 101, 102], "rank": [39, 40, 41, 46, 47, 49, 52, 65, 79, 89, 99, 101, 102], "low": [39, 40, 41, 46, 47, 49, 52, 65, 98, 101, 102], "approxim": [39, 40, 41, 46, 47, 49, 52, 65, 101], "factor": [39, 40, 41, 46, 47, 49, 52, 65, 98], "llama2_70b": 40, "llama2_7b": [41, 101], "qlora": [42, 43, 48, 51, 54, 62, 92, 94, 100, 101], "per": [42, 43, 48, 51, 54, 57, 62, 100, 102], "paper": [42, 43, 48, 51, 54, 101, 102], "2305": [42, 43, 48, 51, 54, 55], "14314": [42, 43, 48, 51, 54], "lora_llama2_13b": 42, "lora_llama2_7b": [43, 101], "llama3_70b": 46, "llama3_8b": [47, 74, 100], "lora_llama3_8b": 48, "announc": 50, "lora_mistral_7b": 51, "phi3": [52, 53, 54], "phi3_mini": 52, "ref": [53, 85], "phi": 53, "128k": 53, "nor": 53, "slide": 53, "window": 53, "lora_phi3_mini": 54, "head_dim": [55, 57, 59, 60], "pos_embed": [55, 101], "kv_cach": 55, "kvcach": [55, 60], "attn_dropout": [55, 60], "head": [55, 57, 59, 60, 100], "queri": [55, 57, 60, 61, 100], "gqa": 55, "introduc": [55, 58, 65, 96, 101, 102], "pdf": [55, 58], "13245v1": 55, "version": [55, 74, 93, 100, 102], "multihead": 55, "mha": [55, 60], "n": [55, 68, 69, 91, 95, 96, 97], "extrem": 55, "share": [55, 97, 98], "mqa": 55, "credit": 55, "document": [55, 70, 77], "lightn": 55, "lit": 55, "lit_gpt": 55, "v": [55, 60, 101], "k": [55, 101], "q": [55, 101], "n_kv_head": 55, "dimens": [55, 57, 59, 60, 65, 100, 101, 102], "calcul": [55, 60, 100], "e": [55, 62, 64, 67, 93, 98, 100, 101, 102], "g": [55, 64, 100, 101, 102], "rotarypositionalembed": [55, 101], "cach": [55, 57, 59, 60, 93], "rope": [55, 59], "dropout": [55, 65, 101, 102], "onto": 55, "scaled_dot_product_attent": 55, "seq_length": [55, 61, 74], "boolean": [55, 60, 61, 70], "softmax": [55, 60, 61], "row": [55, 60, 61, 96], "j": [55, 60, 61], "seq_len": [55, 59], "bigger": 55, "n_h": [55, 59], "num": [55, 59], "n_kv": 55, "kv": [55, 57, 60], "emb": [55, 60, 61], "h_d": [55, 59], "gate_proj": 56, "down_proj": 56, "up_proj": 56, "silu": 56, "feed": [56, 61], "network": [56, 101, 102], "fed": [56, 96], "multipli": 56, "subclass": [56, 73], "although": [56, 101], "afterward": 56, "former": 56, "regist": [56, 62, 102], "hook": [56, 62, 102], "latter": 56, "standalon": 57, "past": 57, "becaus": [57, 60, 96, 98, 100], "expand": 57, "dpython": [57, 60, 62], "reset": [57, 60], "zero": [57, 58, 98, 100], "k_val": 57, "v_val": 57, "h": [57, 93], "longer": 57, "ep": 58, "1e": 58, "06": [58, 101], "root": [58, 84, 85], "squar": 58, "1910": 58, "07467": 58, "verfic": [58, 59], "small": [58, 98], "divis": 58, "10000": 59, "rotari": [59, 100], "propos": 59, "2104": 59, "09864": 59, "l450": 59, "upto": 59, "init": [59, 85, 102], "exceed": 59, "freq": 59, "recomput": 59, "geometr": 59, "progress": [59, 99], "rotat": 59, "angl": 59, "bsz": [59, 74], "todo": 59, "effici": [59, 77, 92, 94, 98, 99, 101], "transformerdecoderlay": 60, "norm": [60, 61], "space": 60, "belong": 60, "reduc": [60, 94, 97, 101, 102], "statement": 60, "improv": [60, 77, 98, 100, 101], "readabl": [60, 98], "At": 60, "arang": 60, "prompt_length": 60, "causal_mask": 60, "m_": 60, "seq": 60, "reset_cach": 60, "setup_cach": 60, "attn": [61, 101, 102], "causalselfattent": [61, 101], "sa_norm": 61, "mlp_norm": 61, "ff": 61, "common_util": 62, "bfloat16": [62, 98, 99, 100, 101], "offload_to_cpu": 62, "nf4": [62, 102], "restor": 62, "higher": [62, 100, 102], "offload": [62, 102], "increas": [62, 63, 100, 101], "peak": [62, 98, 100, 101, 102], "gpu": [62, 98, 99, 100, 101, 102], "usag": [62, 93, 98, 99, 100, 102], "_register_state_dict_hook": 62, "m": [62, 69, 74, 96], "mymodul": 62, "_after_": 62, "nf4tensor": [62, 102], "unquant": [62, 98, 102], "unus": 62, "num_warmup_step": 63, "num_training_step": 63, "num_cycl": 63, "last_epoch": 63, "lambdalr": 63, "rate": [63, 94, 99], "schedul": [63, 87, 99], "linearli": 63, "lr": 63, "decreas": [63, 101, 102], "cosin": 63, "v4": 63, "23": [63, 100], "src": 63, "l104": 63, "warmup": [63, 87], "phase": 63, "wave": 63, "half": 63, "lr_schedul": 63, "peft": [64, 65, 66, 67, 101, 102], "protocol": 64, "adapter_param": [64, 65, 66, 67], "proj": 64, "in_dim": [64, 65, 101, 102], "out_dim": [64, 65, 101, 102], "bia": [64, 65, 101, 102], "loralinear": [64, 101, 102], "alpha": [65, 101, 102], "use_bia": 65, "perturb": 65, "decomposit": [65, 101], "matric": [65, 101, 102], "trainabl": [65, 67, 101, 102], "mapsto": 65, "w_0x": 65, "r": [65, 69, 101], "bax": 65, "probabl": [65, 74, 98], "lora_a": [65, 101, 102], "lora_b": [65, 101, 102], "subset": 66, "get_adapter_param": [67, 101], "sentencepieceprocessor": 68, "pretrain": [68, 69, 96, 99, 101, 102], "non": 68, "spm_model": [68, 96], "tokenized_text": 68, "hello": [68, 96, 98, 100], "world": [68, 79, 98], "add_bo": [68, 69, 96], "add_eo": [68, 69, 96], "31587": 68, "29644": 68, "102": 68, "trim_leading_whitespac": 68, "prefix": 68, "unbatch": 68, "prepend": [68, 69], "bo": [68, 69, 96], "append": [68, 93], "eo": [68, 69, 96], "trim": 68, "whitespac": 68, "underli": [68, 102], "sentencepiec": [68, 100], "due": [68, 101, 102], "tokenize_messag": [68, 69, 96, 97], "problem": 68, "slice": 68, "tokenizer_path": 68, "separ": [68, 71, 96, 99, 100, 101, 102], "concat": 68, "1788": 68, "2643": 68, "13": [68, 98, 100, 102], "1792": 68, "9508": 68, "465": 68, "22137": 68, "2933": 68, "join": 68, "attribut": 68, "llama3_tiktoken": 69, "p": [69, 70, 101, 102], "l": 69, "all_special_token": 69, "bos_token": 69, "begin_of_text": [69, 96], "eos_token": 69, "end_of_text": 69, "start_header_id": [69, 96], "end_header_id": [69, 96], "step_id": 69, "eom_id": 69, "eot_id": [69, 96], "python_tag": 69, "identif": 69, "regex": 69, "second": [69, 98, 100, 101, 102], "uniqu": 69, "256": [69, 97, 98, 100], "header": [69, 96], "token_id": 69, "truncate_at_eo": 69, "tokenize_head": 69, "datatyp": [70, 102], "polici": [70, 77, 88], "denot": 70, "integ": [70, 86, 89], "auto_wrap_polici": [70, 77, 88], "submodul": 70, "obei": 70, "contract": 70, "get_fsdp_polici": 70, "modules_to_wrap": [70, 77], "min_num_param": 70, "my_fsdp_polici": 70, "recurs": [70, 84], "isinst": 70, "sum": [70, 101], "numel": [70, 101], "1000": 70, "functool": 70, "partial": 70, "stabl": [70, 84, 87, 89, 93], "html": [70, 73, 78, 84, 87, 88, 89], "alia": 70, "few": [71, 97, 100, 101, 102], "0001_of_0003": 71, "0002_of_0003": 71, "preserv": [71, 102], "weight_map": [71, 98], "intermediate_checkpoint": [71, 72], "parit": 71, "_weight_map": 71, "shard": [72, 100], "wip": 72, "argpars": 73, "argumentpars": 73, "builtin": 73, "said": 73, "noth": 73, "treat": [73, 96], "consult": 73, "info": [73, 99], "librari": [73, 78, 89, 92, 94, 102], "parse_known_arg": 73, "namespac": 73, "act": 73, "precid": 73, "parse_arg": 73, "properti": [73, 101], "too": [73, 100], "max_generated_token": 74, "pad_id": 74, "temperatur": [74, 98], "top_k": [74, 98], "stop_token": 74, "custom_generate_next_token": 74, "condit": [74, 97], "predict": 74, "prune": [74, 102], "stop": 74, "compil": [74, 98, 100, 102], "generate_next_token": 74, "llama3_token": [74, 100], "hi": [74, 96], "my": [74, 96, 98, 100], "jeremi": 74, "availab": 75, "distribut": [75, 80, 88, 89, 94, 99, 100], "bf16": [76, 102], "request": [76, 97, 98], "inde": [76, 98], "kernel": 76, "runtimeerror": [76, 80], "float32": 76, "isn": 76, "hardwar": [76, 94, 98, 101], "memory_efficient_fsdp_wrap": 77, "maxim": [77, 92, 94], "been": [77, 100], "workload": 77, "15": [77, 96, 98, 101, 102], "alongsid": 77, "ac": 77, "fullyshardeddataparallel": 77, "const": 77, "fsdppolicytyp": 77, "handler": 78, "aka": 79, "filenam": 82, "log_": 82, "unixtimestamp": 82, "txt": [82, 99], "thread": 82, "safe": 82, "flush": [82, 83, 84, 85], "union": [82, 83, 84, 85, 88, 89], "ndarrai": [82, 83, 84, 85], "scalar": [82, 83, 84, 85], "record": [82, 83, 84, 85], "payload": [82, 83, 84, 85], "organize_log": 84, "tensorboard": 84, "subdirectori": 84, "compar": [84, 98, 101, 102], "logdir": 84, "startup": 84, "tree": [84, 97, 98], "tfevent": 84, "encount": 84, "frontend": 84, "organ": 84, "accordingli": 84, "my_log_dir": 84, "view": [84, 98, 99], "my_metr": [84, 85], "termin": [84, 85], "entiti": 85, "bias": 85, "sent": 85, "usernam": 85, "my_project": 85, "my_ent": 85, "my_group": 85, "importerror": 85, "account": [85, 101, 102], "log_config": 85, "local": [85, 89, 93, 96, 98, 99], "link": [85, 98], "capecap": 85, "6053ofw0": 85, "torchtune_config_j67sb73v": 85, "padding_idx": 86, "ignore_idx": 86, "longest": 86, "token_pair": 86, "torchtune_perf_trac": 87, "contextmanag": 87, "wait": 87, "trace": 87, "speed": [87, 100, 102], "reduct": [87, 101], "acwrappolicytyp": 88, "describ": [88, 97], "author": [88, 94, 99, 102], "intermedi": [88, 100, 102], "fsdp_adavnced_tutori": 88, "debug_mod": 89, "pseudo": 89, "random": [89, 99], "commonli": [89, 98, 101, 102], "numpi": 89, "own": [89, 96, 97, 98, 101], "determinist": 89, "global": 89, "warn": 89, "nondeterminist": 89, "addition": [89, 101], "cudnn": 89, "set_deterministic_debug_mod": 89, "algorithm": 89, "generated_examples_python": 90, "zip": 90, "galleri": [90, 95], "sphinx": 90, "000": [91, 95, 100], "execut": [91, 95], "generated_exampl": 91, "mem": [91, 95], "mb": [91, 95], "topic": 92, "gentl": 92, "introduct": 92, "readi": [92, 96], "workflow": [92, 97, 99, 101], "requisit": 93, "proper": [93, 99], "host": [93, 99], "page": [93, 94, 99, 100], "latest": [93, 99, 102], "confirm": 93, "And": [93, 98, 100], "ls": [93, 98, 99, 100], "welcom": 93, "show": [93, 96, 101], "greatest": [93, 99], "contributor": 93, "cd": [93, 98], "even": [93, 96, 100, 101, 102], "commit": 93, "branch": 93, "url": 93, "whl": 93, "therebi": [93, 102], "forc": 93, "reinstal": 93, "opt": [93, 99], "suffix": 93, "cu121": 93, "On": [94, 101], "pointer": 94, "emphas": 94, "aspect": 94, "simplic": 94, "component": 94, "reus": 94, "prove": 94, "democrat": 94, "box": [94, 102], "zoo": 94, "varieti": [94, 101], "techniqu": [94, 98, 99, 101], "integr": [94, 98, 99, 100, 101, 102], "excit": 94, "checkout": 94, "quickstart": 94, "attain": 94, "better": [94, 96, 97, 98], "chekckpoint": 94, "hyperparamet": [94, 99, 101, 102], "embodi": 94, "philosophi": 94, "usabl": 94, "composit": 94, "hard": 94, "outlin": 94, "unecessari": 94, "never": 94, "thoroughli": 94, "unit": 94, "know": [96, 97, 98, 100, 101], "align": 96, "intend": 96, "nice": 96, "meet": 96, "overhaul": 96, "sai": [96, 97, 99], "accompani": 96, "who": 96, "influenti": 96, "hip": 96, "hop": 96, "artist": [96, 100], "2pac": 96, "rakim": 96, "c": 96, "na": 96, "flavor": [96, 97], "certain": 96, "msg": 96, "formatted_messag": [96, 97], "nyou": [96, 97], "nwho": 96, "sentencepiecetoken": 96, "why": [96, 99, 101], "user_messag": 96, "518": 96, "25580": 96, "29962": 96, "3532": 96, "14816": 96, "29903": 96, "6778": 96, "piece_to_id": 96, "reserv": [96, 102], "vector": 96, "place": 96, "manual": [96, 102], "529": 96, "29879": 96, "29958": 96, "tiktokentoken": 96, "nhere": 96, "_encode_special_token": 96, "128000": 96, "128009": 96, "pure": 96, "That": 96, "won": [96, 98, 100], "mess": 96, "govern": 96, "prime": 96, "strictli": 96, "summarizetempl": [96, 97], "lightweight": 96, "ask": 96, "untouch": 96, "nsummari": 96, "robust": 96, "csv": 96, "question": [96, 97, 98, 100], "answer": [96, 98, 100], "onlin": 96, "forum": 96, "panda": 96, "pd": 96, "df": 96, "read_csv": 96, "your_fil": 96, "nrow": 96, "tolist": 96, "iloc": 96, "gp": 96, "receiv": 96, "commun": [96, 98], "satellit": 96, "thing": [96, 102], "message_convert": 96, "input_msg": 96, "output_msg": 96, "assistant_messag": 96, "But": [96, 98, 100, 101], "mistralchatformat": 96, "custom_dataset": 96, "2048": 96, "data_fil": 96, "honor": 96, "copi": [96, 98, 99, 100, 102], "8b_lora_single_devic": [96, 100], "launch": [96, 99], "custom_8b_lora_single_devic": 96, "steer": 97, "wheel": 97, "publicli": 97, "great": [97, 98], "iter": [97, 102], "knob": 97, "tweak": 97, "footprint": [97, 101], "could": [97, 101], "achiev": [97, 98, 100, 101, 102], "concatdataset": 97, "instruct_dataset": 97, "vicgal": 97, "gpt4": 97, "alpacainstructtempl": 97, "demonstr": 97, "fix": 97, "goal": 97, "agnost": 97, "respond": 97, "further": [97, 101, 102], "classifi": 97, "anim": 97, "plant": 97, "miner": 97, "oak": 97, "copper": 97, "ore": 97, "eleph": 97, "mydataset": 97, "onthehub": 97, "customtempl": 97, "similar": [97, 98, 100, 101, 102], "quit": [97, 102], "similarli": 97, "chat_dataset": 97, "incorpor": 97, "advanc": 97, "preferencedataset": 97, "rlhf": 97, "adjust": 97, "chosen": 97, "reject": 97, "chosen_messag": 97, "transformed_sampl": 97, "key_chosen": 97, "rejected_messag": 97, "key_reject": 97, "chosen_input_id": 97, "c_mask": 97, "chosen_label": 97, "np": 97, "cross_entropy_ignore_idx": 97, "rejected_input_id": 97, "r_mask": 97, "rejected_label": 97, "purpos": [97, 99, 100], "stack_exchanged_paired_dataset": 97, "had": 97, "lvwerra": 97, "stack": 97, "exchang": 97, "stackexchangedpairedtempl": 97, "response_j": 97, "response_k": 97, "data_dir": 97, "rl": 97, "favorit": [98, 100, 101], "seemlessli": 98, "beyond": [98, 102], "connect": 98, "larger": [98, 100], "amount": 98, "natur": 98, "export": 98, "mobil": 98, "phone": 98, "leverag": [98, 100, 102], "mode": 98, "lot": 98, "plai": 98, "freez": [98, 101], "percentag": 98, "learnabl": 98, "keep": [98, 101], "16gb": [98, 101], "rtx": 98, "3090": 98, "4090": 98, "hour": 98, "full_finetune_single_devic": [98, 99], "7b_full_low_memori": [98, 99], "full_finetune_distribut": [98, 99], "7b_full": [98, 99], "13b_full": [98, 99], "7b_qlora_single_devic": [98, 99, 102], "473": 98, "98": [98, 102], "gb": [98, 100, 101, 102], "50": 98, "484": 98, "01": [98, 99], "fact": [98, 100, 101], "third": 98, "realli": 98, "eleuther_ev": [98, 100], "eleuther_evalu": [98, 100], "lm_eval": [98, 100], "plan": 98, "custom_eval_config": [98, 100], "truthfulqa_mc2": [98, 100, 101], "measur": [98, 100], "propens": [98, 100], "shot": [98, 100], "accuraci": [98, 100, 101, 102], "baselin": [98, 101], "324": 98, "loglikelihood": 98, "195": 98, "121": 98, "27": 98, "197": 98, "acc": 98, "388": 98, "38": 98, "shown": 98, "489": 98, "48": [98, 102], "seem": 98, "custom_generation_config": [98, 100], "kick": 98, "300": 98, "interest": 98, "site": 98, "visit": 98, "bai": 98, "area": 98, "92": [98, 100], "exploratorium": 98, "san": 98, "francisco": 98, "magazin": 98, "awesom": 98, "bridg": 98, "pretti": 98, "cool": 98, "96": [98, 102], "61": 98, "sec": [98, 100], "25": 98, "83": 98, "99": [98, 101], "72": 98, "littl": 98, "saw": 98, "took": [98, 100], "torchao": [98, 100, 102], "bit": [98, 100, 101, 102], "custom_quantization_config": [98, 100], "68": 98, "19": [98, 100, 102], "76": 98, "69": 98, "95": [98, 100], "67": 98, "4w": [98, 100], "unlik": [98, 100], "engin": [98, 100], "fullmodeltorchtunecheckpoint": [98, 100], "int4weightonlyquant": [98, 100], "groupsiz": [98, 100], "did": [98, 100, 102], "park": 98, "sit": 98, "top": [98, 102], "hill": 98, "beauti": 98, "62": [98, 100], "17": [98, 101], "85": 98, "hood": [98, 102], "sped": 98, "almost": [98, 100, 101], "3x": [98, 100], "benefit": 98, "doesn": 98, "yet": 98, "fast": 98, "clone": [98, 101, 102], "assumpt": 98, "satisfi": 98, "new_dir": 98, "output_dict": 98, "sd_1": 98, "sd_2": 98, "dump": 98, "convert_hf_checkpoint": 98, "checkpoint_path": 98, "justin": 98, "school": 98, "math": 98, "teacher": 98, "ws": 98, "94": [98, 100], "103": 98, "28": 98, "bandwidth": [98, 100], "1391": 98, "84": 98, "thats": 98, "seamlessli": 98, "authent": [98, 99], "hopefulli": 98, "gave": 98, "gate": 99, "grant": 99, "minut": 99, "agreement": 99, "altern": 99, "hackabl": 99, "singularli": 99, "technic": 99, "depth": 99, "principl": 99, "minim": [99, 101, 102], "boilerpl": 99, "hold": 99, "substanti": [99, 101], "custom_config": 99, "replic": 99, "lorafinetunerecipesingledevic": 99, "lora_finetune_output": 99, "log_1713194212": 99, "52": 99, "3697006702423096": 99, "25880": [99, 102], "24": [99, 100], "55": 99, "83it": 99, "monitor": 99, "tqdm": 99, "interv": 99, "e2": 99, "releas": 100, "focu": 100, "128": [100, 101], "theta": 100, "gain": 100, "illustr": 100, "basic": 100, "observ": 100, "18": 100, "consum": [100, 102], "vram": [100, 101], "overal": 100, "nproc_per_nod": [100, 101], "lora_finetune_distribut": [100, 101], "8b_lora": 100, "8b_qlora_single_devic": 100, "alloc": [100, 102], "coupl": [100, 101, 102], "122": 100, "sarah": 100, "busi": 100, "mum": 100, "young": 100, "children": 100, "live": 100, "north": 100, "east": 100, "england": 100, "135": 100, "88": 100, "138": 100, "346": 100, "09": 100, "139": 100, "31": 100, "far": 100, "drill": 100, "90": 100, "93": 100, "91": 100, "104": 100, "four": [100, 101], "again": 100, "jake": 100, "disciplin": 100, "passion": 100, "draw": 100, "paint": 100, "57": [100, 101, 102], "speedup": 100, "broader": 100, "teach": 101, "straight": 101, "jump": 101, "neural": [101, 102], "unfamiliar": 101, "oppos": [101, 102], "momentum": 101, "adamw": 101, "arbitrari": 101, "relat": 101, "aghajanyan": 101, "et": 101, "al": 101, "hypothes": 101, "intrins": 101, "down": [101, 102], "often": 101, "eight": 101, "practic": 101, "imag": 101, "left": 101, "blue": 101, "rememb": 101, "approx": 101, "15m": 101, "8192": 101, "65k": 101, "requires_grad": [101, 102], "frozen_out": [101, 102], "lora_out": [101, 102], "omit": 101, "base_model": 101, "choos": 101, "lora_model": 101, "lora_llama_2_7b": [101, 102], "alon": 101, "in_featur": 101, "out_featur": 101, "inplac": 101, "feel": 101, "free": 101, "strict": 101, "whenev": 101, "validate_state_dict_for_lora": 101, "peft_util": 101, "set_trainable_param": 101, "fetch": 101, "lora_param": 101, "total_param": 101, "trainable_param": 101, "2f": 101, "6742609920": 101, "4194304": 101, "nnode": 101, "7b_lora": 101, "my_model_checkpoint_path": [101, 102], "tokenizer_checkpoint": [101, 102], "my_tokenizer_checkpoint_path": [101, 102], "constraint": 101, "factori": 101, "benefici": 101, "impact": 101, "minor": 101, "good": 101, "64": 101, "lora_experiment_1": 101, "smooth": [101, 102], "curv": [101, 102], "500": 101, "ran": 101, "commod": 101, "cogniz": 101, "ax": 101, "parallel": 101, "truthfulqa": 101, "previous": 101, "475": 101, "87": 101, "508": 101, "86": 101, "504": 101, "04": 101, "514": 101, "lowest": 101, "absolut": 101, "4gb": 101, "tradeoff": 101, "potenti": 101, "highli": 102, "vanilla": 102, "held": 102, "therefor": 102, "bespok": 102, "normalfloat": 102, "8x": 102, "retain": 102, "vast": 102, "major": 102, "highlight": 102, "degrad": 102, "normatfloat": 102, "doubl": 102, "themselv": 102, "deepdiv": 102, "idea": 102, "distinct": 102, "storag": 102, "de": 102, "incur": 102, "counterpart": 102, "set_default_devic": 102, "qlora_linear": 102, "memory_alloc": 102, "177": 102, "152": 102, "byte": 102, "del": 102, "empty_cach": 102, "lora_linear": 102, "081": 102, "344": 102, "qlora_llama2_7b": 102, "qlora_model": 102, "essenti": 102, "reparametrize_as_dtype_state_dict_post_hook": 102, "stat": 102, "against": 102, "35": 102, "40": 102, "29": 102, "slow": 102, "slower": 102, "149": 102, "9157477021217346": 102, "02": 102, "08": 102, "14": 102, "15it": 102, "nightli": 102, "200": 102, "hundr": 102, "228": 102, "8158286809921265": 102, "59": 102, "95it": 102, "exercis": 102, "portion": 102, "augment": 102, "linear_nf4": 102, "to_nf4": 102, "linear_weight": 102, "autograd": 102, "regular": 102, "incom": 102}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "Message"], [20, 1, 1, "", "MistralChatFormat"], [21, 1, 1, "", "SummarizeTemplate"], [22, 0, 1, "", "sharegpt_to_llama2_messages"], [23, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.Message": [[19, 2, 1, "", "from_dict"]], "torchtune.data.MistralChatFormat": [[20, 2, 1, "", "format"], [20, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[21, 2, 1, "", "format"]], "torchtune.datasets": [[24, 1, 1, "", "ChatDataset"], [25, 1, 1, "", "ConcatDataset"], [26, 1, 1, "", "InstructDataset"], [27, 1, 1, "", "PackedDataset"], [28, 0, 1, "", "alpaca_cleaned_dataset"], [29, 0, 1, "", "alpaca_dataset"], [30, 0, 1, "", "chat_dataset"], [31, 0, 1, "", "grammar_dataset"], [32, 0, 1, "", "instruct_dataset"], [33, 0, 1, "", "samsum_dataset"], [34, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[35, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[36, 0, 1, "", "llama2_13b"], [37, 0, 1, "", "llama2_70b"], [38, 0, 1, "", "llama2_7b"], [39, 0, 1, "", "lora_llama2_13b"], [40, 0, 1, "", "lora_llama2_70b"], [41, 0, 1, "", "lora_llama2_7b"], [42, 0, 1, "", "qlora_llama2_13b"], [43, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[44, 0, 1, "", "llama3_70b"], [45, 0, 1, "", "llama3_8b"], [46, 0, 1, "", "lora_llama3_70b"], [47, 0, 1, "", "lora_llama3_8b"], [48, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[49, 0, 1, "", "lora_mistral_7b"], [50, 0, 1, "", "mistral_7b"], [51, 0, 1, "", "qlora_mistral_7b"]], "torchtune.models.phi3": [[52, 0, 1, "", "lora_phi3_mini"], [53, 0, 1, "", "phi3_mini"], [54, 0, 1, "", "qlora_phi3_mini"]], "torchtune.modules": [[55, 1, 1, "", "CausalSelfAttention"], [56, 1, 1, "", "FeedForward"], [57, 1, 1, "", "KVCache"], [58, 1, 1, "", "RMSNorm"], [59, 1, 1, "", "RotaryPositionalEmbeddings"], [60, 1, 1, "", "TransformerDecoder"], [61, 1, 1, "", "TransformerDecoderLayer"], [63, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[55, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[56, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[57, 2, 1, "", "reset"], [57, 2, 1, "", "update"]], "torchtune.modules.RMSNorm": [[58, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[59, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[60, 2, 1, "", "forward"], [60, 2, 1, "", "reset_caches"], [60, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerDecoderLayer": [[61, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[62, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[64, 1, 1, "", "AdapterModule"], [65, 1, 1, "", "LoRALinear"], [66, 0, 1, "", "get_adapter_params"], [67, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[64, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[65, 2, 1, "", "adapter_params"], [65, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[68, 1, 1, "", "SentencePieceTokenizer"], [69, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[68, 2, 1, "", "decode"], [68, 2, 1, "", "encode"], [68, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[69, 2, 1, "", "decode"], [69, 2, 1, "", "encode"], [69, 2, 1, "", "tokenize_message"], [69, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[70, 4, 1, "", "FSDPPolicyType"], [71, 1, 1, "", "FullModelHFCheckpointer"], [72, 1, 1, "", "FullModelMetaCheckpointer"], [73, 1, 1, "", "TuneRecipeArgumentParser"], [74, 0, 1, "", "generate"], [75, 0, 1, "", "get_device"], [76, 0, 1, "", "get_dtype"], [77, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [78, 0, 1, "", "get_logger"], [79, 0, 1, "", "get_world_size_and_rank"], [80, 0, 1, "", "init_distributed"], [81, 0, 1, "", "list_dtypes"], [86, 0, 1, "", "padded_collate"], [87, 0, 1, "", "profiler"], [88, 0, 1, "", "set_activation_checkpointing"], [89, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[71, 2, 1, "", "load_checkpoint"], [71, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[72, 2, 1, "", "load_checkpoint"], [72, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[73, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[82, 1, 1, "", "DiskLogger"], [83, 1, 1, "", "StdoutLogger"], [84, 1, 1, "", "TensorBoardLogger"], [85, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[82, 2, 1, "", "close"], [82, 2, 1, "", "log"], [82, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[83, 2, 1, "", "close"], [83, 2, 1, "", "log"], [83, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[84, 2, 1, "", "close"], [84, 2, 1, "", "log"], [84, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[85, 2, 1, "", "close"], [85, 2, 1, "", "log"], [85, 2, 1, "", "log_config"], [85, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:data"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "data", "Python data"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 70, 92, 94, 98, 100, 101, 102], "config": [0, 7, 8, 99], "data": [1, 5, 96], "instruct": [1, 93, 97, 100], "templat": [1, 96, 97], "chat": [1, 96, 97], "format": [1, 6, 97], "type": 1, "convert": 1, "helper": 1, "func": 1, "dataset": [2, 96, 97], "exampl": 2, "gener": [2, 74, 98, 100], "builder": 2, "class": [2, 8], "model": [3, 4, 9, 98, 99, 100, 101], "llama3": [3, 96, 100], "llama2": [3, 96, 98, 101, 102], "phi": 3, "3": 3, "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 93, 102], "block": 4, "token": [4, 96], "peft": 4, "util": [4, 5, 70], "checkpoint": [5, 6, 9, 98], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 101, 102], "manag": 5, "perform": [5, 101], "profil": [5, 87], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 94, 98], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 98, 101, 102], "put": [6, 102], "thi": 6, "all": [6, 7, 102], "togeth": [6, 102], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 97], "us": [7, 8, 96, 98, 102], "instanti": [7, 10], "referenc": 7, "other": [7, 98], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 98, 99], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 94, 101, 102], "ar": 8, "recip": [8, 99, 101], "script": 8, "run": [8, 98], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "messag": 19, "mistralchatformat": 20, "summarizetempl": 21, "sharegpt_to_llama2_messag": 22, "validate_messag": 23, "chatdataset": 24, "concatdataset": 25, "instructdataset": 26, "packeddataset": 27, "alpaca_cleaned_dataset": 28, "alpaca_dataset": 29, "chat_dataset": 30, "grammar_dataset": 31, "instruct_dataset": 32, "samsum_dataset": 33, "slimorca_dataset": 34, "gemma_2b": 35, "llama2_13b": 36, "llama2_70b": 37, "llama2_7b": 38, "lora_llama2_13b": 39, "lora_llama2_70b": 40, "lora_llama2_7b": 41, "qlora_llama2_13b": 42, "qlora_llama2_7b": 43, "llama3_70b": 44, "llama3_8b": 45, "lora_llama3_70b": 46, "lora_llama3_8b": 47, "qlora_llama3_8b": 48, "lora_mistral_7b": 49, "mistral_7b": 50, "qlora_mistral_7b": 51, "lora_phi3_mini": 52, "phi3_mini": 53, "qlora_phi3_mini": 54, "causalselfattent": 55, "todo": [55, 61], "feedforward": 56, "kvcach": 57, "rmsnorm": 58, "rotarypositionalembed": 59, "transformerdecod": 60, "transformerdecoderlay": 61, "reparametrize_as_dtype_state_dict_post_hook": 62, "get_cosine_schedule_with_warmup": 63, "adaptermodul": 64, "loralinear": 65, "get_adapter_param": 66, "set_trainable_param": 67, "sentencepiecetoken": 68, "tiktokentoken": 69, "fsdppolicytyp": 70, "fullmodelhfcheckpoint": 71, "fullmodelmetacheckpoint": 72, "tunerecipeargumentpars": 73, "get_devic": 75, "get_dtyp": 76, "get_full_finetune_fsdp_wrap_polici": 77, "get_logg": 78, "get_world_size_and_rank": 79, "init_distribut": 80, "list_dtyp": 81, "disklogg": 82, "stdoutlogg": 83, "tensorboardlogg": 84, "wandblogg": 85, "padded_col": 86, "set_activation_checkpoint": 88, "set_se": 89, "comput": [91, 95], "time": [91, 95], "welcom": 92, "document": 92, "get": [92, 100], "start": 92, "tutori": 92, "instal": 93, "via": [93, 100], "pypi": 93, "git": 93, "clone": 93, "nightli": 93, "kei": 94, "concept": 94, "design": 94, "principl": 94, "fine": [96, 97, 99, 100], "tune": [96, 97, 99, 100], "chang": 96, "from": [96, 102], "prompt": 96, "special": 96, "when": 96, "should": 96, "i": 96, "custom": [96, 97], "fulli": 97, "end": 98, "workflow": 98, "download": [98, 99], "7b": 98, "finetun": [98, 101, 102], "evalu": [98, 100], "eleutherai": [98, 100], "s": [98, 100], "eval": [98, 100], "har": [98, 100], "speed": 98, "up": 98, "quantiz": [98, 100], "librari": 98, "upload": 98, "hug": 98, "face": 98, "hub": 98, "first": 99, "llm": 99, "select": 99, "modifi": 99, "train": 99, "next": 99, "step": 99, "meta": 100, "8b": 100, "access": 100, "text": 100, "our": 100, "faster": 100, "how": 101, "doe": 101, "work": 101, "appli": 101, "trade": 101, "off": 101, "qlora": 102, "save": 102, "deep": 102, "dive": 102}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})