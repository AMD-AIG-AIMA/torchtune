Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_generation", "api_ref_models", "api_ref_modules", "api_ref_training", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/comet_logging", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.log_config", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLTemplate", "generated/torchtune.data.ChosenRejectedToMessages", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InputOutputToMessages", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.JSONToMessages", "generated/torchtune.data.Message", "generated/torchtune.data.PromptTemplate", "generated/torchtune.data.PromptTemplateInterface", "generated/torchtune.data.QuestionAnswerTemplate", "generated/torchtune.data.Role", "generated/torchtune.data.ShareGPTToMessages", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.get_openai_messages", "generated/torchtune.data.get_sharegpt_messages", "generated/torchtune.data.left_pad_sequence", "generated/torchtune.data.padded_collate", "generated/torchtune.data.padded_collate_dpo", "generated/torchtune.data.padded_collate_sft", "generated/torchtune.data.truncate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.ChatDataset", "generated/torchtune.datasets.ConcatDataset", "generated/torchtune.datasets.InstructDataset", "generated/torchtune.datasets.PackedDataset", "generated/torchtune.datasets.PreferenceDataset", "generated/torchtune.datasets.SFTDataset", "generated/torchtune.datasets.TextCompletionDataset", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.chat_dataset", "generated/torchtune.datasets.cnn_dailymail_articles_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.instruct_dataset", "generated/torchtune.datasets.llava_instruct_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.datasets.stack_exchange_paired_dataset", "generated/torchtune.datasets.text_completion_dataset", "generated/torchtune.datasets.the_cauldron_dataset", "generated/torchtune.datasets.wikitext_dataset", "generated/torchtune.generation.generate", "generated/torchtune.models.clip.TilePositionalEmbedding", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding", "generated/torchtune.models.clip.TokenPositionalEmbedding", "generated/torchtune.models.clip.clip_vision_encoder", "generated/torchtune.models.code_llama2.code_llama2_13b", "generated/torchtune.models.code_llama2.code_llama2_70b", "generated/torchtune.models.code_llama2.code_llama2_7b", "generated/torchtune.models.code_llama2.lora_code_llama2_13b", "generated/torchtune.models.code_llama2.lora_code_llama2_70b", "generated/torchtune.models.code_llama2.lora_code_llama2_7b", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b", "generated/torchtune.models.gemma.GemmaTokenizer", "generated/torchtune.models.gemma.gemma", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.gemma.gemma_7b", "generated/torchtune.models.gemma.gemma_tokenizer", "generated/torchtune.models.gemma.lora_gemma", "generated/torchtune.models.gemma.lora_gemma_2b", "generated/torchtune.models.gemma.lora_gemma_7b", "generated/torchtune.models.gemma.qlora_gemma_2b", "generated/torchtune.models.gemma.qlora_gemma_7b", "generated/torchtune.models.llama2.Llama2ChatTemplate", "generated/torchtune.models.llama2.Llama2Tokenizer", "generated/torchtune.models.llama2.llama2", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.llama2_reward_7b", "generated/torchtune.models.llama2.llama2_tokenizer", "generated/torchtune.models.llama2.lora_llama2", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.lora_llama2_reward_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_70b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_reward_7b", "generated/torchtune.models.llama3.Llama3Tokenizer", "generated/torchtune.models.llama3.llama3", "generated/torchtune.models.llama3.llama3_70b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.llama3_tokenizer", "generated/torchtune.models.llama3.lora_llama3", "generated/torchtune.models.llama3.lora_llama3_70b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_70b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.llama3_1.llama3_1", "generated/torchtune.models.llama3_1.llama3_1_405b", "generated/torchtune.models.llama3_1.llama3_1_70b", "generated/torchtune.models.llama3_1.llama3_1_8b", "generated/torchtune.models.llama3_1.lora_llama3_1", "generated/torchtune.models.llama3_1.lora_llama3_1_70b", "generated/torchtune.models.llama3_1.lora_llama3_1_8b", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b", "generated/torchtune.models.mistral.MistralChatTemplate", "generated/torchtune.models.mistral.MistralTokenizer", "generated/torchtune.models.mistral.lora_mistral", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.lora_mistral_classifier", "generated/torchtune.models.mistral.lora_mistral_reward_7b", "generated/torchtune.models.mistral.mistral", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.mistral_classifier", "generated/torchtune.models.mistral.mistral_reward_7b", "generated/torchtune.models.mistral.mistral_tokenizer", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_reward_7b", "generated/torchtune.models.phi3.Phi3MiniTokenizer", "generated/torchtune.models.phi3.lora_phi3", "generated/torchtune.models.phi3.lora_phi3_mini", "generated/torchtune.models.phi3.phi3", "generated/torchtune.models.phi3.phi3_mini", "generated/torchtune.models.phi3.phi3_mini_tokenizer", "generated/torchtune.models.phi3.qlora_phi3_mini", "generated/torchtune.models.qwen2.Qwen2Tokenizer", "generated/torchtune.models.qwen2.lora_qwen2", "generated/torchtune.models.qwen2.lora_qwen2_0_5b", "generated/torchtune.models.qwen2.lora_qwen2_1_5b", "generated/torchtune.models.qwen2.lora_qwen2_7b", "generated/torchtune.models.qwen2.qwen2", "generated/torchtune.models.qwen2.qwen2_0_5b", "generated/torchtune.models.qwen2.qwen2_1_5b", "generated/torchtune.models.qwen2.qwen2_7b", "generated/torchtune.models.qwen2.qwen2_tokenizer", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.Fp32LayerNorm", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.MultiHeadAttention", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TanhGate", "generated/torchtune.modules.TiedEmbeddingTransformerDecoder", "generated/torchtune.modules.TransformerCrossAttentionLayer", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerSelfAttentionLayer", "generated/torchtune.modules.VisionTransformer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss", "generated/torchtune.modules.model_fusion.DeepFusionModel", "generated/torchtune.modules.model_fusion.FusionEmbedding", "generated/torchtune.modules.model_fusion.FusionLayer", "generated/torchtune.modules.model_fusion.register_fusion_module", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.disable_adapter", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora", "generated/torchtune.modules.peft.validate_state_dict_for_lora", "generated/torchtune.modules.rlhf.estimate_advantages", "generated/torchtune.modules.rlhf.get_rewards_ppo", "generated/torchtune.modules.rlhf.loss.DPOLoss", "generated/torchtune.modules.rlhf.loss.IPOLoss", "generated/torchtune.modules.rlhf.loss.PPOLoss", "generated/torchtune.modules.rlhf.loss.RSOLoss", "generated/torchtune.modules.rlhf.loss.SimPOLoss", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token", "generated/torchtune.modules.tokenizers.BaseTokenizer", "generated/torchtune.modules.tokenizers.ModelTokenizer", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens", "generated/torchtune.modules.transforms.Transform", "generated/torchtune.modules.transforms.VisionCrossAttentionMask", "generated/torchtune.training.FSDPPolicyType", "generated/torchtune.training.FullModelHFCheckpointer", "generated/torchtune.training.FullModelMetaCheckpointer", "generated/torchtune.training.FullModelTorchTuneCheckpointer", "generated/torchtune.training.ModelType", "generated/torchtune.training.OptimizerInBackwardWrapper", "generated/torchtune.training.apply_selective_activation_checkpointing", "generated/torchtune.training.create_optim_in_bwd_wrapper", "generated/torchtune.training.get_dtype", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy", "generated/torchtune.training.get_memory_stats", "generated/torchtune.training.get_quantizer_mode", "generated/torchtune.training.get_unmasked_sequence_lengths", "generated/torchtune.training.get_world_size_and_rank", "generated/torchtune.training.init_distributed", "generated/torchtune.training.is_distributed", "generated/torchtune.training.log_memory_stats", "generated/torchtune.training.lora_fsdp_wrap_policy", "generated/torchtune.training.metric_logging.CometLogger", "generated/torchtune.training.metric_logging.DiskLogger", "generated/torchtune.training.metric_logging.StdoutLogger", "generated/torchtune.training.metric_logging.TensorBoardLogger", "generated/torchtune.training.metric_logging.WandBLogger", "generated/torchtune.training.register_optim_in_bwd_hooks", "generated/torchtune.training.set_activation_checkpointing", "generated/torchtune.training.set_default_dtype", "generated/torchtune.training.set_seed", "generated/torchtune.training.setup_torch_profiler", "generated/torchtune.training.update_state_dict_for_classifier", "generated/torchtune.training.validate_expected_param_dtype", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.torch_version_ge", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "recipes/lora_finetune_single_device", "recipes/qat_distributed", "recipes/recipes_overview", "sg_execution_times", "tune_cli", "tutorials/chat", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/memory_optimizations", "tutorials/qat_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_generation.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_training.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/comet_logging.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.log_config.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLTemplate.rst", "generated/torchtune.data.ChosenRejectedToMessages.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InputOutputToMessages.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.JSONToMessages.rst", "generated/torchtune.data.Message.rst", "generated/torchtune.data.PromptTemplate.rst", "generated/torchtune.data.PromptTemplateInterface.rst", "generated/torchtune.data.QuestionAnswerTemplate.rst", "generated/torchtune.data.Role.rst", "generated/torchtune.data.ShareGPTToMessages.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.get_openai_messages.rst", "generated/torchtune.data.get_sharegpt_messages.rst", "generated/torchtune.data.left_pad_sequence.rst", "generated/torchtune.data.padded_collate.rst", "generated/torchtune.data.padded_collate_dpo.rst", "generated/torchtune.data.padded_collate_sft.rst", "generated/torchtune.data.truncate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.ChatDataset.rst", "generated/torchtune.datasets.ConcatDataset.rst", "generated/torchtune.datasets.InstructDataset.rst", "generated/torchtune.datasets.PackedDataset.rst", "generated/torchtune.datasets.PreferenceDataset.rst", "generated/torchtune.datasets.SFTDataset.rst", "generated/torchtune.datasets.TextCompletionDataset.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.chat_dataset.rst", "generated/torchtune.datasets.cnn_dailymail_articles_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.instruct_dataset.rst", "generated/torchtune.datasets.llava_instruct_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.datasets.stack_exchange_paired_dataset.rst", "generated/torchtune.datasets.text_completion_dataset.rst", "generated/torchtune.datasets.the_cauldron_dataset.rst", "generated/torchtune.datasets.wikitext_dataset.rst", "generated/torchtune.generation.generate.rst", "generated/torchtune.models.clip.TilePositionalEmbedding.rst", "generated/torchtune.models.clip.TiledTokenPositionalEmbedding.rst", "generated/torchtune.models.clip.TokenPositionalEmbedding.rst", "generated/torchtune.models.clip.clip_vision_encoder.rst", "generated/torchtune.models.code_llama2.code_llama2_13b.rst", "generated/torchtune.models.code_llama2.code_llama2_70b.rst", "generated/torchtune.models.code_llama2.code_llama2_7b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.lora_code_llama2_7b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_13b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_70b.rst", "generated/torchtune.models.code_llama2.qlora_code_llama2_7b.rst", "generated/torchtune.models.gemma.GemmaTokenizer.rst", "generated/torchtune.models.gemma.gemma.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.gemma.gemma_7b.rst", "generated/torchtune.models.gemma.gemma_tokenizer.rst", "generated/torchtune.models.gemma.lora_gemma.rst", "generated/torchtune.models.gemma.lora_gemma_2b.rst", "generated/torchtune.models.gemma.lora_gemma_7b.rst", "generated/torchtune.models.gemma.qlora_gemma_2b.rst", "generated/torchtune.models.gemma.qlora_gemma_7b.rst", "generated/torchtune.models.llama2.Llama2ChatTemplate.rst", "generated/torchtune.models.llama2.Llama2Tokenizer.rst", "generated/torchtune.models.llama2.llama2.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.llama2_reward_7b.rst", "generated/torchtune.models.llama2.llama2_tokenizer.rst", "generated/torchtune.models.llama2.lora_llama2.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_reward_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_70b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_reward_7b.rst", "generated/torchtune.models.llama3.Llama3Tokenizer.rst", "generated/torchtune.models.llama3.llama3.rst", "generated/torchtune.models.llama3.llama3_70b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.llama3_tokenizer.rst", "generated/torchtune.models.llama3.lora_llama3.rst", "generated/torchtune.models.llama3.lora_llama3_70b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_70b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.llama3_1.llama3_1.rst", "generated/torchtune.models.llama3_1.llama3_1_405b.rst", "generated/torchtune.models.llama3_1.llama3_1_70b.rst", "generated/torchtune.models.llama3_1.llama3_1_8b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.lora_llama3_1_8b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_70b.rst", "generated/torchtune.models.llama3_1.qlora_llama3_1_8b.rst", "generated/torchtune.models.mistral.MistralChatTemplate.rst", "generated/torchtune.models.mistral.MistralTokenizer.rst", "generated/torchtune.models.mistral.lora_mistral.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.lora_mistral_classifier.rst", "generated/torchtune.models.mistral.lora_mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.mistral_classifier.rst", "generated/torchtune.models.mistral.mistral_reward_7b.rst", "generated/torchtune.models.mistral.mistral_tokenizer.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_reward_7b.rst", "generated/torchtune.models.phi3.Phi3MiniTokenizer.rst", "generated/torchtune.models.phi3.lora_phi3.rst", "generated/torchtune.models.phi3.lora_phi3_mini.rst", "generated/torchtune.models.phi3.phi3.rst", "generated/torchtune.models.phi3.phi3_mini.rst", "generated/torchtune.models.phi3.phi3_mini_tokenizer.rst", "generated/torchtune.models.phi3.qlora_phi3_mini.rst", "generated/torchtune.models.qwen2.Qwen2Tokenizer.rst", "generated/torchtune.models.qwen2.lora_qwen2.rst", "generated/torchtune.models.qwen2.lora_qwen2_0_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_1_5b.rst", "generated/torchtune.models.qwen2.lora_qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2.rst", "generated/torchtune.models.qwen2.qwen2_0_5b.rst", "generated/torchtune.models.qwen2.qwen2_1_5b.rst", "generated/torchtune.models.qwen2.qwen2_7b.rst", "generated/torchtune.models.qwen2.qwen2_tokenizer.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.Fp32LayerNorm.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.MultiHeadAttention.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TanhGate.rst", "generated/torchtune.modules.TiedEmbeddingTransformerDecoder.rst", "generated/torchtune.modules.TransformerCrossAttentionLayer.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerSelfAttentionLayer.rst", "generated/torchtune.modules.VisionTransformer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.loss.CEWithChunkedOutputLoss.rst", "generated/torchtune.modules.model_fusion.DeepFusionModel.rst", "generated/torchtune.modules.model_fusion.FusionEmbedding.rst", "generated/torchtune.modules.model_fusion.FusionLayer.rst", "generated/torchtune.modules.model_fusion.register_fusion_module.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.disable_adapter.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.peft.validate_missing_and_unexpected_for_lora.rst", "generated/torchtune.modules.peft.validate_state_dict_for_lora.rst", "generated/torchtune.modules.rlhf.estimate_advantages.rst", "generated/torchtune.modules.rlhf.get_rewards_ppo.rst", "generated/torchtune.modules.rlhf.loss.DPOLoss.rst", "generated/torchtune.modules.rlhf.loss.IPOLoss.rst", "generated/torchtune.modules.rlhf.loss.PPOLoss.rst", "generated/torchtune.modules.rlhf.loss.RSOLoss.rst", "generated/torchtune.modules.rlhf.loss.SimPOLoss.rst", "generated/torchtune.modules.rlhf.truncate_sequence_at_first_stop_token.rst", "generated/torchtune.modules.tokenizers.BaseTokenizer.rst", "generated/torchtune.modules.tokenizers.ModelTokenizer.rst", "generated/torchtune.modules.tokenizers.SentencePieceBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenBaseTokenizer.rst", "generated/torchtune.modules.tokenizers.parse_hf_tokenizer_json.rst", "generated/torchtune.modules.tokenizers.tokenize_messages_no_special_tokens.rst", "generated/torchtune.modules.transforms.Transform.rst", "generated/torchtune.modules.transforms.VisionCrossAttentionMask.rst", "generated/torchtune.training.FSDPPolicyType.rst", "generated/torchtune.training.FullModelHFCheckpointer.rst", "generated/torchtune.training.FullModelMetaCheckpointer.rst", "generated/torchtune.training.FullModelTorchTuneCheckpointer.rst", "generated/torchtune.training.ModelType.rst", "generated/torchtune.training.OptimizerInBackwardWrapper.rst", "generated/torchtune.training.apply_selective_activation_checkpointing.rst", "generated/torchtune.training.create_optim_in_bwd_wrapper.rst", "generated/torchtune.training.get_dtype.rst", "generated/torchtune.training.get_full_finetune_fsdp_wrap_policy.rst", "generated/torchtune.training.get_memory_stats.rst", "generated/torchtune.training.get_quantizer_mode.rst", "generated/torchtune.training.get_unmasked_sequence_lengths.rst", "generated/torchtune.training.get_world_size_and_rank.rst", "generated/torchtune.training.init_distributed.rst", "generated/torchtune.training.is_distributed.rst", "generated/torchtune.training.log_memory_stats.rst", "generated/torchtune.training.lora_fsdp_wrap_policy.rst", "generated/torchtune.training.metric_logging.CometLogger.rst", "generated/torchtune.training.metric_logging.DiskLogger.rst", "generated/torchtune.training.metric_logging.StdoutLogger.rst", "generated/torchtune.training.metric_logging.TensorBoardLogger.rst", "generated/torchtune.training.metric_logging.WandBLogger.rst", "generated/torchtune.training.register_optim_in_bwd_hooks.rst", "generated/torchtune.training.set_activation_checkpointing.rst", "generated/torchtune.training.set_default_dtype.rst", "generated/torchtune.training.set_seed.rst", "generated/torchtune.training.setup_torch_profiler.rst", "generated/torchtune.training.update_state_dict_for_classifier.rst", "generated/torchtune.training.validate_expected_param_dtype.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.torch_version_ge.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "recipes/lora_finetune_single_device.rst", "recipes/qat_distributed.rst", "recipes/recipes_overview.rst", "sg_execution_times.rst", "tune_cli.rst", "tutorials/chat.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/memory_optimizations.rst", "tutorials/qat_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.generation", "torchtune.models", "torchtune.modules", "torchtune.training", "torchtune.utils", "Checkpointing in torchtune", "Logging to Comet", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "log_config", "parse", "validate", "ChatFormat", "ChatMLTemplate", "ChosenRejectedToMessages", "torchtune.data.GrammarErrorCorrectionTemplate", "InputOutputToMessages", "InstructTemplate", "JSONToMessages", "Message", "PromptTemplate", "PromptTemplateInterface", "torchtune.data.QuestionAnswerTemplate", "torchtune.data.Role", "ShareGPTToMessages", "torchtune.data.SummarizeTemplate", "get_openai_messages", "get_sharegpt_messages", "left_pad_sequence", "padded_collate", "padded_collate_dpo", "padded_collate_sft", "truncate", "validate_messages", "torchtune.datasets.ChatDataset", "ConcatDataset", "torchtune.datasets.InstructDataset", "PackedDataset", "PreferenceDataset", "SFTDataset", "TextCompletionDataset", "alpaca_cleaned_dataset", "alpaca_dataset", "chat_dataset", "cnn_dailymail_articles_dataset", "grammar_dataset", "instruct_dataset", "llava_instruct_dataset", "samsum_dataset", "slimorca_dataset", "stack_exchange_paired_dataset", "text_completion_dataset", "the_cauldron_dataset", "wikitext_dataset", "generate", "TilePositionalEmbedding", "TiledTokenPositionalEmbedding", "TokenPositionalEmbedding", "clip_vision_encoder", "code_llama2_13b", "code_llama2_70b", "code_llama2_7b", "lora_code_llama2_13b", "lora_code_llama2_70b", "lora_code_llama2_7b", "qlora_code_llama2_13b", "qlora_code_llama2_70b", "qlora_code_llama2_7b", "GemmaTokenizer", "gemma", "gemma_2b", "gemma_7b", "gemma_tokenizer", "lora_gemma", "lora_gemma_2b", "lora_gemma_7b", "qlora_gemma_2b", "qlora_gemma_7b", "Llama2ChatTemplate", "Llama2Tokenizer", "llama2", "llama2_13b", "llama2_70b", "llama2_7b", "llama2_reward_7b", "llama2_tokenizer", "lora_llama2", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "lora_llama2_reward_7b", "qlora_llama2_13b", "qlora_llama2_70b", "qlora_llama2_7b", "qlora_llama2_reward_7b", "Llama3Tokenizer", "llama3", "llama3_70b", "llama3_8b", "llama3_tokenizer", "lora_llama3", "lora_llama3_70b", "lora_llama3_8b", "qlora_llama3_70b", "qlora_llama3_8b", "llama3_1", "llama3_1_405b", "llama3_1_70b", "llama3_1_8b", "lora_llama3_1", "lora_llama3_1_70b", "lora_llama3_1_8b", "qlora_llama3_1_70b", "qlora_llama3_1_8b", "MistralChatTemplate", "MistralTokenizer", "lora_mistral", "lora_mistral_7b", "lora_mistral_classifier", "lora_mistral_reward_7b", "mistral", "mistral_7b", "mistral_classifier", "mistral_reward_7b", "mistral_tokenizer", "qlora_mistral_7b", "qlora_mistral_reward_7b", "Phi3MiniTokenizer", "lora_phi3", "lora_phi3_mini", "phi3", "phi3_mini", "phi3_mini_tokenizer", "qlora_phi3_mini", "Qwen2Tokenizer", "lora_qwen2", "lora_qwen2_0_5b", "lora_qwen2_1_5b", "lora_qwen2_7b", "qwen2", "qwen2_0_5b", "qwen2_1_5b", "qwen2_7b", "qwen2_tokenizer", "FeedForward", "Fp32LayerNorm", "KVCache", "MultiHeadAttention", "RMSNorm", "RotaryPositionalEmbeddings", "TanhGate", "TiedEmbeddingTransformerDecoder", "TransformerCrossAttentionLayer", "TransformerDecoder", "TransformerSelfAttentionLayer", "VisionTransformer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "CEWithChunkedOutputLoss", "DeepFusionModel", "FusionEmbedding", "FusionLayer", "register_fusion_module", "AdapterModule", "LoRALinear", "disable_adapter", "get_adapter_params", "set_trainable_params", "validate_missing_and_unexpected_for_lora", "validate_state_dict_for_lora", "estimate_advantages", "get_rewards_ppo", "DPOLoss", "IPOLoss", "PPOLoss", "RSOLoss", "SimPOLoss", "truncate_sequence_at_first_stop_token", "BaseTokenizer", "ModelTokenizer", "SentencePieceBaseTokenizer", "TikTokenBaseTokenizer", "parse_hf_tokenizer_json", "tokenize_messages_no_special_tokens", "Transform", "VisionCrossAttentionMask", "torchtune.training.FSDPPolicyType", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "FullModelTorchTuneCheckpointer", "ModelType", "OptimizerInBackwardWrapper", "apply_selective_activation_checkpointing", "create_optim_in_bwd_wrapper", "get_dtype", "get_full_finetune_fsdp_wrap_policy", "get_memory_stats", "get_quantizer_mode", "get_unmasked_sequence_lengths", "get_world_size_and_rank", "init_distributed", "is_distributed", "log_memory_stats", "lora_fsdp_wrap_policy", "CometLogger", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "register_optim_in_bwd_hooks", "set_activation_checkpointing", "set_default_dtype", "set_seed", "setup_torch_profiler", "update_state_dict_for_classifier", "validate_expected_param_dtype", "get_device", "get_logger", "torch_version_ge", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "LoRA Single Device Finetuning", "Distributed Quantization-Aware Training (QAT)", "Recipes Overview", "Computation times", "torchtune CLI", "Fine-tuning Llama3 with Chat Data", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Meta Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Memory Optimization Overview", "Finetuning Llama3 with QAT", "Finetuning Llama2 with QLoRA"], "terms": {"instruct": [1, 2, 4, 18, 19, 21, 22, 23, 29, 41, 42, 44, 47, 50, 51, 52, 53, 54, 56, 57, 100, 119, 128, 135, 136, 137, 145, 146, 147, 226, 229, 230, 233, 234, 237, 239, 241, 242], "prompt": [1, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 39, 41, 43, 44, 47, 48, 50, 51, 52, 53, 54, 55, 57, 59, 73, 77, 83, 84, 90, 100, 104, 119, 120, 129, 132, 137, 139, 148, 156, 158, 164, 188, 235, 236, 238], "chat": [1, 2, 17, 18, 23, 29, 31, 32, 39, 44, 48, 83, 84, 137, 229], "includ": [1, 8, 10, 11, 17, 22, 25, 26, 44, 63, 74, 84, 85, 101, 110, 125, 137, 144, 156, 158, 169, 183, 192, 193, 228, 231, 233, 234, 235, 236, 237, 238, 239, 242], "some": [1, 8, 10, 18, 123, 165, 171, 172, 226, 228, 229, 230, 233, 234, 235, 236, 237, 239, 240, 241, 242], "specif": [1, 5, 10, 11, 13, 43, 44, 52, 57, 73, 84, 100, 120, 132, 139, 184, 200, 230, 234, 235, 236, 240, 241, 242], "format": [1, 2, 6, 17, 22, 24, 31, 33, 39, 41, 43, 44, 47, 48, 51, 73, 83, 84, 100, 119, 120, 132, 139, 184, 192, 193, 194, 195, 233, 234, 236, 237, 238, 239, 240], "differ": [1, 10, 12, 35, 40, 41, 48, 51, 60, 61, 62, 120, 156, 160, 177, 185, 195, 220, 228, 229, 230, 233, 234, 236, 238, 239, 240, 241, 242], "dataset": [1, 10, 19, 21, 22, 23, 24, 29, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 177, 178, 228, 231, 237, 238, 241], "model": [1, 2, 8, 9, 10, 11, 13, 18, 19, 21, 23, 24, 29, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 151, 152, 153, 154, 156, 158, 159, 160, 161, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 183, 184, 187, 188, 189, 192, 193, 194, 195, 197, 198, 200, 201, 208, 209, 214, 215, 219, 226, 228, 229, 230, 231, 234, 235, 242], "from": [1, 2, 4, 8, 9, 10, 11, 12, 13, 19, 22, 23, 24, 29, 32, 33, 34, 39, 40, 41, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 75, 76, 83, 86, 87, 88, 89, 104, 120, 126, 128, 137, 139, 145, 146, 147, 148, 149, 152, 156, 157, 158, 159, 160, 162, 163, 166, 167, 168, 171, 174, 177, 178, 180, 181, 185, 187, 190, 192, 193, 194, 196, 198, 209, 212, 213, 214, 219, 225, 227, 230, 232, 233, 235, 236, 237, 238, 239, 240, 241], "common": [1, 2, 5, 10, 188, 233, 234, 235, 238, 239, 240, 241], "json": [1, 8, 23, 29, 31, 32, 39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 104, 137, 139, 148, 187, 192, 233, 235, 236, 241], "schema": 1, "convers": [1, 8, 17, 19, 29, 31, 32, 38, 39, 43, 44, 48, 52, 54, 192, 194, 195, 228, 234, 235, 236, 239, 240, 242], "list": [1, 8, 10, 17, 19, 24, 25, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 48, 49, 52, 57, 58, 59, 63, 67, 68, 69, 70, 71, 72, 73, 77, 78, 79, 80, 81, 82, 84, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 129, 130, 131, 132, 133, 134, 137, 138, 139, 140, 141, 142, 143, 156, 158, 160, 163, 164, 165, 166, 168, 169, 173, 174, 183, 184, 185, 186, 188, 190, 192, 193, 194, 209, 222, 234, 235, 236, 237, 238, 240, 241], "us": [1, 2, 4, 5, 8, 9, 12, 13, 15, 17, 18, 21, 22, 24, 25, 31, 32, 34, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 73, 83, 84, 85, 91, 100, 101, 104, 105, 110, 114, 120, 132, 133, 137, 139, 140, 144, 148, 149, 152, 153, 154, 156, 158, 159, 160, 161, 163, 164, 165, 167, 170, 173, 175, 176, 177, 179, 181, 185, 186, 190, 191, 192, 193, 195, 196, 199, 200, 201, 203, 208, 209, 210, 211, 212, 213, 217, 219, 221, 226, 227, 228, 229, 230, 231, 233, 235, 237, 238, 239, 240, 241], "collect": [1, 10, 59, 237], "sampl": [1, 9, 12, 17, 19, 21, 22, 23, 24, 29, 31, 32, 39, 41, 42, 43, 44, 45, 50, 52, 53, 54, 56, 57, 59, 152, 154, 156, 158, 159, 160, 164, 180, 189, 190, 234, 236, 240], "batch": [1, 11, 34, 35, 36, 42, 47, 50, 52, 53, 57, 61, 151, 152, 154, 156, 157, 158, 159, 160, 164, 166, 175, 176, 177, 178, 180, 181, 203, 218, 228, 235, 237, 238, 239, 240], "handl": [1, 10, 15, 40, 44, 84, 120, 185, 186, 234, 236, 239, 240, 242], "ani": [1, 5, 8, 10, 11, 13, 15, 16, 19, 22, 23, 25, 29, 31, 32, 34, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 62, 84, 120, 150, 161, 164, 166, 171, 172, 173, 174, 183, 184, 185, 188, 192, 193, 194, 196, 205, 208, 209, 217, 220, 233, 234, 235, 237, 239, 240, 241], "pad": [1, 33, 34, 35, 36, 42, 139, 160, 176, 179, 182, 190, 203, 235], "miscellan": 1, "modifi": [1, 10, 11, 12, 161, 228, 236, 238, 239, 240, 241, 242], "For": [2, 6, 8, 10, 11, 19, 23, 24, 25, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 74, 78, 84, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 156, 158, 160, 163, 165, 167, 192, 198, 202, 209, 213, 215, 217, 227, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "detail": [2, 8, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 84, 127, 151, 160, 163, 179, 191, 200, 208, 217, 229, 230, 233, 235, 236, 237, 238, 239, 241, 242], "usag": [2, 161, 163, 195, 196, 218, 227, 229, 233, 235, 236, 237, 238, 240, 241, 242], "guid": [2, 9, 10, 12, 19, 21, 23, 29, 50, 51, 52, 53, 54, 57, 181, 209, 228, 234, 235, 237, 239], "pleas": [2, 6, 17, 20, 22, 27, 30, 31, 32, 39, 41, 60, 61, 62, 63, 70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 160, 163, 191, 200, 208, 215, 227, 236, 238, 242], "see": [2, 6, 8, 9, 12, 20, 27, 30, 34, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 81, 82, 83, 84, 96, 97, 98, 99, 108, 109, 117, 118, 119, 127, 130, 131, 138, 139, 151, 160, 168, 191, 195, 200, 208, 209, 213, 215, 217, 222, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "our": [2, 8, 11, 228, 229, 230, 231, 234, 235, 236, 237, 239, 240, 241, 242], "tutori": [2, 8, 84, 215, 228, 229, 230, 231, 234, 235, 236, 237, 238, 239, 240, 241, 242], "support": [2, 4, 8, 9, 11, 12, 13, 24, 39, 41, 42, 43, 44, 47, 48, 49, 50, 52, 53, 54, 57, 58, 78, 91, 105, 114, 119, 121, 123, 133, 136, 137, 140, 150, 152, 160, 165, 166, 169, 180, 193, 194, 196, 199, 201, 202, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "sever": [2, 240], "wide": [2, 152], "help": [2, 8, 83, 156, 158, 160, 164, 192, 209, 226, 227, 228, 231, 233, 234, 235, 236, 237, 240, 241, 242], "quickli": [2, 10, 25, 45, 229, 234, 235, 240], "bootstrap": 2, "your": [2, 6, 9, 12, 13, 25, 39, 45, 48, 51, 61, 62, 63, 84, 160, 165, 209, 212, 213, 219, 226, 227, 228, 229, 230, 231, 233, 234, 235, 238, 239, 240, 241, 242], "fine": [2, 8, 9, 11, 12, 24, 42, 43, 44, 56, 219, 226, 228, 229, 230, 231, 236, 239, 241], "tune": [2, 4, 8, 9, 10, 11, 12, 15, 24, 42, 43, 44, 56, 219, 226, 227, 228, 229, 230, 231, 233, 236, 239, 241, 242], "also": [2, 8, 9, 10, 11, 12, 13, 40, 48, 51, 56, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 137, 140, 144, 152, 156, 158, 169, 181, 200, 201, 208, 209, 213, 219, 221, 227, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "like": [2, 5, 8, 9, 10, 11, 12, 39, 137, 160, 163, 165, 194, 227, 229, 233, 234, 235, 236, 237, 239, 240, 241], "These": [2, 5, 8, 10, 11, 13, 42, 43, 160, 190, 229, 231, 234, 235, 236, 237, 238, 239, 240, 241, 242], "ar": [2, 5, 8, 9, 10, 12, 13, 17, 22, 25, 26, 31, 32, 33, 34, 35, 38, 41, 42, 43, 44, 47, 48, 51, 52, 59, 61, 67, 68, 69, 70, 71, 72, 73, 78, 79, 80, 81, 82, 83, 84, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 156, 157, 158, 159, 160, 164, 165, 166, 169, 170, 173, 174, 176, 190, 191, 192, 193, 195, 196, 198, 199, 201, 206, 208, 218, 219, 227, 228, 229, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "especi": [2, 228, 233, 236, 240], "specifi": [2, 8, 10, 11, 13, 19, 21, 23, 29, 48, 50, 51, 52, 53, 54, 57, 59, 77, 85, 90, 91, 101, 104, 105, 110, 114, 129, 137, 140, 144, 148, 152, 156, 158, 159, 164, 191, 200, 202, 208, 213, 215, 218, 230, 231, 233, 234, 235, 236, 237, 238, 240, 241, 242], "yaml": [2, 10, 11, 13, 14, 15, 40, 48, 51, 56, 213, 228, 231, 233, 234, 235, 236, 237, 238, 239, 241, 242], "config": [2, 8, 9, 12, 13, 14, 15, 16, 40, 48, 51, 56, 152, 173, 192, 196, 209, 213, 218, 228, 229, 230, 231, 234, 235, 236, 238, 239, 240, 241, 242], "represent": [2, 239, 241, 242], "abov": [2, 4, 8, 161, 206, 227, 230, 236, 238, 239, 240, 241, 242], "all": [4, 5, 11, 16, 24, 25, 33, 34, 40, 42, 43, 44, 63, 104, 137, 139, 148, 149, 152, 156, 158, 160, 161, 164, 165, 166, 167, 170, 189, 192, 196, 198, 206, 214, 220, 224, 226, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241], "famili": [4, 8, 11, 47, 49, 52, 54, 55, 57, 58, 195, 228, 233, 238], "request": [4, 199, 235, 236], "access": [4, 8, 10, 11, 40, 192, 198, 229, 230, 233, 235, 236, 237], "hug": [4, 8, 18, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 104, 137, 148, 162, 187, 228, 233, 237, 238], "face": [4, 8, 18, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 104, 137, 148, 162, 187, 228, 233, 237, 238], "To": [4, 8, 10, 11, 12, 42, 52, 160, 166, 192, 219, 227, 228, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "download": [4, 8, 52, 224, 227, 229, 230, 234, 235, 238, 239, 241, 242], "8b": [4, 103, 107, 109, 113, 116, 118, 134, 229, 230, 233, 234, 241], "meta": [4, 8, 83, 84, 100, 154, 192, 193, 229, 230, 233, 234, 236, 237], "hf": [4, 8, 132, 177, 178, 180, 192, 233, 234, 236, 237, 238], "token": [4, 8, 10, 11, 24, 34, 36, 37, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 73, 74, 77, 78, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 121, 123, 125, 127, 129, 132, 133, 135, 137, 139, 140, 144, 148, 152, 154, 156, 157, 158, 159, 160, 163, 164, 165, 166, 176, 179, 182, 183, 184, 185, 186, 187, 188, 190, 200, 203, 229, 233, 235, 236, 237, 238, 239, 240, 241, 242], "hf_token": [4, 230], "70b": [4, 65, 68, 71, 87, 93, 97, 102, 106, 108, 112, 115, 117, 238], "ignor": [4, 8, 56, 132, 149, 152, 197, 219, 229, 230, 233], "pattern": [4, 186, 229, 230, 233], "origin": [4, 8, 46, 47, 161, 165, 166, 169, 229, 230, 234, 236, 238, 239, 240, 241, 242], "consolid": [4, 8, 229, 230], "weight": [4, 8, 11, 67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 152, 156, 161, 168, 169, 173, 177, 185, 192, 193, 194, 195, 202, 213, 219, 226, 230, 233, 234, 236, 237, 238, 239, 240, 241, 242], "you": [4, 8, 9, 10, 11, 12, 13, 22, 24, 25, 39, 41, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 83, 100, 160, 163, 166, 167, 195, 209, 212, 213, 219, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "can": [4, 5, 8, 9, 10, 11, 12, 13, 16, 19, 21, 23, 24, 25, 26, 29, 40, 41, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 84, 120, 153, 154, 156, 157, 158, 160, 163, 164, 166, 167, 170, 185, 186, 191, 192, 195, 197, 200, 208, 209, 212, 213, 215, 218, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "instead": [4, 8, 11, 31, 32, 33, 42, 43, 56, 63, 139, 149, 151, 160, 169, 181, 233, 238, 239, 240, 241], "405b": [4, 111], "The": [4, 8, 9, 10, 11, 12, 15, 16, 17, 18, 22, 24, 31, 32, 34, 38, 39, 40, 41, 42, 43, 44, 48, 51, 52, 55, 57, 60, 61, 62, 63, 67, 68, 69, 73, 78, 79, 80, 84, 91, 92, 93, 94, 95, 100, 105, 106, 107, 114, 115, 116, 120, 121, 123, 132, 133, 134, 139, 140, 141, 142, 143, 150, 153, 154, 155, 160, 161, 162, 163, 164, 165, 166, 170, 175, 177, 178, 179, 180, 181, 183, 184, 185, 186, 187, 188, 190, 191, 192, 194, 199, 202, 209, 213, 216, 218, 221, 222, 223, 227, 228, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "reus": [4, 228], "llama3_token": [4, 52, 57, 59, 234, 238], "builder": [4, 8, 46, 48, 49, 51, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 79, 80, 81, 82, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 102, 103, 106, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 122, 124, 126, 128, 130, 131, 134, 136, 138, 141, 142, 143, 145, 146, 147, 234, 235, 240, 242], "class": [4, 10, 12, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 39, 40, 41, 42, 43, 44, 45, 52, 57, 60, 61, 62, 63, 73, 77, 83, 84, 89, 90, 100, 104, 119, 120, 123, 127, 128, 129, 132, 137, 139, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 168, 169, 171, 172, 177, 178, 179, 180, 181, 183, 184, 185, 186, 189, 190, 192, 193, 194, 195, 196, 209, 210, 211, 212, 213, 231, 234, 235, 237, 239, 240, 242], "7b": [4, 8, 41, 49, 58, 66, 69, 72, 76, 80, 88, 89, 94, 95, 98, 99, 122, 124, 126, 128, 131, 143, 147, 192, 193, 234, 237, 238, 239, 242], "13b": [4, 8, 64, 67, 70, 86, 92, 96], "codellama": 4, "size": [4, 8, 11, 13, 33, 47, 50, 52, 53, 57, 61, 62, 63, 139, 151, 152, 153, 154, 156, 157, 158, 159, 160, 163, 164, 165, 166, 175, 176, 190, 203, 204, 206, 228, 230, 233, 235, 236, 237, 238, 239, 240, 241], "0": [4, 8, 11, 33, 34, 35, 36, 42, 48, 51, 59, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 84, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 105, 110, 114, 120, 121, 123, 125, 127, 132, 133, 135, 139, 140, 141, 142, 143, 144, 145, 146, 152, 156, 158, 160, 162, 165, 169, 177, 178, 179, 180, 181, 182, 188, 202, 203, 209, 212, 213, 217, 223, 225, 230, 232, 234, 235, 236, 237, 238, 239, 240, 241, 242], "5b": [4, 141, 142, 145, 146, 240], "qwen2": [4, 139, 140, 141, 142, 143, 145, 146, 147, 148, 195, 240], "exampl": [4, 8, 9, 10, 11, 12, 13, 15, 19, 23, 25, 33, 34, 35, 36, 40, 41, 42, 44, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 59, 63, 73, 84, 100, 120, 132, 139, 152, 160, 163, 164, 165, 166, 167, 168, 170, 177, 178, 180, 181, 182, 185, 186, 188, 191, 192, 193, 195, 196, 202, 203, 209, 212, 213, 216, 219, 223, 224, 225, 227, 229, 230, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242], "output": [4, 8, 21, 22, 33, 40, 41, 43, 44, 47, 50, 51, 53, 54, 59, 63, 67, 68, 69, 74, 78, 85, 89, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 128, 133, 134, 140, 143, 144, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 169, 172, 173, 174, 190, 194, 200, 211, 218, 219, 227, 229, 230, 233, 234, 235, 236, 237, 238, 239, 240, 242], "dir": [4, 8, 213, 227, 229, 230, 233, 236, 237, 238, 241], "tmp": [4, 10, 196, 229, 230, 234, 237], "none": [4, 11, 12, 14, 16, 19, 21, 22, 23, 29, 37, 38, 41, 42, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 73, 77, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 129, 132, 137, 139, 148, 149, 151, 152, 154, 156, 157, 158, 159, 160, 164, 166, 170, 172, 173, 174, 175, 176, 179, 185, 188, 190, 192, 193, 194, 195, 197, 199, 202, 207, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 233, 234, 235, 236, 241], "mini": [4, 132, 134, 135, 136, 137, 138], "4k": [4, 135, 136, 137], "microsoft": [4, 136, 137], "ai": [4, 43, 44, 126, 152, 213, 234, 238], "v0": [4, 119], "mistralai": [4, 233], "2b": [4, 75, 79], "googl": [4, 75, 76], "gguf": 4, "vision": [4, 44, 63, 165, 190], "compon": [4, 8, 11, 16, 35, 43, 44, 52, 57, 228, 231, 235, 237, 239, 242], "multimod": [4, 24, 44, 52, 57, 164, 227], "encod": [4, 5, 44, 63, 73, 84, 100, 120, 132, 139, 156, 157, 158, 164, 165, 166, 167, 177, 181, 183, 185, 186, 188, 190, 234], "perform": [5, 8, 42, 59, 84, 149, 160, 163, 170, 181, 189, 228, 229, 230, 234, 236, 238, 241, 242], "direct": [5, 11, 35, 67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134, 177, 227], "text": [5, 24, 25, 26, 39, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 84, 100, 120, 132, 139, 165, 166, 183, 185, 186, 188, 190, 234, 236, 241], "id": [5, 8, 34, 35, 36, 39, 41, 42, 49, 52, 57, 58, 59, 84, 100, 120, 132, 139, 152, 154, 156, 158, 159, 164, 183, 184, 185, 186, 187, 188, 190, 192, 194, 209, 234, 235, 236], "decod": [5, 48, 51, 59, 74, 78, 85, 91, 100, 101, 105, 110, 114, 120, 121, 123, 125, 127, 132, 133, 135, 139, 140, 144, 152, 156, 157, 158, 164, 166, 167, 183, 185, 186, 234], "typic": [5, 10, 19, 23, 29, 42, 43, 44, 45, 56, 137, 167, 177, 181, 235, 240, 241, 242], "byte": [5, 139, 186, 240, 242], "pair": [5, 10, 35, 36, 55, 178, 186, 235], "underli": [5, 120, 185, 240, 242], "helper": 5, "method": [5, 8, 10, 11, 12, 15, 39, 41, 43, 45, 47, 48, 49, 50, 51, 53, 54, 55, 56, 58, 139, 161, 163, 164, 167, 168, 171, 173, 183, 184, 196, 202, 227, 228, 235, 239, 242], "two": [5, 8, 10, 21, 38, 52, 57, 61, 160, 165, 167, 182, 190, 228, 229, 230, 236, 237, 238, 239, 240, 241, 242], "pre": [5, 42, 43, 44, 45, 52, 56, 57, 83, 84, 160, 164, 166, 167, 229, 230, 231, 234, 235], "train": [5, 8, 9, 10, 11, 12, 19, 21, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 83, 84, 100, 120, 132, 139, 150, 152, 154, 156, 158, 159, 161, 162, 163, 164, 165, 166, 167, 177, 181, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 226, 228, 229, 231, 233, 234, 235, 236, 238, 239, 240, 241, 242], "function": [5, 8, 10, 11, 13, 15, 33, 34, 35, 39, 41, 48, 51, 59, 61, 62, 63, 149, 152, 160, 161, 170, 173, 174, 177, 179, 191, 192, 204, 217, 219, 221, 228, 234, 235, 242], "preprocess": [5, 42, 160], "imag": [5, 24, 44, 52, 57, 60, 61, 62, 63, 160, 165, 190, 239], "algorithm": [5, 175, 181, 217], "ppo": [5, 175, 176, 177, 179], "offer": 6, "allow": [6, 40, 166, 173, 212, 230, 233, 240, 241, 242], "seamless": 6, "transit": 6, "between": [6, 8, 43, 48, 139, 156, 157, 158, 164, 176, 178, 179, 181, 192, 195, 209, 235, 236, 238, 239, 241, 242], "interoper": [6, 8, 11, 228, 236, 242], "rest": [6, 234, 240, 242], "ecosystem": [6, 8, 11, 228, 236, 238, 242], "comprehens": [6, 240], "overview": [6, 10, 12, 164, 226, 229, 230, 237, 239, 242], "deep": [6, 8, 9, 10, 11, 12, 166, 167, 228, 231, 237, 238, 240], "dive": [6, 8, 9, 10, 11, 12, 228, 231, 237, 238, 240], "util": [6, 8, 10, 11, 13, 33, 34, 197, 212, 214, 215, 221, 222, 223, 228, 236, 237, 240, 242], "work": [6, 8, 11, 165, 166, 228, 230, 233, 236, 238, 240, 242], "set": [6, 8, 9, 10, 11, 12, 19, 23, 24, 29, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 85, 91, 100, 101, 105, 110, 114, 121, 123, 125, 127, 132, 133, 135, 139, 140, 144, 152, 154, 156, 157, 158, 170, 172, 191, 200, 206, 208, 209, 215, 216, 217, 218, 221, 228, 231, 233, 234, 236, 237, 238, 239, 240, 241], "enabl": [6, 9, 10, 11, 12, 40, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 92, 93, 94, 95, 96, 97, 98, 99, 106, 107, 108, 109, 115, 116, 117, 118, 122, 124, 130, 131, 134, 138, 141, 142, 143, 145, 146, 152, 166, 169, 217, 218, 229, 230, 238, 239, 240, 242], "consumpt": [6, 40, 229, 240], "dure": [6, 8, 41, 42, 47, 48, 50, 51, 53, 54, 151, 152, 154, 156, 158, 159, 160, 161, 164, 165, 181, 190, 201, 229, 230, 234, 236, 238, 239, 240, 241, 242], "variou": [6, 22, 231], "provid": [6, 8, 10, 11, 13, 18, 19, 21, 23, 29, 34, 37, 40, 41, 42, 63, 152, 156, 158, 160, 164, 170, 177, 194, 200, 209, 213, 218, 221, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 240], "debug": [6, 8, 10, 11, 209, 233], "finetun": [6, 8, 10, 11, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143, 164, 226, 228, 230, 237, 238, 240], "job": [6, 12, 217, 237], "walk": [8, 11, 212, 228, 234, 235, 236, 237, 241, 242], "through": [8, 9, 10, 11, 12, 43, 63, 149, 160, 166, 170, 228, 229, 230, 231, 233, 234, 235, 236, 237, 240, 241, 242], "design": [8, 11, 181], "behavior": [8, 208, 234, 235], "associ": [8, 10, 11, 59, 63, 74, 85, 101, 110, 125, 144, 209, 236, 239], "what": [8, 9, 10, 12, 43, 44, 48, 51, 83, 119, 160, 226, 231, 234, 235, 236, 237, 238, 240], "cover": [8, 9, 10, 11, 12, 234, 236, 242], "how": [8, 9, 10, 11, 12, 100, 160, 191, 209, 215, 226, 229, 230, 233, 234, 235, 236, 237, 238, 240, 241, 242], "we": [8, 9, 10, 11, 12, 34, 41, 42, 43, 44, 48, 49, 58, 59, 84, 120, 139, 151, 152, 154, 156, 158, 160, 163, 164, 169, 177, 181, 192, 193, 194, 199, 202, 208, 214, 219, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "them": [8, 10, 40, 41, 73, 84, 120, 132, 149, 160, 161, 166, 188, 230, 233, 234, 235, 236, 239, 240, 241, 242], "scenario": [8, 40, 73, 84, 100, 120, 132, 139], "full": [8, 10, 11, 20, 27, 30, 43, 58, 70, 71, 72, 73, 81, 82, 84, 96, 97, 98, 99, 108, 109, 117, 118, 120, 130, 131, 132, 138, 164, 173, 174, 188, 197, 228, 233, 235, 236, 238, 239, 240, 241], "compos": [8, 160], "which": [8, 10, 11, 34, 40, 41, 42, 45, 47, 48, 50, 51, 53, 54, 56, 67, 68, 69, 77, 78, 79, 80, 90, 91, 92, 93, 94, 95, 100, 104, 105, 106, 107, 114, 115, 116, 119, 120, 121, 122, 123, 124, 129, 133, 134, 137, 140, 141, 142, 143, 152, 154, 156, 158, 159, 160, 162, 164, 166, 173, 174, 185, 192, 193, 194, 196, 199, 210, 213, 215, 219, 228, 229, 230, 231, 233, 234, 235, 236, 237, 239, 240, 241, 242], "plug": [8, 240], "recip": [8, 9, 10, 12, 13, 14, 15, 149, 156, 158, 173, 192, 193, 194, 228, 229, 230, 234, 235, 236, 238, 240, 242], "evalu": [8, 11, 226, 228, 230, 231, 237, 239, 242], "gener": [8, 11, 34, 39, 41, 42, 49, 56, 84, 100, 120, 170, 175, 209, 216, 217, 218, 224, 226, 230, 234, 235, 239, 240, 241, 242], "each": [8, 11, 17, 22, 25, 26, 34, 35, 40, 42, 43, 44, 60, 61, 62, 63, 67, 68, 69, 73, 78, 79, 80, 84, 91, 92, 93, 94, 95, 105, 106, 107, 114, 115, 116, 120, 121, 122, 123, 124, 132, 133, 134, 140, 141, 142, 143, 152, 154, 156, 158, 159, 160, 163, 164, 166, 173, 174, 175, 176, 177, 178, 180, 181, 188, 190, 203, 217, 218, 228, 230, 231, 233, 235, 236, 237, 239, 240, 241], "make": [8, 9, 10, 11, 12, 159, 160, 228, 233, 234, 236, 237, 238, 239, 240, 241, 242], "easi": [8, 11, 228, 235, 239, 240], "understand": [8, 10, 11, 166, 226, 228, 229, 234, 235, 239, 240, 242], "extend": [8, 11, 228], "befor": [8, 25, 38, 41, 42, 60, 61, 63, 74, 78, 84, 152, 156, 157, 158, 159, 160, 163, 164, 166, 169, 186, 192, 209, 230, 233, 236, 240, 241], "let": [8, 10, 12, 233, 234, 235, 236, 237, 238, 239, 240, 242], "s": [8, 10, 11, 12, 13, 15, 17, 18, 23, 29, 31, 32, 38, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 67, 68, 69, 73, 83, 84, 91, 92, 93, 94, 95, 100, 105, 106, 107, 114, 115, 116, 119, 120, 121, 122, 123, 124, 132, 133, 134, 137, 139, 140, 143, 144, 151, 152, 154, 156, 158, 159, 160, 161, 164, 167, 168, 171, 173, 174, 177, 178, 180, 181, 182, 186, 191, 192, 193, 196, 200, 201, 203, 208, 209, 212, 215, 216, 219, 221, 228, 233, 234, 235, 237, 239, 240, 241, 242], "defin": [8, 10, 11, 25, 39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 149, 152, 156, 157, 158, 164, 168, 169, 171, 176, 235, 237, 239], "concept": [8, 231, 236, 237, 240], "In": [8, 10, 11, 39, 61, 62, 63, 154, 160, 169, 178, 191, 208, 212, 213, 230, 234, 236, 238, 239, 240, 241, 242], "ll": [8, 10, 11, 59, 202, 228, 230, 234, 235, 236, 237, 238, 240, 241, 242], "talk": 8, "about": [8, 11, 52, 57, 160, 177, 181, 209, 213, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 241, 242], "take": [8, 10, 11, 13, 35, 43, 44, 52, 57, 149, 151, 160, 161, 166, 192, 194, 221, 230, 234, 235, 236, 237, 238, 239, 242], "close": [8, 11, 209, 210, 211, 212, 213, 239], "look": [8, 10, 11, 198, 212, 227, 234, 235, 236, 237, 238, 239, 241], "veri": [8, 40, 156, 158, 164, 233, 236, 240], "simpli": [8, 10, 23, 42, 44, 177, 178, 233, 234, 235, 236, 238, 240, 242], "dictat": 8, "state_dict": [8, 161, 165, 166, 173, 192, 193, 194, 195, 196, 219, 239, 242], "store": [8, 43, 44, 209, 210, 213, 239, 240, 242], "file": [8, 9, 10, 11, 12, 13, 14, 15, 39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 84, 100, 104, 120, 132, 137, 139, 148, 185, 186, 187, 192, 193, 194, 210, 213, 218, 225, 228, 230, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "disk": [8, 45, 210], "string": [8, 21, 24, 25, 48, 49, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 168, 183, 185, 186, 188, 197, 199, 202, 209, 221, 233, 235, 240], "kei": [8, 10, 12, 19, 21, 23, 29, 31, 34, 35, 39, 41, 43, 44, 47, 48, 50, 51, 52, 53, 54, 55, 57, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 156, 157, 158, 159, 164, 166, 172, 173, 174, 181, 192, 194, 196, 209, 218, 233, 236, 237, 239, 240, 242], "identifi": [8, 209], "state": [8, 11, 160, 161, 171, 172, 173, 174, 175, 177, 192, 193, 194, 196, 198, 219, 229, 236, 238, 239, 240, 242], "dict": [8, 10, 11, 12, 13, 19, 21, 22, 23, 24, 25, 29, 31, 32, 34, 35, 36, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 77, 90, 100, 104, 129, 132, 137, 139, 148, 157, 159, 161, 164, 166, 171, 172, 173, 174, 183, 184, 186, 187, 189, 192, 193, 194, 196, 198, 201, 205, 207, 209, 214, 219, 235], "If": [8, 10, 16, 19, 21, 22, 23, 24, 29, 31, 34, 37, 38, 39, 41, 47, 48, 50, 51, 52, 53, 54, 56, 57, 59, 63, 77, 85, 90, 91, 100, 101, 104, 105, 110, 114, 129, 132, 137, 139, 140, 144, 148, 152, 154, 156, 158, 159, 160, 161, 163, 164, 169, 174, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 205, 209, 212, 213, 217, 218, 220, 221, 227, 233, 234, 235, 236, 237, 238, 239, 240, 241], "don": [8, 10, 11, 213, 217, 233, 234, 235, 236, 237, 240, 242], "t": [8, 10, 11, 33, 163, 166, 178, 199, 213, 217, 231, 233, 234, 235, 236, 237, 240, 242], "match": [8, 41, 132, 174, 209, 219, 227, 233, 235, 236, 238, 239], "up": [8, 9, 11, 12, 41, 42, 49, 58, 139, 186, 190, 198, 209, 218, 229, 230, 231, 233, 234, 235, 237, 238, 239, 240, 242], "exactli": [8, 174, 241], "those": [8, 195, 236, 238, 239], "definit": [8, 239], "either": [8, 34, 43, 44, 59, 174, 192, 209, 215, 227, 233, 239, 241, 242], "run": [8, 9, 10, 12, 15, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 139, 140, 144, 149, 151, 156, 158, 161, 163, 192, 193, 194, 196, 197, 198, 206, 209, 212, 213, 214, 227, 228, 229, 230, 234, 235, 237, 238, 239, 240, 241, 242], "explicit": 8, "error": [8, 10, 20, 38, 139, 151, 192, 217, 233], "load": [8, 11, 39, 40, 41, 42, 43, 44, 45, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 164, 173, 192, 193, 194, 196, 212, 219, 234, 235, 236, 238, 239], "rais": [8, 13, 16, 19, 21, 23, 29, 31, 34, 38, 41, 47, 48, 50, 51, 52, 53, 54, 56, 57, 63, 132, 139, 140, 151, 152, 156, 157, 158, 160, 173, 174, 188, 192, 193, 194, 196, 199, 201, 205, 209, 213, 217, 219, 220], "an": [8, 9, 10, 11, 12, 13, 38, 40, 41, 45, 48, 50, 51, 52, 53, 56, 57, 58, 60, 61, 62, 91, 105, 114, 121, 123, 127, 133, 139, 140, 141, 142, 145, 146, 151, 152, 156, 158, 160, 164, 165, 166, 167, 168, 170, 171, 172, 177, 190, 191, 192, 193, 194, 196, 197, 200, 209, 213, 218, 221, 228, 230, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "except": [8, 24, 119, 188, 235], "wors": [8, 240], "silent": [8, 149], "succe": 8, "infer": [8, 39, 44, 74, 83, 84, 125, 151, 152, 154, 156, 158, 159, 164, 190, 221, 226, 230, 231, 234, 236, 237, 238, 241, 242], "expect": [8, 10, 13, 19, 21, 22, 23, 29, 39, 41, 43, 44, 47, 48, 50, 51, 52, 53, 54, 55, 57, 73, 84, 100, 120, 132, 139, 154, 164, 174, 196, 209, 213, 220, 234, 235, 239, 241], "addit": [8, 10, 11, 13, 39, 41, 43, 44, 45, 47, 48, 49, 51, 52, 56, 57, 58, 84, 119, 165, 166, 173, 177, 191, 192, 193, 194, 199, 200, 205, 208, 209, 210, 212, 213, 215, 228, 234, 237, 239, 240], "line": [8, 9, 11, 231, 233, 235, 237, 238], "need": [8, 9, 10, 11, 12, 22, 25, 39, 42, 44, 149, 152, 156, 158, 160, 164, 165, 181, 208, 209, 212, 213, 214, 227, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 242], "shape": [8, 34, 59, 60, 61, 62, 63, 151, 152, 154, 156, 157, 158, 159, 160, 163, 164, 165, 166, 169, 175, 176, 177, 178, 179, 180, 181, 182, 190, 203, 218, 219], "valu": [8, 10, 19, 21, 23, 29, 32, 33, 34, 35, 47, 48, 50, 51, 53, 54, 55, 59, 64, 65, 66, 74, 75, 76, 78, 85, 86, 87, 88, 89, 91, 100, 101, 102, 103, 105, 110, 111, 112, 113, 114, 121, 123, 125, 126, 127, 128, 133, 135, 140, 144, 145, 146, 147, 151, 152, 153, 156, 157, 158, 159, 162, 164, 166, 173, 175, 176, 179, 182, 192, 195, 196, 203, 209, 210, 211, 212, 213, 217, 230, 233, 235, 237, 238, 239, 240, 241], "popular": [8, 164, 228, 235, 236], "llama2": [8, 10, 11, 13, 39, 41, 49, 58, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 83, 84, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 120, 132, 139, 149, 158, 159, 195, 226, 228, 229, 233, 237, 238, 240, 241], "offici": [8, 83, 139, 234, 237, 238], "implement": [8, 11, 39, 41, 43, 45, 47, 48, 49, 50, 51, 53, 54, 55, 56, 58, 73, 84, 120, 132, 149, 153, 154, 155, 160, 162, 168, 169, 177, 178, 179, 180, 181, 183, 184, 192, 202, 212, 228, 230, 235, 239, 241, 242], "when": [8, 10, 11, 15, 40, 42, 43, 44, 45, 56, 59, 84, 139, 152, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 173, 176, 200, 212, 214, 219, 229, 233, 236, 238, 239, 240, 241, 242], "llama": [8, 39, 83, 84, 100, 153, 154, 192, 193, 229, 230, 233, 234, 236, 237, 238, 239], "websit": 8, "get": [8, 9, 10, 11, 12, 39, 73, 84, 100, 120, 132, 139, 199, 201, 204, 209, 222, 227, 228, 229, 230, 234, 235, 236, 237, 239, 240, 241], "singl": [8, 10, 13, 17, 19, 21, 22, 23, 29, 31, 32, 34, 40, 42, 43, 44, 45, 48, 56, 61, 62, 63, 77, 89, 90, 100, 104, 128, 129, 137, 139, 152, 160, 164, 173, 192, 193, 194, 195, 196, 198, 231, 233, 234, 235, 236, 237, 238, 239, 240, 242], "pth": [8, 229, 230, 236], "inspect": [8, 236, 239, 242], "content": [8, 17, 19, 23, 24, 25, 26, 29, 31, 32, 39, 43, 44, 48, 73, 84, 100, 120, 132, 139, 188, 234, 235], "easili": [8, 10, 228, 235, 239, 241, 242], "torch": [8, 10, 33, 34, 35, 36, 59, 60, 61, 62, 63, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 175, 176, 177, 178, 179, 180, 181, 182, 194, 196, 198, 199, 201, 202, 203, 205, 206, 212, 214, 215, 216, 217, 218, 219, 220, 221, 223, 227, 230, 236, 237, 238, 239, 240, 242], "import": [8, 10, 13, 48, 51, 52, 56, 57, 160, 177, 209, 212, 213, 234, 235, 236, 237, 238, 239, 241, 242], "00": [8, 48, 51, 225, 229, 230, 232, 237], "mmap": [8, 236], "true": [8, 10, 24, 31, 32, 33, 40, 41, 42, 45, 46, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 63, 70, 71, 72, 73, 74, 78, 81, 82, 84, 96, 97, 98, 99, 100, 108, 109, 117, 118, 120, 130, 131, 132, 138, 139, 152, 156, 157, 158, 159, 161, 164, 166, 170, 175, 179, 182, 185, 186, 188, 190, 191, 192, 193, 194, 200, 201, 203, 205, 206, 209, 212, 218, 223, 229, 233, 234, 235, 236, 238, 239, 240, 241, 242], "weights_onli": [8, 194], "map_loc": [8, 236], "cpu": [8, 11, 161, 199, 218, 227, 233, 236, 242], "tensor": [8, 33, 34, 35, 36, 59, 60, 61, 62, 63, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 169, 175, 176, 177, 178, 179, 180, 181, 182, 192, 203, 209, 210, 211, 212, 213, 216, 219, 239, 240, 242], "item": 8, "print": [8, 12, 40, 47, 50, 52, 53, 54, 57, 59, 73, 84, 100, 120, 132, 139, 160, 185, 186, 188, 223, 234, 235, 237, 239, 241, 242], "f": [8, 12, 47, 50, 52, 53, 57, 234, 236, 239, 242], "tok_embed": [8, 156, 158, 164, 165], "32000": [8, 13, 239], "4096": [8, 13, 41, 49, 58, 152, 154, 235, 239, 241], "len": [8, 40, 47, 50, 52, 53, 57, 156, 158, 160, 164], "292": 8, "contain": [8, 19, 21, 24, 31, 34, 35, 36, 42, 43, 44, 45, 48, 56, 73, 84, 100, 104, 120, 132, 137, 139, 148, 151, 152, 154, 156, 158, 159, 164, 168, 171, 172, 173, 175, 182, 186, 188, 192, 193, 194, 196, 198, 201, 207, 212, 218, 219, 234, 236, 238, 239], "input": [8, 21, 22, 34, 35, 36, 39, 41, 42, 43, 44, 47, 49, 50, 51, 52, 53, 54, 57, 58, 60, 61, 62, 63, 77, 90, 100, 104, 120, 129, 132, 137, 140, 144, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 164, 165, 166, 169, 185, 186, 190, 192, 194, 217, 220, 234, 235, 239, 242], "embed": [8, 60, 61, 62, 63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 153, 154, 156, 157, 158, 160, 164, 165, 166, 167, 200, 234, 238, 240, 241], "tabl": [8, 165, 234, 236, 238, 240, 242], "call": [8, 13, 24, 25, 44, 52, 57, 119, 149, 152, 156, 158, 160, 161, 173, 209, 210, 211, 212, 213, 214, 218, 219, 234, 235, 239, 242], "layer": [8, 11, 63, 67, 68, 69, 70, 71, 72, 74, 78, 79, 80, 81, 82, 85, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 127, 128, 130, 131, 133, 134, 135, 138, 140, 141, 142, 143, 144, 152, 155, 156, 157, 158, 159, 160, 164, 166, 167, 169, 173, 174, 191, 197, 200, 228, 229, 238, 239, 240, 241, 242], "have": [8, 10, 13, 21, 24, 43, 48, 61, 62, 63, 151, 152, 160, 163, 168, 174, 181, 190, 194, 196, 200, 212, 220, 227, 234, 235, 236, 237, 238, 239, 240, 241, 242], "dim": [8, 149, 152, 153, 154, 156, 158, 163, 164], "most": [8, 10, 24, 25, 234, 237, 239, 240, 242], "within": [8, 10, 13, 39, 42, 59, 60, 78, 91, 105, 114, 121, 123, 133, 140, 149, 160, 212, 217, 218, 233, 235, 239, 242], "hub": [8, 43, 44, 233, 235, 237], "default": [8, 10, 18, 19, 21, 23, 24, 29, 31, 32, 33, 35, 36, 37, 39, 41, 42, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 154, 156, 157, 158, 159, 161, 162, 164, 169, 173, 175, 176, 177, 181, 185, 186, 188, 190, 192, 193, 194, 199, 204, 208, 209, 210, 213, 216, 217, 218, 227, 230, 233, 234, 235, 236, 238, 239, 240, 241, 242], "everi": [8, 11, 50, 53, 54, 60, 61, 62, 149, 160, 212, 218, 227, 233, 240, 242], "2": [8, 12, 33, 34, 35, 36, 38, 42, 54, 60, 61, 73, 84, 100, 119, 120, 132, 139, 152, 160, 178, 179, 181, 182, 185, 186, 188, 192, 193, 202, 203, 216, 217, 218, 223, 230, 234, 236, 237, 238, 239, 240, 241], "repo": [8, 52, 192, 193, 195, 233, 236], "first": [8, 10, 13, 38, 42, 52, 63, 151, 156, 158, 160, 164, 182, 192, 226, 228, 229, 234, 235, 236, 238, 239, 241, 242], "big": 8, "split": [8, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 139, 186, 234, 235, 236, 241], "across": [8, 11, 40, 192, 212, 217, 236, 238, 241], "bin": [8, 233, 236], "correctli": [8, 11, 16, 173, 192, 227, 231, 234, 237, 242], "piec": 8, "one": [8, 11, 19, 21, 23, 29, 34, 38, 48, 57, 73, 84, 120, 132, 139, 149, 160, 163, 188, 194, 209, 234, 235, 236, 237, 238, 240, 242], "pytorch_model": [8, 236], "00001": [8, 233], "00002": [8, 233], "embed_token": 8, "241": 8, "Not": [8, 231], "onli": [8, 9, 12, 24, 42, 43, 44, 49, 59, 63, 78, 91, 105, 114, 119, 121, 123, 133, 140, 152, 156, 158, 160, 163, 169, 171, 173, 185, 192, 193, 194, 196, 199, 200, 201, 202, 208, 233, 235, 236, 237, 239, 240, 241, 242], "doe": [8, 31, 39, 42, 56, 74, 84, 119, 125, 136, 152, 156, 158, 159, 164, 168, 188, 192, 194, 196, 219, 233, 234, 236, 241], "fewer": [8, 152], "sinc": [8, 10, 13, 43, 44, 149, 163, 192, 194, 229, 234, 236, 238, 240, 241], "mismatch": 8, "name": [8, 9, 10, 12, 14, 19, 21, 22, 23, 29, 41, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 168, 172, 174, 186, 192, 193, 194, 195, 196, 198, 209, 210, 211, 212, 213, 219, 220, 221, 233, 234, 236, 238, 240, 241], "caus": [8, 120, 185], "try": [8, 10, 234, 236, 237, 238, 242], "same": [8, 10, 25, 33, 60, 61, 67, 68, 69, 73, 79, 80, 84, 92, 93, 94, 95, 106, 107, 115, 116, 120, 132, 134, 141, 142, 143, 151, 157, 159, 160, 164, 166, 179, 181, 182, 188, 196, 200, 213, 219, 230, 233, 234, 236, 238, 239, 240, 241, 242], "As": [8, 10, 11, 12, 169, 228, 236, 240, 242], "re": [8, 10, 166, 181, 194, 228, 229, 230, 231, 234, 236, 237, 239, 240], "care": [8, 149, 192, 194, 236, 238, 239], "end": [8, 11, 24, 45, 56, 100, 120, 139, 186, 188, 226, 228, 229, 234, 238, 239, 241], "number": [8, 11, 39, 41, 42, 49, 58, 59, 60, 61, 63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 156, 158, 160, 162, 190, 192, 193, 194, 197, 204, 217, 218, 233, 237, 239, 240], "just": [8, 228, 229, 230, 233, 234, 235, 237, 238, 239, 240, 241], "save": [8, 11, 12, 156, 158, 161, 163, 192, 193, 194, 196, 200, 208, 213, 226, 230, 233, 234, 235, 236, 238, 239, 240, 241], "less": [8, 236, 237, 238, 240, 242], "prone": 8, "manag": [8, 40, 170, 209, 216, 234], "invari": 8, "accept": [8, 10, 191, 235, 237, 240, 242], "multipl": [8, 10, 11, 19, 23, 24, 29, 35, 39, 40, 44, 152, 156, 158, 159, 160, 164, 169, 209, 210, 211, 212, 213, 218, 231, 237, 238], "sourc": [8, 10, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 234, 235, 236, 241], "worri": [8, 231, 234, 237], "explicitli": [8, 168, 228, 239], "convert": [8, 19, 21, 23, 29, 31, 32, 36, 39, 43, 44, 48, 52, 57, 139, 192, 234, 236, 241, 242], "time": [8, 48, 51, 73, 74, 84, 120, 125, 132, 163, 175, 188, 210, 212, 218, 230, 233, 234, 235, 236, 238, 242], "produc": [8, 196, 230, 241, 242], "back": [8, 38, 170, 192, 231, 235, 239, 242], "form": [8, 10, 11, 38, 43, 44, 233], "One": [8, 241], "advantag": [8, 175, 179, 230, 239], "being": [8, 44, 192, 193, 194, 198, 221, 241, 242], "should": [8, 10, 11, 17, 19, 21, 22, 23, 24, 25, 29, 31, 32, 34, 42, 47, 50, 51, 52, 53, 54, 55, 56, 57, 67, 68, 69, 78, 79, 80, 83, 85, 91, 92, 93, 94, 95, 100, 101, 105, 106, 107, 110, 114, 115, 116, 119, 121, 122, 123, 124, 125, 127, 133, 134, 135, 139, 140, 141, 142, 143, 144, 149, 152, 156, 158, 160, 168, 173, 174, 175, 179, 191, 207, 209, 210, 211, 212, 213, 227, 228, 235, 236, 237, 238, 239, 240, 241, 242], "abl": [8, 11, 236, 237, 241], "post": [8, 160, 214, 218, 230, 236, 238, 241, 242], "tool": [8, 24, 25, 44, 119, 209, 235, 236, 237], "quantiz": [8, 67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 169, 194, 202, 226, 227, 229, 231, 237, 242], "eval": [8, 226, 228, 241], "without": [8, 10, 12, 152, 173, 227, 228, 230, 234, 236, 239, 240, 241], "code": [8, 11, 64, 65, 66, 67, 68, 69, 70, 71, 72, 156, 158, 209, 224, 228, 235, 237], "chang": [8, 9, 10, 12, 19, 21, 51, 55, 57, 194, 227, 233, 236, 237, 238, 239, 240, 241, 242], "OR": [8, 31], "script": [8, 12, 231, 233, 235, 236, 237, 238], "wai": [8, 10, 39, 43, 44, 173, 233, 234, 235, 236, 237, 238], "surround": [8, 11, 228], "load_checkpoint": [8, 11, 192, 193, 194, 195], "save_checkpoint": [8, 11, 12, 192, 193, 194], "map": [8, 19, 21, 22, 23, 25, 29, 31, 32, 34, 39, 40, 41, 42, 47, 50, 51, 52, 53, 54, 55, 57, 77, 90, 100, 104, 129, 132, 137, 148, 172, 186, 187, 192, 196, 198, 209, 210, 211, 212, 213, 214, 218, 234, 235, 236, 239], "appli": [8, 11, 34, 39, 41, 43, 44, 47, 52, 57, 67, 68, 69, 70, 71, 72, 74, 78, 79, 80, 81, 82, 84, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 130, 131, 133, 134, 138, 140, 141, 142, 143, 144, 152, 153, 154, 156, 157, 158, 159, 164, 173, 174, 215, 228, 229, 240, 242], "permut": 8, "certain": [8, 10, 218, 231, 234], "ensur": [8, 10, 16, 38, 43, 44, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 192, 194, 199, 228, 235, 237], "behav": 8, "further": [8, 160, 181, 230, 233, 235, 239, 240, 241, 242], "illustr": [8, 52, 57, 238], "whilst": [8, 229, 240], "other": [8, 11, 13, 21, 25, 40, 178, 194, 200, 218, 229, 230, 235, 237, 238, 239, 240, 241], "phi3": [8, 132, 133, 134, 136, 137, 138, 195, 233], "own": [8, 25, 208, 217, 233, 234, 235, 236, 238, 239], "found": [8, 9, 10, 12, 153, 154, 192, 193, 194, 230, 233, 239, 242], "folder": [8, 234], "three": [8, 11, 73, 84, 100, 120, 132, 139, 177, 178, 180, 181, 231, 237], "read": [8, 192, 193, 194, 228, 240], "write": [8, 11, 192, 193, 194, 210, 234, 235, 237], "compat": [8, 192, 194, 240, 241], "transform": [8, 11, 19, 21, 23, 39, 41, 43, 44, 47, 48, 50, 52, 53, 54, 57, 63, 67, 68, 69, 74, 78, 79, 80, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 127, 133, 134, 135, 139, 140, 141, 142, 143, 144, 156, 157, 158, 159, 160, 162, 166, 190, 215, 239, 240, 241], "framework": [8, 11, 228], "mention": [8, 236, 242], "assum": [8, 22, 33, 34, 41, 52, 77, 90, 104, 129, 137, 148, 152, 154, 156, 158, 159, 162, 164, 165, 171, 186, 196, 198, 199, 234, 236, 239], "checkpoint_dir": [8, 10, 192, 193, 194, 236, 238, 241], "necessari": [8, 43, 44, 209, 210, 211, 212, 213, 234, 239], "easiest": [8, 236, 237], "sure": [8, 10, 234, 236, 237, 238, 239, 240, 241, 242], "everyth": [8, 11, 228, 231, 237], "follow": [8, 11, 23, 24, 25, 29, 31, 32, 39, 42, 43, 44, 51, 139, 152, 157, 162, 179, 190, 194, 195, 196, 206, 213, 218, 226, 227, 230, 233, 235, 236, 237, 238, 239, 240, 241, 242], "flow": [8, 39, 41, 42, 241, 242], "By": [8, 139, 230, 233, 239, 240, 241, 242], "safetensor": [8, 192, 233], "output_dir": [8, 10, 192, 193, 194, 218, 236, 238, 239, 241, 242], "here": [8, 9, 10, 12, 18, 50, 52, 57, 153, 154, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "argument": [8, 10, 13, 20, 22, 27, 30, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 152, 191, 200, 205, 209, 210, 212, 213, 215, 233, 234, 235, 239, 241], "snippet": 8, "explain": [8, 240], "setup": [8, 10, 11, 152, 156, 157, 158, 159, 164, 166, 197, 218, 233, 235, 236, 239, 242], "_component_": [8, 9, 10, 12, 13, 40, 48, 51, 56, 218, 230, 234, 235, 236, 238, 239, 240, 241], "fullmodelhfcheckpoint": [8, 236], "directori": [8, 10, 52, 192, 193, 194, 210, 212, 213, 218, 233, 234, 235, 236, 237, 238], "sort": [8, 192, 194], "so": [8, 10, 42, 52, 160, 192, 227, 228, 234, 236, 237, 238, 239, 240, 241, 242], "order": [8, 9, 11, 192, 194, 212, 213, 237], "matter": [8, 192, 194, 233, 239], "checkpoint_fil": [8, 10, 12, 192, 193, 194, 236, 238, 239, 241, 242], "restart": [8, 233], "previou": [8, 42, 192, 193, 194], "more": [8, 10, 11, 25, 39, 41, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 84, 139, 151, 154, 160, 163, 167, 173, 191, 194, 209, 213, 215, 217, 228, 229, 230, 231, 233, 235, 236, 237, 238, 239, 240, 241, 242], "next": [8, 42, 56, 59, 63, 160, 190, 229, 238, 242], "section": [8, 11, 201, 226, 236, 238, 240, 242], "recipe_checkpoint": [8, 192, 193, 194, 241], "null": [8, 10, 241], "usual": [8, 154, 182, 192, 203, 213, 233, 236, 239, 240], "model_typ": [8, 192, 193, 194, 236, 238, 241], "resume_from_checkpoint": [8, 192, 193, 194], "fals": [8, 10, 19, 21, 23, 24, 29, 31, 32, 33, 39, 40, 41, 42, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 63, 67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144, 152, 156, 158, 159, 164, 165, 166, 169, 170, 173, 182, 185, 192, 193, 194, 206, 218, 219, 233, 234, 235, 236, 238, 239, 241, 242], "requir": [8, 10, 34, 35, 40, 43, 44, 45, 52, 56, 57, 73, 84, 100, 120, 132, 139, 165, 192, 194, 196, 202, 205, 206, 208, 209, 212, 213, 217, 218, 227, 230, 233, 234, 235, 237, 240, 241, 242], "param": [8, 11, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143, 167, 169, 171, 172, 174, 192, 239, 241, 242], "directli": [8, 10, 11, 13, 43, 44, 48, 51, 52, 56, 177, 191, 192, 233, 236, 237, 238, 239, 240, 241, 242], "out": [8, 10, 11, 41, 47, 48, 50, 51, 53, 54, 190, 192, 193, 203, 226, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 242], "case": [8, 11, 12, 24, 25, 61, 62, 63, 160, 192, 196, 199, 202, 208, 210, 215, 228, 233, 234, 235, 236, 238, 239, 240, 242], "discrep": [8, 192], "along": [8, 239], "github": [8, 13, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 139, 141, 142, 143, 152, 153, 154, 162, 163, 173, 177, 178, 179, 180, 181, 227, 235, 236, 238], "repositori": [8, 39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 83, 229, 230, 236, 237], "fullmodelmetacheckpoint": [8, 238, 241], "current": [8, 42, 74, 78, 91, 105, 114, 121, 123, 125, 133, 136, 140, 151, 152, 154, 156, 158, 159, 164, 179, 193, 194, 200, 202, 204, 210, 212, 214, 217, 230, 231, 235, 237, 238, 240, 241], "test": [8, 10, 11, 228, 229, 230, 234, 240], "complet": [8, 11, 42, 49, 56, 137, 178, 234, 235, 236, 237, 238, 240], "written": [8, 10, 11, 192, 193, 209, 210, 211, 212, 213, 228], "begin": [8, 42, 56, 84, 120, 139, 160, 186, 234, 238, 242], "partit": [8, 192, 242], "ha": [8, 51, 84, 120, 156, 157, 158, 160, 163, 164, 168, 170, 171, 174, 182, 194, 196, 219, 220, 234, 235, 236, 237, 238, 239, 240, 242], "standard": [8, 20, 31, 43, 44, 48, 73, 84, 85, 91, 100, 101, 105, 110, 114, 120, 121, 123, 125, 127, 132, 133, 135, 139, 140, 144, 152, 211, 228, 234, 236, 238], "key_1": [8, 194], "weight_1": 8, "key_2": 8, "weight_2": 8, "mid": 8, "chekpoint": 8, "middl": [8, 166, 236, 240], "inform": [8, 139, 209, 213, 215, 228, 231, 233, 236, 237], "subsequ": [8, 11, 160, 190], "recipe_st": [8, 192, 193, 194], "pt": [8, 12, 192, 193, 194, 236, 238, 241], "epoch": [8, 11, 12, 162, 192, 193, 194, 233, 234, 236, 237, 238, 241], "optim": [8, 10, 11, 35, 40, 43, 74, 84, 125, 136, 162, 177, 179, 180, 181, 194, 196, 198, 201, 214, 218, 229, 230, 231, 234, 236, 237, 238, 239, 242], "etc": [8, 11, 192, 201, 237], "prevent": [8, 42, 177, 233, 240], "flood": 8, "overwritten": 8, "note": [8, 10, 22, 78, 139, 156, 158, 164, 168, 196, 214, 217, 219, 229, 230, 234, 235, 236, 239, 240, 241, 242], "updat": [8, 10, 11, 25, 151, 152, 177, 179, 189, 196, 218, 227, 234, 236, 237, 238, 239, 240, 241, 242], "hf_model_0001_0": [8, 236], "hf_model_0002_0": [8, 236], "both": [8, 40, 166, 167, 174, 233, 236, 239, 241, 242], "adapt": [8, 67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134, 164, 166, 168, 169, 170, 171, 172, 192, 193, 194, 208, 229, 234, 236, 239, 242], "merg": [8, 13, 14, 139, 148, 192, 236, 238, 242], "would": [8, 10, 12, 25, 42, 156, 158, 160, 164, 178, 227, 234, 235, 236, 239, 240, 242], "addition": [8, 181, 185, 186, 217, 235, 239], "option": [8, 10, 11, 19, 21, 22, 23, 29, 37, 39, 41, 42, 43, 44, 45, 46, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 62, 63, 67, 68, 69, 73, 77, 78, 79, 80, 84, 85, 90, 91, 92, 93, 94, 95, 100, 101, 104, 105, 106, 107, 110, 114, 115, 116, 120, 121, 122, 123, 124, 129, 132, 133, 134, 137, 139, 140, 141, 142, 143, 144, 148, 149, 152, 154, 156, 157, 158, 159, 160, 161, 164, 173, 174, 175, 176, 179, 183, 185, 188, 190, 192, 193, 194, 197, 199, 202, 209, 210, 213, 217, 218, 221, 222, 227, 228, 233, 235, 236], "save_adapter_weights_onli": 8, "choos": [8, 48, 239], "primari": [8, 10, 11, 43, 44, 231, 237], "want": [8, 10, 11, 12, 13, 39, 43, 44, 59, 167, 227, 231, 233, 234, 235, 236, 237, 238, 239, 240], "resum": [8, 11, 162, 192, 193, 194, 242], "initi": [8, 11, 15, 40, 42, 64, 65, 66, 75, 76, 86, 87, 88, 89, 102, 103, 111, 112, 113, 126, 128, 145, 146, 147, 177, 196, 205, 206, 219, 230, 237, 239, 242], "frozen": [8, 165, 177, 239, 240, 242], "base": [8, 13, 24, 25, 41, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 84, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 130, 131, 132, 133, 134, 135, 138, 139, 140, 141, 142, 143, 144, 154, 162, 169, 170, 172, 173, 174, 176, 177, 178, 180, 181, 192, 200, 208, 210, 219, 221, 226, 234, 236, 237, 238, 239, 240, 242], "well": [8, 10, 11, 228, 233, 235, 236, 238, 240, 242], "learnt": [8, 234, 236], "someth": [8, 11, 12, 234, 236, 241], "NOT": [8, 74, 125], "refer": [8, 10, 11, 153, 154, 160, 163, 170, 176, 177, 178, 179, 180, 181, 209, 228, 239, 240, 241], "adapter_checkpoint": [8, 192, 193, 194], "adapter_0": [8, 236], "now": [8, 196, 198, 230, 234, 235, 236, 237, 238, 239, 241, 242], "knowledg": 8, "creat": [8, 10, 13, 23, 25, 42, 44, 48, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 79, 80, 81, 82, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 98, 99, 102, 103, 106, 107, 108, 109, 111, 112, 113, 115, 116, 117, 118, 122, 124, 126, 128, 130, 131, 134, 136, 138, 141, 142, 143, 145, 146, 147, 151, 160, 162, 191, 192, 193, 194, 198, 209, 210, 212, 233, 234, 235, 236, 242], "simpl": [8, 11, 160, 181, 226, 235, 237, 239, 241, 242], "forward": [8, 11, 60, 61, 62, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 169, 177, 178, 179, 180, 181, 201, 218, 238, 239, 240, 242], "modeltyp": [8, 192, 193, 194], "llama2_13b": [8, 92], "right": [8, 34, 192, 236, 238, 239], "pytorch_fil": 8, "00003": 8, "torchtune_sd": 8, "load_state_dict": [8, 164, 165, 166, 173, 196, 219, 239], "successfulli": [8, 233, 237], "vocab": [8, 13, 139, 148, 156, 158, 164, 165, 238], "70": [8, 102], "x": [8, 33, 59, 60, 61, 62, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 164, 165, 166, 169, 203, 216, 239, 241, 242], "randint": 8, "1": [8, 11, 33, 34, 35, 36, 42, 54, 59, 60, 61, 73, 84, 85, 91, 100, 101, 105, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 123, 125, 127, 132, 133, 135, 139, 140, 141, 142, 144, 145, 146, 152, 156, 158, 160, 162, 163, 177, 178, 179, 180, 181, 185, 186, 188, 193, 195, 203, 206, 209, 212, 213, 216, 217, 229, 233, 234, 236, 237, 239, 240, 241, 242], "no_grad": 8, "6": [8, 33, 34, 35, 36, 42, 74, 78, 153, 160, 203, 230, 241, 242], "3989": 8, "9": [8, 33, 34, 35, 160, 203, 236, 241, 242], "0531": 8, "3": [8, 33, 34, 35, 36, 42, 63, 100, 119, 134, 136, 137, 139, 160, 195, 202, 203, 216, 222, 229, 230, 233, 234, 236, 237, 238, 241, 242], "2375": 8, "5": [8, 10, 33, 34, 35, 36, 160, 162, 177, 181, 182, 236, 237, 238, 240], "2822": 8, "4": [8, 10, 33, 34, 35, 36, 63, 152, 160, 202, 203, 223, 228, 230, 233, 235, 236, 238, 239, 240, 241, 242], "4872": 8, "7469": 8, "8": [8, 33, 34, 35, 47, 50, 52, 53, 57, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 92, 93, 94, 95, 96, 97, 98, 99, 106, 107, 108, 109, 110, 115, 116, 117, 118, 122, 124, 130, 131, 134, 138, 139, 141, 142, 143, 160, 163, 236, 239, 240, 241, 242], "6737": 8, "11": [8, 33, 34, 35, 160, 236, 241, 242], "0023": 8, "8235": 8, "6819": 8, "2424": 8, "0109": 8, "6915": 8, "7": [8, 33, 34, 35, 36, 160, 179, 190], "3618": 8, "1628": 8, "8594": 8, "5857": 8, "1151": 8, "7808": 8, "2322": 8, "8850": 8, "9604": 8, "7624": 8, "6040": 8, "3159": 8, "5849": 8, "8039": 8, "9322": 8, "2010": [8, 160], "6824": 8, "8929": 8, "8465": 8, "3794": 8, "3500": 8, "6145": 8, "5931": 8, "do": [8, 9, 11, 24, 34, 41, 52, 139, 173, 188, 209, 213, 219, 233, 234, 235, 236, 237, 238, 239, 240, 241], "find": [8, 9, 11, 12, 177, 231, 233, 236, 237, 239, 240], "hope": 8, "deeper": [8, 229, 237, 240], "insight": [8, 230, 236], "happi": [8, 236], "thi": [9, 10, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 73, 74, 78, 84, 85, 91, 100, 101, 105, 110, 114, 119, 120, 121, 123, 125, 127, 132, 133, 135, 136, 137, 139, 140, 144, 149, 152, 154, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 170, 173, 174, 176, 177, 178, 179, 181, 185, 186, 188, 190, 191, 192, 193, 194, 196, 199, 201, 203, 206, 208, 209, 210, 212, 213, 214, 215, 217, 219, 221, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242], "torchtun": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 227, 229, 230, 231, 234, 235, 237, 240], "start": [9, 11, 12, 188, 195, 209, 227, 228, 234, 235, 236, 237, 240, 241], "cometlogg": 9, "checkpoint": [9, 10, 11, 161, 164, 166, 186, 192, 193, 194, 195, 196, 197, 213, 215, 219, 228, 229, 230, 233, 238, 239, 241, 242], "workspac": [9, 12, 209], "seen": [9, 12, 239, 242], "screenshot": [9, 12], "below": [9, 12, 34, 154, 191, 235, 238, 239, 242], "instal": [9, 10, 12, 206, 209, 212, 213, 226, 233, 235, 236, 237, 238, 239, 240, 241, 242], "comet_ml": [9, 209], "packag": [9, 12, 209, 212, 213, 227, 235], "featur": [9, 11, 12, 227, 228, 229, 230, 231, 236, 237, 240], "via": [9, 10, 12, 43, 48, 51, 56, 169, 192, 239, 242], "pip": [9, 12, 209, 212, 213, 227, 236, 238, 240], "login": [9, 12, 209, 213, 233, 236], "data": [9, 17, 18, 19, 21, 22, 23, 24, 25, 26, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 83, 84, 120, 139, 148, 160, 177, 180, 189, 201, 209, 210, 211, 212, 213, 229, 230, 235, 236, 240, 241, 242], "command": [9, 11, 12, 227, 230, 231, 233, 234, 235, 236, 237, 238, 239, 241, 242], "add": [9, 10, 12, 39, 42, 45, 56, 73, 84, 100, 119, 120, 132, 139, 160, 167, 186, 188, 194, 195, 235, 236, 238, 239, 242], "built": [9, 10, 12, 55, 227, 231, 234, 237, 242], "metric_logg": [9, 10, 11, 12], "metric_log": [9, 10, 12, 209, 210, 211, 212, 213], "project": [9, 12, 63, 67, 68, 69, 74, 78, 85, 89, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 128, 133, 134, 140, 143, 144, 149, 152, 156, 160, 164, 167, 173, 174, 195, 200, 209, 213, 226, 234, 239, 240, 242], "experiment_nam": [9, 209], "my": [9, 59, 233, 234, 235, 236, 238], "experi": [9, 10, 209, 213, 226, 228, 234, 238, 239], "automat": [9, 10, 12, 13, 47, 48, 233, 236, 242], "grab": [9, 12, 238], "hyperparamet": [9, 181, 196, 228, 237, 239, 242], "tab": [9, 12], "actual": [9, 10, 12, 19, 21, 39, 43, 44, 47, 50, 51, 53, 55, 57, 73, 84, 100, 120, 132, 139, 230, 234, 241], "asset": 9, "artifact": [9, 12, 218], "click": [9, 12], "after": [9, 11, 25, 44, 52, 57, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 151, 152, 153, 155, 156, 158, 159, 164, 166, 173, 182, 208, 209, 210, 211, 212, 213, 230, 234, 236, 238, 241, 242], "pars": [10, 13, 14, 187, 231, 234, 237], "effect": [10, 181, 240, 241], "cli": [10, 12, 14, 15, 227, 229, 236, 237, 240], "prerequisit": [10, 234, 235, 236, 237, 238, 239, 241, 242], "Be": [10, 234, 236, 237, 238, 239, 240, 241, 242], "familiar": [10, 234, 236, 237, 238, 239, 241, 242], "fundament": [10, 241], "There": [10, 38, 61, 234, 237, 238, 239, 240], "entri": [10, 11, 34, 231, 237, 240], "point": [10, 11, 31, 32, 48, 188, 231, 235, 236, 237, 238, 239, 241, 242], "locat": [10, 233, 235, 238, 239, 241, 242], "thei": [10, 11, 40, 63, 156, 158, 160, 166, 174, 200, 233, 234, 235, 239, 240, 241], "truth": [10, 163, 236, 238], "reproduc": [10, 209], "overridden": [10, 149, 218], "quick": 10, "experiment": 10, "serv": [10, 19, 21, 23, 29, 50, 51, 52, 53, 54, 57, 188, 191, 235, 239], "particular": [10, 39, 40, 73, 84, 100, 120, 132, 139, 191, 229, 235, 239, 242], "seed": [10, 11, 12, 217, 237, 241], "shuffl": [10, 42, 241], "devic": [10, 11, 173, 196, 199, 201, 221, 231, 233, 234, 236, 237, 238, 239, 240, 242], "cuda": [10, 199, 201, 218, 221, 227, 236, 242], "dtype": [10, 11, 151, 152, 156, 157, 158, 159, 161, 164, 166, 199, 216, 220, 236, 240, 241, 242], "fp32": [10, 163, 240, 241, 242], "enable_fsdp": 10, "mani": [10, 42, 229, 230, 235, 236], "object": [10, 13, 14, 17, 63, 84, 120, 139, 152, 177, 181, 191, 202, 234], "keyword": [10, 13, 39, 41, 43, 44, 45, 47, 48, 49, 51, 52, 56, 57, 58, 161, 234, 235], "loss": [10, 11, 24, 25, 41, 43, 44, 47, 48, 50, 51, 53, 54, 163, 177, 178, 179, 180, 181, 237, 239, 242], "subfield": 10, "dotpath": [10, 77, 90, 104, 129, 137, 148, 235], "wish": [10, 219, 235], "exact": [10, 13, 236], "path": [10, 11, 12, 13, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 185, 186, 187, 192, 193, 194, 218, 233, 234, 235, 236, 238, 239], "normal": [10, 39, 42, 73, 84, 100, 120, 132, 139, 150, 152, 153, 156, 157, 158, 159, 163, 185, 234, 235, 239, 241, 242], "python": [10, 139, 209, 213, 217, 222, 224, 233, 235, 236, 241], "alpaca_dataset": [10, 46, 235], "custom": [10, 11, 17, 22, 25, 39, 41, 43, 44, 48, 51, 52, 56, 57, 77, 90, 104, 129, 137, 148, 215, 228, 229, 230, 231, 233, 237, 238, 239, 240], "train_on_input": [10, 19, 21, 23, 29, 31, 32, 39, 40, 41, 46, 47, 48, 50, 51, 52, 53, 54, 55, 57, 234, 235], "onc": [10, 25, 170, 236, 237, 238, 239, 242], "ve": [10, 151, 230, 233, 234, 235, 236, 238, 239, 240], "instanc": [10, 13, 40, 41, 91, 105, 114, 121, 123, 133, 140, 141, 142, 145, 146, 149, 161, 171, 172, 239], "cfg": [10, 11, 14, 15, 16], "under": [10, 218, 235, 240, 242], "preced": [10, 13, 233, 238, 239], "throw": 10, "notic": [10, 60, 61, 62, 160, 234, 235, 239], "miss": [10, 173, 174, 218, 239], "posit": [10, 13, 42, 60, 61, 62, 63, 74, 78, 121, 123, 125, 127, 133, 135, 151, 152, 154, 156, 157, 158, 159, 160, 164, 165, 238], "anoth": [10, 44, 209, 236, 240], "def": [10, 11, 12, 15, 52, 57, 191, 195, 234, 235, 239, 242], "dictconfig": [10, 11, 13, 14, 15, 16, 209, 213, 218], "arg": [10, 13, 26, 62, 150, 156, 158, 161, 166, 168, 183, 184, 189, 211, 218, 230, 241], "tupl": [10, 13, 25, 35, 62, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 151, 160, 161, 175, 176, 177, 178, 179, 180, 181, 182, 184, 188, 191, 204, 218, 219, 220], "kwarg": [10, 13, 26, 148, 150, 157, 159, 161, 166, 168, 183, 184, 189, 205, 209, 210, 211, 212, 213, 215, 218, 235], "str": [10, 13, 14, 19, 21, 22, 23, 24, 25, 29, 31, 32, 34, 35, 36, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 161, 165, 166, 168, 169, 171, 172, 173, 174, 183, 184, 185, 186, 187, 192, 193, 194, 195, 196, 197, 199, 201, 202, 205, 207, 209, 210, 211, 212, 213, 217, 218, 219, 220, 221, 222, 223, 234, 235, 240], "mean": [10, 152, 153, 156, 157, 158, 159, 164, 175, 208, 233, 234, 235, 237, 239, 241], "pass": [10, 13, 24, 25, 39, 40, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 149, 152, 156, 158, 161, 164, 170, 174, 179, 186, 191, 194, 199, 200, 201, 205, 208, 209, 212, 213, 215, 218, 233, 234, 235, 239, 241, 242], "d": [10, 24, 52, 151, 152, 156, 158, 164, 231, 233, 234, 239, 241], "llama2_token": [10, 234, 236], "llama2token": [10, 90], "modeltoken": [10, 24, 39, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 56, 58, 188, 234, 235], "bool": [10, 19, 21, 23, 24, 29, 31, 32, 33, 39, 41, 42, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 63, 67, 68, 69, 70, 71, 72, 73, 74, 78, 79, 80, 81, 82, 84, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 130, 131, 132, 133, 134, 138, 139, 140, 141, 142, 143, 144, 152, 156, 157, 158, 159, 161, 164, 166, 169, 173, 174, 175, 182, 184, 185, 186, 188, 191, 192, 193, 194, 200, 201, 205, 206, 208, 209, 212, 215, 218, 219, 223, 234, 240, 242], "max_seq_len": [10, 13, 34, 37, 39, 41, 42, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 73, 74, 77, 78, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 121, 123, 125, 127, 129, 132, 133, 135, 137, 139, 140, 144, 148, 151, 152, 154, 156, 158, 188, 234, 235, 241], "int": [10, 12, 34, 35, 36, 37, 39, 41, 42, 49, 52, 57, 58, 59, 60, 61, 62, 63, 67, 68, 69, 70, 71, 72, 73, 74, 77, 78, 79, 80, 81, 82, 84, 85, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 104, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 127, 129, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140, 141, 142, 143, 144, 148, 151, 152, 153, 154, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 169, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 197, 200, 204, 208, 209, 210, 211, 212, 213, 215, 217, 218, 233, 234, 235, 239, 240, 242], "512": [10, 63, 235, 242], "instructdataset": [10, 235], "alreadi": [10, 19, 23, 29, 50, 52, 53, 54, 57, 152, 164, 195, 205, 227, 233, 235, 236, 239], "overwrit": [10, 194, 219, 227, 233], "duplic": [10, 11, 228, 233], "sometim": 10, "than": [10, 38, 151, 152, 156, 158, 160, 177, 191, 194, 195, 220, 223, 234, 235, 236, 237, 238, 239, 240, 242], "resolv": [10, 14, 237], "alpaca": [10, 40, 46, 47, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143, 235], "disklogg": 10, "log_dir": [10, 210, 212, 213], "conveni": [10, 11, 233], "verifi": [10, 199, 200, 221, 234, 237, 239], "properli": [10, 173, 206, 233], "wa": [10, 61, 62, 63, 160, 173, 234, 239, 241, 242], "cp": [10, 227, 233, 234, 236, 237, 238, 241], "7b_lora_single_devic": [10, 236, 237, 239, 242], "my_config": [10, 233], "discuss": [10, 236, 237, 238, 239], "guidelin": 10, "while": [10, 11, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143, 149, 165, 228, 230, 236, 241, 242], "mai": [10, 12, 48, 160, 165, 200, 219, 229, 230, 234, 235, 237, 239, 240], "tempt": 10, "put": [10, 11, 231, 237, 239, 241], "much": [10, 165, 181, 236, 238, 239, 241, 242], "give": [10, 235, 239, 240], "maximum": [10, 34, 37, 39, 41, 42, 49, 58, 60, 61, 63, 74, 77, 78, 85, 90, 91, 100, 101, 104, 105, 110, 114, 121, 123, 125, 127, 129, 133, 135, 137, 140, 144, 151, 152, 154, 156, 158, 190, 233], "flexibl": [10, 40, 235, 240], "switch": 10, "encourag": [10, 181, 239, 240], "clariti": 10, "significantli": [10, 177, 229, 230, 240], "easier": [10, 236, 237], "dont": 10, "slimorca_dataset": 10, "privat": 10, "expos": [10, 11, 194, 231, 234, 237], "parent": [10, 233], "modul": [10, 13, 52, 57, 60, 61, 62, 63, 123, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 195, 197, 198, 200, 208, 214, 215, 217, 235, 237, 239, 242], "__init__": [10, 11, 52, 57, 239, 242], "py": [10, 13, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 139, 141, 142, 143, 151, 152, 153, 154, 162, 177, 178, 179, 180, 181, 233, 236, 238], "guarante": 10, "stabil": [10, 163, 228, 230, 240, 241, 242], "underscor": 10, "_alpaca": 10, "itself": 10, "k1": [10, 11], "v1": [10, 11, 58], "k2": [10, 11], "v2": [10, 11, 209, 235], "lora": [10, 67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 169, 170, 173, 174, 192, 208, 226, 228, 231, 234, 237, 238], "lora_finetune_single_devic": [10, 229, 233, 234, 236, 237, 238, 239, 240, 242], "home": 10, "my_model_checkpoint": 10, "file_1": 10, "file_2": 10, "my_tokenizer_path": 10, "assign": [10, 43, 44], "nest": 10, "dot": 10, "notat": [10, 152, 154, 156, 158, 164, 175, 176, 203], "flag": [10, 11, 24, 41, 47, 48, 50, 51, 53, 54, 191, 194, 200, 233, 240, 242], "bitsandbyt": [10, 240], "pagedadamw8bit": [10, 240], "delet": 10, "foreach": [10, 39], "pytorch": [10, 11, 84, 156, 158, 161, 163, 173, 191, 206, 212, 215, 217, 218, 226, 227, 228, 230, 236, 238, 239, 240, 241, 242], "llama3": [10, 39, 52, 57, 59, 100, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 133, 163, 195, 200, 226, 229, 230, 233, 235, 236, 240], "8b_full": [10, 233, 235], "adamw": [10, 239, 240], "lr": [10, 162, 240], "2e": [10, 240], "fuse": [10, 164, 165, 166, 214, 241], "nproc_per_nod": [10, 230, 235, 238, 239, 241], "full_finetune_distribut": [10, 233, 235, 236, 237], "core": [11, 43, 44, 228, 231, 235, 237, 242], "i": [11, 24, 59, 83, 119, 152, 156, 157, 158, 159, 160, 161, 164, 172, 196, 235, 236, 238, 240, 241, 242], "structur": [11, 17, 23, 26, 29, 31, 32, 39, 48, 73, 84, 100, 104, 120, 132, 137, 139, 148, 190, 234, 235, 236, 241], "new": [11, 23, 29, 31, 32, 47, 49, 50, 52, 53, 54, 126, 151, 165, 166, 195, 209, 210, 212, 234, 236, 237, 238, 239, 242], "user": [11, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 38, 39, 43, 44, 48, 51, 73, 77, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 121, 123, 125, 127, 129, 132, 133, 135, 137, 140, 144, 148, 152, 188, 231, 234, 235, 237, 241], "thought": [11, 228, 231, 237, 242], "target": [11, 181, 228], "pipelin": [11, 228, 230], "llm": [11, 164, 166, 226, 227, 228, 229, 231, 235, 236, 238, 239], "eg": [11, 156, 158, 164, 192, 228], "meaning": [11, 228, 236], "fsdp": [11, 191, 196, 200, 208, 228, 231, 237, 238], "activ": [11, 63, 149, 197, 201, 207, 215, 218, 228, 229, 230, 241, 242], "gradient": [11, 208, 214, 218, 228, 229, 230, 236, 238, 239, 242], "accumul": [11, 214, 218, 228, 229, 230], "mix": [11, 150, 233, 235, 236, 240], "precis": [11, 150, 161, 199, 228, 229, 230, 237, 242], "given": [11, 13, 22, 34, 38, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 59, 139, 169, 170, 176, 178, 183, 184, 199, 202, 208, 214, 221, 223, 228, 239], "complex": 11, "becom": [11, 160, 178, 227, 235], "harder": 11, "anticip": 11, "architectur": [11, 83, 119, 158, 160, 164, 166, 195, 233, 235], "methodolog": 11, "reason": [11, 59, 236, 240, 241], "possibl": [11, 42, 233, 235, 240], "trade": [11, 240], "off": [11, 25, 84, 120, 229, 230, 236, 241], "memori": [11, 40, 41, 42, 45, 49, 56, 58, 139, 156, 158, 161, 163, 173, 200, 201, 207, 208, 218, 226, 228, 229, 230, 231, 236, 237, 238, 241], "vs": [11, 178, 237], "qualiti": [11, 236, 239, 241], "believ": 11, "best": [11, 230, 234, 240], "suit": [11, 237, 240], "b": [11, 33, 34, 151, 152, 154, 156, 158, 159, 164, 169, 175, 176, 181, 203, 213, 239, 242], "fit": [11, 39, 41, 42, 49, 56, 58, 160, 177, 178, 235], "solut": [11, 178], "result": [11, 52, 63, 73, 84, 120, 132, 160, 163, 188, 190, 218, 229, 230, 236, 238, 239, 240, 241, 242], "meant": [11, 161, 196], "depend": [11, 12, 192, 218, 233, 235, 236, 239, 242], "level": [11, 43, 44, 139, 163, 189, 198, 208, 222, 228, 242], "expertis": 11, "routin": 11, "yourself": [11, 233, 238, 239], "exist": [11, 166, 209, 227, 233, 236, 237, 238, 242], "ad": [11, 25, 60, 61, 62, 120, 127, 160, 164, 165, 167, 185, 194, 195, 234, 235, 239, 240, 241, 242], "ones": 11, "modular": [11, 228], "build": [11, 56, 63, 74, 85, 101, 110, 125, 127, 144, 228, 238, 239], "block": [11, 42, 67, 68, 69, 74, 78, 79, 80, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 133, 134, 140, 141, 142, 143, 144, 173, 174, 228], "wandb": [11, 12, 213, 237], "log": [11, 14, 177, 178, 179, 180, 181, 201, 207, 209, 210, 211, 212, 213, 222, 236, 237, 238, 239, 240, 242], "fulli": [11, 40], "nativ": [11, 226, 228, 239, 241, 242], "correct": [11, 20, 50, 153, 154, 156, 158, 221, 228, 234, 235], "numer": [11, 57, 228, 230, 241], "pariti": [11, 228], "verif": 11, "extens": [11, 194, 228], "comparison": [11, 239, 242], "benchmark": [11, 217, 228, 236, 238, 239, 241], "limit": [11, 196, 235, 240, 241], "hidden": [11, 63, 149, 160], "behind": 11, "100": [11, 35, 36, 41, 47, 48, 50, 51, 53, 54, 59, 163, 165, 239, 242], "prefer": [11, 35, 43, 55, 177, 178, 179, 180, 181, 228, 233, 235, 240], "over": [11, 24, 44, 162, 177, 178, 228, 230, 233, 236, 239, 240, 242], "unnecessari": 11, "abstract": [11, 17, 22, 183, 184, 228, 237, 242], "No": [11, 194, 228], "inherit": [11, 228, 235], "go": [11, 63, 73, 83, 84, 119, 120, 132, 160, 188, 228, 235, 236, 237, 240, 242], "upon": [11, 40, 238], "figur": [11, 239, 242], "spectrum": 11, "decid": 11, "interact": [11, 226, 231, 237], "avail": [11, 58, 164, 166, 199, 206, 221, 228, 233, 236, 238, 239], "paradigm": [11, 139, 229, 231, 240], "consist": [11, 19, 23, 29, 52, 57, 58, 231, 237], "configur": [11, 41, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 78, 91, 100, 105, 114, 121, 132, 133, 140, 159, 209, 228, 229, 230, 231, 234, 237, 238, 239, 240, 241, 242], "paramet": [11, 13, 14, 15, 16, 17, 19, 21, 22, 23, 24, 25, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 226, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 241, 242], "overrid": [11, 14, 15, 19, 23, 29, 50, 52, 53, 54, 57, 219, 231, 233, 236, 237, 238, 242], "togeth": [11, 42, 163, 213, 231, 237, 239, 240, 241], "valid": [11, 38, 173, 174, 176, 219, 220, 227, 231, 236, 237], "environ": [11, 206, 209, 221, 227, 231, 233, 235, 236, 237, 241], "logic": [11, 44, 184, 195, 228, 231, 237, 239], "api": [11, 12, 20, 27, 30, 31, 43, 44, 47, 52, 57, 70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 173, 209, 227, 231, 233, 234, 237, 238, 242], "closer": [11, 239], "monolith": [11, 228], "trainer": [11, 177, 178, 180, 181], "A": [11, 12, 19, 20, 23, 27, 29, 30, 31, 32, 34, 35, 36, 40, 42, 63, 73, 84, 120, 132, 139, 148, 152, 156, 157, 158, 159, 160, 161, 164, 169, 173, 175, 176, 177, 178, 179, 180, 181, 182, 185, 186, 188, 190, 191, 195, 196, 201, 202, 207, 208, 225, 226, 232, 233, 234, 239, 240, 241, 242], "wrapper": [11, 150, 185, 186, 196, 198, 233, 239], "around": [11, 39, 73, 84, 100, 120, 132, 139, 150, 185, 186, 201, 233, 234, 236, 239, 240, 241, 242], "extern": [11, 235], "primarili": [11, 40, 239], "eleutherai": [11, 58, 228, 239, 241], "har": [11, 228, 239, 241], "control": [11, 24, 41, 47, 48, 50, 51, 53, 54, 166, 170, 178, 209, 217, 230, 236, 240], "multi": [11, 39, 152, 173, 238], "stage": [11, 160], "distil": 11, "oper": [11, 139, 160, 170, 189, 217, 241], "turn": [11, 19, 23, 24, 29, 38, 39, 100, 234, 240], "dataload": [11, 42, 47, 50, 52, 53, 57], "applic": [11, 192, 193, 213], "clean": [11, 12, 46], "process": [11, 12, 43, 44, 45, 52, 56, 57, 63, 139, 160, 161, 204, 205, 217, 235, 237, 241, 242], "group": [11, 152, 204, 205, 209, 210, 211, 212, 213, 233, 238, 241], "init_process_group": [11, 205], "backend": [11, 233, 241], "gloo": 11, "els": [11, 213, 228, 242], "nccl": 11, "fullfinetunerecipedistribut": 11, "cleanup": 11, "stuff": 11, "carri": [11, 44], "relev": [11, 157, 159, 233, 236, 239, 240], "interfac": [11, 17, 22, 25, 26, 40, 168, 189, 235], "metric": [11, 237, 240, 241], "logger": [11, 207, 209, 210, 211, 212, 213, 222, 237], "self": [11, 12, 42, 52, 57, 67, 68, 69, 74, 78, 79, 80, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 127, 133, 134, 135, 140, 141, 142, 143, 144, 152, 156, 157, 158, 159, 163, 166, 168, 173, 174, 192, 195, 196, 235, 239, 242], "_devic": 11, "get_devic": 11, "_dtype": 11, "get_dtyp": 11, "ckpt_dict": 11, "wrap": [11, 166, 191, 197, 200, 208, 215, 234], "_model": [11, 196], "_setup_model": 11, "_token": [11, 235], "_setup_token": 11, "_optim": 11, "_setup_optim": 11, "_loss_fn": 11, "_setup_loss": 11, "_sampler": 11, "_dataload": 11, "_setup_data": 11, "backward": [11, 196, 198, 214, 218, 242], "zero_grad": 11, "curr_epoch": 11, "rang": [11, 165, 177, 179, 181, 217, 233, 238, 241], "epochs_run": [11, 12], "total_epoch": [11, 12], "idx": [11, 42], "enumer": 11, "_autocast": 11, "logit": [11, 59, 163, 203], "label": [11, 34, 35, 36, 39, 41, 42, 49, 54, 58, 163, 177, 181], "global_step": 11, "_log_every_n_step": 11, "_metric_logg": 11, "log_dict": [11, 209, 210, 211, 212, 213], "step": [11, 42, 43, 44, 52, 57, 156, 158, 162, 164, 175, 198, 209, 210, 211, 212, 213, 214, 218, 226, 229, 230, 236, 239, 241, 242], "learn": [11, 40, 162, 165, 166, 167, 178, 228, 229, 230, 231, 234, 235, 237, 238, 239, 240, 241, 242], "decor": [11, 15], "recipe_main": [11, 15], "fullfinetunerecip": 11, "wandblogg": [12, 239, 242], "Then": [12, 170, 237, 240], "tip": 12, "straggler": 12, "background": 12, "crash": 12, "otherwis": [12, 33, 34, 61, 62, 63, 160, 206, 209, 234, 241], "exit": [12, 227, 233], "resourc": [12, 209, 210, 211, 212, 213, 241], "kill": 12, "ps": 12, "aux": 12, "grep": 12, "awk": 12, "xarg": 12, "desir": [12, 39, 43, 44, 216, 234, 240], "suggest": 12, "approach": [12, 40, 235], "full_finetun": 12, "joinpath": 12, "_checkpoint": [12, 236], "_output_dir": [12, 192, 193, 194], "torchtune_model_": 12, "with_suffix": 12, "wandb_at": 12, "type": [12, 13, 15, 24, 31, 32, 33, 34, 35, 36, 37, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 105, 106, 107, 110, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 169, 171, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 208, 215, 216, 217, 218, 220, 221, 222, 223, 230, 231, 235, 236, 239, 240, 241, 242], "descript": [12, 233], "whatev": 12, "metadata": [12, 241], "seed_kei": 12, "epochs_kei": 12, "total_epochs_kei": 12, "max_steps_kei": 12, "max_steps_per_epoch": [12, 241], "add_fil": 12, "log_artifact": 12, "field": [13, 21, 22, 24, 31, 32, 39, 42, 43, 44, 47, 52, 57, 207, 235], "hydra": 13, "facebook": 13, "research": 13, "http": [13, 39, 41, 45, 49, 52, 56, 58, 64, 65, 66, 67, 68, 69, 70, 71, 72, 75, 76, 78, 79, 80, 81, 82, 84, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 105, 106, 107, 108, 109, 115, 116, 117, 118, 121, 122, 123, 124, 126, 128, 130, 131, 133, 134, 136, 137, 138, 139, 141, 142, 143, 145, 146, 147, 152, 153, 154, 160, 162, 163, 173, 175, 177, 178, 179, 180, 181, 190, 191, 192, 193, 206, 209, 212, 213, 215, 217, 222, 227, 235, 236, 238], "com": [13, 67, 68, 69, 79, 80, 84, 92, 93, 94, 95, 100, 106, 107, 115, 116, 134, 139, 141, 142, 143, 152, 153, 154, 162, 163, 173, 177, 178, 179, 180, 181, 209, 227, 236, 238], "facebookresearch": [13, 153], "blob": [13, 67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 137, 139, 141, 142, 143, 152, 153, 154, 162, 177, 178, 179, 180, 181], "main": [13, 15, 84, 137, 152, 153, 154, 227, 230, 236, 238], "_intern": 13, "_instantiate2": 13, "l148": 13, "omegaconf": 13, "num_lay": [13, 63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 156, 158, 160, 164, 166], "32": [13, 160, 164, 166, 209, 238, 239, 240, 241, 242], "num_head": [13, 63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 154, 156, 158], "num_kv_head": [13, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152], "vocab_s": [13, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 163, 165], "must": [13, 25, 40, 52, 57, 152, 168, 209, 242], "return": [13, 15, 17, 22, 24, 25, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 75, 76, 77, 78, 79, 80, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 100, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 162, 163, 164, 165, 166, 168, 169, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 194, 196, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 216, 217, 218, 221, 222, 223, 234, 235, 239, 242], "nn": [13, 33, 34, 63, 149, 151, 152, 156, 157, 158, 159, 160, 161, 164, 165, 166, 167, 168, 170, 171, 172, 191, 197, 198, 208, 214, 215, 219, 220, 239, 242], "parsed_yaml": 13, "embed_dim": [13, 60, 61, 62, 63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 154, 157, 159, 160, 165, 166, 219, 239], "valueerror": [13, 19, 21, 23, 29, 31, 34, 38, 41, 47, 48, 50, 51, 52, 53, 54, 56, 57, 132, 140, 152, 156, 158, 160, 192, 193, 194, 199, 201, 217, 220], "recipe_nam": 14, "rank": [14, 67, 68, 69, 78, 79, 80, 91, 92, 93, 94, 95, 105, 106, 107, 114, 115, 116, 121, 122, 123, 124, 133, 134, 140, 141, 142, 143, 169, 204, 206, 217, 229, 237, 239, 242], "zero": [14, 151, 153, 236, 238, 241], "displai": 14, "callabl": [15, 39, 41, 43, 44, 45, 52, 56, 57, 59, 63, 156, 158, 170, 191, 200, 202, 208, 215], "With": [15, 236, 239, 241, 242], "my_recip": 15, "foo": 15, "bar": [15, 228, 237, 240], "instanti": [16, 25, 64, 65, 66, 67, 68, 69, 74, 75, 76, 77, 78, 79, 80, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 101, 102, 103, 104, 105, 106, 107, 110, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 127, 128, 129, 133, 134, 135, 136, 137, 140, 141, 142, 143, 144, 145, 146, 147, 148, 196], "configerror": 16, "cannot": [16, 139, 194, 238], "deprec": [17, 22, 31, 32, 39, 41], "remov": [17, 22, 31, 32, 39, 41, 139, 240], "futur": [17, 22, 31, 32, 39, 41, 230, 241], "releas": [17, 22, 31, 32, 39, 41, 233, 238], "prompttempl": [17, 20, 22, 27, 30, 73, 84, 100, 120, 132, 139], "tag": [17, 25, 39, 73, 77, 83, 84, 90, 100, 104, 119, 120, 129, 132, 137, 139, 148, 209, 210, 211, 212, 213, 234], "system": [17, 18, 19, 21, 23, 24, 25, 26, 28, 29, 31, 32, 38, 39, 43, 44, 50, 51, 52, 53, 54, 57, 73, 77, 83, 84, 90, 104, 119, 120, 129, 132, 137, 148, 188, 234, 235], "assist": [17, 18, 19, 21, 23, 24, 25, 26, 28, 29, 31, 32, 38, 39, 43, 44, 48, 59, 73, 77, 83, 84, 90, 100, 104, 120, 129, 132, 137, 148, 188, 234, 235], "role": [17, 19, 23, 24, 25, 26, 29, 31, 32, 39, 43, 44, 48, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 188, 234, 235], "prepend": [17, 19, 21, 23, 25, 26, 29, 50, 51, 52, 53, 54, 57, 77, 84, 90, 100, 104, 120, 129, 137, 148, 185], "append": [17, 25, 26, 77, 90, 100, 104, 120, 129, 132, 137, 148, 156, 158, 164, 185, 209, 227, 235], "messag": [17, 18, 19, 21, 23, 25, 26, 29, 31, 32, 38, 39, 43, 44, 47, 48, 50, 51, 52, 53, 54, 57, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 184, 188, 227, 233, 234, 235], "classmethod": [17, 22, 24, 235], "accord": [17, 52, 57, 119, 234], "openai": [18, 31, 179, 235], "markup": 18, "languag": [18, 59, 139, 165, 166, 169, 177, 219, 239, 240], "It": [18, 24, 25, 43, 44, 48, 50, 51, 52, 53, 55, 57, 119, 160, 164, 177, 181, 209, 233, 234, 235, 242], "templat": [18, 20, 22, 25, 26, 27, 30, 39, 40, 41, 43, 44, 47, 50, 53, 73, 77, 83, 84, 90, 100, 104, 119, 120, 129, 132, 137, 139, 148], "im_start": 18, "context": [18, 136, 170, 216, 218, 235, 240], "im_end": 18, "goe": [18, 170], "respons": [18, 19, 21, 23, 24, 29, 43, 44, 50, 51, 52, 53, 54, 57, 73, 84, 120, 132, 175, 176, 177, 178, 180, 181, 188, 235, 236, 237, 238], "column_map": [19, 21, 22, 23, 29, 40, 41, 46, 47, 50, 51, 52, 53, 54, 55, 57, 235], "new_system_prompt": [19, 21, 23, 29, 50, 51, 52, 53, 54, 57], "chosen": [19, 43, 55, 177, 178, 180, 181, 218, 235], "reject": [19, 43, 55, 177, 178, 180, 181, 235], "column": [19, 21, 22, 23, 29, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 152, 156, 158, 159, 164, 234, 235, 241], "q1": [19, 43, 48], "a1": [19, 43, 48], "a2": [19, 43], "whether": [19, 21, 23, 24, 29, 31, 32, 34, 39, 41, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 67, 68, 69, 74, 78, 79, 80, 91, 92, 93, 94, 95, 100, 105, 106, 107, 114, 115, 116, 120, 121, 122, 123, 124, 132, 133, 134, 139, 140, 141, 142, 143, 144, 161, 169, 173, 174, 185, 186, 191, 199, 201, 209, 219, 234, 235], "keep": [19, 21, 23, 29, 51, 55, 57, 165, 236, 239, 240], "present": [19, 23, 29, 50, 52, 53, 54, 57, 186, 194, 219], "functool": [20, 27, 30, 191], "partial": [20, 27, 30, 191], "_prompt_templ": [20, 27, 30, 84, 120, 139], "english": 20, "n": [20, 25, 27, 30, 73, 84, 120, 132, 152, 160, 188, 225, 232, 233, 234, 235, 241], "ncorrect": 20, "grammar": [20, 50, 235], "task": [20, 27, 30, 40, 49, 73, 84, 100, 120, 132, 139, 229, 234, 235, 236, 238, 239, 240, 241, 242], "user_messag": [20, 27, 30, 234], "assistant_messag": [20, 27, 30, 234], "equival": [21, 31, 32, 61, 178, 180, 181], "respect": [21, 40, 83, 172, 218, 234, 235], "placehold": [22, 41, 235], "ident": [22, 23, 33, 34, 41, 42, 52, 119, 178, 236, 241], "alwai": [22, 178, 209, 219, 240], "dataclass": [23, 234], "remain": [23, 29, 31, 32, 162, 239, 240], "unmask": [23, 29, 31, 32], "liter": [24, 25, 28, 67, 68, 69, 70, 71, 72, 77, 78, 79, 80, 81, 82, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 104, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 129, 130, 131, 133, 134, 137, 138, 140, 141, 142, 143, 148, 173, 174], "ipython": [24, 25, 28, 43, 44, 77, 90, 104, 129, 137, 148], "union": [24, 34, 46, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 77, 90, 104, 129, 137, 140, 144, 148, 156, 158, 164, 174, 197, 209, 210, 211, 212, 213, 215, 217], "mask": [24, 25, 41, 42, 44, 47, 48, 50, 51, 52, 53, 54, 57, 73, 84, 100, 120, 132, 139, 152, 156, 157, 158, 159, 164, 175, 179, 184, 188, 190, 203, 234, 235], "eot": [24, 100], "repres": [24, 35, 60, 61, 160, 197, 234, 240, 241], "individu": [24, 42, 164, 201, 213, 215, 234, 235], "interleav": [24, 190], "tokenize_messag": [24, 39, 41, 43, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 73, 84, 100, 120, 132, 139, 184, 188, 234, 235], "attach": 24, "appropri": [24, 40, 83, 162, 165, 192, 235, 242], "special": [24, 39, 73, 84, 100, 104, 120, 132, 137, 139, 148, 160, 165, 183, 184, 186, 187, 188, 190, 196, 235], "writer": 24, "human": [24, 29, 32, 48, 83, 177, 179, 180, 234], "dictionari": [24, 25, 34, 35, 36, 42, 43, 44, 77, 90, 104, 129, 137, 148, 201, 207, 209, 210, 211, 212, 213, 236], "hello": [24, 73, 84, 100, 120, 132, 139, 185, 186, 234, 236, 238], "world": [24, 73, 84, 100, 120, 132, 139, 185, 186, 204, 206, 236], "calcul": [24, 25, 152, 156, 157, 158, 159, 160, 164, 175, 176, 179, 238], "correspond": [24, 35, 168, 171, 175, 179, 199, 230, 237, 238, 240, 241], "where": [24, 25, 33, 35, 39, 47, 61, 84, 89, 120, 128, 152, 156, 158, 160, 163, 164, 169, 175, 177, 178, 179, 182, 185, 190, 200, 203, 208, 235, 240], "hand": 24, "consecut": [24, 38, 190], "e": [24, 39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 139, 152, 160, 161, 164, 168, 172, 190, 192, 196, 201, 218, 227, 230, 236, 238, 239, 240, 241, 242], "last": [24, 37, 42, 56, 100, 162, 176, 235], "properti": [24, 157, 159, 166, 239, 240], "contains_media": 24, "non": [24, 139, 174, 176], "from_dict": [24, 234], "construct": [24, 139, 190, 231, 239], "text_cont": [24, 234], "achiev": [25, 214, 230, 236, 238, 239, 241, 242], "prepend_tag": 25, "append_tag": 25, "thu": [25, 43, 44, 178, 240, 241], "consid": [25, 40, 43, 44, 61, 62, 63, 160, 240], "come": [25, 38, 168, 231, 239, 240], "question": [27, 51, 234, 235, 236, 238], "nanswer": 27, "answer": [27, 51, 234, 236, 238], "alia": [28, 191], "adher": [29, 31, 32], "sharegpt": [29, 32, 48], "gpt": [29, 32, 48, 139, 152, 236], "summar": [30, 53, 234, 235, 240], "dialogu": [30, 53, 234], "nsummari": [30, 234], "summari": [30, 40, 53, 160, 201, 235], "jsontomessag": [31, 48], "transformed_sampl": [31, 32, 235], "could": [31, 239], "sharegpttomessag": [32, 48, 54], "sequenc": [33, 34, 35, 36, 41, 42, 45, 49, 52, 56, 57, 58, 73, 74, 77, 78, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 121, 123, 125, 127, 129, 132, 133, 135, 137, 139, 140, 144, 148, 151, 152, 154, 156, 158, 160, 164, 176, 181, 182, 186, 188, 190, 203, 234], "batch_first": 33, "padding_valu": 33, "float": [33, 59, 67, 68, 69, 70, 71, 72, 74, 78, 79, 80, 81, 82, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 105, 106, 107, 108, 109, 110, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 127, 130, 131, 133, 134, 135, 138, 140, 141, 142, 143, 144, 152, 153, 162, 169, 175, 176, 177, 178, 179, 180, 181, 201, 207, 209, 210, 211, 212, 213, 239, 240, 241, 242], "rnn": [33, 34], "pad_sequ": [33, 34], "variabl": [33, 41, 195, 206, 209, 235, 242], "length": [33, 34, 36, 37, 38, 40, 41, 42, 49, 58, 73, 74, 77, 78, 84, 85, 90, 91, 100, 101, 104, 105, 110, 114, 120, 121, 123, 125, 127, 129, 132, 133, 135, 136, 137, 139, 140, 144, 148, 151, 152, 154, 156, 158, 163, 164, 175, 176, 186, 188, 190, 193, 203, 209, 240], "left": [33, 34, 100, 132, 239], "longest": [33, 36], "trail": 33, "dimens": [33, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 154, 156, 158, 160, 165, 169, 238, 239, 240, 242], "element": [33, 34, 40, 203, 236], "c": [33, 34, 52, 234], "10": [33, 34, 35, 36, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 160, 165, 236, 238, 240, 241, 242], "12": [33, 34, 35, 54, 160, 227, 241], "pad_direct": 34, "keys_to_pad": 34, "padding_idx": [34, 35, 36, 42], "collat": [34, 36, 42, 235], "left_pad_sequ": 34, "subset": [34, 47, 49, 50, 52, 53, 54, 55, 56, 57, 58, 78, 91, 105, 114, 121, 123, 133, 140, 171], "integ": [34, 36, 165, 191, 197, 217], "per": [34, 70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 151, 160, 161, 176, 177, 190, 233, 240, 241, 242], "batch_siz": [34, 47, 50, 52, 53, 57, 151, 152, 156, 157, 158, 159, 163, 164, 165, 166, 177, 178, 180, 182, 236, 240, 241], "empti": [34, 38, 233], "ignore_idx": [35, 36], "dpo": [35, 43, 170, 177, 178, 180, 181], "input_id": [35, 203], "chosen_input_id": [35, 235], "chosen_label": [35, 235], "rejected_input_id": [35, 235], "rejected_label": [35, 235], "index": [35, 36, 40, 42, 152, 154, 156, 158, 159, 162, 164, 176, 227, 228, 234, 236, 237], "concaten": [35, 40, 73, 84, 120, 132, 139, 184, 188], "13": [35, 73, 84, 120, 132, 160, 182, 188, 242], "14": [35, 160, 241, 242], "15": [35, 160, 200, 234, 236, 239, 242], "16": [35, 67, 68, 69, 70, 71, 72, 79, 80, 81, 82, 92, 93, 94, 95, 96, 97, 98, 99, 106, 107, 108, 109, 115, 116, 117, 118, 122, 124, 130, 131, 134, 138, 141, 142, 143, 160, 239, 242], "17": [35, 160, 239], "18": [35, 160, 238], "19": [35, 160, 242], "20": [35, 160, 182, 241], "token_pair": 36, "padded_col": [36, 235], "eos_id": [37, 139, 186, 188], "replac": [37, 41, 47, 48, 50, 51, 53, 54, 139, 156, 161, 165, 219, 239], "forth": [38, 235], "shorter": 38, "min": [38, 239], "invalid": 38, "sftdataset": [39, 41, 43, 46, 47, 48, 50, 51, 52, 53, 54, 57], "chat_dataset": [39, 235], "multiturn": [39, 234], "convert_to_messag": [39, 234], "prepar": [39, 234, 241], "truncat": [39, 41, 42, 49, 56, 58, 73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148, 182, 186, 188, 235], "local": [39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 104, 137, 148, 209, 213, 217, 227, 233, 234, 236, 237], "g": [39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63, 139, 152, 160, 164, 168, 190, 192, 201, 218, 230, 238, 239, 240, 241, 242], "csv": [39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 234, 235], "filepath": [39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58], "data_fil": [39, 41, 43, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 234, 235], "load_dataset": [39, 41, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 234, 235], "huggingfac": [39, 41, 45, 49, 56, 58, 128, 136, 137, 139, 145, 146, 147, 162, 177, 178, 180, 181, 192, 193, 233, 236], "co": [39, 41, 45, 49, 56, 58, 128, 136, 137, 145, 146, 147, 192, 193, 236], "doc": [39, 41, 44, 45, 49, 56, 58, 84, 100, 139, 191, 206, 209, 212, 213, 217, 222, 233, 235, 236], "en": [39, 41, 45, 49, 56, 58, 241], "package_refer": [39, 41, 45, 49, 56, 58], "loading_method": [39, 41, 45, 49, 56, 58], "chat_format": [39, 234, 235], "chatformat": [39, 235], "inst": [39, 73, 83, 84, 100, 119, 120, 132, 139, 234, 235], "mistral": [39, 73, 84, 100, 119, 120, 121, 122, 123, 124, 126, 127, 128, 129, 130, 131, 132, 139, 195, 233, 234, 236, 237], "extra": [39, 73, 84, 100, 120, 132, 139, 164, 227, 234, 239, 241, 242], "still": [39, 73, 84, 100, 120, 132, 139, 163, 165, 166, 229, 239, 241, 242], "unless": 39, "load_dataset_kwarg": [39, 41, 43, 44, 45, 46, 47, 48, 49, 51, 52, 56, 57, 58], "sub": [40, 212], "unifi": [40, 128], "were": [40, 160, 170, 179, 234, 237, 241], "simplifi": [40, 177, 233, 239], "simultan": 40, "intern": 40, "aggreg": 40, "transpar": 40, "howev": [40, 137, 227, 229, 240], "constitu": 40, "might": [40, 165, 167, 233, 236, 240], "larg": [40, 139, 163, 169, 218, 233, 240, 242], "comput": [40, 43, 44, 85, 91, 101, 105, 110, 114, 140, 144, 149, 152, 154, 156, 158, 163, 164, 177, 178, 180, 181, 190, 201, 217, 230, 236, 240, 241, 242], "cumul": 40, "maintain": [40, 166, 229, 240, 242], "indic": [40, 41, 42, 63, 152, 154, 156, 158, 159, 160, 164, 165, 175, 179, 182, 190, 191, 203, 206, 234], "deleg": 40, "retriev": [40, 43, 44, 200], "lead": [40, 120, 185, 230], "high": [40, 43, 44, 228, 239, 240], "scale": [40, 59, 67, 68, 69, 78, 79, 80, 91, 92, 93, 94, 95, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 133, 134, 140, 141, 142, 143, 155, 157, 159, 169, 176, 178, 181, 229, 239, 240, 241, 242], "strategi": [40, 230], "stream": [40, 222], "demand": 40, "deriv": [40, 149, 158, 159], "dataset1": 40, "mycustomdataset": 40, "params1": 40, "dataset2": 40, "params2": 40, "concat_dataset": 40, "total": [40, 162, 176, 179, 204, 225, 232, 236, 238, 239, 240], "data_point": 40, "1500": 40, "accomplish": [40, 48, 51, 56], "instruct_dataset": [40, 41, 235], "vicgal": [40, 235], "gpt4": [40, 235], "alpacainstructtempl": [40, 235], "samsum": [40, 53, 235], "summarizetempl": [40, 53, 234, 235], "focus": [40, 231, 237, 240], "enhanc": [40, 160, 181, 240, 242], "divers": 40, "machin": [40, 180, 221, 233, 236], "instructtempl": [41, 235], "contribut": [41, 47, 48, 50, 51, 53, 54, 176, 179, 231], "disabl": [41, 49, 58, 170, 217, 241], "recommend": [41, 48, 49, 50, 53, 55, 58, 119, 163, 209, 212, 234, 236, 240, 242], "highest": [41, 49, 58], "ds": [42, 54], "max_pack": 42, "split_across_pack": [42, 56], "greedi": 42, "pack": [42, 46, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 152, 154, 156, 158, 159, 164, 241], "done": [42, 173, 199, 208, 219, 239, 241, 242], "outsid": [42, 217, 218, 239], "sampler": [42, 237], "part": [42, 165, 180, 234, 242], "style": [42, 46, 47, 48, 54, 166, 242], "buffer": [42, 240], "long": [42, 139, 186, 234, 235, 239], "enough": [42, 234], "attent": [42, 63, 67, 68, 69, 74, 78, 79, 80, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 127, 133, 134, 135, 136, 140, 141, 142, 143, 144, 151, 152, 154, 156, 157, 158, 159, 164, 166, 173, 174, 190, 238, 239, 240, 242], "lower": [42, 229, 230, 239], "triangular": 42, "cross": [42, 157, 163, 164, 166, 190], "attend": [42, 152, 156, 157, 158, 159, 164, 190], "rel": [42, 152, 154, 156, 158, 159, 164, 177, 201, 239], "its": [42, 83, 119, 123, 152, 154, 156, 158, 159, 164, 166, 178, 214, 217, 233, 234, 235, 236, 238, 239, 240], "max": [42, 73, 84, 120, 132, 139, 148, 156, 158, 160, 162, 164, 186, 188, 233, 239], "wise": 42, "made": [42, 48, 51, 56, 154, 236], "smaller": [42, 165, 178, 236, 238, 239, 240, 241, 242], "jam": 42, "vari": 42, "s1": [42, 84, 120, 185], "s2": [42, 84, 120, 185], "s3": 42, "s4": 42, "contamin": 42, "input_po": [42, 151, 152, 154, 156, 158, 159, 164], "matrix": [42, 156, 157, 158, 164], "causal": [42, 152, 156, 158, 159, 164], "continu": [42, 160, 209, 231, 235], "increment": 42, "move": [42, 56, 156, 158], "entir": [42, 56, 163, 167, 208, 234, 242], "avoid": [42, 56, 153, 160, 161, 178, 217, 233, 241, 242], "sentenc": [42, 56, 120], "message_transform": [43, 44], "techniqu": [43, 228, 229, 231, 236, 237, 238, 239, 240, 241], "rlhf": [43, 175, 176, 177, 178, 179, 180, 181, 182, 235], "remot": [43, 44], "separ": [43, 73, 84, 120, 132, 166, 188, 192, 234, 237, 238, 239, 242], "repons": 43, "At": [43, 44, 156, 158, 164], "uniqu": [43, 44, 84, 195], "extract": [43, 44, 49, 187], "becaus": [43, 44, 78, 151, 156, 158, 160, 164, 194, 233, 234, 241], "against": [43, 44, 181, 223, 241, 242], "unit": [43, 44, 208, 228], "row": [43, 44, 152, 156, 158, 159, 164, 234], "final": [43, 44, 67, 68, 69, 74, 78, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 133, 134, 140, 143, 144, 149, 156, 158, 164, 170, 173, 174, 236, 238, 239, 240, 242], "model_transform": [43, 44, 50, 52, 53, 54, 57], "ref": [43, 44, 47, 52, 57, 136, 137, 213], "filter_fn": [44, 45, 56], "supervis": 44, "round": [44, 241], "involv": [44, 241], "incorpor": [44, 177, 235], "media": 44, "happen": [44, 163], "ti": [44, 78, 140, 144, 156, 240], "agnost": [44, 235], "treat": [44, 160, 170, 234], "modal": [44, 166], "minimum": [44, 52, 57], "filter": [44, 45, 56, 241], "prior": [44, 45, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 219], "add_eo": [45, 56, 73, 84, 100, 120, 132, 139, 185, 186, 234], "freeform": [45, 56], "unstructur": [45, 56, 58], "corpu": [45, 49, 56, 58], "tabular": [45, 56], "txt": [45, 56, 139, 148, 210, 235, 237], "eo": [45, 56, 120, 132, 137, 185, 188, 234, 235], "yahma": 46, "packeddataset": [46, 47, 48, 50, 51, 52, 53, 54, 56, 57, 58, 235], "variant": [46, 50, 53], "version": [46, 59, 78, 91, 105, 114, 121, 123, 133, 140, 152, 223, 227, 234, 238, 241, 242], "page": [46, 58, 227, 228, 233, 237, 238, 240], "tatsu": 47, "lab": 47, "codebas": [47, 236], "independ": 47, "alpacatomessag": 47, "alpaca_d": 47, "conversation_column": 48, "conversation_styl": [48, 235], "altern": [48, 51, 237, 240], "friendli": [48, 51, 56, 59, 234], "similar": [48, 49, 52, 55, 56, 57, 58, 173, 177, 235, 236, 238, 239, 242], "toward": [48, 178, 181], "my_dataset": [48, 51], "london": [48, 51], "am": [48, 51, 83, 119, 234, 235, 236, 238], "ccdv": 49, "cnn_dailymail": 49, "textcompletiondataset": [49, 56, 58, 235], "cnn": 49, "dailymail": 49, "articl": [49, 58], "highlight": [49, 242], "anyth": 49, "liweili": 50, "c4_200m": 50, "grammarerrorcorrectiontempl": 50, "conjunct": [50, 53, 55], "inputoutputtomessag": [50, 53], "grammar_d": 50, "liuhaotian": 52, "llava": 52, "150k": 52, "llava_instruct_150k": 52, "coco": 52, "2017": 52, "visit": [52, 236], "cocodataset": 52, "org": [52, 64, 65, 66, 67, 68, 70, 71, 72, 78, 79, 80, 81, 82, 84, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 107, 108, 109, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 139, 152, 153, 154, 160, 175, 177, 178, 179, 180, 181, 190, 191, 206, 212, 215, 217, 222, 227], "wget": 52, "zip": [52, 224], "train2017": 52, "unzip": 52, "minim": [52, 57, 235, 237, 239, 241, 242], "purpos": [52, 57, 237, 238], "clip": [52, 57, 60, 61, 62, 63, 160, 179], "clipimagetransform": [52, 57, 160], "mymodeltransform": [52, 57], "tokenizer_path": [52, 57, 73, 84, 120, 132], "image_transform": [52, 57], "__call__": [52, 57], "align": [52, 57, 177, 234], "llava_instruct_d": 52, "samsung": 53, "samsum_d": 53, "open": [54, 75, 76, 235, 236], "orca": 54, "slimorca": 54, "dedup": 54, "351": 54, "82": 54, "391": 54, "221": 54, "220": 54, "193": 54, "471": 54, "lvwerra": [55, 235], "stack": [55, 160, 218, 235], "exchang": [55, 235], "preferencedataset": [55, 235], "questionanswertempl": 55, "allenai": [56, 235, 241], "c4": [56, 235, 241], "data_dir": [56, 235], "realnewslik": [56, 235], "huggingfacem4": 57, "the_cauldron": 57, "cauldron": 57, "card": [57, 84, 100], "cauldron_d": 57, "ai2d": 57, "wikitext_document_level": 58, "wikitext": [58, 241], "103": [58, 236], "wikipedia": 58, "transformerdecod": [59, 64, 65, 66, 67, 68, 69, 70, 71, 72, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 133, 134, 135, 136, 138, 140, 143, 144, 147, 156, 164, 165, 166, 239], "max_generated_token": 59, "temperatur": [59, 177, 178, 180, 181, 236], "top_k": [59, 236], "stop_token": [59, 182], "custom_generate_next_token": 59, "condit": [59, 206, 233, 235], "seq_length": [59, 152, 157, 159, 165, 166], "bsz": [59, 60, 61, 160, 163], "predict": [59, 175, 176, 179, 229], "prune": [59, 242], "probabl": [59, 67, 69, 78, 91, 94, 105, 114, 121, 123, 133, 140, 169, 177, 178, 179, 180, 181, 236], "stop": [59, 182], "compil": [59, 163, 236, 238, 240, 242], "generate_next_token": 59, "llama3_8b": [59, 107, 164, 238, 241], "hi": [59, 234], "jeremi": 59, "m": [59, 161, 234, 241], "max_num_til": [60, 61, 63, 160], "tile": [60, 61, 62, 63, 160, 190], "patch": [60, 61, 62, 63, 160, 190], "check": [60, 61, 62, 63, 156, 157, 158, 159, 160, 164, 166, 173, 199, 206, 223, 226, 228, 229, 230, 231, 234, 236, 237, 239, 240], "document": [60, 61, 62, 63, 152, 178, 191, 200, 208, 229, 231, 233, 235, 240], "vision_transform": [60, 61, 62, 63], "visiontransform": [60, 61, 62, 63], "divid": [60, 61, 62, 63, 160, 190], "dimension": [60, 61, 62, 63, 160], "aspect_ratio": [60, 61, 160], "n_img": [60, 61, 160], "n_tile": [60, 61, 160], "n_token": [60, 61, 62, 160], "aspect": [60, 61, 228], "ratio": [60, 61, 177, 178, 179], "crop": [60, 61, 62, 63, 160], "tile_s": [61, 62, 63, 160, 190], "patch_siz": [61, 62, 63, 160, 190], "local_token_positional_embed": 61, "_position_embed": [61, 160], "tokenpositionalembed": [61, 160], "gate": [61, 155, 195, 229, 230, 233, 237], "global_token_positional_embed": 61, "advanc": [61, 62, 63, 160, 235], "40": [61, 62, 63, 139, 160, 190, 240, 242], "400": [61, 62, 63, 160, 190], "10x10": [61, 62, 63, 160, 190], "grid": [61, 62, 63, 160, 190], "k": [61, 152, 239], "th": 61, "silu": [63, 149], "cls_output_dim": [63, 160], "attn_bia": 63, "out_indic": [63, 160], "output_cls_project": 63, "in_channel": [63, 160], "intermediate_act": 63, "transformerencoderlay": 63, "cl": [63, 160, 235], "head": [63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 154, 156, 158, 164, 167, 195, 238], "mlp": [63, 67, 68, 69, 74, 78, 79, 80, 85, 91, 92, 93, 94, 95, 101, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 125, 127, 133, 134, 135, 140, 141, 142, 143, 144, 156, 157, 158, 159, 173, 174, 238, 239, 240], "boolean": [63, 152, 156, 157, 158, 159, 164, 166, 191, 203], "bia": [63, 168, 169, 219, 239, 241, 242], "intermedi": [63, 74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 160, 194, 215, 238, 242], "fourth": [63, 160], "determin": [63, 174], "channel": [63, 160, 241], "assertionerror": [63, 156, 157, 158, 173, 174, 219], "divis": [63, 153], "code_llama2": [64, 65, 66, 67, 68, 69, 70, 71, 72, 233], "w": [64, 65, 66, 75, 76, 86, 87, 88, 89, 102, 103, 111, 112, 113, 126, 128, 145, 146, 147, 160, 209, 212, 213, 234, 236, 239, 242], "arxiv": [64, 65, 66, 67, 68, 70, 71, 72, 78, 79, 80, 81, 82, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 107, 108, 109, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 152, 153, 154, 160, 175, 177, 178, 179, 180, 181, 190], "pdf": [64, 65, 66, 175, 190], "2308": [64, 65, 66], "12950": [64, 65, 66], "lora_attn_modul": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 173, 174, 229, 239, 240, 242], "q_proj": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 152, 173, 174, 229, 239, 240, 241, 242], "k_proj": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 152, 173, 174, 229, 239, 240, 241, 242], "v_proj": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 152, 173, 174, 229, 239, 240, 241, 242], "output_proj": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 152, 173, 174, 239, 240, 241, 242], "apply_lora_to_mlp": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 173, 174, 229, 239, 240], "apply_lora_to_output": [67, 68, 69, 70, 71, 72, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 143, 173, 174, 239, 240], "lora_rank": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 229, 239, 240], "lora_alpha": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 229, 239, 240], "lora_dropout": [67, 68, 69, 70, 71, 72, 78, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 114, 121, 123, 133, 140, 141, 142, 143, 240], "05": [67, 68, 69, 70, 71, 72, 85, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 141, 142, 143, 144], "use_dora": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143], "quantize_bas": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 169, 242], "code_llama2_13b": 67, "tloen": [67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143], "8bb8579e403dc78e37fe81ffbb253c413007323f": [67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143], "l41": [67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143], "l43": [67, 68, 69, 79, 80, 92, 93, 94, 95, 106, 107, 115, 116, 134, 141, 142, 143], "linear": [67, 68, 69, 70, 71, 72, 78, 79, 80, 81, 82, 91, 92, 93, 94, 95, 96, 97, 98, 99, 105, 106, 107, 108, 109, 114, 115, 116, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 140, 141, 142, 143, 158, 168, 169, 173, 174, 239, 240, 241, 242], "low": [67, 68, 69, 78, 79, 80, 91, 92, 93, 94, 95, 105, 106, 107, 114, 115, 116, 121, 122, 123, 124, 133, 134, 140, 141, 142, 143, 169, 229, 236, 239, 242], "approxim": [67, 68, 69, 78, 79, 80, 91, 92, 93, 94, 95, 105, 106, 107, 114, 115, 116, 121, 122, 123, 124, 133, 134, 140, 141, 142, 143, 169, 239], "factor": [67, 68, 69, 78, 79, 80, 91, 92, 93, 94, 95, 105, 106, 107, 110, 114, 115, 116, 121, 122, 123, 124, 133, 134, 140, 141, 142, 143, 169, 175, 236], "dropout": [67, 69, 74, 78, 85, 91, 94, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 169, 239, 240, 242], "decompos": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134], "magnitud": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134, 240], "introduc": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134, 152, 153, 166, 169, 181, 230, 234, 235, 239, 240, 241, 242], "dora": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134], "ab": [67, 68, 70, 71, 72, 78, 79, 80, 81, 82, 86, 87, 88, 89, 91, 92, 93, 94, 96, 97, 98, 99, 105, 106, 107, 108, 109, 117, 118, 121, 122, 123, 124, 130, 131, 133, 134, 138, 152, 153, 154, 160, 177, 178, 179, 180, 181], "2402": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134], "09353": [67, 68, 78, 79, 80, 91, 92, 93, 94, 105, 106, 107, 121, 122, 123, 124, 133, 134], "code_llama2_70b": 68, "code_llama2_7b": 69, "qlora": [70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 161, 226, 228, 229, 238, 239], "paper": [70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 177, 178, 180, 181, 190, 239, 242], "2305": [70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138, 152, 177, 180], "14314": [70, 71, 72, 81, 82, 96, 97, 98, 99, 108, 109, 117, 118, 130, 131, 138], "lora_code_llama2_13b": 70, "lora_code_llama2_70b": 71, "lora_code_llama2_7b": 72, "gemma": [73, 75, 76, 77, 78, 79, 80, 81, 82, 195, 240], "prompt_templ": [73, 77, 84, 90, 100, 104, 120, 129, 132, 137, 139, 148], "sentencepiec": [73, 84, 120, 132, 185, 238], "pretrain": [73, 84, 100, 120, 132, 164, 166, 167, 185, 186, 233, 234, 237, 239, 242], "gear": [73, 84, 100, 120, 132, 139], "whenev": [73, 84, 100, 120, 132, 139, 239], "commun": [73, 84, 100, 120, 132, 139, 234, 235, 236], "chatmltempl": [73, 84, 100, 120, 132, 139, 148], "spm_model": [73, 84, 120, 132, 185, 234], "tokenized_text": [73, 84, 100, 120, 132, 139, 185, 186], "add_bo": [73, 84, 100, 120, 132, 139, 185, 186, 234], "31587": [73, 84, 100, 120, 132, 185, 186], "29644": [73, 84, 100, 120, 132, 185, 186], "102": [73, 84, 100, 120, 132, 185, 186], "concat": [73, 84, 120, 132, 188], "1788": [73, 84, 120, 132, 188], "2643": [73, 84, 120, 132, 188], "1792": [73, 84, 120, 132, 188], "9508": [73, 84, 120, 132, 188], "465": [73, 84, 120, 132, 188], "22137": [73, 84, 120, 132, 188], "2933": [73, 84, 120, 132, 188], "join": [73, 84, 120, 132, 188], "attribut": [73, 84, 120, 132, 170, 181, 188, 198], "head_dim": [74, 78, 151, 152, 156, 158], "intermediate_dim": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144], "attn_dropout": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 156, 158], "norm_ep": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144], "1e": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 153], "06": [74, 78, 153, 239], "rope_bas": [74, 78, 85, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144], "10000": [74, 78, 85, 121, 123, 125, 127, 133, 135, 154], "norm_embed": [74, 78], "gemmatransformerdecod": [74, 75, 76, 78, 79, 80, 81, 82], "transformerselfattentionlay": [74, 85, 101, 110, 125, 144, 157, 164, 166], "rm": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144], "norm": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 156, 158, 159], "space": [74, 85, 101, 110, 125, 139, 144, 156, 158, 167, 240], "slide": [74, 125, 136], "window": [74, 125, 136, 235], "vocabulari": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 139, 140, 144, 163, 239, 240], "queri": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 151, 152, 156, 158, 159, 164, 238, 240], "mha": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 156, 158], "onto": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152, 167], "scaled_dot_product_attent": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152], "epsilon": [74, 78, 85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 179], "rotari": [74, 78, 85, 121, 123, 125, 127, 133, 135, 154, 238], "10_000": [74, 78, 121, 123, 125, 127, 135], "blog": [75, 76], "technolog": [75, 76], "develop": [75, 76, 242], "gemmatoken": 77, "_templatetyp": [77, 90, 104, 129, 137, 148], "prompttemplateinterfac": [77, 90, 104, 129, 137, 148], "gemma_2b": 79, "gemma_7b": 80, "lora_gemma_2b": 81, "lora_gemma_7b": 82, "taken": [83, 239, 242], "sy": [83, 84, 234, 235], "honest": [83, 234, 235], "pari": [83, 119, 235], "capit": [83, 119, 235], "franc": [83, 119, 235], "known": [83, 84, 119, 120, 202, 235, 241], "stun": [83, 119, 235], "llama2chattempl": [84, 90, 119, 148], "describ": [84, 100, 215, 235], "regist": [84, 100, 104, 132, 137, 148, 149, 161, 214, 242], "html": [84, 139, 191, 206, 212, 215, 217, 222, 226], "problem": [84, 120], "due": [84, 120, 185, 229, 239, 240, 242], "whitespac": [84, 120, 185], "slice": [84, 120], "gqa": [85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152], "mqa": [85, 91, 101, 105, 110, 114, 121, 123, 125, 127, 133, 135, 140, 144, 152], "kvcach": [85, 91, 101, 105, 110, 114, 133, 140, 144, 152, 156, 158], "scale_hidden_dim_for_mlp": [85, 91, 101, 105, 110, 114, 140, 144], "2307": [86, 87, 88, 89], "09288": [86, 87, 88, 89], "classif": [89, 123, 127, 128, 195], "reward": [89, 95, 99, 124, 128, 131, 175, 176, 177, 178, 180, 181, 195], "llama2_70b": 93, "llama2_7b": [94, 239], "classifi": [95, 123, 127, 128, 219, 235, 240], "llama2_reward_7b": [95, 195], "lora_llama2_13b": 96, "lora_llama2_70b": 97, "lora_llama2_7b": [98, 239], "lora_llama2_reward_7b": 99, "special_token": [100, 132, 139, 186, 234], "tiktoken": [100, 186, 238], "canon": [100, 104, 132, 137, 148], "tt_model": [100, 186], "token_id": [100, 120, 139, 183, 186], "truncate_at_eo": [100, 186], "skip_special_token": [100, 139, 186], "show": [100, 186, 190, 227, 229, 230, 233, 234, 239], "skip": [100, 152, 186], "add_start_token": 100, "add_end_token": 100, "header": [100, 234], "eom": 100, "seq": [100, 156, 158, 164], "500000": [101, 105, 110, 114], "special_tokens_path": [104, 137, 148], "llama3token": [104, 234], "similarli": [104, 137, 148, 235, 241], "llama3_70b": 106, "lora_llama3_70b": 108, "lora_llama3_8b": 109, "scale_factor": 110, "rope": [110, 140, 144, 152, 154], "llama3_1": [111, 112, 113, 114, 115, 116, 117, 118, 229], "rtype": 111, "llama3_1_70b": 115, "llama3_1_8b": 116, "lora_llama3_1_70b": 117, "lora_llama3_1_8b": 118, "yet": [119, 234, 236], "mistralchattempl": [120, 129], "trim_leading_whitespac": [120, 185], "unbatch": [120, 185], "bo": [120, 137, 185, 188, 234, 235], "trim": [120, 185], "num_class": [123, 127, 219], "announc": 126, "ray2333": 128, "feedback": [128, 177], "mistraltoken": [129, 234], "lora_mistral_7b": 130, "lora_mistral_reward_7b": 131, "ignore_system_prompt": 132, "phi3_mini": [134, 195], "phi": [136, 137, 195], "128k": 136, "nor": 136, "phi3minitoken": 137, "tokenizer_config": 137, "spm": 137, "lm": [137, 179], "unk": 137, "augment": [137, 242], "endoftext": [137, 139], "phi3minisentencepiecebasetoken": 137, "lora_phi3_mini": 138, "merges_fil": [139, 148], "unk_token": 139, "bos_token": 139, "eos_token": 139, "pad_token": 139, "bpe_cache_s": 139, "151646": 139, "bpe": 139, "v4": [139, 162], "src": [139, 162], "tokenization_qwen2": 139, "word": [139, 140, 144, 178, 241], "utf": 139, "librari": [139, 177, 178, 180, 199, 217, 222, 226, 227, 228, 233, 235, 240, 242], "stdtype": 139, "unknown": 139, "cach": [139, 151, 152, 154, 156, 157, 158, 159, 164, 166, 227, 233], "speed": [139, 186, 218, 238, 240, 241, 242], "realli": [139, 236], "esp": 139, "chines": 139, "technic": [139, 231, 237], "leak": 139, "appear": [139, 240], "equal": [139, 190, 223], "39": [139, 160], "385": 139, "78": 139, "675": 139, "2000": [139, 241], "41": [139, 160], "tokenization_util": 139, "l541": 139, "l262": 139, "wether": 139, "runtimeerror": [139, 188, 196, 199, 205], "1000000": [140, 144], "tie_word_embed": [140, 141, 142, 144, 145, 146], "tiedembeddingtransformerdecod": [140, 141, 142, 144, 145, 146], "qwen2transformerdecod": 140, "period": [140, 144], "qwen2_0_5b": 141, "qwen2_1_5b": 142, "qwen2_7b": 143, "qwen": [145, 146, 147], "qwen2token": 148, "gate_proj": 149, "down_proj": 149, "up_proj": 149, "feed": [149, 157, 159], "network": [149, 170, 239, 242], "fed": [149, 234], "multipli": [149, 240], "subclass": 149, "although": [149, 239, 241], "afterward": 149, "former": 149, "hook": [149, 161, 214, 242], "latter": 149, "layernorm": 150, "standalon": 151, "kv": [151, 152, 156, 158, 164, 241], "past": 151, "expand": 151, "dpython": [151, 152, 156, 157, 158, 159, 161, 164, 166, 216, 220], "reset": [151, 152, 156, 157, 158, 159, 164, 166, 201], "k_val": 151, "v_val": 151, "assert": 151, "longer": [151, 235, 240], "h": [151, 160, 163, 227, 233], "pos_embed": [152, 157, 239, 241], "q_norm": 152, "k_norm": 152, "kv_cach": 152, "is_caus": 152, "13245v1": 152, "multihead": 152, "extrem": 152, "share": [152, 235, 236], "credit": 152, "lightn": 152, "lit": 152, "lit_gpt": 152, "v": [152, 156, 158, 164, 239], "q": [152, 239], "n_kv_head": 152, "rotarypositionalembed": [152, 239, 241], "rmsnorm": 152, "vice": [152, 233], "versa": [152, 233], "y": 152, "s_x": 152, "second": [152, 165, 236, 239, 240, 242], "s_y": 152, "softmax": [152, 156, 158, 159, 164], "j": [152, 156, 157, 158, 159, 164], "n_h": [152, 154], "num": [152, 154], "n_kv": 152, "emb": [152, 156, 157, 158, 164], "h_d": [152, 154], "reset_cach": [152, 156, 157, 158, 159, 164, 166], "setup_cach": [152, 156, 157, 158, 159, 164, 166], "ep": 153, "root": [153, 212, 213], "squar": 153, "1910": 153, "07467": 153, "verfic": [153, 154], "small": [153, 236], "propos": [154, 240], "2104": 154, "09864": 154, "l80": 154, "upto": 154, "init": [154, 201, 213, 242], "exceed": 154, "freq": 154, "recomput": [154, 240], "geometr": 154, "progress": [154, 237, 240], "rotat": 154, "angl": 154, "todo": 154, "effici": [154, 173, 200, 226, 228, 229, 236, 237, 239, 241], "basic": [155, 238], "learnabl": [155, 164, 166, 236], "output_hidden_st": [156, 158, 164], "belong": [156, 158, 198], "reduc": [156, 158, 177, 228, 229, 230, 235, 239, 240, 241, 242], "statement": [156, 158], "improv": [156, 158, 180, 186, 200, 230, 238, 239, 240], "readabl": [156, 158, 236], "caches_are_en": [156, 158, 164], "encoder_input": [156, 157, 158, 164], "encoder_mask": [156, 157, 158, 164], "s_e": [156, 158, 164], "d_e": [156, 158, 164], "relat": [156, 157, 158, 164, 239], "arang": [156, 158, 164], "prompt_length": [156, 158, 164], "seq_len": [156, 158], "bigger": [156, 158], "mode": [156, 158, 197, 202, 209, 236], "m_": [156, 158, 164], "set_num_output_chunk": [156, 158], "num_output_chunk": [156, 158, 163], "combin": [156, 158, 164, 166, 167, 176], "cewithchunkedoutputloss": [156, 158], "attn": [157, 159, 239, 241, 242], "multiheadattent": [157, 159, 239, 241], "ca_norm": 157, "mlp_norm": [157, 159], "ca_scal": 157, "mlp_scale": [157, 159], "convent": 157, "ff": [157, 159], "cache_en": [157, 159, 166], "token_sequ": 157, "embed_sequ": 157, "sa_norm": 159, "sa_scal": 159, "token_pos_embed": 160, "pre_tile_pos_emb": 160, "post_tile_pos_emb": 160, "cls_project": 160, "vit": 160, "11929": 160, "convolut": 160, "flatten": 160, "downscal": 160, "800x400": 160, "400x400": 160, "_transform": 160, "broken": [160, 166], "down": [160, 194, 235, 239, 240, 242], "whole": 160, "num_til": 160, "101": 160, "pool": 160, "tiledtokenpositionalembed": 160, "tilepositionalembed": 160, "tile_pos_emb": 160, "even": [160, 219, 227, 233, 234, 235, 238, 239, 240, 242], "8x8": 160, "21": 160, "22": 160, "23": [160, 162], "24": [160, 237, 238], "25": [160, 236], "26": 160, "27": [160, 236], "28": [160, 236], "29": [160, 242], "30": [160, 182, 241], "31": [160, 238], "33": 160, "34": 160, "35": [160, 242], "36": 160, "37": 160, "38": [160, 236], "42": 160, "43": 160, "44": 160, "45": 160, "46": 160, "47": 160, "48": [160, 236, 242], "49": 160, "50": [160, 182, 209, 236], "51": 160, "52": [160, 237], "53": 160, "54": 160, "55": [160, 237], "56": 160, "57": [160, 239, 242], "58": 160, "59": [160, 242], "60": 160, "61": [160, 236], "62": 160, "63": 160, "64": [160, 229, 239], "num_patches_per_til": 160, "emb_dim": 160, "greater": [160, 223], "constain": 160, "anim": [160, 235], "max_n_img": 160, "n_channel": 160, "hidden_st": 160, "vision_util": 160, "tile_crop": 160, "num_channel": 160, "image_s": 160, "800": 160, "patch_grid_s": 160, "random": [160, 217, 237], "rand": 160, "nch": 160, "tile_cropped_imag": 160, "batch_imag": 160, "unsqueez": 160, "batch_aspect_ratio": 160, "clip_vision_encod": 160, "common_util": 161, "bfloat16": [161, 216, 236, 237, 238, 239, 240, 241], "offload_to_cpu": 161, "nf4": [161, 240, 242], "restor": 161, "higher": [161, 178, 229, 238, 240, 241, 242], "offload": [161, 242], "increas": [161, 162, 177, 229, 238, 239, 240, 241], "peak": [161, 201, 207, 236, 238, 239, 242], "gpu": [161, 230, 233, 236, 237, 238, 239, 241, 242], "_register_state_dict_hook": 161, "mymodul": 161, "_after_": 161, "nf4tensor": [161, 242], "unquant": [161, 241, 242], "unus": 161, "num_warmup_step": 162, "num_training_step": 162, "num_cycl": [162, 218], "last_epoch": 162, "lambdalr": 162, "rate": [162, 228, 237, 240], "schedul": [162, 218, 237, 240], "linearli": 162, "decreas": [162, 235, 239, 240, 241, 242], "cosin": 162, "l104": 162, "warmup": [162, 218], "phase": 162, "wave": 162, "half": [162, 240], "lr_schedul": 162, "ignore_index": 163, "ce": 163, "chunk": [163, 186], "upcast": 163, "bf16": [163, 199, 240, 242], "better": [163, 181, 228, 234, 235, 236, 240, 241], "accuraci": [163, 229, 230, 236, 238, 239, 240, 241, 242], "doubl": [163, 242], "therefor": [163, 242], "num_token": 163, "entropi": 163, "consider": 163, "compute_cross_entropi": 163, "gain": [163, 230, 238], "won": [163, 234], "realiz": 163, "pull": [163, 173, 229, 230, 233], "1390": 163, "ground": [163, 240], "loss_fn": 163, "chunkedcrossentropyloss": 163, "output_chunk": 163, "num_chunk": 163, "model_fus": [164, 165, 166, 167], "deepfus": 164, "evolut": 164, "signatur": 164, "interchang": 164, "fusion_param": [164, 165, 166, 167], "fusionembed": 164, "fusionlay": 164, "fusion_lay": [164, 166], "transformercrossattentionlay": [164, 166], "clip_vit_224": [164, 167], "projection_head": [164, 167], "feedforward": [164, 167], "register_fusion_modul": 164, "sequenti": [164, 167], "flamingo": [164, 166, 190], "Or": 164, "strict": [164, 165, 166, 173, 239], "fusion_vocab_s": 165, "fusion": [165, 166, 167], "necessit": 165, "rout": 165, "drop": [165, 241], "128": [165, 229, 238, 239, 240], "fusion_first": 166, "visual": 166, "few": [166, 235, 238, 239, 242], "shot": [166, 236, 238, 241], "infus": 166, "interpret": [166, 235], "enocd": 166, "isn": [166, 199, 233], "trainabl": [166, 169, 172, 208, 239, 240, 242], "fused_lay": 166, "insert": [166, 241], "mark": 167, "earli": 167, "peft": [168, 169, 170, 171, 172, 173, 174, 192, 229, 239, 242], "protocol": 168, "adapter_param": [168, 169, 170, 171, 172], "proj": 168, "in_dim": [168, 169, 239, 240, 242], "out_dim": [168, 169, 239, 240, 242], "loralinear": [168, 239, 242], "alpha": [169, 239, 240, 242], "use_bia": 169, "perturb": 169, "decomposit": [169, 239, 240], "matric": [169, 239, 242], "mapsto": 169, "w_0x": 169, "r": [169, 239], "bax": 169, "lora_a": [169, 239, 242], "lora_b": [169, 239, 242], "temporarili": 170, "neural": [170, 239, 242], "polici": [170, 176, 177, 178, 179, 180, 181, 191, 200, 208, 215, 229], "caller": 170, "whose": [170, 209, 214], "yield": 170, "get_adapter_param": [172, 239], "base_miss": 173, "base_unexpect": 173, "lora_miss": 173, "lora_unexpect": 173, "validate_state_dict_for_lora": [173, 239], "unlik": [173, 178], "reli": [173, 188, 236, 238], "unexpect": 173, "120600": 173, "nonempti": 173, "full_model_state_dict_kei": 174, "lora_state_dict_kei": 174, "base_model_state_dict_kei": 174, "confirm": [174, 227], "lora_modul": 174, "complement": 174, "disjoint": 174, "overlap": 174, "gamma": [175, 180, 181], "lmbda": 175, "estim": [175, 176], "1506": 175, "02438": 175, "reponse_len": [175, 176], "receiv": [175, 234], "discount": 175, "gae": 175, "lambda": 175, "particip": [175, 190], "score": 176, "logprob": [176, 178, 181], "ref_logprob": 176, "kl_coeff": 176, "valid_score_idx": 176, "kl": 176, "coeffici": [176, 179], "response_len": 176, "total_reward": 176, "diverg": 176, "kl_reward": 176, "beta": [177, 181], "label_smooth": [177, 181], "18290": 177, "intuit": [177, 178, 180, 181], "dispref": 177, "dynam": [177, 241], "degener": 177, "occur": [177, 230], "naiv": 177, "trl": [177, 178, 180, 181], "5d1deb1445828cfd0e947cb3a7925b1c03a283fc": 177, "dpo_train": [177, 178, 180], "l844": 177, "retain": [177, 240, 242], "2009": 177, "01325": 177, "regular": [177, 178, 181, 240, 241, 242], "baselin": [177, 179, 236, 239], "rather": [177, 240], "overhead": [177, 230, 240, 241], "uncertainti": [177, 181], "policy_chosen_logp": [177, 178, 180, 181], "policy_rejected_logp": [177, 178, 180, 181], "reference_chosen_logp": [177, 178, 180], "reference_rejected_logp": [177, 178, 180], "chosen_reward": [177, 178, 180, 181], "rejected_reward": [177, 178, 180, 181], "tau": 178, "optimis": 178, "ipo": 178, "2310": 178, "12036": 178, "pi": 178, "pi_ref": 178, "regress": [178, 180], "gap": 178, "likelihood": 178, "he": 178, "weaker": 178, "regularis": 178, "4dce042a3863db1d375358e8c8092b874b02934b": [178, 180], "l1143": 178, "reciproc": 178, "larger": [178, 181, 194, 236, 238, 240], "value_clip_rang": 179, "value_coeff": 179, "proxim": 179, "1707": 179, "06347": 179, "eqn": 179, "vwxyzjn": 179, "ccc19538e817e98a60d3253242ac15e2a562cb49": 179, "lm_human_preference_detail": 179, "train_policy_acceler": 179, "l719": 179, "ea25b9e8b234e6ee1bca43083f8f3cf974143998": 179, "ppo2": 179, "l68": 179, "l75": 179, "pi_old_logprob": 179, "pi_logprob": 179, "phi_old_valu": 179, "phi_valu": 179, "padding_mask": [179, 182], "value_padding_mask": 179, "old": 179, "participag": 179, "five": 179, "policy_loss": 179, "value_loss": 179, "clipfrac": 179, "fraction": 179, "statist": [180, 240], "rso": 180, "hing": 180, "2309": 180, "06657": 180, "logist": 180, "slic": 180, "10425": 180, "almost": [180, 239], "vector": [180, 234], "svm": 180, "counter": 180, "l1141": 180, "simpo": 181, "free": [181, 239], "2405": 181, "14734": 181, "averag": 181, "implicit": 181, "margin": 181, "bradlei": 181, "terri": 181, "win": 181, "lose": 181, "98ad01ddfd1e1b67ec018014b83cba40e0caea66": 181, "cpo_train": 181, "l603": 181, "pretti": [181, 236], "identitc": 181, "elimin": 181, "kind": 181, "ipoloss": 181, "fill_valu": 182, "sequence_length": 182, "pad_id": 182, "been": [182, 200, 234, 241], "stop_token_id": 182, "869": 182, "eos_mask": 182, "truncated_sequ": 182, "light": 185, "sentencepieceprocessor": 185, "prefix": [185, 240], "bos_id": [186, 188], "lightweight": [186, 234], "break": 186, "substr": 186, "repetit": 186, "identif": 186, "regex": 186, "absent": 186, "tokenizer_json_path": 187, "heavili": 188, "beggin": 188, "satisfi": [188, 236], "loos": 189, "image_token_id": 190, "encoder_max_seq_len": 190, "laid": 190, "fig": 190, "2204": 190, "14198": 190, "immedi": [190, 240], "until": [190, 240], "img1": 190, "img2": 190, "img3": 190, "dog": 190, "cat": 190, "text_seq_len": 190, "image_seq_len": 190, "datatyp": [191, 240, 242], "denot": 191, "auto_wrap_polici": [191, 200, 215], "submodul": [191, 208], "obei": 191, "contract": 191, "get_fsdp_polici": 191, "modules_to_wrap": [191, 200, 208], "min_num_param": 191, "my_fsdp_polici": 191, "recurs": [191, 208, 212], "isinst": [191, 235], "sum": [191, 239], "p": [191, 196, 239, 241, 242], "numel": [191, 239], "1000": [191, 241], "stabl": [191, 206, 212, 217, 227, 240], "safe_seri": 192, "from_pretrain": 192, "0001_of_0003": 192, "0002_of_0003": 192, "preserv": [192, 242], "weight_map": [192, 236], "convert_weight": 192, "_model_typ": [192, 195], "intermediate_checkpoint": [192, 193, 194], "adapter_onli": [192, 193, 194], "_weight_map": 192, "shard": [193, 238], "wip": 193, "qualnam": 195, "boundari": 195, "distinguish": 195, "mistral_reward_7b": 195, "my_new_model": 195, "my_custom_state_dict_map": 195, "optim_map": 196, "bare": 196, "bone": 196, "distribut": [196, 205, 206, 215, 217, 221, 228, 231, 233, 237, 238, 240], "optim_dict": [196, 198, 214], "cfg_optim": 196, "ckpt": 196, "optim_ckpt": 196, "placeholder_optim_dict": 196, "optiminbackwardwrapp": 196, "get_optim_kei": 196, "arbitrari": [196, 239, 240], "optim_ckpt_map": 196, "loadabl": 196, "ac_mod": 197, "ac_opt": 197, "select": 197, "op": [197, 241], "ac": [197, 200], "optimizerinbackwardwrapp": 198, "top": [198, 240, 242], "named_paramet": [198, 219], "float32": 199, "inde": [199, 236], "kernel": 199, "hardwar": [199, 228, 235, 236, 239, 240], "memory_efficient_fsdp_wrap": [200, 241], "maxim": [200, 208, 226, 228], "workload": [200, 230, 241], "alongsid": 200, "fullyshardeddataparallel": [200, 208], "fsdppolicytyp": [200, 208], "reset_stat": 201, "track": [201, 209], "alloc": [201, 207, 208, 238, 242], "reserv": [201, 207, 234, 242], "stat": [201, 207, 242], "int4": [202, 241], "4w": 202, "recogn": 202, "int8dynactint4weightquant": [202, 230, 241], "8da4w": [202, 241], "int8dynactint4weightqatquant": [202, 230, 241], "qat": [202, 226], "exclud": 203, "get_last_unmasked_token_idx": 203, "aka": 204, "master": 206, "port": [206, 233], "address": [206, 240], "hold": [206, 237], "peak_memory_act": 207, "peak_memory_alloc": 207, "peak_memory_reserv": 207, "get_memory_stat": 207, "hierarch": 208, "api_kei": 209, "experiment_kei": 209, "onlin": [209, 234], "log_cod": 209, "comet": 209, "www": 209, "site": [209, 235, 236], "ml": 209, "team": 209, "compar": [209, 212, 223, 236, 238, 239, 241, 242], "sdk": 209, "uncategor": 209, "alphanumer": 209, "charact": 209, "get_or_cr": 209, "fresh": 209, "persist": 209, "hpo": 209, "sweep": 209, "server": 209, "offlin": 209, "auto": [209, 233], "creation": 209, "experimentconfig": 209, "project_nam": 209, "my_project": [209, 213], "my_workspac": 209, "my_metr": [209, 212, 213], "importerror": [209, 213], "termin": [209, 212, 213], "comet_api_kei": 209, "flush": [209, 210, 211, 212, 213], "ndarrai": [209, 210, 211, 212, 213], "scalar": [209, 210, 211, 212, 213], "record": [209, 210, 211, 212, 213, 218], "log_config": [209, 213], "payload": [209, 210, 211, 212, 213], "filenam": 210, "log_": 210, "unixtimestamp": 210, "thread": 210, "safe": 210, "organize_log": 212, "tensorboard": 212, "subdirectori": 212, "logdir": 212, "startup": 212, "tree": [212, 235, 236, 238], "tfevent": 212, "encount": 212, "frontend": 212, "organ": [212, 233], "accordingli": [212, 241], "my_log_dir": 212, "view": 212, "entiti": 213, "bias": [213, 239, 242], "sent": 213, "usernam": 213, "my_ent": 213, "my_group": 213, "account": [213, 239, 242], "link": [213, 236, 238], "capecap": 213, "6053ofw0": 213, "torchtune_config_j67sb73v": 213, "soon": [214, 231, 240], "readi": [214, 226, 234, 241], "grad": 214, "acwrappolicytyp": 215, "author": [215, 228, 237, 240, 242], "fsdp_adavnced_tutori": 215, "insid": 216, "contextmanag": 216, "debug_mod": 217, "pseudo": 217, "commonli": [217, 239, 240, 242], "numpi": 217, "determinist": 217, "global": [217, 235, 240], "warn": 217, "nondeterminist": 217, "cudnn": 217, "set_deterministic_debug_mod": 217, "profile_memori": 218, "with_stack": 218, "record_shap": 218, "with_flop": 218, "wait_step": 218, "warmup_step": 218, "active_step": 218, "profil": 218, "layout": 218, "trace": 218, "profileract": 218, "gradient_accumul": 218, "sensibl": 218, "default_schedul": 218, "reduct": [218, 230, 239], "iter": [218, 219, 220, 242], "scope": 218, "flop": 218, "wait": 218, "cycl": 218, "repeat": [218, 240], "model_named_paramet": 219, "force_overrid": 219, "behaviour": 219, "concret": [219, 240], "vocab_dim": 219, "randomli": 219, "place": [219, 234, 235, 240], "named_param": 220, "handler": 222, "__version__": 223, "generated_examples_python": 224, "galleri": [224, 232], "sphinx": 224, "000": [225, 232, 238], "execut": [225, 232], "generated_exampl": 225, "mem": [225, 232], "mb": [225, 232], "topic": 226, "gentl": 226, "introduct": 226, "first_finetune_tutori": 226, "workflow": [226, 229, 235, 237, 239], "proper": [227, 237], "host": [227, 233, 237, 240], "torchvis": 227, "torchao": [227, 230, 236, 238, 241, 242], "latest": [227, 229, 230, 237, 240, 242], "url": 227, "whl": 227, "cu121": 227, "And": [227, 236], "ls": [227, 233, 236, 237, 238], "welcom": [227, 233], "greatest": [227, 237], "contributor": 227, "cd": [227, 236], "commit": 227, "branch": 227, "therebi": [227, 240, 241, 242], "forc": 227, "reinstal": 227, "opt": [227, 237], "suffix": 227, "On": [228, 239], "pointer": 228, "emphas": 228, "simplic": 228, "component": 228, "prove": 228, "democrat": 228, "box": [228, 229, 230, 242], "zoo": 228, "varieti": [228, 231, 239], "integr": [228, 236, 237, 238, 239, 241, 242], "excit": 228, "checkout": 228, "quickstart": 228, "attain": 228, "eager": 228, "embodi": 228, "philosophi": 228, "usabl": 228, "composit": 228, "hard": [228, 235], "outlin": 228, "unecessari": 228, "never": 228, "thoroughli": 228, "competit": 229, "grant": [229, 230, 237], "interest": [229, 230, 231, 236], "8b_lora_single_devic": [229, 233, 234, 238, 240], "adjust": [229, 230, 235, 240, 241], "imapct": 229, "aggress": 229, "tradeoff": [229, 239], "slower": [229, 240, 242], "lever": [229, 230], "too": [229, 230, 238], "action": [229, 230], "degrad": [230, 240, 241, 242], "simul": [230, 240, 241], "compromis": 230, "blogpost": [230, 240], "qat_distribut": [230, 241], "8b_qat_ful": [230, 241], "least": [230, 238, 239, 241], "vram": [230, 238, 239, 241], "80gb": [230, 241], "delai": 230, "fake": [230, 241], "empir": [230, 241], "potenti": [230, 239, 240], "fake_quant_after_n_step": [230, 241], "idea": [230, 242], "roughli": 230, "total_step": 230, "plan": [230, 236], "un": 230, "groupsiz": [230, 241], "256": [230, 238, 241], "hackabl": [231, 237], "singularli": [231, 237], "funetun": 231, "awar": [231, 240, 241], "favourit": 231, "love": 231, "tracker": 231, "issu": [231, 241], "short": 233, "subcommand": 233, "anytim": 233, "symlink": 233, "wrote": 233, "readm": [233, 236, 238], "md": 233, "lot": [233, 236, 240], "recent": 233, "agre": 233, "term": [233, 240], "perman": 233, "eat": 233, "bandwith": 233, "storag": [233, 242], "00030": 233, "ootb": 233, "full_finetune_single_devic": [233, 235, 236, 237], "7b_full_low_memori": [233, 236, 237], "8b_full_single_devic": [233, 235], "mini_full_low_memori": 233, "7b_full": [233, 236, 237], "13b_full": [233, 236, 237], "70b_full": 233, "edit": 233, "clobber": 233, "destin": 233, "lora_finetune_distribut": [233, 238, 239], "torchrun": 233, "launch": [233, 234, 237], "nproc": 233, "node": 233, "worker": 233, "nnode": [233, 239, 241], "minimum_nod": 233, "maximum_nod": 233, "fail": 233, "rdzv": 233, "rendezv": 233, "endpoint": 233, "8b_lora": [233, 238], "bypass": 233, "fancy_lora": 233, "8b_fancy_lora": 233, "sai": [233, 234, 237], "know": [234, 235, 236, 239], "intend": 234, "nice": 234, "meet": 234, "overhaul": 234, "begin_of_text": 234, "start_header_id": 234, "end_header_id": 234, "eot_id": 234, "untrain": 234, "accompani": 234, "who": 234, "influenti": 234, "hip": 234, "hop": 234, "artist": 234, "2pac": 234, "rakim": 234, "na": 234, "llama2chatformat": [234, 235], "flavor": [234, 235], "msg": 234, "formatted_messag": [234, 235], "nyou": [234, 235], "nwho": 234, "why": [234, 237, 239], "518": 234, "25580": 234, "29962": 234, "3532": 234, "14816": 234, "29903": 234, "6778": 234, "_spm_model": 234, "piece_to_id": 234, "manual": [234, 242], "529": 234, "29879": 234, "29958": 234, "nhere": 234, "128000": [234, 241], "128009": 234, "pure": 234, "That": 234, "mess": 234, "govern": 234, "prime": 234, "strictli": 234, "ask": [234, 240], "untouch": 234, "though": 234, "robust": 234, "forum": 234, "panda": 234, "pd": 234, "df": 234, "read_csv": 234, "your_fil": 234, "nrow": 234, "tolist": 234, "iloc": 234, "gp": 234, "satellit": 234, "thing": [234, 240, 242], "message_convert": 234, "input_msg": 234, "output_msg": 234, "But": [234, 236, 239], "mistralchatformat": 234, "chatdataset": [234, 235], "custom_dataset": 234, "2048": 234, "honor": 234, "copi": [234, 236, 237, 238, 241, 242], "custom_8b_lora_single_devic": 234, "steer": 235, "wheel": 235, "publicli": 235, "great": [235, 236, 240], "hood": [235, 242], "text_completion_dataset": [235, 241], "upper": 235, "constraint": [235, 239], "slow": [235, 240, 242], "signific": [235, 240, 241], "speedup": [235, 236, 238], "my_data": 235, "fix": [235, 241], "goal": [235, 241], "respond": 235, "plant": 235, "miner": 235, "oak": 235, "copper": 235, "ore": 235, "eleph": 235, "customtempl": 235, "importlib": 235, "import_modul": 235, "mechan": 235, "search": 235, "often": [235, 239, 240], "runtim": 235, "pythonpath": 235, "quit": [235, 240, 242], "customchatformat": 235, "concatdataset": 235, "drive": 235, "rajpurkar": 235, "io": 235, "squad": 235, "explor": 235, "chosen_messag": 235, "key_chosen": 235, "rejected_messag": 235, "key_reject": 235, "c_mask": 235, "np": 235, "cross_entropy_ignore_idx": 235, "r_mask": 235, "stack_exchanged_paired_dataset": 235, "had": 235, "1024": [235, 241], "stackexchangedpairedtempl": 235, "response_j": 235, "response_k": 235, "rl": 235, "favorit": [236, 239], "seemlessli": 236, "beyond": [236, 240, 242], "connect": [236, 241], "amount": 236, "natur": 236, "export": 236, "mobil": 236, "phone": 236, "leverag": [236, 238, 242], "plai": [236, 240], "freez": [236, 239], "percentag": 236, "16gb": [236, 239], "rtx": 236, "3090": 236, "4090": 236, "hour": 236, "7b_qlora_single_devic": [236, 237, 242], "473": 236, "98": [236, 242], "gb": [236, 238, 239, 241, 242], "484": 236, "01": [236, 237], "fact": [236, 238, 239], "third": 236, "eleuther_ev": [236, 238, 241], "eleuther_evalu": [236, 238, 241], "lm_eval": [236, 238], "custom_eval_config": [236, 238], "truthfulqa_mc2": [236, 238, 239], "measur": [236, 238], "propens": [236, 238], "324": 236, "loglikelihood": 236, "195": 236, "121": 236, "197": 236, "acc": [236, 241], "388": 236, "shown": [236, 241], "489": 236, "seem": 236, "custom_generation_config": [236, 238], "kick": 236, "300": 236, "bai": 236, "area": 236, "92": 236, "exploratorium": 236, "san": 236, "francisco": 236, "magazin": 236, "awesom": 236, "bridg": 236, "cool": 236, "96": [236, 242], "sec": [236, 238], "83": 236, "99": [236, 239], "72": 236, "littl": 236, "int8_weight_onli": [236, 238], "int8_dynamic_activation_int8_weight": [236, 238], "ao": [236, 238], "quant_api": [236, 238], "quantize_": [236, 238], "int4_weight_onli": [236, 238], "previous": [236, 238, 239], "benefit": 236, "doesn": 236, "fast": 236, "clone": [236, 239, 241, 242], "assumpt": 236, "new_dir": 236, "output_dict": 236, "sd_1": 236, "sd_2": 236, "dump": 236, "convert_hf_checkpoint": 236, "checkpoint_path": 236, "justin": 236, "school": 236, "math": 236, "teacher": 236, "ws": 236, "94": [236, 238], "bandwidth": [236, 238], "1391": 236, "84": 236, "thats": 236, "seamlessli": 236, "authent": [236, 237], "hopefulli": 236, "gave": 236, "minut": 237, "agreement": 237, "depth": 237, "principl": 237, "boilerpl": 237, "substanti": [237, 239], "custom_config": 237, "replic": 237, "info": 237, "lorafinetunerecipesingledevic": 237, "lora_finetune_output": 237, "log_1713194212": 237, "3697006702423096": 237, "25880": [237, 242], "83it": 237, "monitor": 237, "tqdm": 237, "interv": 237, "e2": 237, "focu": 238, "theta": 238, "observ": [238, 241], "consum": [238, 242], "overal": 238, "8b_qlora_single_devic": [238, 240], "coupl": [238, 239, 242], "meta_model_0": [238, 241], "did": [238, 242], "122": 238, "sarah": 238, "busi": 238, "mum": 238, "young": 238, "children": 238, "live": 238, "north": 238, "east": 238, "england": 238, "135": 238, "88": 238, "138": 238, "346": 238, "09": 238, "139": 238, "broader": 238, "teach": 239, "straight": 239, "jump": 239, "unfamiliar": 239, "oppos": [239, 242], "momentum": [239, 240], "aghajanyan": 239, "et": 239, "al": 239, "hypothes": 239, "intrins": 239, "four": 239, "eight": 239, "practic": 239, "blue": 239, "rememb": 239, "approx": 239, "15m": 239, "8192": [239, 241], "65k": 239, "requires_grad": [239, 242], "frozen_out": [239, 242], "lora_out": [239, 242], "omit": [239, 240], "base_model": 239, "lora_model": 239, "lora_llama_2_7b": [239, 242], "alon": 239, "bit": [239, 240, 241, 242], "in_featur": [239, 241], "out_featur": [239, 241], "inplac": 239, "feel": 239, "validate_missing_and_unexpected_for_lora": 239, "peft_util": 239, "set_trainable_param": 239, "fetch": 239, "lora_param": 239, "total_param": 239, "trainable_param": 239, "2f": 239, "6742609920": 239, "4194304": 239, "7b_lora": 239, "my_model_checkpoint_path": [239, 241, 242], "tokenizer_checkpoint": [239, 241, 242], "my_tokenizer_checkpoint_path": [239, 241, 242], "factori": 239, "benefici": 239, "impact": [239, 240], "minor": 239, "good": [239, 240], "lora_experiment_1": 239, "smooth": [239, 242], "curv": [239, 242], "500": 239, "ran": 239, "footprint": [239, 241], "commod": 239, "cogniz": 239, "ax": 239, "parallel": 239, "truthfulqa": 239, "475": 239, "87": 239, "508": 239, "86": 239, "504": 239, "04": 239, "514": 239, "lowest": 239, "absolut": 239, "4gb": 239, "salman": 240, "mohammadi": 240, "brief": 240, "glossari": 240, "leav": 240, "struggl": 240, "constrain": [240, 241], "particularli": 240, "gradient_accumulation_step": 240, "throughput": 240, "cost": 240, "sebastian": 240, "raschka": 240, "fp16": 240, "sound": 240, "quot": 240, "aliv": 240, "region": 240, "enable_activation_checkpoint": 240, "total_batch_s": 240, "count": 240, "suppos": 240, "log_every_n_step": 240, "translat": 240, "frequent": 240, "slowli": 240, "num_devic": 240, "adamw8bit": 240, "pagedadamw": 240, "modern": 240, "converg": 240, "stateless": 240, "stochast": 240, "descent": 240, "sacrif": 240, "optimizer_in_bwd": 240, "greatli": 240, "lora_": 240, "lora_llama3": 240, "aim": 240, "_lora": 240, "firstli": 240, "secondli": 240, "affect": 240, "fashion": 240, "jointli": 240, "sens": 240, "novel": 240, "normalfloat": [240, 242], "8x": [240, 242], "worth": 240, "cast": [240, 241], "incur": [240, 241, 242], "penalti": 240, "demonstr": [240, 241], "qlora_": 240, "qlora_llama3": 240, "_qlora": 240, "perplex": 241, "ultim": 241, "ptq": 241, "kept": 241, "width": 241, "nois": 241, "henc": 241, "x_q": 241, "int8": 241, "zp": 241, "x_float": 241, "qmin": 241, "qmax": 241, "clamp": 241, "x_fq": 241, "dequant": 241, "proce": 241, "prepared_model": 241, "swap": 241, "int8dynactint4weightqatlinear": 241, "int8dynactint4weightlinear": 241, "train_loop": 241, "converted_model": 241, "qat_distributed_recipe_label": 241, "recov": 241, "modif": 241, "custom_8b_qat_ful": 241, "led": 241, "presum": 241, "mutat": 241, "5gb": 241, "custom_quant": 241, "poorli": 241, "custom_eleuther_evalu": 241, "fullmodeltorchtunecheckpoint": 241, "hellaswag": 241, "max_seq_length": 241, "my_eleuther_evalu": 241, "stderr": 241, "word_perplex": 241, "9148": 241, "byte_perplex": 241, "5357": 241, "bits_per_byt": 241, "6189": 241, "5687": 241, "0049": 241, "acc_norm": 241, "7536": 241, "0043": 241, "portion": [241, 242], "74": 241, "048": 241, "190": 241, "7735": 241, "5598": 241, "6413": 241, "5481": 241, "0050": 241, "7390": 241, "0044": 241, "7251": 241, "4994": 241, "5844": 241, "5740": 241, "7610": 241, "outperform": 241, "importantli": 241, "characterist": 241, "187": 241, "958": 241, "halv": 241, "int4weightonlyquant": 241, "motiv": 241, "edg": 241, "smartphon": 241, "executorch": 241, "xnnpack": 241, "export_llama": 241, "use_sdpa_with_kv_cach": 241, "qmode": 241, "group_siz": 241, "get_bos_id": 241, "get_eos_id": 241, "128001": 241, "output_nam": 241, "llama3_8da4w": 241, "pte": 241, "881": 241, "oneplu": 241, "709": 241, "tok": 241, "815": 241, "316": 241, "364": 241, "highli": 242, "vanilla": 242, "held": 242, "bespok": 242, "vast": 242, "major": 242, "normatfloat": 242, "themselv": 242, "deepdiv": 242, "distinct": 242, "de": 242, "counterpart": 242, "set_default_devic": 242, "qlora_linear": 242, "memory_alloc": 242, "177": 242, "152": 242, "del": 242, "empty_cach": 242, "lora_linear": 242, "081": 242, "344": 242, "qlora_llama2_7b": 242, "qlora_model": 242, "essenti": 242, "reparametrize_as_dtype_state_dict_post_hook": 242, "149": 242, "9157477021217346": 242, "02": 242, "08": 242, "15it": 242, "nightli": 242, "200": 242, "hundr": 242, "228": 242, "8158286809921265": 242, "95it": 242, "exercis": 242, "linear_nf4": 242, "to_nf4": 242, "linear_weight": 242, "autograd": 242, "incom": 242}, "objects": {"torchtune.config": [[13, 0, 1, "", "instantiate"], [14, 0, 1, "", "log_config"], [15, 0, 1, "", "parse"], [16, 0, 1, "", "validate"]], "torchtune.data": [[17, 1, 1, "", "ChatFormat"], [18, 1, 1, "", "ChatMLTemplate"], [19, 1, 1, "", "ChosenRejectedToMessages"], [20, 3, 1, "", "GrammarErrorCorrectionTemplate"], [21, 1, 1, "", "InputOutputToMessages"], [22, 1, 1, "", "InstructTemplate"], [23, 1, 1, "", "JSONToMessages"], [24, 1, 1, "", "Message"], [25, 1, 1, "", "PromptTemplate"], [26, 1, 1, "", "PromptTemplateInterface"], [27, 3, 1, "", "QuestionAnswerTemplate"], [28, 3, 1, "", "Role"], [29, 1, 1, "", "ShareGPTToMessages"], [30, 3, 1, "", "SummarizeTemplate"], [31, 0, 1, "", "get_openai_messages"], [32, 0, 1, "", "get_sharegpt_messages"], [33, 0, 1, "", "left_pad_sequence"], [34, 0, 1, "", "padded_collate"], [35, 0, 1, "", "padded_collate_dpo"], [36, 0, 1, "", "padded_collate_sft"], [37, 0, 1, "", "truncate"], [38, 0, 1, "", "validate_messages"]], "torchtune.data.ChatFormat": [[17, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[22, 2, 1, "", "format"]], "torchtune.data.Message": [[24, 4, 1, "", "contains_media"], [24, 2, 1, "", "from_dict"], [24, 4, 1, "", "text_content"]], "torchtune.datasets": [[39, 3, 1, "", "ChatDataset"], [40, 1, 1, "", "ConcatDataset"], [41, 3, 1, "", "InstructDataset"], [42, 1, 1, "", "PackedDataset"], [43, 1, 1, "", "PreferenceDataset"], [44, 1, 1, "", "SFTDataset"], [45, 1, 1, "", "TextCompletionDataset"], [46, 0, 1, "", "alpaca_cleaned_dataset"], [47, 0, 1, "", "alpaca_dataset"], [48, 0, 1, "", "chat_dataset"], [49, 0, 1, "", "cnn_dailymail_articles_dataset"], [50, 0, 1, "", "grammar_dataset"], [51, 0, 1, "", "instruct_dataset"], [52, 0, 1, "", "llava_instruct_dataset"], [53, 0, 1, "", "samsum_dataset"], [54, 0, 1, "", "slimorca_dataset"], [55, 0, 1, "", "stack_exchange_paired_dataset"], [56, 0, 1, "", "text_completion_dataset"], [57, 0, 1, "", "the_cauldron_dataset"], [58, 0, 1, "", "wikitext_dataset"]], "torchtune.generation": [[59, 0, 1, "", "generate"]], "torchtune.models.clip": [[60, 1, 1, "", "TilePositionalEmbedding"], [61, 1, 1, "", "TiledTokenPositionalEmbedding"], [62, 1, 1, "", "TokenPositionalEmbedding"], [63, 0, 1, "", "clip_vision_encoder"]], "torchtune.models.clip.TilePositionalEmbedding": [[60, 2, 1, "", "forward"]], "torchtune.models.clip.TiledTokenPositionalEmbedding": [[61, 2, 1, "", "forward"]], "torchtune.models.clip.TokenPositionalEmbedding": [[62, 2, 1, "", "forward"]], "torchtune.models.code_llama2": [[64, 0, 1, "", "code_llama2_13b"], [65, 0, 1, "", "code_llama2_70b"], [66, 0, 1, "", "code_llama2_7b"], [67, 0, 1, "", "lora_code_llama2_13b"], [68, 0, 1, "", "lora_code_llama2_70b"], [69, 0, 1, "", "lora_code_llama2_7b"], [70, 0, 1, "", "qlora_code_llama2_13b"], [71, 0, 1, "", "qlora_code_llama2_70b"], [72, 0, 1, "", "qlora_code_llama2_7b"]], "torchtune.models.gemma": [[73, 1, 1, "", "GemmaTokenizer"], [74, 0, 1, "", "gemma"], [75, 0, 1, "", "gemma_2b"], [76, 0, 1, "", "gemma_7b"], [77, 0, 1, "", "gemma_tokenizer"], [78, 0, 1, "", "lora_gemma"], [79, 0, 1, "", "lora_gemma_2b"], [80, 0, 1, "", "lora_gemma_7b"], [81, 0, 1, "", "qlora_gemma_2b"], [82, 0, 1, "", "qlora_gemma_7b"]], "torchtune.models.gemma.GemmaTokenizer": [[73, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama2": [[83, 1, 1, "", "Llama2ChatTemplate"], [84, 1, 1, "", "Llama2Tokenizer"], [85, 0, 1, "", "llama2"], [86, 0, 1, "", "llama2_13b"], [87, 0, 1, "", "llama2_70b"], [88, 0, 1, "", "llama2_7b"], [89, 0, 1, "", "llama2_reward_7b"], [90, 0, 1, "", "llama2_tokenizer"], [91, 0, 1, "", "lora_llama2"], [92, 0, 1, "", "lora_llama2_13b"], [93, 0, 1, "", "lora_llama2_70b"], [94, 0, 1, "", "lora_llama2_7b"], [95, 0, 1, "", "lora_llama2_reward_7b"], [96, 0, 1, "", "qlora_llama2_13b"], [97, 0, 1, "", "qlora_llama2_70b"], [98, 0, 1, "", "qlora_llama2_7b"], [99, 0, 1, "", "qlora_llama2_reward_7b"]], "torchtune.models.llama2.Llama2Tokenizer": [[84, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3": [[100, 1, 1, "", "Llama3Tokenizer"], [101, 0, 1, "", "llama3"], [102, 0, 1, "", "llama3_70b"], [103, 0, 1, "", "llama3_8b"], [104, 0, 1, "", "llama3_tokenizer"], [105, 0, 1, "", "lora_llama3"], [106, 0, 1, "", "lora_llama3_70b"], [107, 0, 1, "", "lora_llama3_8b"], [108, 0, 1, "", "qlora_llama3_70b"], [109, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.llama3.Llama3Tokenizer": [[100, 2, 1, "", "decode"], [100, 2, 1, "", "tokenize_message"], [100, 2, 1, "", "tokenize_messages"]], "torchtune.models.llama3_1": [[110, 0, 1, "", "llama3_1"], [111, 0, 1, "", "llama3_1_405b"], [112, 0, 1, "", "llama3_1_70b"], [113, 0, 1, "", "llama3_1_8b"], [114, 0, 1, "", "lora_llama3_1"], [115, 0, 1, "", "lora_llama3_1_70b"], [116, 0, 1, "", "lora_llama3_1_8b"], [117, 0, 1, "", "qlora_llama3_1_70b"], [118, 0, 1, "", "qlora_llama3_1_8b"]], "torchtune.models.mistral": [[119, 1, 1, "", "MistralChatTemplate"], [120, 1, 1, "", "MistralTokenizer"], [121, 0, 1, "", "lora_mistral"], [122, 0, 1, "", "lora_mistral_7b"], [123, 0, 1, "", "lora_mistral_classifier"], [124, 0, 1, "", "lora_mistral_reward_7b"], [125, 0, 1, "", "mistral"], [126, 0, 1, "", "mistral_7b"], [127, 0, 1, "", "mistral_classifier"], [128, 0, 1, "", "mistral_reward_7b"], [129, 0, 1, "", "mistral_tokenizer"], [130, 0, 1, "", "qlora_mistral_7b"], [131, 0, 1, "", "qlora_mistral_reward_7b"]], "torchtune.models.mistral.MistralTokenizer": [[120, 2, 1, "", "decode"], [120, 2, 1, "", "encode"], [120, 2, 1, "", "tokenize_messages"]], "torchtune.models.phi3": [[132, 1, 1, "", "Phi3MiniTokenizer"], [133, 0, 1, "", "lora_phi3"], [134, 0, 1, "", "lora_phi3_mini"], [135, 0, 1, "", "phi3"], [136, 0, 1, "", "phi3_mini"], [137, 0, 1, "", "phi3_mini_tokenizer"], [138, 0, 1, "", "qlora_phi3_mini"]], "torchtune.models.phi3.Phi3MiniTokenizer": [[132, 2, 1, "", "decode"], [132, 2, 1, "", "tokenize_messages"]], "torchtune.models.qwen2": [[139, 1, 1, "", "Qwen2Tokenizer"], [140, 0, 1, "", "lora_qwen2"], [141, 0, 1, "", "lora_qwen2_0_5b"], [142, 0, 1, "", "lora_qwen2_1_5b"], [143, 0, 1, "", "lora_qwen2_7b"], [144, 0, 1, "", "qwen2"], [145, 0, 1, "", "qwen2_0_5b"], [146, 0, 1, "", "qwen2_1_5b"], [147, 0, 1, "", "qwen2_7b"], [148, 0, 1, "", "qwen2_tokenizer"]], "torchtune.models.qwen2.Qwen2Tokenizer": [[139, 2, 1, "", "decode"], [139, 2, 1, "", "encode"], [139, 2, 1, "", "tokenize_messages"]], "torchtune.modules": [[149, 1, 1, "", "FeedForward"], [150, 1, 1, "", "Fp32LayerNorm"], [151, 1, 1, "", "KVCache"], [152, 1, 1, "", "MultiHeadAttention"], [153, 1, 1, "", "RMSNorm"], [154, 1, 1, "", "RotaryPositionalEmbeddings"], [155, 1, 1, "", "TanhGate"], [156, 1, 1, "", "TiedEmbeddingTransformerDecoder"], [157, 1, 1, "", "TransformerCrossAttentionLayer"], [158, 1, 1, "", "TransformerDecoder"], [159, 1, 1, "", "TransformerSelfAttentionLayer"], [160, 1, 1, "", "VisionTransformer"], [162, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.FeedForward": [[149, 2, 1, "", "forward"]], "torchtune.modules.Fp32LayerNorm": [[150, 2, 1, "", "forward"]], "torchtune.modules.KVCache": [[151, 2, 1, "", "reset"], [151, 2, 1, "", "update"]], "torchtune.modules.MultiHeadAttention": [[152, 2, 1, "", "forward"], [152, 2, 1, "", "reset_cache"], [152, 2, 1, "", "setup_cache"]], "torchtune.modules.RMSNorm": [[153, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[154, 2, 1, "", "forward"]], "torchtune.modules.TanhGate": [[155, 2, 1, "", "forward"]], "torchtune.modules.TiedEmbeddingTransformerDecoder": [[156, 2, 1, "", "caches_are_enabled"], [156, 2, 1, "", "forward"], [156, 2, 1, "", "reset_caches"], [156, 2, 1, "", "set_num_output_chunks"], [156, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerCrossAttentionLayer": [[157, 4, 1, "", "cache_enabled"], [157, 2, 1, "", "forward"], [157, 2, 1, "", "reset_cache"], [157, 2, 1, "", "setup_cache"]], "torchtune.modules.TransformerDecoder": [[158, 2, 1, "", "caches_are_enabled"], [158, 2, 1, "", "forward"], [158, 2, 1, "", "reset_caches"], [158, 2, 1, "", "set_num_output_chunks"], [158, 2, 1, "", "setup_caches"]], "torchtune.modules.TransformerSelfAttentionLayer": [[159, 4, 1, "", "cache_enabled"], [159, 2, 1, "", "forward"], [159, 2, 1, "", "reset_cache"], [159, 2, 1, "", "setup_cache"]], "torchtune.modules.VisionTransformer": [[160, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[161, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.loss": [[163, 1, 1, "", "CEWithChunkedOutputLoss"]], "torchtune.modules.loss.CEWithChunkedOutputLoss": [[163, 2, 1, "", "compute_cross_entropy"], [163, 2, 1, "", "forward"]], "torchtune.modules.model_fusion": [[164, 1, 1, "", "DeepFusionModel"], [165, 1, 1, "", "FusionEmbedding"], [166, 1, 1, "", "FusionLayer"], [167, 0, 1, "", "register_fusion_module"]], "torchtune.modules.model_fusion.DeepFusionModel": [[164, 2, 1, "", "caches_are_enabled"], [164, 2, 1, "", "forward"], [164, 2, 1, "", "reset_caches"], [164, 2, 1, "", "setup_caches"]], "torchtune.modules.model_fusion.FusionEmbedding": [[165, 2, 1, "", "forward"], [165, 2, 1, "", "fusion_params"]], "torchtune.modules.model_fusion.FusionLayer": [[166, 4, 1, "", "cache_enabled"], [166, 2, 1, "", "forward"], [166, 2, 1, "", "fusion_params"], [166, 2, 1, "", "reset_cache"], [166, 2, 1, "", "setup_cache"]], "torchtune.modules.peft": [[168, 1, 1, "", "AdapterModule"], [169, 1, 1, "", "LoRALinear"], [170, 0, 1, "", "disable_adapter"], [171, 0, 1, "", "get_adapter_params"], [172, 0, 1, "", "set_trainable_params"], [173, 0, 1, "", "validate_missing_and_unexpected_for_lora"], [174, 0, 1, "", "validate_state_dict_for_lora"]], "torchtune.modules.peft.AdapterModule": [[168, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[169, 2, 1, "", "adapter_params"], [169, 2, 1, "", "forward"]], "torchtune.modules.rlhf": [[175, 0, 1, "", "estimate_advantages"], [176, 0, 1, "", "get_rewards_ppo"], [182, 0, 1, "", "truncate_sequence_at_first_stop_token"]], "torchtune.modules.rlhf.loss": [[177, 1, 1, "", "DPOLoss"], [178, 1, 1, "", "IPOLoss"], [179, 1, 1, "", "PPOLoss"], [180, 1, 1, "", "RSOLoss"], [181, 1, 1, "", "SimPOLoss"]], "torchtune.modules.rlhf.loss.DPOLoss": [[177, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.IPOLoss": [[178, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.PPOLoss": [[179, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.RSOLoss": [[180, 2, 1, "", "forward"]], "torchtune.modules.rlhf.loss.SimPOLoss": [[181, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[183, 1, 1, "", "BaseTokenizer"], [184, 1, 1, "", "ModelTokenizer"], [185, 1, 1, "", "SentencePieceBaseTokenizer"], [186, 1, 1, "", "TikTokenBaseTokenizer"], [187, 0, 1, "", "parse_hf_tokenizer_json"], [188, 0, 1, "", "tokenize_messages_no_special_tokens"]], "torchtune.modules.tokenizers.BaseTokenizer": [[183, 2, 1, "", "decode"], [183, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.ModelTokenizer": [[184, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.SentencePieceBaseTokenizer": [[185, 2, 1, "", "decode"], [185, 2, 1, "", "encode"]], "torchtune.modules.tokenizers.TikTokenBaseTokenizer": [[186, 2, 1, "", "decode"], [186, 2, 1, "", "encode"]], "torchtune.modules.transforms": [[189, 1, 1, "", "Transform"], [190, 1, 1, "", "VisionCrossAttentionMask"]], "torchtune.training": [[191, 3, 1, "", "FSDPPolicyType"], [192, 1, 1, "", "FullModelHFCheckpointer"], [193, 1, 1, "", "FullModelMetaCheckpointer"], [194, 1, 1, "", "FullModelTorchTuneCheckpointer"], [195, 1, 1, "", "ModelType"], [196, 1, 1, "", "OptimizerInBackwardWrapper"], [197, 0, 1, "", "apply_selective_activation_checkpointing"], [198, 0, 1, "", "create_optim_in_bwd_wrapper"], [199, 0, 1, "", "get_dtype"], [200, 0, 1, "", "get_full_finetune_fsdp_wrap_policy"], [201, 0, 1, "", "get_memory_stats"], [202, 0, 1, "", "get_quantizer_mode"], [203, 0, 1, "", "get_unmasked_sequence_lengths"], [204, 0, 1, "", "get_world_size_and_rank"], [205, 0, 1, "", "init_distributed"], [206, 0, 1, "", "is_distributed"], [207, 0, 1, "", "log_memory_stats"], [208, 0, 1, "", "lora_fsdp_wrap_policy"], [214, 0, 1, "", "register_optim_in_bwd_hooks"], [215, 0, 1, "", "set_activation_checkpointing"], [216, 0, 1, "", "set_default_dtype"], [217, 0, 1, "", "set_seed"], [218, 0, 1, "", "setup_torch_profiler"], [219, 0, 1, "", "update_state_dict_for_classifier"], [220, 0, 1, "", "validate_expected_param_dtype"]], "torchtune.training.FullModelHFCheckpointer": [[192, 2, 1, "", "load_checkpoint"], [192, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelMetaCheckpointer": [[193, 2, 1, "", "load_checkpoint"], [193, 2, 1, "", "save_checkpoint"]], "torchtune.training.FullModelTorchTuneCheckpointer": [[194, 2, 1, "", "load_checkpoint"], [194, 2, 1, "", "save_checkpoint"]], "torchtune.training.OptimizerInBackwardWrapper": [[196, 2, 1, "", "get_optim_key"], [196, 2, 1, "", "load_state_dict"], [196, 2, 1, "", "state_dict"]], "torchtune.training.metric_logging": [[209, 1, 1, "", "CometLogger"], [210, 1, 1, "", "DiskLogger"], [211, 1, 1, "", "StdoutLogger"], [212, 1, 1, "", "TensorBoardLogger"], [213, 1, 1, "", "WandBLogger"]], "torchtune.training.metric_logging.CometLogger": [[209, 2, 1, "", "close"], [209, 2, 1, "", "log"], [209, 2, 1, "", "log_config"], [209, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.DiskLogger": [[210, 2, 1, "", "close"], [210, 2, 1, "", "log"], [210, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.StdoutLogger": [[211, 2, 1, "", "close"], [211, 2, 1, "", "log"], [211, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.TensorBoardLogger": [[212, 2, 1, "", "close"], [212, 2, 1, "", "log"], [212, 2, 1, "", "log_dict"]], "torchtune.training.metric_logging.WandBLogger": [[213, 2, 1, "", "close"], [213, 2, 1, "", "log"], [213, 2, 1, "", "log_config"], [213, 2, 1, "", "log_dict"]], "torchtune.utils": [[221, 0, 1, "", "get_device"], [222, 0, 1, "", "get_logger"], [223, 0, 1, "", "torch_version_ge"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:data", "4": "py:property"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "data", "Python data"], "4": ["py", "property", "Python property"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 7, 8, 20, 27, 28, 30, 39, 41, 191, 226, 228, 233, 236, 238, 239, 241, 242], "config": [0, 10, 11, 233, 237], "data": [1, 20, 27, 28, 30, 234], "text": [1, 2, 235, 238], "templat": [1, 234, 235], "type": 1, "convert": 1, "messag": [1, 24], "transform": [1, 5, 189], "collat": 1, "helper": 1, "function": 1, "dataset": [2, 39, 41, 234, 235], "onli": [2, 10], "multimod": 2, "gener": [2, 3, 59, 236, 238], "builder": 2, "class": [2, 11], "model": [4, 5, 12, 233, 236, 237, 238, 239, 240, 241], "llama3": [4, 101, 234, 238, 241], "1": 4, "llama2": [4, 85, 234, 236, 239, 242], "code": 4, "llama": 4, "qwen": 4, "2": 4, "phi": 4, "3": 4, "mistral": [4, 125], "gemma": [4, 74], "clip": 4, "modul": 5, "compon": [5, 10, 240], "build": [5, 227, 242], "block": 5, "base": 5, "token": [5, 234], "util": [5, 7], "peft": [5, 240], "fusion": 5, "vision": 5, "reinforc": 5, "learn": 5, "from": [5, 234, 242], "human": 5, "feedback": 5, "rlhf": 5, "loss": 5, "train": [6, 191, 230, 237], "checkpoint": [6, 8, 12, 236, 240], "reduc": 6, "precis": [6, 240], "distribut": [6, 230], "memori": [6, 235, 239, 240, 242], "manag": 6, "metric": [6, 9, 12], "log": [6, 9, 12], "perform": [6, 239], "profil": 6, "miscellan": [6, 7], "overview": [8, 228, 231, 236, 240], "format": [8, 235], "handl": 8, "differ": 8, "hfcheckpoint": 8, "metacheckpoint": 8, "torchtunecheckpoint": 8, "intermedi": 8, "vs": 8, "final": 8, "lora": [8, 229, 236, 239, 240, 242], "put": [8, 242], "thi": 8, "all": [8, 10, 242], "togeth": [8, 242], "comet": 9, "logger": [9, 12], "about": 10, "where": 10, "do": 10, "paramet": [10, 240], "live": 10, "write": 10, "configur": [10, 235], "us": [10, 11, 234, 236, 242], "instanti": [10, 13], "referenc": 10, "other": [10, 236], "field": 10, "interpol": 10, "valid": [10, 16, 233], "your": [10, 11, 236, 237], "best": 10, "practic": 10, "airtight": 10, "public": 10, "api": 10, "command": 10, "line": 10, "overrid": 10, "remov": 10, "what": [11, 228, 239, 241, 242], "ar": 11, "recip": [11, 231, 233, 237, 239, 241], "script": 11, "run": [11, 233, 236], "cli": [11, 233], "pars": [11, 15], "weight": 12, "bias": 12, "w": 12, "b": 12, "log_config": 14, "chatformat": 17, "chatmltempl": 18, "chosenrejectedtomessag": 19, "grammarerrorcorrectiontempl": 20, "inputoutputtomessag": 21, "instructtempl": 22, "jsontomessag": 23, "prompttempl": 25, "prompttemplateinterfac": 26, "questionanswertempl": 27, "role": 28, "sharegpttomessag": 29, "summarizetempl": 30, "get_openai_messag": 31, "get_sharegpt_messag": 32, "left_pad_sequ": 33, "padded_col": 34, "padded_collate_dpo": 35, "padded_collate_sft": 36, "truncat": 37, "validate_messag": 38, "chatdataset": 39, "concatdataset": 40, "instructdataset": 41, "packeddataset": 42, "preferencedataset": 43, "sftdataset": 44, "textcompletiondataset": 45, "alpaca_cleaned_dataset": 46, "alpaca_dataset": 47, "chat_dataset": 48, "cnn_dailymail_articles_dataset": 49, "grammar_dataset": 50, "instruct_dataset": 51, "llava_instruct_dataset": 52, "samsum_dataset": 53, "slimorca_dataset": 54, "stack_exchange_paired_dataset": 55, "text_completion_dataset": 56, "the_cauldron_dataset": 57, "wikitext_dataset": 58, "tilepositionalembed": 60, "tiledtokenpositionalembed": 61, "tokenpositionalembed": 62, "clip_vision_encod": 63, "code_llama2_13b": 64, "code_llama2_70b": 65, "code_llama2_7b": 66, "lora_code_llama2_13b": 67, "lora_code_llama2_70b": 68, "lora_code_llama2_7b": 69, "qlora_code_llama2_13b": 70, "qlora_code_llama2_70b": 71, "qlora_code_llama2_7b": 72, "gemmatoken": 73, "gemma_2b": 75, "gemma_7b": 76, "gemma_token": 77, "lora_gemma": 78, "lora_gemma_2b": 79, "lora_gemma_7b": 80, "qlora_gemma_2b": 81, "qlora_gemma_7b": 82, "llama2chattempl": 83, "llama2token": 84, "llama2_13b": 86, "llama2_70b": 87, "llama2_7b": 88, "llama2_reward_7b": 89, "llama2_token": 90, "lora_llama2": 91, "lora_llama2_13b": 92, "lora_llama2_70b": 93, "lora_llama2_7b": 94, "lora_llama2_reward_7b": 95, "qlora_llama2_13b": 96, "qlora_llama2_70b": 97, "qlora_llama2_7b": 98, "qlora_llama2_reward_7b": 99, "llama3token": 100, "llama3_70b": 102, "llama3_8b": 103, "llama3_token": 104, "lora_llama3": 105, "lora_llama3_70b": 106, "lora_llama3_8b": 107, "qlora_llama3_70b": 108, "qlora_llama3_8b": 109, "llama3_1": 110, "llama3_1_405b": 111, "llama3_1_70b": 112, "llama3_1_8b": 113, "lora_llama3_1": 114, "lora_llama3_1_70b": 115, "lora_llama3_1_8b": 116, "qlora_llama3_1_70b": 117, "qlora_llama3_1_8b": 118, "mistralchattempl": 119, "mistraltoken": 120, "lora_mistr": 121, "lora_mistral_7b": 122, "lora_mistral_classifi": 123, "lora_mistral_reward_7b": 124, "mistral_7b": 126, "mistral_classifi": 127, "mistral_reward_7b": 128, "mistral_token": 129, "qlora_mistral_7b": 130, "qlora_mistral_reward_7b": 131, "phi3minitoken": 132, "lora_phi3": 133, "lora_phi3_mini": 134, "phi3": 135, "phi3_mini": 136, "phi3_mini_token": 137, "qlora_phi3_mini": 138, "qwen2token": 139, "lora_qwen2": 140, "lora_qwen2_0_5b": 141, "lora_qwen2_1_5b": 142, "lora_qwen2_7b": 143, "qwen2": 144, "qwen2_0_5b": 145, "qwen2_1_5b": 146, "qwen2_7b": 147, "qwen2_token": 148, "feedforward": 149, "fp32layernorm": 150, "kvcach": 151, "multiheadattent": 152, "todo": [152, 159], "rmsnorm": 153, "rotarypositionalembed": 154, "tanhgat": 155, "tiedembeddingtransformerdecod": 156, "transformercrossattentionlay": 157, "transformerdecod": 158, "transformerselfattentionlay": 159, "visiontransform": 160, "reparametrize_as_dtype_state_dict_post_hook": 161, "get_cosine_schedule_with_warmup": 162, "cewithchunkedoutputloss": 163, "deepfusionmodel": 164, "fusionembed": 165, "fusionlay": 166, "register_fusion_modul": 167, "adaptermodul": 168, "loralinear": 169, "disable_adapt": 170, "get_adapter_param": 171, "set_trainable_param": 172, "validate_missing_and_unexpected_for_lora": 173, "validate_state_dict_for_lora": 174, "estimate_advantag": 175, "get_rewards_ppo": 176, "dpoloss": 177, "ipoloss": 178, "ppoloss": 179, "rsoloss": 180, "simpoloss": 181, "truncate_sequence_at_first_stop_token": 182, "basetoken": 183, "modeltoken": 184, "sentencepiecebasetoken": 185, "tiktokenbasetoken": 186, "parse_hf_tokenizer_json": 187, "tokenize_messages_no_special_token": 188, "visioncrossattentionmask": 190, "fsdppolicytyp": 191, "fullmodelhfcheckpoint": 192, "fullmodelmetacheckpoint": 193, "fullmodeltorchtunecheckpoint": 194, "modeltyp": 195, "optimizerinbackwardwrapp": 196, "apply_selective_activation_checkpoint": 197, "create_optim_in_bwd_wrapp": 198, "get_dtyp": 199, "get_full_finetune_fsdp_wrap_polici": 200, "get_memory_stat": 201, "get_quantizer_mod": 202, "get_unmasked_sequence_length": 203, "get_world_size_and_rank": 204, "init_distribut": 205, "is_distribut": 206, "log_memory_stat": 207, "lora_fsdp_wrap_polici": 208, "cometlogg": 209, "disklogg": 210, "stdoutlogg": 211, "tensorboardlogg": 212, "wandblogg": 213, "register_optim_in_bwd_hook": 214, "set_activation_checkpoint": 215, "set_default_dtyp": 216, "set_se": 217, "setup_torch_profil": 218, "update_state_dict_for_classifi": 219, "validate_expected_param_dtyp": 220, "get_devic": 221, "get_logg": 222, "torch_version_g": 223, "comput": [225, 232], "time": [225, 232], "welcom": 226, "document": 226, "get": [226, 233, 238], "start": [226, 233], "tutori": 226, "instal": 227, "instruct": [227, 235, 238], "pre": 227, "requisit": 227, "via": [227, 238], "pypi": 227, "git": 227, "clone": 227, "nightli": 227, "kei": 228, "concept": 228, "design": 228, "principl": 228, "singl": 229, "devic": [229, 241], "finetun": [229, 231, 236, 239, 241, 242], "quantiz": [230, 236, 238, 240, 241], "awar": 230, "qat": [230, 241], "supervis": 231, "download": [233, 236, 237], "list": 233, "built": [233, 235], "copi": 233, "fine": [234, 235, 237, 238, 240], "tune": [234, 235, 237, 238, 240], "chat": [234, 235], "chang": 234, "prompt": 234, "special": 234, "when": 234, "should": 234, "i": 234, "custom": [234, 235], "hug": [235, 236], "face": [235, 236], "set": 235, "max": 235, "sequenc": 235, "length": 235, "sampl": 235, "pack": 235, "unstructur": 235, "corpu": 235, "multipl": 235, "local": 235, "remot": 235, "fulli": 235, "end": 236, "workflow": 236, "7b": 236, "evalu": [236, 238, 241], "eleutherai": [236, 238], "s": [236, 238], "eval": [236, 238], "har": [236, 238], "speed": 236, "up": 236, "librari": 236, "upload": 236, "hub": 236, "first": 237, "llm": 237, "select": 237, "modifi": 237, "next": 237, "step": [237, 240], "meta": 238, "8b": 238, "access": 238, "our": 238, "faster": 238, "how": 239, "doe": 239, "work": 239, "appli": [239, 241], "trade": 239, "off": 239, "optim": 240, "activ": 240, "gradient": 240, "accumul": 240, "lower": [240, 241], "fuse": 240, "backward": 240, "pass": 240, "effici": 240, "low": 240, "rank": 240, "adapt": 240, "qlora": [240, 242], "option": 241, "save": 242, "deep": 242, "dive": 242}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})