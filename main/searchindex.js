Search.setIndex({"docnames": ["api_ref_config", "api_ref_data", "api_ref_datasets", "api_ref_models", "api_ref_modules", "api_ref_utilities", "deep_dives/checkpointer", "deep_dives/configs", "deep_dives/recipe_deepdive", "deep_dives/wandb_logging", "generated/torchtune.config.instantiate", "generated/torchtune.config.parse", "generated/torchtune.config.validate", "generated/torchtune.data.AlpacaInstructTemplate", "generated/torchtune.data.ChatFormat", "generated/torchtune.data.ChatMLFormat", "generated/torchtune.data.GrammarErrorCorrectionTemplate", "generated/torchtune.data.InstructTemplate", "generated/torchtune.data.Llama2ChatFormat", "generated/torchtune.data.MistralChatFormat", "generated/torchtune.data.SummarizeTemplate", "generated/torchtune.data.validate_messages", "generated/torchtune.datasets.alpaca_cleaned_dataset", "generated/torchtune.datasets.alpaca_dataset", "generated/torchtune.datasets.grammar_dataset", "generated/torchtune.datasets.samsum_dataset", "generated/torchtune.datasets.slimorca_dataset", "generated/torchtune.models.gemma.gemma_2b", "generated/torchtune.models.llama2.llama2_13b", "generated/torchtune.models.llama2.llama2_70b", "generated/torchtune.models.llama2.llama2_7b", "generated/torchtune.models.llama2.lora_llama2_13b", "generated/torchtune.models.llama2.lora_llama2_70b", "generated/torchtune.models.llama2.lora_llama2_7b", "generated/torchtune.models.llama2.qlora_llama2_13b", "generated/torchtune.models.llama2.qlora_llama2_7b", "generated/torchtune.models.llama3.llama3_8b", "generated/torchtune.models.llama3.lora_llama3_8b", "generated/torchtune.models.llama3.qlora_llama3_8b", "generated/torchtune.models.mistral.lora_mistral_7b", "generated/torchtune.models.mistral.mistral_7b", "generated/torchtune.models.mistral.qlora_mistral_7b", "generated/torchtune.modules.CausalSelfAttention", "generated/torchtune.modules.FeedForward", "generated/torchtune.modules.KVCache", "generated/torchtune.modules.RMSNorm", "generated/torchtune.modules.RotaryPositionalEmbeddings", "generated/torchtune.modules.TransformerDecoder", "generated/torchtune.modules.TransformerDecoderLayer", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook", "generated/torchtune.modules.get_cosine_schedule_with_warmup", "generated/torchtune.modules.peft.AdapterModule", "generated/torchtune.modules.peft.LoRALinear", "generated/torchtune.modules.peft.get_adapter_params", "generated/torchtune.modules.peft.set_trainable_params", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer", "generated/torchtune.modules.tokenizers.TikTokenTokenizer", "generated/torchtune.utils.FullModelHFCheckpointer", "generated/torchtune.utils.FullModelMetaCheckpointer", "generated/torchtune.utils.TuneRecipeArgumentParser", "generated/torchtune.utils.get_device", "generated/torchtune.utils.get_dtype", "generated/torchtune.utils.get_logger", "generated/torchtune.utils.get_world_size_and_rank", "generated/torchtune.utils.init_distributed", "generated/torchtune.utils.list_dtypes", "generated/torchtune.utils.metric_logging.DiskLogger", "generated/torchtune.utils.metric_logging.StdoutLogger", "generated/torchtune.utils.metric_logging.TensorBoardLogger", "generated/torchtune.utils.metric_logging.WandBLogger", "generated/torchtune.utils.padded_collate", "generated/torchtune.utils.profiler", "generated/torchtune.utils.set_activation_checkpointing", "generated/torchtune.utils.set_seed", "generated_examples/index", "generated_examples/sg_execution_times", "index", "install", "overview", "sg_execution_times", "tutorials/datasets", "tutorials/e2e_flow", "tutorials/first_finetune_tutorial", "tutorials/llama3", "tutorials/lora_finetune", "tutorials/qlora_finetune"], "filenames": ["api_ref_config.rst", "api_ref_data.rst", "api_ref_datasets.rst", "api_ref_models.rst", "api_ref_modules.rst", "api_ref_utilities.rst", "deep_dives/checkpointer.rst", "deep_dives/configs.rst", "deep_dives/recipe_deepdive.rst", "deep_dives/wandb_logging.rst", "generated/torchtune.config.instantiate.rst", "generated/torchtune.config.parse.rst", "generated/torchtune.config.validate.rst", "generated/torchtune.data.AlpacaInstructTemplate.rst", "generated/torchtune.data.ChatFormat.rst", "generated/torchtune.data.ChatMLFormat.rst", "generated/torchtune.data.GrammarErrorCorrectionTemplate.rst", "generated/torchtune.data.InstructTemplate.rst", "generated/torchtune.data.Llama2ChatFormat.rst", "generated/torchtune.data.MistralChatFormat.rst", "generated/torchtune.data.SummarizeTemplate.rst", "generated/torchtune.data.validate_messages.rst", "generated/torchtune.datasets.alpaca_cleaned_dataset.rst", "generated/torchtune.datasets.alpaca_dataset.rst", "generated/torchtune.datasets.grammar_dataset.rst", "generated/torchtune.datasets.samsum_dataset.rst", "generated/torchtune.datasets.slimorca_dataset.rst", "generated/torchtune.models.gemma.gemma_2b.rst", "generated/torchtune.models.llama2.llama2_13b.rst", "generated/torchtune.models.llama2.llama2_70b.rst", "generated/torchtune.models.llama2.llama2_7b.rst", "generated/torchtune.models.llama2.lora_llama2_13b.rst", "generated/torchtune.models.llama2.lora_llama2_70b.rst", "generated/torchtune.models.llama2.lora_llama2_7b.rst", "generated/torchtune.models.llama2.qlora_llama2_13b.rst", "generated/torchtune.models.llama2.qlora_llama2_7b.rst", "generated/torchtune.models.llama3.llama3_8b.rst", "generated/torchtune.models.llama3.lora_llama3_8b.rst", "generated/torchtune.models.llama3.qlora_llama3_8b.rst", "generated/torchtune.models.mistral.lora_mistral_7b.rst", "generated/torchtune.models.mistral.mistral_7b.rst", "generated/torchtune.models.mistral.qlora_mistral_7b.rst", "generated/torchtune.modules.CausalSelfAttention.rst", "generated/torchtune.modules.FeedForward.rst", "generated/torchtune.modules.KVCache.rst", "generated/torchtune.modules.RMSNorm.rst", "generated/torchtune.modules.RotaryPositionalEmbeddings.rst", "generated/torchtune.modules.TransformerDecoder.rst", "generated/torchtune.modules.TransformerDecoderLayer.rst", "generated/torchtune.modules.common_utils.reparametrize_as_dtype_state_dict_post_hook.rst", "generated/torchtune.modules.get_cosine_schedule_with_warmup.rst", "generated/torchtune.modules.peft.AdapterModule.rst", "generated/torchtune.modules.peft.LoRALinear.rst", "generated/torchtune.modules.peft.get_adapter_params.rst", "generated/torchtune.modules.peft.set_trainable_params.rst", "generated/torchtune.modules.tokenizers.SentencePieceTokenizer.rst", "generated/torchtune.modules.tokenizers.TikTokenTokenizer.rst", "generated/torchtune.utils.FullModelHFCheckpointer.rst", "generated/torchtune.utils.FullModelMetaCheckpointer.rst", "generated/torchtune.utils.TuneRecipeArgumentParser.rst", "generated/torchtune.utils.get_device.rst", "generated/torchtune.utils.get_dtype.rst", "generated/torchtune.utils.get_logger.rst", "generated/torchtune.utils.get_world_size_and_rank.rst", "generated/torchtune.utils.init_distributed.rst", "generated/torchtune.utils.list_dtypes.rst", "generated/torchtune.utils.metric_logging.DiskLogger.rst", "generated/torchtune.utils.metric_logging.StdoutLogger.rst", "generated/torchtune.utils.metric_logging.TensorBoardLogger.rst", "generated/torchtune.utils.metric_logging.WandBLogger.rst", "generated/torchtune.utils.padded_collate.rst", "generated/torchtune.utils.profiler.rst", "generated/torchtune.utils.set_activation_checkpointing.rst", "generated/torchtune.utils.set_seed.rst", "generated_examples/index.rst", "generated_examples/sg_execution_times.rst", "index.rst", "install.rst", "overview.rst", "sg_execution_times.rst", "tutorials/datasets.rst", "tutorials/e2e_flow.rst", "tutorials/first_finetune_tutorial.rst", "tutorials/llama3.rst", "tutorials/lora_finetune.rst", "tutorials/qlora_finetune.rst"], "titles": ["torchtune.config", "torchtune.data", "torchtune.datasets", "torchtune.models", "torchtune.modules", "torchtune.utils", "Checkpointing in torchtune", "All About Configs", "What Are Recipes?", "Logging to Weights &amp; Biases", "instantiate", "parse", "validate", "AlpacaInstructTemplate", "ChatFormat", "ChatMLFormat", "GrammarErrorCorrectionTemplate", "InstructTemplate", "Llama2ChatFormat", "MistralChatFormat", "SummarizeTemplate", "validate_messages", "alpaca_cleaned_dataset", "alpaca_dataset", "grammar_dataset", "samsum_dataset", "slimorca_dataset", "gemma_2b", "llama2_13b", "llama2_70b", "llama2_7b", "lora_llama2_13b", "lora_llama2_70b", "lora_llama2_7b", "qlora_llama2_13b", "qlora_llama2_7b", "llama3_8b", "lora_llama3_8b", "qlora_llama3_8b", "lora_mistral_7b", "mistral_7b", "qlora_mistral_7b", "CausalSelfAttention", "FeedForward", "KVCache", "RMSNorm", "RotaryPositionalEmbeddings", "TransformerDecoder", "TransformerDecoderLayer", "reparametrize_as_dtype_state_dict_post_hook", "get_cosine_schedule_with_warmup", "AdapterModule", "LoRALinear", "get_adapter_params", "set_trainable_params", "SentencePieceTokenizer", "TikTokenTokenizer", "FullModelHFCheckpointer", "FullModelMetaCheckpointer", "TuneRecipeArgumentParser", "get_device", "get_dtype", "get_logger", "get_world_size_and_rank", "init_distributed", "list_dtypes", "DiskLogger", "StdoutLogger", "TensorBoardLogger", "WandBLogger", "padded_collate", "profiler", "set_activation_checkpointing", "set_seed", "&lt;no title&gt;", "Computation times", "Welcome to the torchtune Documentation", "Install Instructions", "torchtune Overview", "Computation times", "Configuring Datasets for Fine-Tuning", "End-to-End Workflow with torchtune", "Fine-Tune Your First LLM", "Llama3 in torchtune", "Finetuning Llama2 with LoRA", "Finetuning Llama2 with QLoRA"], "terms": {"all": [3, 4, 8, 12, 42, 43, 47, 49, 56, 57, 59, 74, 76, 78, 79, 81, 82, 83, 84], "from": [3, 6, 7, 8, 9, 10, 13, 16, 17, 18, 20, 22, 23, 24, 25, 27, 28, 29, 30, 40, 43, 47, 48, 50, 51, 53, 55, 57, 58, 59, 68, 69, 75, 77, 79, 80, 81, 82, 83, 84], "famili": [3, 8, 22, 23, 26, 78, 83], "tune": [3, 6, 7, 8, 9, 11, 76, 77, 78, 81, 84, 85], "download": [3, 6, 74, 77, 83, 84, 85], "meta": [3, 6, 18, 57, 58, 81, 82, 83], "llama": [3, 6, 18, 45, 46, 57, 58, 81, 82, 83, 84], "3": [3, 6, 56, 59, 62, 70, 81, 82, 83, 85], "8b": [3, 36, 37], "hf": [3, 6, 57, 81, 82, 83], "token": [3, 6, 7, 8, 22, 23, 24, 25, 26, 42, 46, 47, 48, 55, 56, 80, 81, 82, 83, 84, 85], "access_token": 3, "pre": [3, 18, 77], "train": [3, 5, 6, 8, 9, 18, 22, 23, 24, 25, 26, 42, 49, 50, 57, 58, 61, 71, 76, 78, 80, 81, 83, 84, 85], "can": [3, 4, 6, 7, 8, 9, 10, 12, 22, 23, 45, 46, 55, 57, 59, 68, 69, 76, 77, 78, 80, 81, 82, 83, 84, 85], "hug": [3, 6, 22, 23, 24, 25, 26, 50, 78, 80, 82, 83], "face": [3, 6, 22, 23, 24, 25, 26, 50, 78, 80, 82, 83], "hub": [3, 6, 82], "follow": [3, 6, 8, 42, 50, 69, 76, 77, 80, 81, 82, 83, 84, 85], "command": [3, 8, 9, 59, 77, 81, 82, 83, 84, 85], "2": [3, 6, 9, 21, 26, 42, 55, 57, 58, 70, 73, 81, 82, 83, 84], "7b": [3, 6, 22, 23, 30, 33, 39, 40, 57, 58, 82, 83, 84, 85], "ai": [3, 40, 42, 69, 83], "mistralai": 3, "v0": 3, "1": [3, 6, 8, 26, 42, 47, 50, 55, 56, 58, 68, 69, 70, 73, 81, 82, 83, 84, 85], "googl": [3, 27], "2b": [3, 27], "ignor": [3, 6, 42, 43], "pattern": [3, 56], "These": [4, 6, 7, 8, 10, 59, 80, 81, 82, 83, 84, 85], "ar": [4, 6, 7, 9, 10, 13, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 31, 32, 33, 34, 35, 37, 38, 39, 41, 47, 52, 57, 58, 61, 77, 78, 80, 81, 82, 83, 84, 85], "common": [4, 7, 80, 83, 84], "us": [4, 6, 9, 10, 11, 15, 18, 22, 23, 24, 25, 26, 42, 43, 45, 46, 47, 48, 49, 55, 56, 57, 58, 59, 60, 61, 66, 67, 68, 69, 73, 76, 77, 78, 80, 82, 83, 84], "offer": 5, "allow": [5, 68, 85], "seamless": 5, "transit": 5, "between": [5, 6, 57, 81, 83, 84, 85], "format": [5, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 26, 57, 58, 81, 82, 83, 84], "interoper": [5, 6, 8, 78, 81, 85], "rest": [5, 85], "ecosystem": [5, 6, 8, 78, 81, 83, 85], "For": [5, 6, 7, 8, 22, 23, 42, 47, 59, 69, 73, 77, 80, 81, 82, 83, 84, 85], "comprehens": 5, "overview": [5, 7, 9, 82, 84, 85], "pleas": [5, 35, 38, 41, 77, 85], "see": [5, 6, 9, 18, 19, 35, 38, 41, 44, 51, 59, 62, 69, 71, 73, 77, 78, 80, 81, 82, 83, 84, 85], "deep": [5, 6, 7, 8, 9, 78, 82, 83], "dive": [5, 6, 7, 8, 9, 78, 82, 83], "enabl": [5, 7, 8, 9, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 71, 73, 83, 84, 85], "work": [5, 6, 8, 59, 78, 81, 83, 85], "set": [5, 6, 7, 8, 9, 22, 23, 24, 25, 26, 46, 47, 54, 60, 72, 73, 78, 80, 81, 82, 83, 84], "consumpt": 5, "dure": [5, 6, 22, 23, 24, 25, 42, 44, 46, 47, 48, 49, 81, 83, 84, 85], "provid": [5, 6, 7, 8, 10, 15, 19, 26, 47, 59, 69, 78, 80, 81, 82, 83], "debug": [5, 6, 7, 8], "your": [5, 9, 10, 68, 69, 76, 77, 78, 80, 83, 84, 85], "finetun": [5, 6, 7, 8, 31, 32, 33, 34, 37, 65, 76, 78, 82, 83], "job": [5, 9, 73, 82], "variou": [5, 17], "dataset": [5, 7, 13, 16, 17, 20, 22, 23, 24, 25, 26, 78, 82, 83], "walk": [6, 8, 68, 78, 80, 81, 82, 85], "you": [6, 7, 8, 9, 10, 17, 18, 22, 23, 59, 68, 69, 76, 77, 78, 80, 81, 82, 83, 84, 85], "through": [6, 7, 8, 9, 43, 78, 80, 81, 82, 85], "design": [6, 8], "behavior": [6, 80], "associ": [6, 7, 8, 81, 84], "util": [6, 7, 8, 9, 10, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 78, 81, 82, 83, 85], "what": [6, 7, 9, 18, 19, 24, 25, 76, 80, 81, 82, 83], "cover": [6, 7, 8, 9, 81, 85], "how": [6, 7, 8, 9, 76, 80, 81, 82, 83, 85], "we": [6, 7, 8, 9, 22, 23, 42, 44, 46, 47, 52, 55, 57, 58, 61, 78, 80, 81, 82, 83, 84, 85], "them": [6, 7, 43, 49, 55, 80, 81, 84, 85], "scenario": 6, "full": [6, 7, 8, 35, 38, 41, 55, 78, 83, 84], "compos": 6, "compon": [6, 8, 12, 71, 78, 80, 82, 84, 85], "which": [6, 8, 22, 23, 24, 25, 31, 32, 33, 34, 37, 39, 42, 46, 47, 48, 50, 55, 57, 58, 61, 66, 69, 78, 80, 81, 82, 83, 84, 85], "plug": 6, "ani": [6, 7, 8, 10, 11, 12, 13, 16, 17, 20, 49, 53, 54, 55, 57, 58, 73, 80, 81, 82, 83, 84], "recip": [6, 7, 9, 10, 11, 43, 57, 58, 78, 80, 81, 83, 85], "evalu": [6, 8, 76, 78, 82, 84, 85], "gener": [6, 8, 13, 16, 20, 26, 55, 73, 74, 76, 80, 84, 85], "each": [6, 8, 14, 17, 31, 32, 33, 34, 37, 39, 42, 46, 47, 55, 56, 73, 78, 80, 81, 82, 83, 84], "support": [6, 8, 9, 10, 19, 22, 23, 24, 25, 26, 42, 52, 58, 61, 65, 78, 80, 81, 82, 83, 84, 85], "model": [6, 7, 8, 10, 15, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 51, 52, 53, 54, 57, 58, 71, 72, 76, 78, 80, 85], "make": [6, 7, 8, 9, 42, 48, 78, 81, 82, 83, 84, 85], "easi": [6, 8, 78, 84], "understand": [6, 7, 8, 76, 78, 80, 84, 85], "extend": [6, 8, 78], "befor": [6, 21, 47, 48, 52, 57, 81], "let": [6, 7, 9, 80, 81, 82, 83, 84, 85], "s": [6, 7, 8, 9, 10, 11, 13, 14, 15, 18, 19, 21, 22, 23, 24, 25, 26, 31, 32, 33, 34, 37, 39, 42, 46, 47, 48, 49, 51, 53, 56, 57, 58, 60, 68, 71, 78, 80, 82, 84, 85], "defin": [6, 7, 8, 43, 51, 52, 53, 82, 84], "some": [6, 7, 15, 53, 54, 76, 78, 81, 82, 84, 85], "concept": [6, 81, 82], "In": [6, 7, 8, 46, 52, 68, 69, 81, 83, 84, 85], "ll": [6, 7, 8, 56, 78, 80, 81, 82, 83, 85], "talk": 6, "about": [6, 8, 57, 69, 78, 81, 82, 83, 84, 85], "take": [6, 7, 8, 10, 43, 44, 49, 57, 59, 60, 80, 81, 82, 83, 84, 85], "close": [6, 8, 66, 67, 68, 69, 84], "look": [6, 7, 8, 68, 77, 80, 81, 82, 83, 84], "veri": [6, 47, 81], "simpli": [6, 7, 80, 81, 83, 85], "dictat": 6, "state_dict": [6, 49, 57, 58, 84, 85], "store": [6, 66, 69, 84, 85], "file": [6, 7, 8, 9, 10, 11, 55, 56, 57, 58, 59, 66, 69, 71, 75, 78, 79, 80, 81, 82, 83, 84, 85], "disk": [6, 66], "weight": [6, 8, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 49, 51, 52, 57, 58, 69, 76, 81, 82, 83, 84, 85], "string": [6, 22, 23, 24, 25, 26, 51, 55, 56, 60, 61], "kei": [6, 7, 9, 42, 44, 47, 54, 57, 81, 82, 84, 85], "identifi": 6, "state": [6, 8, 49, 53, 54, 57, 58, 81, 83, 84, 85], "dict": [6, 7, 8, 9, 10, 13, 16, 17, 20, 49, 53, 54, 57, 58, 64], "If": [6, 7, 12, 13, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 42, 49, 52, 57, 58, 60, 61, 64, 68, 69, 73, 77, 80, 81, 82, 83, 84], "identif": [6, 56], "don": [6, 7, 8, 69, 73, 81, 82, 83, 85], "t": [6, 7, 8, 26, 56, 61, 69, 73, 81, 82, 83, 85], "match": [6, 77, 81, 83, 84], "up": [6, 8, 9, 22, 23, 80, 82, 83, 84, 85], "exactli": 6, "those": [6, 84], "definit": [6, 84], "either": [6, 57, 84, 85], "run": [6, 7, 9, 11, 43, 44, 47, 49, 57, 58, 68, 69, 77, 78, 82, 83, 84, 85], "explicit": 6, "error": [6, 7, 21, 57, 73], "load": [6, 8, 57, 58, 59, 68, 81, 83, 84], "rais": [6, 10, 12, 19, 21, 26, 42, 47, 57, 58, 61, 64, 69, 73], "an": [6, 7, 8, 9, 10, 13, 21, 22, 23, 24, 25, 26, 42, 47, 51, 53, 54, 57, 58, 69, 78, 80, 81, 82, 83, 84, 85], "except": [6, 19, 80], "wors": 6, "silent": [6, 43], "succe": 6, "infer": [6, 18, 42, 44, 46, 47, 48, 76, 81, 82, 83, 85], "expect": [6, 7, 10, 13, 16, 17, 20, 46, 69, 80, 84], "addit": [6, 7, 8, 10, 57, 58, 61, 64, 66, 68, 69, 72, 78, 82, 84], "line": [6, 8, 59, 82, 83], "also": [6, 7, 8, 9, 10, 42, 47, 52, 60, 69, 77, 80, 81, 82, 83, 84, 85], "need": [6, 7, 8, 9, 17, 26, 42, 43, 47, 68, 69, 77, 80, 81, 82, 83, 84, 85], "shape": [6, 42, 44, 46, 47, 48, 52], "valu": [6, 7, 26, 27, 28, 29, 30, 36, 40, 42, 44, 45, 47, 50, 57, 59, 66, 67, 68, 69, 73, 82, 83, 84], "two": [6, 7, 21, 78, 81, 82, 83, 84, 85], "popular": [6, 78, 80, 81], "llama2": [6, 7, 8, 10, 18, 22, 23, 26, 28, 29, 30, 31, 32, 33, 34, 35, 43, 47, 48, 55, 76, 78, 82, 83], "offici": [6, 18, 82, 83], "implement": [6, 8, 22, 23, 24, 25, 26, 43, 45, 46, 50, 51, 52, 57, 68, 78, 84, 85], "when": [6, 7, 8, 11, 47, 49, 50, 68, 81, 83, 84, 85], "websit": 6, "get": [6, 7, 8, 9, 55, 61, 62, 63, 77, 78, 80, 81, 82, 84], "access": [6, 7, 8, 57, 81, 82], "singl": [6, 7, 10, 13, 14, 15, 16, 17, 18, 19, 20, 42, 57, 58, 81, 82, 83, 84, 85], "pth": [6, 81, 83], "inspect": [6, 81, 84, 85], "content": [6, 55, 80], "easili": [6, 7, 78, 80, 84, 85], "torch": [6, 44, 47, 49, 50, 60, 61, 64, 71, 72, 73, 81, 82, 83, 84, 85], "import": [6, 7, 10, 68, 69, 80, 81, 82, 84, 85], "consolid": [6, 83], "00": [6, 75, 79, 82, 83], "mmap": [6, 81], "true": [6, 7, 22, 23, 24, 25, 34, 35, 38, 41, 49, 55, 56, 57, 58, 64, 68, 80, 81, 83, 84, 85], "weights_onli": 6, "map_loc": [6, 81], "cpu": [6, 8, 49, 61, 77, 81, 85], "tensor": [6, 42, 43, 44, 45, 46, 47, 48, 49, 52, 57, 66, 67, 68, 69, 70, 84, 85], "item": 6, "print": [6, 9, 22, 23, 24, 25, 26, 55, 80, 82, 84, 85], "f": [6, 9, 22, 23, 24, 25, 81, 84, 85], "tok_embed": [6, 47], "size": [6, 8, 10, 22, 23, 24, 25, 42, 44, 45, 46, 47, 48, 63, 78, 80, 81, 82, 83, 84], "32000": [6, 10, 84], "4096": [6, 10, 22, 23, 42, 46, 84], "len": [6, 22, 23, 24, 25, 47], "292": 6, "The": [6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 31, 32, 33, 34, 37, 45, 46, 49, 50, 55, 56, 57, 59, 60, 61, 62, 69, 71, 73, 77, 78, 81, 82, 83, 84, 85], "contain": [6, 42, 44, 46, 47, 48, 51, 53, 54, 55, 56, 57, 58, 59, 68, 70, 81, 83, 84], "includ": [6, 7, 8, 14, 17, 52, 57, 58, 59, 78, 81, 82, 83, 84, 85], "input": [6, 13, 14, 17, 22, 23, 24, 26, 42, 43, 45, 46, 47, 48, 52, 55, 57, 70, 73, 80, 84, 85], "embed": [6, 42, 44, 45, 46, 47, 83], "tabl": [6, 85], "call": [6, 10, 43, 49, 59, 66, 67, 68, 69, 84, 85], "layer": [6, 8, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 47, 48, 52, 78, 83, 84, 85], "have": [6, 7, 10, 42, 44, 51, 59, 68, 71, 77, 80, 81, 82, 83, 84, 85], "dim": [6, 42, 43, 45, 46, 47, 48], "most": [6, 7, 56, 82, 84, 85], "within": [6, 7, 10, 26, 43, 68, 73, 81, 83, 84, 85], "default": [6, 7, 15, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 42, 43, 45, 46, 47, 48, 49, 50, 52, 55, 56, 57, 58, 59, 61, 66, 69, 70, 71, 73, 77, 81, 83, 84, 85], "everi": [6, 8, 43, 68, 77, 85], "config": [6, 9, 10, 11, 12, 42, 57, 59, 69, 78, 80, 81, 83, 84, 85], "repo": [6, 57, 58, 81], "first": [6, 7, 10, 21, 47, 56, 57, 59, 76, 78, 81, 83, 84, 85], "big": [6, 81], "split": [6, 80, 81], "across": [6, 8, 57, 68, 73, 81, 83], "bin": [6, 81], "To": [6, 7, 8, 9, 57, 77, 78, 80, 81, 82, 83, 84, 85], "correctli": [6, 8, 12, 57, 77, 82, 85], "piec": 6, "one": [6, 8, 21, 43, 55, 80, 81, 82, 83, 85], "pytorch_model": [6, 81], "00001": 6, "00002": 6, "embed_token": 6, "241": 6, "Not": 6, "onli": [6, 9, 42, 46, 47, 48, 52, 53, 55, 58, 59, 61, 81, 82, 83, 84, 85], "doe": [6, 19, 51, 57, 59, 81], "fewer": [6, 42], "sinc": [6, 7, 10, 43, 57, 81, 83], "instead": [6, 8, 43, 44, 52, 81, 83, 84], "mismatch": 6, "name": [6, 7, 9, 13, 16, 17, 20, 51, 54, 56, 57, 58, 59, 60, 66, 67, 68, 69, 81, 83], "caus": [6, 55], "try": [6, 7, 81, 82, 83, 85], "same": [6, 7, 31, 32, 33, 34, 37, 42, 44, 48, 55, 59, 69, 81, 83, 84, 85], "As": [6, 7, 8, 9, 52, 78, 81, 83, 85], "re": [6, 7, 56, 78, 81, 82, 83, 84], "care": [6, 43, 57, 81, 83, 84], "like": [6, 7, 8, 9, 77, 80, 81, 82, 84], "end": [6, 8, 56, 76, 78, 83, 84], "number": [6, 8, 22, 23, 26, 42, 44, 47, 50, 57, 58, 63, 73, 82, 84], "just": [6, 13, 78, 80, 82, 83, 84], "save": [6, 8, 9, 49, 57, 58, 69, 76, 81, 83, 84], "less": [6, 26, 81, 82, 83, 85], "prone": 6, "manag": [6, 71], "invari": 6, "accept": [6, 7, 26, 55, 82, 85], "multipl": [6, 7, 8, 52, 66, 67, 68, 69, 82, 83], "sourc": [6, 7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 36, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 80, 81], "worri": [6, 82], "explicitli": [6, 51, 78, 84], "convert": [6, 57, 70, 81, 85], "time": [6, 55, 66, 68, 81, 83, 85], "produc": [6, 85], "back": [6, 21, 57, 84, 85], "origin": [6, 22, 23, 49, 52, 80, 81, 83, 84, 85], "form": [6, 7, 8, 21], "One": [6, 81], "advantag": [6, 84], "being": [6, 57, 58, 60, 85], "should": [6, 7, 8, 14, 17, 18, 19, 31, 32, 33, 34, 37, 39, 42, 43, 51, 59, 66, 67, 68, 69, 77, 78, 80, 81, 82, 83, 84, 85], "abl": [6, 8, 80, 81, 82, 83], "fine": [6, 8, 9, 76, 78, 81, 84], "post": [6, 85], "tool": [6, 81, 82], "quantiz": [6, 31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 76, 82, 85], "eval": [6, 76, 78], "without": [6, 7, 9, 77, 78, 80, 81, 84], "code": [6, 8, 47, 74, 78, 80, 82], "chang": [6, 7, 9, 13, 77, 80, 81, 82, 83, 84, 85], "OR": 6, "convers": [6, 14, 15, 18, 19, 21, 26, 57, 78, 80, 81, 83, 84, 85], "script": [6, 9, 81, 82, 83], "wai": [6, 7, 80, 81, 82, 83], "surround": [6, 8, 78], "load_checkpoint": [6, 8, 57, 58], "save_checkpoint": [6, 8, 9, 57, 58], "method": [6, 7, 8, 9, 11, 22, 23, 24, 25, 26, 49, 51, 53, 59, 77, 78, 81, 83, 84, 85], "convertor": 6, "avail": [6, 8, 59, 60, 61, 78, 81, 83, 84], "here": [6, 7, 9, 15, 24, 45, 46, 80, 81, 82, 83, 84, 85], "three": [6, 8, 82], "hfcheckpoint": 6, "read": [6, 57, 58, 78], "write": [6, 8, 57, 58, 66, 80, 82], "compat": [6, 57], "transform": [6, 8, 31, 32, 33, 34, 37, 39, 47, 48, 50, 84], "framework": [6, 8, 78], "mention": [6, 81, 85], "abov": [6, 49, 77, 81, 83, 84, 85], "assum": [6, 13, 16, 17, 20, 50, 53, 56, 61, 81, 84], "checkpoint_dir": [6, 7, 57, 58, 81, 83], "necessari": [6, 26, 66, 67, 68, 69, 84], "json": [6, 57, 71, 81], "easiest": [6, 81, 82], "sure": [6, 7, 81, 82, 83, 84, 85], "everyth": [6, 8, 59, 78, 82], "flow": [6, 85], "By": [6, 83, 84, 85], "safetensor": 6, "output": [6, 17, 22, 23, 24, 26, 31, 32, 33, 34, 37, 39, 42, 43, 45, 46, 47, 48, 52, 54, 67, 71, 77, 80, 81, 82, 83, 84, 85], "dir": [6, 69, 77, 81, 82, 83], "output_dir": [6, 7, 57, 58, 71, 81, 83, 84, 85], "specifi": [6, 7, 8, 10, 42, 69, 80, 81, 82, 83, 85], "argument": [6, 7, 10, 17, 26, 35, 38, 41, 42, 59, 64, 66, 68, 69, 72, 83, 84], "snippet": 6, "explain": 6, "setup": [6, 7, 8, 47, 72, 81, 84, 85], "_component_": [6, 7, 9, 10, 80, 81, 83, 84], "fullmodelhfcheckpoint": [6, 81], "directori": [6, 7, 57, 58, 66, 68, 69, 81, 82, 83], "sort": [6, 57], "id": [6, 22, 23, 26, 55, 56, 57, 70, 81], "so": [6, 7, 57, 59, 77, 78, 81, 82, 83, 84, 85], "order": [6, 8, 57, 68, 69, 82], "matter": [6, 57, 84], "checkpoint_fil": [6, 7, 9, 57, 58, 81, 83, 84, 85], "restart": 6, "previou": [6, 57, 58], "more": [6, 7, 8, 44, 46, 59, 69, 71, 73, 78, 80, 81, 82, 83, 84, 85], "next": [6, 83, 85], "section": [6, 8, 76, 81, 83, 85], "recipe_checkpoint": [6, 57, 58], "null": [6, 7], "usual": [6, 46, 57, 69, 81, 84], "model_typ": [6, 57, 58, 81, 83], "resume_from_checkpoint": [6, 57, 58], "fals": [6, 7, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 52, 55, 56, 57, 58, 71, 81, 83, 84, 85], "requir": [6, 7, 26, 57, 68, 69, 73, 77, 80, 82, 85], "param": [6, 8, 31, 32, 33, 34, 37, 52, 53, 54, 57, 84, 85], "directli": [6, 7, 8, 10, 57, 80, 81, 82, 83, 84, 85], "help": [6, 18, 47, 57, 59, 76, 77, 78, 80, 81, 82, 83, 85], "ensur": [6, 7, 12, 21, 26, 42, 57, 61, 78, 82], "out": [6, 7, 8, 22, 23, 24, 25, 57, 58, 76, 78, 81, 82, 83, 84, 85], "case": [6, 8, 9, 42, 57, 61, 66, 78, 80, 81, 83, 84, 85], "discrep": [6, 57], "along": [6, 83, 84], "detail": [6, 44, 71, 73, 81, 82, 83, 84, 85], "found": [6, 7, 9, 45, 46, 84, 85], "metacheckpoint": 6, "github": [6, 10, 31, 32, 33, 34, 37, 42, 45, 46, 50, 77, 82], "repositori": [6, 18, 81, 82], "fullmodelmetacheckpoint": [6, 83], "torchtunecheckpoint": 6, "perform": [6, 43, 78, 81, 83, 85], "current": [6, 42, 46, 47, 48, 58, 63, 66, 68, 73, 81, 82, 83], "test": [6, 7, 8, 77, 78], "complet": [6, 8, 80, 81, 82, 83], "written": [6, 7, 8, 57, 58, 66, 67, 68, 69, 78], "begin": [6, 55, 56, 83, 85], "partit": [6, 85], "ha": [6, 51, 53, 55, 80, 81, 82, 83, 84, 85], "standard": [6, 67, 78, 81, 83], "key_1": 6, "weight_1": 6, "key_2": 6, "weight_2": 6, "mid": 6, "chekpoint": 6, "middl": [6, 81], "inform": [6, 69, 78, 81, 82, 83], "subsequ": [6, 8], "recipe_st": [6, 57, 58], "pt": [6, 9, 57, 58, 81, 83], "epoch": [6, 8, 9, 50, 57, 58, 81, 82, 83], "optim": [6, 7, 8, 50, 81, 82, 83, 84, 85], "etc": [6, 8, 57, 82], "prevent": 6, "flood": 6, "overwritten": 6, "note": [6, 7, 17, 47, 51, 55, 57, 71, 73, 80, 81, 84, 85], "updat": [6, 7, 8, 77, 81, 82, 83, 84, 85], "hf_model_0001_0": [6, 81], "hf_model_0002_0": [6, 81], "both": [6, 81, 84, 85], "adapt": [6, 51, 52, 53, 54, 57, 58, 81, 84, 85], "merg": [6, 10, 57, 81, 83, 85], "would": [6, 7, 9, 47, 77, 80, 81, 84, 85], "our": [6, 8, 78, 80, 81, 82, 84, 85], "tutori": [6, 78, 80, 81, 82, 83, 84, 85], "primari": [6, 7, 8, 82], "want": [6, 7, 8, 9, 10, 77, 81, 82, 83, 84], "resum": [6, 8, 50, 57, 58, 85], "initi": [6, 8, 11, 27, 28, 29, 30, 36, 40, 64, 82, 84, 85], "frozen": [6, 84, 85], "base": [6, 10, 26, 31, 32, 33, 34, 35, 37, 38, 39, 41, 46, 50, 52, 54, 57, 59, 66, 76, 81, 82, 83, 84, 85], "well": [6, 7, 8, 78, 80, 81, 83, 85], "learnt": [6, 81], "someth": [6, 8, 9, 81], "NOT": 6, "refer": [6, 7, 8, 45, 46, 78, 84], "adapter_checkpoint": [6, 57, 58], "adapter_0": [6, 81], "now": [6, 55, 80, 81, 82, 83, 84, 85], "knowledg": 6, "creat": [6, 7, 10, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 44, 50, 57, 58, 66, 68, 80, 81, 83, 85], "simpl": [6, 8, 76, 82, 84, 85], "forward": [6, 8, 42, 43, 45, 46, 47, 48, 52, 83, 84, 85], "13b": [6, 28, 31, 34], "modeltyp": [6, 57, 58], "llama2_13b": [6, 31, 34], "right": [6, 57, 81, 83, 84], "pytorch_fil": 6, "00003": 6, "torchtune_sd": 6, "load_state_dict": [6, 84], "successfulli": [6, 82], "vocab": [6, 10, 47, 83], "70": [6, 29], "x": [6, 42, 43, 45, 46, 47, 48, 52, 84, 85], "randint": 6, "0": [6, 8, 32, 33, 35, 42, 47, 50, 52, 55, 68, 69, 70, 73, 75, 79, 81, 82, 83, 84, 85], "no_grad": 6, "6": [6, 45, 70, 81, 85], "3989": 6, "9": [6, 81, 85], "0531": 6, "2375": 6, "5": [6, 50, 70, 71, 81, 82, 83], "2822": 6, "4": [6, 26, 42, 70, 78, 81, 83, 84, 85], "4872": 6, "7469": 6, "8": [6, 22, 23, 24, 25, 31, 32, 33, 34, 35, 37, 38, 39, 41, 81, 84, 85], "6737": 6, "11": [6, 81, 83, 85], "0023": 6, "8235": 6, "6819": 6, "2424": 6, "0109": 6, "6915": 6, "7": [6, 70], "3618": 6, "1628": 6, "8594": 6, "5857": 6, "1151": 6, "7808": 6, "2322": 6, "8850": 6, "9604": 6, "7624": 6, "6040": 6, "3159": 6, "5849": 6, "8039": 6, "9322": 6, "2010": 6, "6824": 6, "8929": 6, "8465": 6, "3794": 6, "3500": 6, "6145": 6, "5931": 6, "do": [6, 8, 69, 80, 81, 82, 83, 84], "find": [6, 8, 9, 81, 82, 84], "list": [6, 7, 14, 15, 18, 19, 21, 22, 23, 26, 31, 32, 33, 34, 35, 37, 38, 39, 41, 51, 52, 55, 56, 57, 58, 59, 62, 65, 70, 80, 82, 83], "builder": [6, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 80, 85], "hope": 6, "deeper": [6, 82], "insight": [6, 81], "happi": [6, 81], "thi": [7, 8, 9, 10, 22, 23, 26, 42, 43, 46, 47, 48, 49, 50, 51, 55, 57, 58, 59, 60, 61, 66, 68, 69, 71, 73, 76, 77, 78, 80, 81, 82, 83, 84, 85], "guid": [7, 9, 78, 80, 82, 84], "yaml": [7, 8, 10, 11, 59, 69, 78, 81, 82, 83, 84, 85], "pars": [7, 10, 56, 59, 82], "effect": 7, "cli": [7, 9, 11, 77, 81, 82], "prerequisit": [7, 80, 81, 82, 83, 84, 85], "Be": [7, 81, 82, 83, 84, 85], "familiar": [7, 81, 82, 83, 84, 85], "torchtun": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 77, 80, 82], "instal": [7, 9, 68, 69, 76, 81, 82, 83, 84, 85], "fundament": 7, "There": [7, 14, 21, 81, 82, 83, 84], "entri": [7, 8, 82], "point": [7, 8, 80, 81, 82, 83, 84, 85], "locat": [7, 83, 84, 85], "thei": [7, 8, 47, 59, 80, 84], "truth": [7, 81, 83], "reproduc": 7, "overridden": [7, 43, 59], "quick": 7, "experiment": 7, "modifi": [7, 8, 9, 49, 78, 81, 83, 84, 85], "serv": [7, 80, 84], "particular": [7, 26, 80, 84, 85], "seed": [7, 8, 9, 73, 82], "shuffl": 7, "devic": [7, 8, 60, 61, 81, 82, 83, 84], "cuda": [7, 60, 61, 77, 81, 85], "dtype": [7, 8, 44, 49, 61, 65, 81, 85], "fp32": [7, 85], "enable_fsdp": 7, "mani": [7, 80, 81], "object": [7, 10, 14, 15, 18, 19, 42, 80], "keyword": [7, 10, 26, 49], "loss": [7, 8, 22, 23, 24, 25, 82, 84, 85], "function": [7, 8, 10, 11, 42, 43, 49, 60, 63, 73, 78, 80, 85], "exampl": [7, 8, 9, 10, 11, 15, 18, 19, 22, 23, 24, 25, 26, 42, 51, 55, 57, 58, 68, 69, 70, 74, 75, 77, 79, 80, 81, 83, 84, 85], "subfield": 7, "dotpath": 7, "wish": 7, "exact": [7, 10, 81], "path": [7, 8, 9, 10, 22, 23, 24, 25, 26, 55, 56, 57, 58, 59, 71, 81, 83, 84], "normal": [7, 45, 47, 48, 55, 80, 84, 85], "python": [7, 56, 59, 62, 69, 73, 74, 81], "alpaca_dataset": [7, 22, 80], "custom": [7, 8, 78, 81, 82, 83, 84], "train_on_input": [7, 22, 23, 24, 25, 26, 80], "onc": [7, 81, 82, 83, 84, 85], "ve": [7, 44, 56, 80, 81, 83, 84], "instanc": [7, 10, 43, 49, 53, 54, 84], "cfg": [7, 8, 11, 12], "automat": [7, 9, 10, 81, 85], "under": [7, 81, 83, 85], "preced": [7, 10, 83, 84], "actual": [7, 9], "throw": 7, "notic": [7, 80, 84], "miss": [7, 84], "posit": [7, 10, 42, 46, 47, 48, 83], "anoth": [7, 81], "handl": [7, 11, 55, 81, 84, 85], "def": [7, 8, 9, 11, 80, 84, 85], "dictconfig": [7, 8, 10, 11, 12, 69], "arg": [7, 10, 47, 49, 51, 59, 67], "tupl": [7, 10, 26, 49, 55, 56, 59, 63, 70], "kwarg": [7, 10, 49, 51, 59, 64, 66, 67, 68, 69, 72], "str": [7, 10, 13, 16, 17, 20, 22, 23, 24, 25, 26, 49, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 65, 66, 67, 68, 69, 71, 73], "mean": [7, 45, 82, 84], "pass": [7, 10, 42, 43, 49, 61, 64, 68, 69, 72, 84, 85], "add": [7, 9, 56, 59, 80, 81, 83, 84, 85], "d": [7, 42, 47, 48, 56, 80, 84], "llama2_token": [7, 81], "tmp": [7, 82, 83], "option": [7, 8, 13, 16, 17, 20, 31, 32, 33, 34, 37, 39, 42, 46, 47, 48, 49, 55, 56, 57, 58, 60, 61, 62, 66, 69, 71, 72, 73, 77, 78, 81], "bool": [7, 22, 23, 24, 25, 26, 31, 32, 33, 34, 35, 37, 38, 39, 41, 49, 52, 55, 56, 57, 58, 64, 68, 71, 85], "max_seq_len": [7, 10, 22, 23, 26, 42, 44, 46, 47, 55, 56, 80], "int": [7, 9, 22, 23, 26, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 44, 45, 46, 47, 50, 52, 55, 56, 57, 58, 63, 66, 67, 68, 69, 70, 73, 80, 84, 85], "512": [7, 22, 23, 80, 85], "instructdataset": [7, 22, 23, 24, 25, 80], "alreadi": [7, 64, 77, 81, 84], "overwrit": [7, 77], "duplic": [7, 8, 78], "sometim": 7, "than": [7, 21, 26, 42, 81, 82, 83, 84, 85], "resolv": [7, 82], "alpaca": [7, 13, 22, 23, 31, 32, 33, 34, 37, 80], "metric_logg": [7, 8, 9], "metric_log": [7, 9, 66, 67, 68, 69], "disklogg": 7, "log_dir": [7, 66, 68, 69], "conveni": [7, 8], "quickli": [7, 80], "verifi": [7, 60, 61, 82, 84], "properli": 7, "experi": [7, 69, 76, 78, 83, 84], "wa": [7, 81, 83, 84, 85], "cp": [7, 77, 81, 82, 83], "7b_lora_single_devic": [7, 81, 82, 84, 85], "my_config": 7, "discuss": [7, 82, 84], "guidelin": 7, "while": [7, 8, 31, 32, 33, 34, 37, 43, 78, 81, 85], "mai": [7, 9, 71, 80, 82, 84], "tempt": 7, "put": [7, 8, 82, 84], "much": [7, 81, 83, 84, 85], "give": [7, 84], "maximum": [7, 22, 23, 26, 42, 44, 46, 47, 56], "flexibl": [7, 80], "switch": 7, "encourag": [7, 84], "clariti": 7, "significantli": 7, "easier": [7, 81, 82], "dont": 7, "slimorca_dataset": 7, "privat": 7, "typic": [7, 85], "expos": [7, 8, 80, 82], "parent": 7, "modul": [7, 10, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 72, 73, 82, 84, 85], "__init__": [7, 8, 84, 85], "py": [7, 10, 31, 32, 33, 34, 37, 42, 44, 45, 46, 50, 81, 83], "guarante": 7, "stabil": [7, 78, 85], "underscor": 7, "_alpaca": 7, "collect": [7, 82], "differ": [7, 9, 55, 78, 81, 83, 84, 85], "itself": 7, "via": [7, 9, 52, 84, 85], "pair": [7, 70, 80], "k1": [7, 8], "v1": [7, 8], "k2": [7, 8], "v2": [7, 8], "lora_finetune_single_devic": [7, 81, 82, 83, 84, 85], "checkpoint": [7, 8, 49, 56, 57, 58, 69, 72, 78, 83, 84, 85], "home": 7, "my_model_checkpoint": 7, "file_1": 7, "file_2": 7, "my_tokenizer_path": 7, "class": [7, 9, 13, 14, 15, 16, 17, 18, 19, 20, 42, 43, 44, 45, 46, 47, 48, 51, 52, 53, 54, 55, 56, 57, 58, 59, 66, 67, 68, 69, 80, 82, 84, 85], "assign": 7, "nest": 7, "dot": 7, "notat": [7, 42, 46, 47, 48], "core": [8, 78, 80, 82, 85], "i": [8, 18, 19, 49, 54, 56, 80, 81, 83, 85], "structur": [8, 14, 15, 18, 19, 80, 81], "new": [8, 40, 66, 68, 81, 82, 83, 84, 85], "user": [8, 14, 15, 18, 19, 21, 42, 55, 80, 82], "thought": [8, 78, 82, 85], "target": [8, 78], "pipelin": [8, 78], "llm": [8, 76, 78, 80, 81, 84], "eg": [8, 47, 57, 78], "meaning": [8, 78, 81], "featur": [8, 9, 77, 78, 81, 82], "fsdp": [8, 78, 82, 83], "activ": [8, 43, 72, 78, 85], "gradient": [8, 78, 81, 83, 84, 85], "accumul": [8, 78], "mix": [8, 81], "precis": [8, 49, 61, 78, 82, 85], "appli": [8, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 45, 46, 47, 48, 78, 85], "given": [8, 10, 17, 21, 52, 60, 61, 78, 84], "complex": 8, "becom": [8, 77, 80], "harder": 8, "anticip": 8, "architectur": [8, 18, 19, 47, 80], "methodolog": 8, "reason": [8, 81], "possibl": 8, "trade": 8, "off": [8, 55, 81], "memori": [8, 22, 23, 49, 76, 78, 80, 81, 82, 83], "vs": [8, 82], "qualiti": [8, 81, 84], "believ": 8, "best": 8, "suit": [8, 82], "specif": [8, 10, 80, 81, 85], "b": [8, 42, 46, 47, 48, 52, 69, 84, 85], "fit": [8, 22, 23], "solut": 8, "result": [8, 55, 81, 83, 84, 85], "meant": [8, 49], "depend": [8, 9, 13, 81, 84, 85], "level": [8, 62, 78, 85], "expertis": 8, "routin": 8, "yourself": [8, 83, 84], "exist": [8, 77, 80, 81, 82, 83, 85], "ad": [8, 55, 84, 85], "ones": 8, "modular": [8, 78], "build": [8, 78, 83, 84], "block": [8, 31, 32, 33, 34, 37, 39, 78], "wandb": [8, 9, 69, 82], "log": [8, 62, 66, 67, 68, 69, 81, 82, 83, 85], "fulli": 8, "nativ": [8, 76, 78, 84, 85], "pytorch": [8, 47, 49, 68, 71, 73, 76, 77, 78, 83, 84, 85], "correct": [8, 16, 24, 45, 46, 47, 60, 78, 80], "numer": [8, 78], "pariti": [8, 78], "verif": 8, "extens": [8, 78], "comparison": [8, 84, 85], "benchmark": [8, 73, 78, 81, 83, 84], "limit": 8, "hidden": [8, 43], "behind": 8, "100": [8, 22, 23, 24, 25, 26, 70, 71, 84, 85], "flag": [8, 22, 23, 24, 25, 85], "prefer": [8, 78, 80], "over": [8, 50, 59, 78, 80, 81, 83, 84, 85], "unnecessari": 8, "abstract": [8, 14, 17, 78, 82, 85], "No": [8, 78], "inherit": [8, 59, 78], "go": [8, 18, 19, 55, 78, 80, 81, 82, 85], "upon": [8, 83], "figur": [8, 84, 85], "spectrum": 8, "decid": 8, "interact": [8, 76, 82], "start": [8, 9, 56, 77, 78, 80, 81, 82], "paradigm": 8, "consist": [8, 82], "configur": [8, 22, 23, 24, 25, 26, 48, 78, 82, 83, 84, 85], "paramet": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 64, 66, 67, 68, 69, 70, 71, 72, 73, 76, 78, 81, 82, 83, 84, 85], "overrid": [8, 11, 81, 82, 83, 85], "togeth": [8, 69, 82, 84], "valid": [8, 21, 77, 81, 82], "environ": [8, 77, 81, 82], "logic": [8, 57, 78, 82, 84], "api": [8, 9, 35, 38, 41, 81, 82, 83, 85], "closer": [8, 84], "monolith": [8, 78], "trainer": [8, 63], "A": [8, 9, 49, 52, 55, 56, 57, 59, 70, 75, 76, 79, 81, 84, 85], "wrapper": [8, 55, 56, 84], "around": [8, 55, 56, 71, 81, 84, 85], "extern": 8, "primarili": [8, 84], "eleutherai": [8, 78, 84], "har": [8, 78, 84], "control": [8, 22, 23, 24, 25, 73, 81], "multi": [8, 42, 83], "stage": 8, "distil": 8, "oper": [8, 71, 73], "turn": [8, 21, 56], "dataload": [8, 22, 23, 24, 25], "applic": [8, 42, 57, 58, 69], "clean": [8, 9, 22], "after": [8, 44, 45, 66, 67, 68, 69, 85], "process": [8, 9, 49, 73, 82, 85], "group": [8, 42, 66, 67, 68, 69, 83], "init_process_group": [8, 64], "backend": 8, "gloo": 8, "els": [8, 59, 69, 78, 85], "nccl": 8, "fullfinetunerecipedistribut": 8, "cleanup": 8, "other": [8, 10, 59, 80, 82, 83, 84], "stuff": 8, "carri": 8, "relev": [8, 81, 84], "interfac": [8, 14, 17, 51], "metric": [8, 82], "logger": [8, 62, 66, 67, 68, 69, 82], "self": [8, 9, 31, 32, 33, 34, 37, 39, 42, 47, 48, 51, 80, 84, 85], "_devic": 8, "get_devic": 8, "_dtype": 8, "get_dtyp": 8, "ckpt_dict": 8, "wrap": [8, 71, 72], "_model": 8, "_setup_model": 8, "_token": [8, 80], "_setup_token": 8, "_optim": 8, "_setup_optim": 8, "_loss_fn": 8, "_setup_loss": 8, "_sampler": 8, "_dataload": 8, "_setup_data": 8, "backward": [8, 85], "zero_grad": 8, "curr_epoch": 8, "rang": [8, 73, 83], "epochs_run": [8, 9], "total_epoch": [8, 9], "idx": 8, "batch": [8, 22, 23, 24, 25, 42, 44, 46, 47, 48, 55, 70, 78, 80, 82, 83, 84], "enumer": 8, "_autocast": 8, "logit": 8, "label": [8, 22, 23, 26, 70], "total_training_step": 8, "_log_every_n_step": 8, "_metric_logg": 8, "log_dict": [8, 66, 67, 68, 69], "step": [8, 47, 50, 56, 66, 67, 68, 69, 71, 76, 81, 84, 85], "learn": [8, 50, 78, 80, 82, 83, 84, 85], "decor": [8, 11], "recipe_main": [8, 11], "none": [8, 9, 12, 13, 16, 17, 19, 20, 21, 42, 46, 47, 48, 54, 55, 56, 57, 58, 60, 61, 62, 66, 67, 68, 69, 72, 73, 81], "fullfinetunerecip": 8, "direct": [8, 77], "wandblogg": [9, 84, 85], "workspac": 9, "seen": [9, 84, 85], "screenshot": 9, "below": [9, 46, 80, 83, 84, 85], "packag": [9, 68, 69, 77], "pip": [9, 68, 69, 77, 81, 83], "Then": [9, 82], "login": [9, 69, 81], "built": [9, 77, 80, 82, 85], "project": [9, 31, 32, 33, 34, 37, 39, 42, 43, 69, 76, 84, 85], "grab": [9, 83], "tab": 9, "tip": 9, "straggler": 9, "background": 9, "crash": 9, "otherwis": 9, "exit": [9, 77], "resourc": [9, 66, 67, 68, 69], "kill": 9, "ps": 9, "aux": 9, "grep": 9, "awk": 9, "xarg": 9, "click": 9, "sampl": [9, 13, 14, 15, 16, 17, 18, 19, 20, 26, 80, 81], "desir": 9, "suggest": 9, "approach": [9, 80], "full_finetun": 9, "joinpath": 9, "_checkpoint": [9, 81], "_output_dir": [9, 57, 58], "torchtune_model_": 9, "with_suffix": 9, "wandb_at": 9, "artifact": 9, "type": [9, 10, 11, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 42, 44, 45, 46, 47, 48, 49, 52, 53, 55, 56, 57, 58, 61, 62, 63, 64, 71, 81, 84, 85], "descript": 9, "whatev": 9, "metadata": 9, "seed_kei": 9, "epochs_kei": 9, "total_epochs_kei": 9, "max_steps_kei": 9, "max_steps_per_epoch": 9, "add_fil": 9, "log_artifact": 9, "field": [10, 17, 22, 23, 24, 25], "hydra": 10, "facebook": 10, "research": 10, "http": [10, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42, 45, 46, 50, 57, 58, 59, 62, 68, 69, 71, 73, 77, 81], "com": [10, 31, 32, 33, 34, 37, 42, 45, 46, 50, 77], "facebookresearch": [10, 45, 46], "blob": [10, 31, 32, 33, 34, 37, 42, 45, 46, 50], "main": [10, 11, 42, 45, 46, 77, 81, 83], "_intern": 10, "_instantiate2": 10, "l148": 10, "omegaconf": 10, "num_lay": [10, 47], "32": [10, 83, 84, 85], "num_head": [10, 42, 44, 46, 47], "num_kv_head": [10, 42, 44], "vocab_s": 10, "must": [10, 22, 23, 24, 25, 26, 51, 56, 59, 85], "return": [10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 42, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 56, 57, 60, 61, 62, 63, 64, 65, 70, 71, 73, 80, 84, 85], "nn": [10, 42, 43, 44, 47, 48, 49, 51, 53, 54, 72, 84, 85], "parsed_yaml": 10, "embed_dim": [10, 42, 46, 48, 84], "valueerror": [10, 19, 21, 26, 42, 47, 57, 58, 61, 73], "callabl": [11, 47], "With": [11, 81, 84, 85], "my_recip": 11, "foo": 11, "bar": [11, 78, 82], "instanti": [12, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40], "configerror": 12, "cannot": [12, 83], "data": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 66, 67, 68, 69, 80, 81, 85], "prompt": [13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 47, 55, 80, 81, 83], "templat": [13, 14, 16, 17, 20, 22, 23, 24, 25, 26], "style": [13, 22, 23, 26, 85], "slightli": 13, "instruct": [13, 15, 17, 19, 22, 23, 76, 82, 83, 84, 85], "classmethod": [13, 14, 15, 16, 17, 18, 19, 20], "map": [13, 16, 17, 20, 54, 57, 66, 67, 68, 69, 81, 84], "column_map": [13, 16, 17, 20, 80], "placehold": [13, 14, 16, 17, 20], "column": [13, 16, 17, 20], "ident": [13, 16, 17, 19, 20, 81], "chat": [14, 15, 18, 26], "role": [14, 55, 80], "system": [14, 15, 18, 19, 21, 55, 80], "assist": [14, 15, 18, 21, 55, 80], "messag": [14, 15, 18, 19, 21, 55, 56, 77, 80], "accord": [14, 19], "openai": 15, "markup": 15, "languag": [15, 52, 84], "It": [15, 19, 80, 85], "huggingfac": [15, 50, 57, 58, 80, 81], "im_start": 15, "context": [15, 71, 80], "im_end": 15, "goe": 15, "respons": [15, 55, 80, 81, 82, 83], "appropri": [15, 18, 19, 50, 80, 85], "tag": [15, 18, 19, 56, 66, 67, 68, 69], "grammar": [16, 24, 80], "sentenc": 16, "alwai": [17, 59], "human": 18, "taken": [18, 84, 85], "inst": [18, 19, 80], "sy": [18, 80], "respect": [18, 54, 80], "honest": [18, 80], "am": [18, 19, 80, 81, 83], "pari": [18, 19, 80], "capit": [18, 19, 80], "franc": [18, 19, 80], "known": [18, 19, 55, 80], "its": [18, 19, 73, 80, 81, 83, 84], "stun": [18, 19, 80], "mistral": [19, 39, 40, 41, 81, 82], "llama2chatformat": [19, 80], "summar": [20, 25, 80], "task": [20, 80, 81, 83, 84, 85], "dialogu": [20, 25], "dialog": 20, "forth": 21, "consecut": 21, "come": [21, 51, 84], "empti": 21, "shorter": 21, "length": [21, 22, 23, 26, 42, 44, 46, 47, 48, 55, 56, 58, 70], "min": [21, 84], "invalid": 21, "yahma": 22, "codebas": [22, 23, 24, 25, 81], "where": [22, 23, 24, 25, 42, 47, 52, 55, 80], "mask": [22, 23, 24, 25, 42, 48, 55, 56, 80], "contribut": [22, 23, 24, 25], "replac": [22, 23, 24, 25, 49, 84], "encod": [22, 23, 24, 25, 26, 55, 56], "decod": [22, 23, 24, 25, 26, 47, 55, 56], "anyth": [22, 23, 24, 25, 26], "load_dataset": [22, 23, 24, 25, 26], "whether": [22, 23, 24, 25, 26, 31, 32, 33, 34, 37, 39, 49, 52, 55, 56, 61], "recommend": [22, 23, 68, 81, 85], "highest": [22, 23], "sequenc": [22, 23, 26, 42, 44, 46, 47, 48, 55, 56, 70], "alpaca_d": [22, 23], "batch_siz": [22, 23, 24, 25, 42, 48, 81], "tatsu": [23, 80], "lab": [23, 80], "liweili": 24, "c4_200m": 24, "variant": [24, 25], "mirror": [24, 25], "llama_recip": [24, 25], "grammar_d": 24, "samsum": 25, "summari": 25, "samsum_d": 25, "open": [26, 27, 80, 81], "orca": [26, 80], "slimorca": [26, 80], "dedup": [26, 80], "1024": [26, 80], "chatdataset": 26, "adher": 26, "doesn": [26, 81], "prescrib": 26, "truncat": [26, 55, 56], "least": [26, 83, 84], "though": 26, "max": [26, 47, 50, 55, 84], "ds": 26, "10": [26, 70, 81, 83, 85], "351": 26, "82": [26, 81], "391": 26, "221": 26, "220": 26, "193": 26, "12": [26, 77], "471": 26, "gemma": 27, "transformerdecod": [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 84], "w": [27, 28, 29, 30, 36, 40, 68, 69, 81, 84, 85], "blog": 27, "technolog": 27, "develop": [27, 85], "arxiv": [28, 29, 30, 35, 38, 41, 42, 45, 46], "org": [28, 29, 30, 35, 38, 41, 42, 45, 46, 59, 62, 68, 71, 73, 77], "ab": [28, 29, 30, 35, 38, 41, 46], "2307": [28, 29, 30], "09288": [28, 29, 30], "lora_attn_modul": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84, 85], "liter": [31, 32, 33, 34, 35, 37, 38, 39, 41], "q_proj": [31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 84, 85], "k_proj": [31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 84, 85], "v_proj": [31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 84, 85], "output_proj": [31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 84, 85], "apply_lora_to_mlp": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84], "apply_lora_to_output": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84], "lora_rank": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84], "lora_alpha": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84], "float": [31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 45, 50, 52, 66, 67, 68, 69, 84, 85], "16": [31, 32, 33, 34, 35, 37, 38, 39, 41, 84, 85], "quantize_bas": [31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 85], "lora": [31, 32, 33, 34, 35, 37, 38, 39, 41, 52, 76, 78, 82, 83], "tloen": [31, 32, 33, 34, 37], "8bb8579e403dc78e37fe81ffbb253c413007323f": [31, 32, 33, 34, 37], "l41": [31, 32, 33, 34, 37], "l43": [31, 32, 33, 34, 37], "linear": [31, 32, 33, 34, 35, 37, 38, 39, 41, 47, 51, 52, 84, 85], "attent": [31, 32, 33, 34, 37, 39, 42, 44, 46, 47, 48, 83, 84, 85], "mlp": [31, 32, 33, 34, 37, 39, 47, 48, 83, 84], "final": [31, 32, 33, 34, 37, 39, 43, 47, 56, 81, 83, 84, 85], "rank": [31, 32, 33, 34, 37, 39, 52, 63, 73, 82, 84, 85], "low": [31, 32, 33, 34, 37, 39, 52, 81, 84, 85], "approxim": [31, 32, 33, 34, 37, 39, 52, 84], "scale": [31, 32, 33, 34, 37, 39, 52, 84, 85], "factor": [31, 32, 33, 34, 37, 39, 52, 81], "lora_dropout": [32, 33, 35], "05": [32, 33, 35], "70b": 32, "llama2_7b": [32, 33, 84], "qlora": [35, 38, 41, 49, 76, 78, 83, 84], "per": [35, 38, 41, 44, 49, 83, 85], "paper": [35, 38, 41, 84, 85], "2305": [35, 38, 41, 42], "14314": [35, 38, 41], "lora_llama2_7b": [35, 84], "llama3": [36, 37, 38, 76], "llama3_8b": [37, 83], "lora_llama3_8b": 38, "announc": 40, "lora_mistral_7b": 41, "head_dim": [42, 44, 46, 47], "pos_embed": [42, 84], "kv_cach": 42, "kvcach": [42, 47], "attn_dropout": [42, 47], "head": [42, 44, 46, 47, 83], "queri": [42, 44, 47, 83], "gqa": 42, "introduc": [42, 45, 52, 84, 85], "pdf": [42, 45], "13245v1": 42, "version": [42, 77, 83, 85], "multihead": 42, "mha": [42, 47], "n": [42, 55, 56, 75, 79, 80], "extrem": 42, "share": [42, 80, 81], "mqa": 42, "credit": 42, "document": 42, "lightn": 42, "lit": 42, "gpt": [42, 81], "lit_gpt": 42, "v": [42, 47, 84], "k": [42, 84], "q": [42, 84], "n_kv_head": 42, "dimens": [42, 44, 46, 47, 52, 83, 84, 85], "calcul": [42, 83], "e": [42, 49, 51, 54, 77, 81, 83, 84, 85], "g": [42, 51, 83, 84, 85], "rotarypositionalembed": [42, 84], "cach": [42, 44, 46, 77], "comput": [42, 43, 46, 47, 73, 81, 85], "rope": [42, 46], "dropout": [42, 52, 84, 85], "onto": 42, "scaled_dot_product_attent": 42, "input_po": [42, 46, 47, 48], "seq_length": [42, 48], "seq_len": [42, 46], "bigger": 42, "n_h": [42, 46], "num": [42, 46], "n_kv": 42, "kv": [42, 44, 47], "emb": [42, 47, 48], "h_d": [42, 46], "gate_proj": 43, "down_proj": 43, "up_proj": 43, "silu": 43, "feed": [43, 48], "network": [43, 84, 85], "deriv": [43, 47, 48], "fed": 43, "multipli": 43, "subclass": [43, 59], "although": [43, 84], "afterward": 43, "former": 43, "regist": [43, 49, 85], "hook": [43, 49, 85], "latter": 43, "max_batch_s": 44, "standalon": 44, "past": 44, "becaus": [44, 47, 81, 83], "expand": 44, "dpython": [44, 49], "ep": 45, "1e": 45, "06": [45, 84], "root": [45, 68, 69], "squar": 45, "1910": 45, "07467": 45, "verfic": [45, 46], "small": [45, 81], "avoid": [45, 49, 73, 85], "divis": 45, "zero": [45, 81, 83], "10000": 46, "rotari": [46, 83], "propos": 46, "2104": 46, "09864": 46, "l450": 46, "upto": 46, "init": [46, 69, 85], "exceed": 46, "freq": 46, "recomput": 46, "geometr": 46, "progress": [46, 82], "rotat": 46, "angl": 46, "bsz": 46, "todo": 46, "made": [46, 81], "effici": [46, 76, 78, 81, 82, 84], "transformerdecoderlay": 47, "norm": [47, 48], "move": 47, "space": 47, "check": [47, 61, 76, 81, 82, 84], "belong": 47, "reduc": [47, 78, 80, 84, 85], "statement": 47, "improv": [47, 81, 83, 84], "readabl": [47, 81], "At": 47, "arang": 47, "prompt_length": 47, "causal_mask": 47, "m_": 47, "seq": 47, "attn": [48, 84, 85], "causalselfattent": [48, 84], "sa_norm": 48, "mlp_norm": 48, "ff": 48, "common_util": 49, "bfloat16": [49, 81, 82, 83, 84], "offload_to_cpu": 49, "nf4": [49, 85], "restor": 49, "higher": [49, 83, 85], "offload": [49, 85], "increas": [49, 50, 83, 84], "peak": [49, 81, 83, 84, 85], "gpu": [49, 81, 82, 83, 84, 85], "usag": [49, 77, 81, 82, 83, 85], "_register_state_dict_hook": 49, "m": [49, 56], "mymodul": 49, "_after_": 49, "nf4tensor": [49, 85], "unquant": [49, 81, 85], "unus": 49, "num_warmup_step": 50, "num_training_step": 50, "num_cycl": 50, "last_epoch": 50, "lambdalr": 50, "rate": [50, 78, 82], "schedul": [50, 71, 82], "linearli": 50, "lr": 50, "decreas": [50, 84, 85], "cosin": 50, "remain": [50, 84], "v4": 50, "23": [50, 83], "src": 50, "l104": 50, "warmup": [50, 71], "phase": 50, "total": [50, 63, 75, 79, 81, 83, 84], "wave": 50, "half": 50, "index": [50, 70, 77, 81], "last": 50, "lr_schedul": 50, "peft": [51, 52, 53, 54, 84, 85], "protocol": 51, "adapter_param": [51, 52, 53, 54], "correspond": [51, 53, 61, 82, 83], "proj": 51, "in_dim": [51, 52, 84, 85], "out_dim": [51, 52, 84, 85], "bia": [51, 52, 84, 85], "loralinear": [51, 84, 85], "alpha": [52, 84, 85], "use_bia": 52, "larg": [52, 85], "perturb": 52, "decomposit": [52, 84], "matric": [52, 84, 85], "trainabl": [52, 54, 84, 85], "mapsto": 52, "w_0x": 52, "r": [52, 56, 84], "bax": 52, "probabl": [52, 81], "lora_a": [52, 84, 85], "lora_b": [52, 84, 85], "subset": 53, "get_adapter_param": [54, 84], "sentencepieceprocessor": 55, "pretrain": [55, 56, 82, 84, 85], "non": 55, "spm_model": 55, "tokenized_text": 55, "hello": [55, 81, 83], "world": [55, 63, 81], "add_bo": [55, 56], "add_eo": [55, 56], "31587": 55, "29644": 55, "102": 55, "text": [55, 56, 80, 81], "trim_leading_whitespac": 55, "prefix": 55, "unbatch": 55, "prepend": [55, 56], "bo": [55, 56], "append": [55, 77], "eo": [55, 56], "trim": 55, "lead": 55, "whitespac": 55, "underli": [55, 85], "sentencepiec": [55, 83], "s1": 55, "s2": 55, "due": [55, 84, 85], "tokenize_messag": [55, 56, 80], "concaten": 55, "problem": 55, "slice": 55, "tokenizer_path": 55, "separ": [55, 57, 82, 83, 84, 85], "concat": 55, "1788": 55, "2643": 55, "13": [55, 81, 83, 85], "1792": 55, "9508": 55, "465": 55, "22137": 55, "2933": 55, "join": 55, "attribut": 55, "llama3_tiktoken": 56, "p": [56, 84, 85], "l": 56, "all_special_token": 56, "bos_token": 56, "begin_of_text": 56, "eos_token": 56, "end_of_text": 56, "start_header_id": 56, "end_header_id": 56, "step_id": 56, "eom_id": 56, "eot_id": 56, "python_tag": 56, "tiktoken": [56, 83], "regex": 56, "special": 56, "element": [56, 81], "second": [56, 81, 83, 84, 85], "uniqu": 56, "256": [56, 80, 81, 83], "header": 56, "token_id": 56, "truncate_at_eo": 56, "tokenize_head": 56, "co": [57, 58, 81], "few": [57, 80, 83, 84, 85], "0001_of_0003": 57, "0002_of_0003": 57, "preserv": [57, 85], "weight_map": [57, 81], "intermediate_checkpoint": [57, 58], "parit": 57, "_weight_map": 57, "shard": [58, 83], "wip": 58, "argpars": 59, "argumentpars": 59, "builtin": 59, "said": 59, "noth": 59, "treat": 59, "still": [59, 84, 85], "consult": 59, "doc": [59, 62, 68, 69, 71, 73, 81], "info": [59, 82], "librari": [59, 62, 73, 76, 78, 85], "html": [59, 62, 68, 71, 73], "parse_known_arg": 59, "namespac": 59, "act": 59, "precid": 59, "parse_arg": 59, "intern": 59, "properti": [59, 84], "too": [59, 83], "availab": 60, "machin": [60, 81], "distribut": [60, 64, 72, 73, 78, 82, 83], "bf16": [61, 85], "request": [61, 80, 81], "inde": [61, 81], "kernel": 61, "runtimeerror": [61, 64], "float32": 61, "done": [61, 84, 85], "isn": 61, "hardwar": [61, 78, 81, 84], "stream": 62, "handler": 62, "aka": 63, "filenam": 66, "log_": 66, "unixtimestamp": 66, "txt": [66, 82], "thread": 66, "safe": 66, "flush": [66, 67, 68, 69], "union": [66, 67, 68, 69, 73], "ndarrai": [66, 67, 68, 69], "scalar": [66, 67, 68, 69], "record": [66, 67, 68, 69], "payload": [66, 67, 68, 69], "dictionari": [66, 67, 68, 69, 81], "organize_log": 68, "tensorboard": 68, "stabl": [68, 71, 73, 77], "subdirectori": 68, "sub": 68, "compar": [68, 81, 84, 85], "logdir": 68, "startup": 68, "recurs": 68, "tree": [68, 80, 81], "tfevent": 68, "encount": 68, "frontend": 68, "organ": 68, "accordingli": 68, "my_log_dir": 68, "view": [68, 81, 82], "my_metr": [68, 69], "termin": [68, 69], "entiti": 69, "bias": 69, "ref": 69, "sent": 69, "usernam": 69, "individu": 69, "my_project": 69, "my_ent": 69, "my_group": 69, "importerror": 69, "account": [69, 84, 85], "log_config": 69, "local": [69, 73, 77, 81, 82], "link": [69, 81], "capecap": 69, "6053ofw0": 69, "torchtune_config_j67sb73v": 69, "padding_idx": 70, "ignore_idx": 70, "pad": 70, "longest": 70, "integ": [70, 73], "tokenpair": 70, "collat": 70, "token_pair": 70, "torchtune_perf_trac": 71, "contextmanag": 71, "wait": 71, "trace": 71, "speed": [71, 83, 85], "reduct": [71, 84], "auto_wrap_polici": 72, "polici": 72, "debug_mod": 73, "pseudo": 73, "random": [73, 82], "commonli": [73, 81, 84, 85], "numpi": 73, "own": [73, 80, 81, 84], "determinist": 73, "global": 73, "warn": 73, "nondeterminist": 73, "addition": [73, 84], "cudnn": 73, "disabl": 73, "set_deterministic_debug_mod": 73, "algorithm": 73, "outsid": [73, 81, 83, 84], "generated_examples_python": 74, "zip": 74, "galleri": [74, 79], "sphinx": 74, "000": [75, 79, 83], "execut": [75, 79], "generated_exampl": 75, "mem": [75, 79], "mb": [75, 79], "topic": 76, "gentl": 76, "introduct": 76, "readi": 76, "maxim": [76, 78], "workflow": [76, 80, 82, 84], "requisit": 77, "proper": [77, 82], "host": [77, 82], "page": [77, 78, 82, 83], "latest": [77, 82, 85], "confirm": 77, "And": [77, 81, 83], "h": 77, "ls": [77, 81, 82, 83], "welcom": 77, "show": [77, 84], "greatest": [77, 82], "contributor": 77, "cd": [77, 81], "even": [77, 83, 84, 85], "commit": 77, "branch": 77, "extra": [77, 84, 85], "url": 77, "whl": 77, "therebi": [77, 85], "howev": 77, "forc": 77, "reinstal": 77, "opt": [77, 82], "suffix": 77, "cu121": 77, "On": [78, 84], "pointer": 78, "author": [78, 82, 85], "emphas": 78, "aspect": 78, "simplic": 78, "component": 78, "reus": 78, "high": [78, 84], "prove": 78, "democrat": 78, "box": [78, 85], "zoo": 78, "varieti": [78, 84], "techniqu": [78, 81, 82, 84], "integr": [78, 81, 82, 83, 84, 85], "excit": 78, "checkout": 78, "quickstart": 78, "attain": 78, "better": [78, 80, 81], "chekckpoint": 78, "hyperparamet": [78, 82, 84, 85], "embodi": 78, "philosophi": 78, "especi": [78, 81], "usabl": 78, "composit": 78, "hard": 78, "outlin": 78, "unecessari": 78, "never": 78, "thoroughli": 78, "unit": 78, "know": [80, 81, 83, 84], "steer": 80, "wheel": 80, "publicli": 80, "great": [80, 81], "sever": 80, "wide": 80, "bootstrap": 80, "indic": 80, "iter": [80, 85], "knob": 80, "tweak": 80, "sai": [80, 82], "footprint": [80, 84], "could": [80, 84], "achiev": [80, 81, 83, 84, 85], "fix": 80, "goal": 80, "flavor": 80, "agnost": 80, "condit": 80, "respond": 80, "alpacainstructtempl": 80, "describ": 80, "further": [80, 84, 85], "classifi": 80, "anim": 80, "plant": 80, "miner": 80, "oak": 80, "copper": 80, "ore": 80, "eleph": 80, "instructtempl": 80, "instruct_dataset": 80, "mydataset": 80, "onthehub": 80, "customtempl": 80, "similar": [80, 81, 83, 84, 85], "chatformat": 80, "quit": [80, 85], "similarli": 80, "chat_dataset": 80, "conversation_styl": 80, "sharegpt": 80, "chat_format": 80, "formatted_messag": 80, "nyou": 80, "incorpor": 80, "advanc": 80, "preferencedataset": 80, "rlhf": 80, "adjust": 80, "chosen": 80, "reject": 80, "chosen_messag": 80, "transformed_sampl": 80, "key_chosen": 80, "rejected_messag": 80, "key_reject": 80, "chosen_input_id": 80, "c_mask": 80, "chosen_label": 80, "np": 80, "cross_entropy_ignore_idx": 80, "rejected_input_id": 80, "r_mask": 80, "rejected_label": 80, "purpos": [80, 82, 83], "stack_exchanged_paired_dataset": 80, "had": 80, "lvwerra": 80, "stack": 80, "exchang": 80, "stackexchangedpairedtempl": 80, "question": [80, 81, 83], "response_j": 80, "response_k": 80, "data_dir": 80, "rl": 80, "favorit": [81, 83, 84], "commun": 81, "seemlessli": 81, "beyond": [81, 85], "connect": 81, "larger": [81, 83], "might": 81, "amount": 81, "natur": 81, "export": 81, "mobil": 81, "phone": 81, "leverag": [81, 83, 85], "mode": 81, "lot": 81, "plai": 81, "freez": [81, 84], "percentag": 81, "learnabl": 81, "keep": [81, 84], "16gb": [81, 84], "rtx": 81, "3090": 81, "4090": 81, "hour": 81, "full_finetune_single_devic": [81, 82], "7b_full_low_memori": [81, 82], "full_finetune_distribut": [81, 82], "7b_full": [81, 82], "13b_full": [81, 82], "7b_qlora_single_devic": [81, 82, 85], "473": 81, "98": [81, 85], "gb": [81, 83, 84, 85], "50": 81, "484": 81, "01": [81, 82], "fact": [81, 83, 84], "third": 81, "smaller": [81, 83, 84, 85], "But": [81, 83, 84], "realli": 81, "eleuther_ev": [81, 83], "eleuther_evalu": [81, 83], "lm_eval": [81, 83], "plan": 81, "copi": [81, 82, 83, 85], "custom_eval_config": [81, 83], "truthfulqa_mc2": [81, 83, 84], "measur": [81, 83], "propens": [81, 83], "answer": [81, 83], "shot": [81, 83], "accuraci": [81, 83, 84, 85], "baselin": [81, 84], "324": 81, "loglikelihood": 81, "195": 81, "121": 81, "27": 81, "197": 81, "acc": 81, "388": 81, "38": 81, "shown": 81, "489": 81, "48": [81, 85], "seem": 81, "custom_generation_config": [81, 83], "kick": 81, "top_k": 81, "300": 81, "temperatur": 81, "interest": 81, "site": 81, "visit": 81, "bai": 81, "area": 81, "92": [81, 83], "exploratorium": 81, "san": 81, "francisco": 81, "magazin": 81, "awesom": 81, "bridg": 81, "pretti": 81, "cool": 81, "96": [81, 85], "61": 81, "sec": [81, 83], "25": 81, "83": 81, "99": [81, 84], "15": [81, 84, 85], "72": 81, "littl": 81, "saw": 81, "took": [81, 83], "torchao": [81, 83, 85], "bit": [81, 83, 84, 85], "custom_quantization_config": [81, 83], "68": 81, "19": [81, 83, 85], "76": 81, "69": 81, "95": [81, 83], "67": 81, "4w": [81, 83], "unlik": [81, 83], "won": [81, 83], "engin": [81, 83], "fullmodeltorchtunecheckpoint": [81, 83], "int4weightonlyquant": [81, 83], "groupsiz": [81, 83], "did": [81, 83, 85], "park": 81, "sit": 81, "top": [81, 85], "hill": 81, "beauti": 81, "62": [81, 83], "17": [81, 84], "85": 81, "compil": [81, 83, 85], "hood": [81, 85], "sped": 81, "almost": [81, 83, 84], "3x": [81, 83], "benefit": 81, "yet": 81, "fast": 81, "clone": [81, 84, 85], "assumpt": 81, "satisfi": 81, "new_dir": 81, "output_dict": 81, "sd_1": 81, "sd_2": 81, "dump": 81, "convert_hf_checkpoint": 81, "checkpoint_path": 81, "my": [81, 83], "justin": 81, "school": 81, "math": 81, "teacher": 81, "ws": 81, "94": [81, 83], "103": 81, "28": 81, "bandwidth": [81, 83], "1391": 81, "84": 81, "thats": 81, "seamlessli": 81, "authent": [81, 82], "hopefulli": 81, "gave": 81, "launch": 82, "gate": 82, "grant": 82, "minut": 82, "agreement": 82, "altern": 82, "hackabl": 82, "singularli": 82, "focus": 82, "technic": 82, "depth": 82, "why": [82, 84], "principl": 82, "minim": [82, 84, 85], "boilerpl": 82, "hold": 82, "substanti": [82, 84], "custom_config": 82, "replic": 82, "lorafinetunerecipesingledevic": 82, "lora_finetune_output": 82, "log_1713194212": 82, "sampler": 82, "52": 82, "3697006702423096": 82, "25880": [82, 85], "24": [82, 83], "55": 82, "83it": 82, "were": 82, "monitor": 82, "tqdm": 82, "interv": 82, "e2": 82, "releas": 83, "128": [83, 84], "intermedi": [83, 85], "theta": 83, "gain": 83, "illustr": 83, "basic": 83, "8b_lora_single_devic": 83, "observ": 83, "18": 83, "consum": [83, 85], "vram": [83, 84], "overal": 83, "nproc_per_nod": [83, 84], "lora_finetune_distribut": [83, 84], "8b_lora": 83, "8b_qlora_single_devic": 83, "alloc": [83, 85], "coupl": [83, 84, 85], "llama3_token": 83, "122": 83, "sarah": 83, "busi": 83, "mum": 83, "young": 83, "children": 83, "live": 83, "north": 83, "east": 83, "england": 83, "135": 83, "88": 83, "138": 83, "346": 83, "09": 83, "139": 83, "31": 83, "been": 83, "far": 83, "drill": 83, "90": 83, "93": 83, "91": 83, "104": 83, "four": [83, 84], "again": 83, "jake": 83, "disciplin": 83, "artist": 83, "passion": 83, "draw": 83, "paint": 83, "57": [83, 84, 85], "speedup": 83, "broader": 83, "teach": 84, "straight": 84, "jump": 84, "neural": [84, 85], "unfamiliar": 84, "oppos": [84, 85], "momentum": 84, "adamw": 84, "arbitrari": 84, "relat": 84, "aghajanyan": 84, "et": 84, "al": 84, "hypothes": 84, "intrins": 84, "lower": 84, "down": [84, 85], "often": 84, "eight": 84, "practic": 84, "imag": 84, "simplifi": 84, "represent": [84, 85], "left": 84, "blue": 84, "rememb": 84, "approx": 84, "15m": 84, "8192": 84, "65k": 84, "requires_grad": [84, 85], "frozen_out": [84, 85], "lora_out": [84, 85], "omit": 84, "construct": 84, "base_model": 84, "choos": 84, "lora_model": 84, "lora_llama_2_7b": [84, 85], "alon": 84, "in_featur": 84, "out_featur": 84, "inplac": 84, "feel": 84, "free": 84, "strict": 84, "whenev": 84, "validate_state_dict_for_lora": 84, "peft_util": 84, "set_trainable_param": 84, "fetch": 84, "lora_param": 84, "total_param": 84, "sum": 84, "numel": 84, "trainable_param": 84, "2f": 84, "6742609920": 84, "4194304": 84, "nnode": 84, "7b_lora": 84, "my_model_checkpoint_path": [84, 85], "tokenizer_checkpoint": [84, 85], "my_tokenizer_checkpoint_path": [84, 85], "constraint": 84, "factori": 84, "benefici": 84, "long": 84, "impact": 84, "rel": 84, "minor": 84, "good": 84, "64": 84, "lora_experiment_1": 84, "smooth": [84, 85], "curv": [84, 85], "500": 84, "ran": 84, "commod": 84, "cogniz": 84, "ax": 84, "parallel": 84, "truthfulqa": 84, "previous": 84, "475": 84, "87": 84, "508": 84, "86": 84, "504": 84, "04": 84, "514": 84, "lowest": 84, "absolut": 84, "4gb": 84, "tradeoff": 84, "potenti": 84, "enhanc": 85, "maintain": 85, "highli": 85, "part": 85, "vanilla": 85, "held": 85, "therefor": 85, "bespok": 85, "normalfloat": 85, "8x": 85, "retain": 85, "vast": 85, "major": 85, "highlight": 85, "degrad": 85, "normatfloat": 85, "doubl": 85, "themselv": 85, "prune": 85, "deepdiv": 85, "idea": 85, "distinct": 85, "storag": 85, "datatyp": 85, "de": 85, "incur": 85, "counterpart": 85, "set_default_devic": 85, "qlora_linear": 85, "memory_alloc": 85, "177": 85, "152": 85, "byte": 85, "del": 85, "empty_cach": 85, "lora_linear": 85, "081": 85, "344": 85, "qlora_llama2_7b": 85, "qlora_model": 85, "essenti": 85, "reparametrize_as_dtype_state_dict_post_hook": 85, "entir": 85, "stat": 85, "reserv": 85, "against": 85, "35": 85, "40": 85, "29": 85, "slow": 85, "slower": 85, "149": 85, "9157477021217346": 85, "02": 85, "08": 85, "14": 85, "15it": 85, "thing": 85, "nightli": 85, "200": 85, "hundr": 85, "228": 85, "8158286809921265": 85, "59": 85, "95it": 85, "exercis": 85, "manual": 85, "portion": 85, "augment": 85, "linear_nf4": 85, "to_nf4": 85, "linear_weight": 85, "autograd": 85, "regular": 85, "incom": 85, "variabl": 85}, "objects": {"torchtune.config": [[10, 0, 1, "", "instantiate"], [11, 0, 1, "", "parse"], [12, 0, 1, "", "validate"]], "torchtune.data": [[13, 1, 1, "", "AlpacaInstructTemplate"], [14, 1, 1, "", "ChatFormat"], [15, 1, 1, "", "ChatMLFormat"], [16, 1, 1, "", "GrammarErrorCorrectionTemplate"], [17, 1, 1, "", "InstructTemplate"], [18, 1, 1, "", "Llama2ChatFormat"], [19, 1, 1, "", "MistralChatFormat"], [20, 1, 1, "", "SummarizeTemplate"], [21, 0, 1, "", "validate_messages"]], "torchtune.data.AlpacaInstructTemplate": [[13, 2, 1, "", "format"]], "torchtune.data.ChatFormat": [[14, 2, 1, "", "format"]], "torchtune.data.ChatMLFormat": [[15, 2, 1, "", "format"]], "torchtune.data.GrammarErrorCorrectionTemplate": [[16, 2, 1, "", "format"]], "torchtune.data.InstructTemplate": [[17, 2, 1, "", "format"]], "torchtune.data.Llama2ChatFormat": [[18, 2, 1, "", "format"]], "torchtune.data.MistralChatFormat": [[19, 2, 1, "", "format"], [19, 3, 1, "", "system"]], "torchtune.data.SummarizeTemplate": [[20, 2, 1, "", "format"]], "torchtune.datasets": [[22, 0, 1, "", "alpaca_cleaned_dataset"], [23, 0, 1, "", "alpaca_dataset"], [24, 0, 1, "", "grammar_dataset"], [25, 0, 1, "", "samsum_dataset"], [26, 0, 1, "", "slimorca_dataset"]], "torchtune.models.gemma": [[27, 0, 1, "", "gemma_2b"]], "torchtune.models.llama2": [[28, 0, 1, "", "llama2_13b"], [29, 0, 1, "", "llama2_70b"], [30, 0, 1, "", "llama2_7b"], [31, 0, 1, "", "lora_llama2_13b"], [32, 0, 1, "", "lora_llama2_70b"], [33, 0, 1, "", "lora_llama2_7b"], [34, 0, 1, "", "qlora_llama2_13b"], [35, 0, 1, "", "qlora_llama2_7b"]], "torchtune.models.llama3": [[36, 0, 1, "", "llama3_8b"], [37, 0, 1, "", "lora_llama3_8b"], [38, 0, 1, "", "qlora_llama3_8b"]], "torchtune.models.mistral": [[39, 0, 1, "", "lora_mistral_7b"], [40, 0, 1, "", "mistral_7b"], [41, 0, 1, "", "qlora_mistral_7b"]], "torchtune.modules": [[42, 1, 1, "", "CausalSelfAttention"], [43, 1, 1, "", "FeedForward"], [44, 1, 1, "", "KVCache"], [45, 1, 1, "", "RMSNorm"], [46, 1, 1, "", "RotaryPositionalEmbeddings"], [47, 1, 1, "", "TransformerDecoder"], [48, 1, 1, "", "TransformerDecoderLayer"], [50, 0, 1, "", "get_cosine_schedule_with_warmup"]], "torchtune.modules.CausalSelfAttention": [[42, 2, 1, "", "forward"]], "torchtune.modules.FeedForward": [[43, 2, 1, "", "forward"]], "torchtune.modules.RMSNorm": [[45, 2, 1, "", "forward"]], "torchtune.modules.RotaryPositionalEmbeddings": [[46, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoder": [[47, 2, 1, "", "forward"]], "torchtune.modules.TransformerDecoderLayer": [[48, 2, 1, "", "forward"]], "torchtune.modules.common_utils": [[49, 0, 1, "", "reparametrize_as_dtype_state_dict_post_hook"]], "torchtune.modules.peft": [[51, 1, 1, "", "AdapterModule"], [52, 1, 1, "", "LoRALinear"], [53, 0, 1, "", "get_adapter_params"], [54, 0, 1, "", "set_trainable_params"]], "torchtune.modules.peft.AdapterModule": [[51, 2, 1, "", "adapter_params"]], "torchtune.modules.peft.LoRALinear": [[52, 2, 1, "", "adapter_params"], [52, 2, 1, "", "forward"]], "torchtune.modules.tokenizers": [[55, 1, 1, "", "SentencePieceTokenizer"], [56, 1, 1, "", "TikTokenTokenizer"]], "torchtune.modules.tokenizers.SentencePieceTokenizer": [[55, 2, 1, "", "decode"], [55, 2, 1, "", "encode"], [55, 2, 1, "", "tokenize_messages"]], "torchtune.modules.tokenizers.TikTokenTokenizer": [[56, 2, 1, "", "decode"], [56, 2, 1, "", "encode"], [56, 2, 1, "", "tokenize_message"], [56, 2, 1, "", "tokenize_messages"]], "torchtune.utils": [[57, 1, 1, "", "FullModelHFCheckpointer"], [58, 1, 1, "", "FullModelMetaCheckpointer"], [59, 1, 1, "", "TuneRecipeArgumentParser"], [60, 0, 1, "", "get_device"], [61, 0, 1, "", "get_dtype"], [62, 0, 1, "", "get_logger"], [63, 0, 1, "", "get_world_size_and_rank"], [64, 0, 1, "", "init_distributed"], [65, 0, 1, "", "list_dtypes"], [70, 0, 1, "", "padded_collate"], [71, 0, 1, "", "profiler"], [72, 0, 1, "", "set_activation_checkpointing"], [73, 0, 1, "", "set_seed"]], "torchtune.utils.FullModelHFCheckpointer": [[57, 2, 1, "", "load_checkpoint"], [57, 2, 1, "", "save_checkpoint"]], "torchtune.utils.FullModelMetaCheckpointer": [[58, 2, 1, "", "load_checkpoint"], [58, 2, 1, "", "save_checkpoint"]], "torchtune.utils.TuneRecipeArgumentParser": [[59, 2, 1, "", "parse_known_args"]], "torchtune.utils.metric_logging": [[66, 1, 1, "", "DiskLogger"], [67, 1, 1, "", "StdoutLogger"], [68, 1, 1, "", "TensorBoardLogger"], [69, 1, 1, "", "WandBLogger"]], "torchtune.utils.metric_logging.DiskLogger": [[66, 2, 1, "", "close"], [66, 2, 1, "", "log"], [66, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.StdoutLogger": [[67, 2, 1, "", "close"], [67, 2, 1, "", "log"], [67, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.TensorBoardLogger": [[68, 2, 1, "", "close"], [68, 2, 1, "", "log"], [68, 2, 1, "", "log_dict"]], "torchtune.utils.metric_logging.WandBLogger": [[69, 2, 1, "", "close"], [69, 2, 1, "", "log"], [69, 2, 1, "", "log_config"], [69, 2, 1, "", "log_dict"]]}, "objtypes": {"0": "py:function", "1": "py:class", "2": "py:method", "3": "py:attribute"}, "objnames": {"0": ["py", "function", "Python function"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"]}, "titleterms": {"torchtun": [0, 1, 2, 3, 4, 5, 6, 76, 78, 81, 83, 84, 85], "config": [0, 7, 8, 82], "data": [1, 5], "dataset": [2, 80], "model": [3, 4, 9, 81, 82, 83, 84], "llama3": [3, 83], "llama2": [3, 81, 84, 85], "mistral": 3, "gemma": 3, "modul": 4, "compon": [4, 7], "build": [4, 77, 85], "block": 4, "token": 4, "peft": 4, "util": [4, 5], "checkpoint": [5, 6, 9, 81], "distribut": 5, "reduc": 5, "precis": 5, "memori": [5, 84, 85], "manag": 5, "perform": [5, 84], "profil": [5, 71], "metric": [5, 9], "log": [5, 9], "miscellan": 5, "overview": [6, 78, 81], "format": [6, 80], "handl": 6, "differ": 6, "intermedi": 6, "vs": 6, "final": 6, "lora": [6, 81, 84, 85], "put": [6, 85], "thi": 6, "all": [6, 7, 85], "togeth": [6, 85], "about": 7, "where": 7, "do": 7, "paramet": 7, "live": 7, "write": 7, "configur": [7, 80], "us": [7, 8, 81, 85], "instanti": [7, 10], "referenc": 7, "other": [7, 81], "field": 7, "interpol": 7, "valid": [7, 12], "your": [7, 8, 81, 82], "best": 7, "practic": 7, "airtight": 7, "public": 7, "api": 7, "onli": 7, "command": 7, "line": 7, "overrid": 7, "what": [8, 78, 84, 85], "ar": 8, "recip": [8, 82, 84], "script": 8, "class": 8, "run": [8, 81], "cli": 8, "pars": [8, 11], "weight": 9, "bias": 9, "logger": 9, "w": 9, "b": 9, "alpacainstructtempl": 13, "chatformat": 14, "chatmlformat": 15, "grammarerrorcorrectiontempl": 16, "instructtempl": 17, "llama2chatformat": 18, "mistralchatformat": 19, "summarizetempl": 20, "validate_messag": 21, "alpaca_cleaned_dataset": 22, "alpaca_dataset": 23, "grammar_dataset": 24, "samsum_dataset": 25, "slimorca_dataset": 26, "gemma_2b": 27, "llama2_13b": 28, "llama2_70b": 29, "llama2_7b": 30, "lora_llama2_13b": 31, "lora_llama2_70b": 32, "lora_llama2_7b": 33, "qlora_llama2_13b": 34, "qlora_llama2_7b": 35, "llama3_8b": 36, "lora_llama3_8b": 37, "qlora_llama3_8b": 38, "lora_mistral_7b": 39, "mistral_7b": 40, "qlora_mistral_7b": 41, "causalselfattent": 42, "todo": [42, 48], "feedforward": 43, "kvcach": 44, "rmsnorm": 45, "rotarypositionalembed": 46, "transformerdecod": 47, "transformerdecoderlay": 48, "reparametrize_as_dtype_state_dict_post_hook": 49, "get_cosine_schedule_with_warmup": 50, "adaptermodul": 51, "loralinear": 52, "get_adapter_param": 53, "set_trainable_param": 54, "sentencepiecetoken": 55, "tiktokentoken": 56, "fullmodelhfcheckpoint": 57, "fullmodelmetacheckpoint": 58, "tunerecipeargumentpars": 59, "get_devic": 60, "get_dtyp": 61, "get_logg": 62, "get_world_size_and_rank": 63, "init_distribut": 64, "list_dtyp": 65, "disklogg": 66, "stdoutlogg": 67, "tensorboardlogg": 68, "wandblogg": 69, "padded_col": 70, "set_activation_checkpoint": 72, "set_se": 73, "comput": [75, 79], "time": [75, 79], "welcom": 76, "document": 76, "get": [76, 83], "start": 76, "tutori": 76, "instal": 77, "instruct": [77, 80], "via": [77, 83], "pypi": 77, "git": 77, "clone": 77, "nightli": 77, "kei": 78, "concept": 78, "design": 78, "principl": 78, "fine": [80, 82, 83], "tune": [80, 82, 83], "custom": 80, "templat": 80, "chat": 80, "fulli": 80, "end": 81, "workflow": 81, "download": [81, 82], "7b": 81, "finetun": [81, 84, 85], "evalu": [81, 83], "eleutherai": [81, 83], "s": [81, 83], "eval": [81, 83], "har": [81, 83], "gener": [81, 83], "speed": 81, "up": 81, "quantiz": [81, 83], "librari": 81, "upload": 81, "hug": 81, "face": 81, "hub": 81, "first": 82, "llm": 82, "select": 82, "modifi": 82, "train": 82, "next": 82, "step": 82, "8b": 83, "access": 83, "text": 83, "our": 83, "faster": 83, "how": 84, "doe": 84, "work": 84, "appli": 84, "trade": 84, "off": 84, "qlora": 85, "save": 85, "deep": 85, "dive": 85, "from": 85}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})